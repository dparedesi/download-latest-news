List of news related to Advanced Micro Devices stock price AMD:

Title: Apple, Nvidia, Meta, and more Magnificent 7 stocks slide
URL: https://qz.com/apple-nvidia-magnificent-7-stocks-slide-1851775625
Time Published: 2025-04-10T16:01:26Z
Full Content:
The roller-coaster week for global stock markets took another sharp turn Thursday, yanking Nvidia (NVDA+0.43%), Apple (AAPL+3.88%), Amazon (AMZN-1.56%), and other Big Tech stocks down with it. The latest shift erased some of Wednesday’s historic gains, which came as President Donald Trump abruptly announced a 90-day pause on tariff hikes for most countries. After the president’s announcement, Apple stock recorded its best performance in roughly 27 years. However, the pause left out China — which is home to much of Big Tech’s meticulously designed supply chain, including Apple’s. By Thursday, markets seemed reluctant to trust a temporary policy pivot that arrived without warning — even as fresh inflation data offered some welcome relief. Thus, another bad day unfolded for investors. Apple closed the day down 4.2%, while Amazon dropped about 5.2%, Nvidia fell about 5.9%, Meta (META-1.69%) plunged 6.7%, Microsoft (MSFT-0.03%) fell 2.3%, and Tesla (TSLA-0.14%) gave up 7.3%. The tech-heavy Nasdaq ended Thursday down 4.3%, while the Dow Jones Industrial Average was down 2.5% and the S&P 500 fell about 3.5%. For Tesla, the plunge came as analysts slashed their price targets for the EV stock — over Trump’s impending tariffs on auto parts and weakening demand for Tesla EVs in Europe and China. Meanwhile, Amazon cautioned that tariffs will drive up prices, Apple fell as the Trump administration clarified the latest tariffs on China (145%), and Meta declined as the company attempted to downplay a tell-all book by former Facebook public-policy executive Sarah Wynn-Williams. Other tech losers included Advanced Micro Devices (AMD+2.46%), which tanked about 8.4%, Intel (INTC+4.05%), which took a roughly 7.7% dive, and Palantir (PLTR+5.70%), which fell 3.7%. Hardly any tech-adjacent stock dodged the carnage, but some of the rare exceptions included U.S.-focused telecom stocks, as well as video game maker Electronic Arts (EA+1.83%). —Catherine Baab contributed to this article. Our free, fast, and fun briefing on the global economy, delivered every weekday morning.
--------------------------------------------------

Title: S&P 500 Gains and Losses Today: Index Roars Back After Reprieve on 'Reciprocal' Tariffs
URL: https://www.investopedia.com/s-and-p-500-gains-and-losses-today-index-roars-back-after-reprieve-on-reciprocal-tariffs-11711797
Time Published: 2025-04-09T20:57:01Z
Full Content:
Major U.S. equities indexes surged after President Donald Trump announced a 90-day pause of "reciprocal" tariffs on imports from U.S. trading partners around the world, less than a day after the levies took effect. After fluctuating in early trading, the S&P 500 screamed higher following the latest shift in trade policy, ending with a daily gain of 9.5%. The Dow advanced 7.9%, while the Nasdaq rocketed 12.2% higher. The technology sector outperformed as stocks that had been clobbered by concerns about the impact of extensive tariffs clawed back some of their recent losses. Shares of microcontroller and analog chipmaker Microchip Technology (MCHP) jumped 27.1%, notching the best daily performance of any S&P 500 stock, while shares of power management semiconductor provider Monolithic Power Systems (MPWR) popped 23.4% higher. Advanced Micro Devices (AMD) shares, which have garnered significant attention because of the firm's artificial intelligence (AI) chips, surged 23.8%. The push higher for the stock came in spite of a downgrade to "sector weight" from "overweight" by analysts at KeyBanc Capital Markets, who cited downside risks to gross margins amid increasing price competition with Intel (INTC). Delta Air Lines (DAL) lifted the airline industry by reporting better-than-expected sales and profit results for its fiscal first quarter. Although Delta announced plans to cut back on capacity growth in the second half of the year as it navigates the uncertain economic environment, its shares took off following the strong earnings report, gaining 23.4% on Wednesday. Meanwhile, shares of rival carrier United Airlines (UAL) soared 26.1%. Telsa (TSLA) shares gained 22.7% in the wake of the tariff reprieve. In addition, the research firm Benchmark included the electric vehicle maker's stock among its best investment ideas, noting a more attractive valuation following recent price declines, the potential for a recovery in sales driven by a new vehicle launch, and potential catalysts from Tesla's robotaxi and self-driving initiatives. The heaviest loss among S&P 500 constituents was on Dollar General (DG) shares, which slipped 1.9%. The stock managed to buck the downtrend in the broader markets this week as concerns about deteriorating macroeconomic conditions boosted the outlook for cost-conscious consumers seeking bargains at the discount retailer's stores. Dollar General shares also attracted an upgrade earlier this week from analysts at Melius Research, who highlighted the company's relative lack of exposure to tariffs. Shares of MarketAxess Holdings (MKTX), which operates an electronic trading platform for fixed-income securities, fell 1.5% after UBS trimmed its price target on the stock. Although analysts lifted their first-quarter earnings estimates to reflect increased credit volume, they anticipate that lower pricing could restrain revenue growth during the period.
--------------------------------------------------

Title: Google's 'Ironwood' TPU aims squarely at the high cost of inference
URL: https://www.zdnet.com/article/googles-ironwood-tpu-aims-squarely-at-the-high-cost-of-inference/
Time Published: 2025-04-09T12:05:59Z
Full Content:
During its Google Cloud Next 25 event Wednesday, the search giant unveiled the latest version of its Tensor Processing Unit (TPU), the custom chip built to run artificial intelligence -- with a twist. Also: Why Google Code Assist may finally be the programming power tool you need For the first time, Google is positioning the chip for inference, the making of predictions for live requests from millions or even billions of users, as opposed to training, the development of neural networks carried out by teams of AI specialists and data scientists. The Ironwood TPU, as the new chip is called, arrives at an economic inflection point in AI. The industry clearly expects AI moving forward to be less about science projects and more about the actual use of AI models by companies. And the rise of DeepSeek AI has focused Wall Street more than ever on the enormous cost of building AI for Google and its competitors. The rise of "reasoning" AI models, such as Google's Gemini, which dramatically increases the number of statements generated by a large language model, creates a sudden surge in the total computing needed to make predictions. As Google put it in describing Ironwood, "reasoning and multi-step inference is shifting the incremental demand for compute -- and therefore cost -- from training to inference time (test-time scaling)." Also: Think DeepSeek has cut AI spending? Think again Thus, Ironwood is a statement by Google that its focus on performance and efficiency is shifting to reflect the rising cost of inference versus the research domain of training. Google has been developing its TPU family of chips for over a decade through six prior generations. However, training chips are generally considered a much lower-volume chip market than inference. That is because training demands rise only as each new, gigantic GenAI research project is inaugurated, which is generally once a year or so. In contrast, inference is expected to meet the needs of thousands or millions of customers who want day-to-day predictions from the trained neural network. Inference is considered a high-volume market in the chip world. Google had previously made the case that the sixth-generation Trillium TPU, introduced last year, which became generally available in December, could serve as both a training and an inference chip in one part, emphasizing its ability to speed up the serving of predictions. In fact, as far back as the TPU version two, in 2017, Google had talked of a combined ability for training and inference. Also: Google reveals new Kubernetes and GKE enhancements for AI innovation The positioning of Ironwood as mainly an inference chip, first and foremost, is a departure. It's a shift that may also mark a change in Google's willingness to rely on Intel, Advanced Micro Devices, and Nvidia as the workhorse of its AI computing fleet. In the past, Google had described the TPU as a necessary investment to achieve cutting-edge research results but not an alternative to its vendors. In Google's cloud computing operations, based on the number of "instances" run by customers, Intel, AMD, and Nvidia chips make up a combined 99% of processors used, versus less than a percent for the TPU, according to research by KeyBanc Capital Markets. That reliance on three dominant vendors has economic implications for Google and the other giants, Microsoft and Amazon. Also: 10 key reasons AI went mainstream overnight - and what happens next Wall Street stock analysts, who compile measures of Google's individual lines of business, have, from time to time, calculated the economic value of the TPU. For example, in January, stock analyst Gil Luria of the boutique research firm DA Davidson wrote that "Google would have generated as much as $24 billion of revenue last year if it was selling TPUs as hardware to NVDA [Nvidia] customers," meaning in competition with Nvidia. Conversely, at a time when the cost of AI escalates into multi-hundred-billion-dollar projects such as Stargate, Wall Street analysts believe that Google's TPU could offer the company a way to save money on the cost of AI infrastructure. Also: DeepSeek's new open-source AI model can outperform o1 for a fraction of the cost Although Google has paid chip-maker Broadcom to help it take each new TPU into commercial production, Google might still save money using more TPUs versus the price it has to pay to Intel, AMD, and Nvidia to consume ever-larger fleets of chips for inference. To make the case for Ironwood, Google on Wednesday emphasized the technical advances of Ironwood versus Trillium. Google said Ironwood gets twice the "performance per watt" of Trillium, as measured by 29.3 trillion floating-point math operations per second. The Ironwood part has 192GB of DRAM memory, dubbed HBM, or high-bandwidth memory, six times as much as Trillium. The memory bandwidth transmitted is 4.5 times as much, 7.2 terabits per second. Also: Nvidia dominates in gen AI benchmarks, clobbering 2 rival AI chips Google said those enhancements are supposed to support much greater movement of data in and out of the chip and between systems. "Ironwood is designed to minimize data movement and latency on chip while carrying out massive tensor manipulations," said Google. The memory and bandwidth advances are all part of Google's emphasis on "scaling" its AI infrastructure. The meaning of scaling is to be able to fully use each chip when grouping together hundreds or thousands of chips to work on a problem in parallel. More chips dedicated to the same problem should lead to a concomitant speed-up in performance. Again, scaling has an economic component. By effectively grouping chips, the TPUs can achieve greater "utilization," the amount of a given resource that is actually used versus the amount being left idle. Successful scaling means higher utilization of chips, which is good because it means less waste of an expensive resource. Also: 5 reasons why Google's Trillium could transform AI and cloud computing - and 2 obstacles That's why, in the past, Google has emphasized Trillium's ability to "scale to hundreds of thousands of chips" in a collection of machines. While Google didn't give explicit details on Ironwood's scaling performance on inference tasks, it once again emphasized on Wednesday the ability of "hundreds of thousands of Ironwood chips to be composed together to rapidly advance the frontiers of GenAI computation." Also: Intel's new CEO vows to run chipmaker 'as a startup, on day one' Google's announcement came with a significant software announcement as well, Pathways on Cloud. The Pathways software is code that distributes parts of the AI computing work to different computers. It had been used internally by Google and is now being made available to the public. Get the morning's top stories in your inbox each day with our Tech Today newsletter.
--------------------------------------------------

Title: Google's latest chip is all about reducing one huge hidden cost in AI
URL: https://www.zdnet.com/article/googles-latest-chip-is-all-about-reducing-one-huge-hidden-cost-in-ai/
Time Published: 2025-04-09T12:05:00Z
Full Content:
During its Google Cloud Next 25 event Wednesday, the search giant unveiled the latest version of its Tensor Processing Unit (TPU), the custom chip built to run artificial intelligence -- with a twist. Also: Why Google Code Assist may finally be the programming power tool you need For the first time, Google is positioning the chip for inference, the making of predictions for live requests from millions or even billions of users, as opposed to training, the development of neural networks carried out by teams of AI specialists and data scientists. The Ironwood TPU, as the new chip is called, arrives at an economic inflection point in AI. The industry clearly expects AI moving forward to be less about science projects and more about the actual use of AI models by companies. And the rise of DeepSeek AI has focused Wall Street more than ever on the enormous cost of building AI for Google and its competitors. The rise of "reasoning" AI models, such as Google's Gemini, which dramatically increases the number of statements generated by a large language model, creates a sudden surge in the total computing needed to make predictions. As Google put it in describing Ironwood, "reasoning and multi-step inference is shifting the incremental demand for compute -- and therefore cost -- from training to inference time (test-time scaling)." Also: Think DeepSeek has cut AI spending? Think again Thus, Ironwood is a statement by Google that its focus on performance and efficiency is shifting to reflect the rising cost of inference versus the research domain of training. Google has been developing its TPU family of chips for over a decade through six prior generations. However, training chips are generally considered a much lower-volume chip market than inference. That is because training demands rise only as each new, gigantic GenAI research project is inaugurated, which is generally once a year or so. In contrast, inference is expected to meet the needs of thousands or millions of customers who want day-to-day predictions from the trained neural network. Inference is considered a high-volume market in the chip world. Google had previously made the case that the sixth-generation Trillium TPU, introduced last year, which became generally available in December, could serve as both a training and an inference chip in one part, emphasizing its ability to speed up the serving of predictions. In fact, as far back as the TPU version two, in 2017, Google had talked of a combined ability for training and inference. Also: Google reveals new Kubernetes and GKE enhancements for AI innovation The positioning of Ironwood as mainly an inference chip, first and foremost, is a departure. It's a shift that may also mark a change in Google's willingness to rely on Intel, Advanced Micro Devices, and Nvidia as the workhorse of its AI computing fleet. In the past, Google had described the TPU as a necessary investment to achieve cutting-edge research results but not an alternative to its vendors. In Google's cloud computing operations, based on the number of "instances" run by customers, Intel, AMD, and Nvidia chips make up a combined 99% of processors used, versus less than a percent for the TPU, according to research by KeyBanc Capital Markets. That reliance on three dominant vendors has economic implications for Google and the other giants, Microsoft and Amazon. Also: 10 key reasons AI went mainstream overnight - and what happens next Wall Street stock analysts, who compile measures of Google's individual lines of business, have, from time to time, calculated the economic value of the TPU. For example, in January, stock analyst Gil Luria of the boutique research firm DA Davidson wrote that "Google would have generated as much as $24 billion of revenue last year if it was selling TPUs as hardware to NVDA [Nvidia] customers," meaning in competition with Nvidia. Conversely, at a time when the cost of AI escalates into multi-hundred-billion-dollar projects such as Stargate, Wall Street analysts believe that Google's TPU could offer the company a way to save money on the cost of AI infrastructure. Also: DeepSeek's new open-source AI model can outperform o1 for a fraction of the cost Although Google has paid chip-maker Broadcom to help it take each new TPU into commercial production, Google might still save money using more TPUs versus the price it has to pay to Intel, AMD, and Nvidia to consume ever-larger fleets of chips for inference. To make the case for Ironwood, Google on Wednesday emphasized the technical advances of Ironwood versus Trillium. Google said Ironwood gets twice the "performance per watt" of Trillium, as measured by 29.3 trillion floating-point math operations per second. The Ironwood part has 192GB of DRAM memory, dubbed HBM, or high-bandwidth memory, six times as much as Trillium. The memory bandwidth transmitted is 4.5 times as much, 7.2 terabits per second. Also: Nvidia dominates in gen AI benchmarks, clobbering 2 rival AI chips Google said those enhancements are supposed to support much greater movement of data in and out of the chip and between systems. "Ironwood is designed to minimize data movement and latency on chip while carrying out massive tensor manipulations," said Google. The memory and bandwidth advances are all part of Google's emphasis on "scaling" its AI infrastructure. The meaning of scaling is to be able to fully use each chip when grouping together hundreds or thousands of chips to work on a problem in parallel. More chips dedicated to the same problem should lead to a concomitant speed-up in performance. Again, scaling has an economic component. By effectively grouping chips, the TPUs can achieve greater "utilization," the amount of a given resource that is actually used versus the amount being left idle. Successful scaling means higher utilization of chips, which is good because it means less waste of an expensive resource. Also: 5 reasons why Google's Trillium could transform AI and cloud computing - and 2 obstacles That's why, in the past, Google has emphasized Trillium's ability to "scale to hundreds of thousands of chips" in a collection of machines. While Google didn't give explicit details on Ironwood's scaling performance on inference tasks, it once again emphasized on Wednesday the ability of "hundreds of thousands of Ironwood chips to be composed together to rapidly advance the frontiers of GenAI computation." Also: Intel's new CEO vows to run chipmaker 'as a startup, on day one' Google's announcement came with a significant software announcement as well, Pathways on Cloud. The Pathways software is code that distributes parts of the AI computing work to different computers. It had been used internally by Google and is now being made available to the public. Get the morning's top stories in your inbox each day with our Tech Today newsletter.
--------------------------------------------------