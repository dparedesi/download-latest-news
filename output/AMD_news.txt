List of news related to Advanced Micro Devices stock price AMD:

Title: Apple, Nvidia, Meta, and more Magnificent 7 stocks slide
URL: https://qz.com/apple-nvidia-magnificent-7-stocks-slide-1851775625
Time Published: 2025-04-10T16:01:26Z
Full Content:
The roller-coaster week for global stock markets took another sharp turn Thursday, yanking Nvidia (NVDA+2.34%), Apple (AAPL+3.96%), Amazon (AMZN+1.49%), and other Big Tech stocks down with it. The latest shift erased some of Wednesday’s historic gains, which came as President Donald Trump abruptly announced a 90-day pause on tariff hikes for most countries. After the president’s announcement, Apple stock recorded its best performance in roughly 27 years. However, the pause left out China — which is home to much of Big Tech’s meticulously designed supply chain, including Apple’s. By Thursday, markets seemed reluctant to trust a temporary policy pivot that arrived without warning — even as fresh inflation data offered some welcome relief. Thus, another bad day unfolded for investors. Apple closed the day down 4.2%, while Amazon dropped about 5.2%, Nvidia fell about 5.9%, Meta (META-0.85%) plunged 6.7%, Microsoft (MSFT+1.99%) fell 2.3%, and Tesla (TSLA-1.87%) gave up 7.3%. The tech-heavy Nasdaq ended Thursday down 4.3%, while the Dow Jones Industrial Average was down 2.5% and the S&P 500 fell about 3.5%. For Tesla, the plunge came as analysts slashed their price targets for the EV stock — over Trump’s impending tariffs on auto parts and weakening demand for Tesla EVs in Europe and China. Meanwhile, Amazon cautioned that tariffs will drive up prices, Apple fell as the Trump administration clarified the latest tariffs on China (145%), and Meta declined as the company attempted to downplay a tell-all book by former Facebook public-policy executive Sarah Wynn-Williams. Other tech losers included Advanced Micro Devices (AMD+4.93%), which tanked about 8.4%, Intel (INTC-2.92%), which took a roughly 7.7% dive, and Palantir (PLTR-1.06%), which fell 3.7%. Hardly any tech-adjacent stock dodged the carnage, but some of the rare exceptions included U.S.-focused telecom stocks, as well as video game maker Electronic Arts (EA+2.49%). —Catherine Baab contributed to this article. Our free, fast, and fun briefing on the global economy, delivered every weekday morning.
--------------------------------------------------

Title: S&P 500 Gains and Losses Today: Index Roars Back After Reprieve on 'Reciprocal' Tariffs
URL: https://www.investopedia.com/s-and-p-500-gains-and-losses-today-index-roars-back-after-reprieve-on-reciprocal-tariffs-11711797
Time Published: 2025-04-09T20:57:01Z
Full Content:
Major U.S. equities indexes surged after President Donald Trump announced a 90-day pause of "reciprocal" tariffs on imports from U.S. trading partners around the world, less than a day after the levies took effect. After fluctuating in early trading, the S&P 500 screamed higher following the latest shift in trade policy, ending with a daily gain of 9.5%. The Dow advanced 7.9%, while the Nasdaq rocketed 12.2% higher. The technology sector outperformed as stocks that had been clobbered by concerns about the impact of extensive tariffs clawed back some of their recent losses. Shares of microcontroller and analog chipmaker Microchip Technology (MCHP) jumped 27.1%, notching the best daily performance of any S&P 500 stock, while shares of power management semiconductor provider Monolithic Power Systems (MPWR) popped 23.4% higher. Advanced Micro Devices (AMD) shares, which have garnered significant attention because of the firm's artificial intelligence (AI) chips, surged 23.8%. The push higher for the stock came in spite of a downgrade to "sector weight" from "overweight" by analysts at KeyBanc Capital Markets, who cited downside risks to gross margins amid increasing price competition with Intel (INTC). Delta Air Lines (DAL) lifted the airline industry by reporting better-than-expected sales and profit results for its fiscal first quarter. Although Delta announced plans to cut back on capacity growth in the second half of the year as it navigates the uncertain economic environment, its shares took off following the strong earnings report, gaining 23.4% on Wednesday. Meanwhile, shares of rival carrier United Airlines (UAL) soared 26.1%. Telsa (TSLA) shares gained 22.7% in the wake of the tariff reprieve. In addition, the research firm Benchmark included the electric vehicle maker's stock among its best investment ideas, noting a more attractive valuation following recent price declines, the potential for a recovery in sales driven by a new vehicle launch, and potential catalysts from Tesla's robotaxi and self-driving initiatives. The heaviest loss among S&P 500 constituents was on Dollar General (DG) shares, which slipped 1.9%. The stock managed to buck the downtrend in the broader markets this week as concerns about deteriorating macroeconomic conditions boosted the outlook for cost-conscious consumers seeking bargains at the discount retailer's stores. Dollar General shares also attracted an upgrade earlier this week from analysts at Melius Research, who highlighted the company's relative lack of exposure to tariffs. Shares of MarketAxess Holdings (MKTX), which operates an electronic trading platform for fixed-income securities, fell 1.5% after UBS trimmed its price target on the stock. Although analysts lifted their first-quarter earnings estimates to reflect increased credit volume, they anticipate that lower pricing could restrain revenue growth during the period.
--------------------------------------------------

Title: Google's 'Ironwood' TPU aims squarely at the high cost of inference
URL: https://www.zdnet.com/article/googles-ironwood-tpu-aims-squarely-at-the-high-cost-of-inference/
Time Published: 2025-04-09T12:05:59Z
Full Content:
During its Google Cloud Next 25 event Wednesday, the search giant unveiled the latest version of its Tensor Processing Unit (TPU), the custom chip built to run artificial intelligence -- with a twist. Also: Why Google Code Assist may finally be the programming power tool you need For the first time, Google is positioning the chip for inference, the making of predictions for live requests from millions or even billions of users, as opposed to training, the development of neural networks carried out by teams of AI specialists and data scientists. The Ironwood TPU, as the new chip is called, arrives at an economic inflection point in AI. The industry clearly expects AI moving forward to be less about science projects and more about the actual use of AI models by companies. And the rise of DeepSeek AI has focused Wall Street more than ever on the enormous cost of building AI for Google and its competitors. The rise of "reasoning" AI models, such as Google's Gemini, which dramatically increases the number of statements generated by a large language model, creates a sudden surge in the total computing needed to make predictions. As Google put it in describing Ironwood, "reasoning and multi-step inference is shifting the incremental demand for compute -- and therefore cost -- from training to inference time (test-time scaling)." Also: Think DeepSeek has cut AI spending? Think again Thus, Ironwood is a statement by Google that its focus on performance and efficiency is shifting to reflect the rising cost of inference versus the research domain of training. Google has been developing its TPU family of chips for over a decade through six prior generations. However, training chips are generally considered a much lower-volume chip market than inference. That is because training demands rise only as each new, gigantic GenAI research project is inaugurated, which is generally once a year or so. In contrast, inference is expected to meet the needs of thousands or millions of customers who want day-to-day predictions from the trained neural network. Inference is considered a high-volume market in the chip world. Google had previously made the case that the sixth-generation Trillium TPU, introduced last year, which became generally available in December, could serve as both a training and an inference chip in one part, emphasizing its ability to speed up the serving of predictions. In fact, as far back as the TPU version two, in 2017, Google had talked of a combined ability for training and inference. Also: Google reveals new Kubernetes and GKE enhancements for AI innovation The positioning of Ironwood as mainly an inference chip, first and foremost, is a departure. It's a shift that may also mark a change in Google's willingness to rely on Intel, Advanced Micro Devices, and Nvidia as the workhorse of its AI computing fleet. In the past, Google had described the TPU as a necessary investment to achieve cutting-edge research results but not an alternative to its vendors. In Google's cloud computing operations, based on the number of "instances" run by customers, Intel, AMD, and Nvidia chips make up a combined 99% of processors used, versus less than a percent for the TPU, according to research by KeyBanc Capital Markets. That reliance on three dominant vendors has economic implications for Google and the other giants, Microsoft and Amazon. Also: 10 key reasons AI went mainstream overnight - and what happens next Wall Street stock analysts, who compile measures of Google's individual lines of business, have, from time to time, calculated the economic value of the TPU. For example, in January, stock analyst Gil Luria of the boutique research firm DA Davidson wrote that "Google would have generated as much as $24 billion of revenue last year if it was selling TPUs as hardware to NVDA [Nvidia] customers," meaning in competition with Nvidia. Conversely, at a time when the cost of AI escalates into multi-hundred-billion-dollar projects such as Stargate, Wall Street analysts believe that Google's TPU could offer the company a way to save money on the cost of AI infrastructure. Also: DeepSeek's new open-source AI model can outperform o1 for a fraction of the cost Although Google has paid chip-maker Broadcom to help it take each new TPU into commercial production, Google might still save money using more TPUs versus the price it has to pay to Intel, AMD, and Nvidia to consume ever-larger fleets of chips for inference. To make the case for Ironwood, Google on Wednesday emphasized the technical advances of Ironwood versus Trillium. Google said Ironwood gets twice the "performance per watt" of Trillium, as measured by 29.3 trillion floating-point math operations per second. The Ironwood part has 192GB of DRAM memory, dubbed HBM, or high-bandwidth memory, six times as much as Trillium. The memory bandwidth transmitted is 4.5 times as much, 7.2 terabits per second. Also: Nvidia dominates in gen AI benchmarks, clobbering 2 rival AI chips Google said those enhancements are supposed to support much greater movement of data in and out of the chip and between systems. "Ironwood is designed to minimize data movement and latency on chip while carrying out massive tensor manipulations," said Google. The memory and bandwidth advances are all part of Google's emphasis on "scaling" its AI infrastructure. The meaning of scaling is to be able to fully use each chip when grouping together hundreds or thousands of chips to work on a problem in parallel. More chips dedicated to the same problem should lead to a concomitant speed-up in performance. Again, scaling has an economic component. By effectively grouping chips, the TPUs can achieve greater "utilization," the amount of a given resource that is actually used versus the amount being left idle. Successful scaling means higher utilization of chips, which is good because it means less waste of an expensive resource. Also: 5 reasons why Google's Trillium could transform AI and cloud computing - and 2 obstacles That's why, in the past, Google has emphasized Trillium's ability to "scale to hundreds of thousands of chips" in a collection of machines. While Google didn't give explicit details on Ironwood's scaling performance on inference tasks, it once again emphasized on Wednesday the ability of "hundreds of thousands of Ironwood chips to be composed together to rapidly advance the frontiers of GenAI computation." Also: Intel's new CEO vows to run chipmaker 'as a startup, on day one' Google's announcement came with a significant software announcement as well, Pathways on Cloud. The Pathways software is code that distributes parts of the AI computing work to different computers. It had been used internally by Google and is now being made available to the public. Get the morning's top stories in your inbox each day with our Tech Today newsletter.
--------------------------------------------------

Title: Google's latest chip is all about reducing one huge hidden cost in AI
URL: https://www.zdnet.com/article/googles-latest-chip-is-all-about-reducing-one-huge-hidden-cost-in-ai/
Time Published: 2025-04-09T12:05:00Z
Full Content:
During its Google Cloud Next 25 event Wednesday, the search giant unveiled the latest version of its Tensor Processing Unit (TPU), the custom chip built to run artificial intelligence -- with a twist. Also: Why Google Code Assist may finally be the programming power tool you need For the first time, Google is positioning the chip for inference, the making of predictions for live requests from millions or even billions of users, as opposed to training, the development of neural networks carried out by teams of AI specialists and data scientists. The Ironwood TPU, as the new chip is called, arrives at an economic inflection point in AI. The industry clearly expects AI moving forward to be less about science projects and more about the actual use of AI models by companies. And the rise of DeepSeek AI has focused Wall Street more than ever on the enormous cost of building AI for Google and its competitors. The rise of "reasoning" AI models, such as Google's Gemini, which dramatically increases the number of statements generated by a large language model, creates a sudden surge in the total computing needed to make predictions. As Google put it in describing Ironwood, "reasoning and multi-step inference is shifting the incremental demand for compute -- and therefore cost -- from training to inference time (test-time scaling)." Also: Think DeepSeek has cut AI spending? Think again Thus, Ironwood is a statement by Google that its focus on performance and efficiency is shifting to reflect the rising cost of inference versus the research domain of training. Google has been developing its TPU family of chips for over a decade through six prior generations. However, training chips are generally considered a much lower-volume chip market than inference. That is because training demands rise only as each new, gigantic GenAI research project is inaugurated, which is generally once a year or so. In contrast, inference is expected to meet the needs of thousands or millions of customers who want day-to-day predictions from the trained neural network. Inference is considered a high-volume market in the chip world. Google had previously made the case that the sixth-generation Trillium TPU, introduced last year, which became generally available in December, could serve as both a training and an inference chip in one part, emphasizing its ability to speed up the serving of predictions. In fact, as far back as the TPU version two, in 2017, Google had talked of a combined ability for training and inference. Also: Google reveals new Kubernetes and GKE enhancements for AI innovation The positioning of Ironwood as mainly an inference chip, first and foremost, is a departure. It's a shift that may also mark a change in Google's willingness to rely on Intel, Advanced Micro Devices, and Nvidia as the workhorse of its AI computing fleet. In the past, Google had described the TPU as a necessary investment to achieve cutting-edge research results but not an alternative to its vendors. In Google's cloud computing operations, based on the number of "instances" run by customers, Intel, AMD, and Nvidia chips make up a combined 99% of processors used, versus less than a percent for the TPU, according to research by KeyBanc Capital Markets. That reliance on three dominant vendors has economic implications for Google and the other giants, Microsoft and Amazon. Also: 10 key reasons AI went mainstream overnight - and what happens next Wall Street stock analysts, who compile measures of Google's individual lines of business, have, from time to time, calculated the economic value of the TPU. For example, in January, stock analyst Gil Luria of the boutique research firm DA Davidson wrote that "Google would have generated as much as $24 billion of revenue last year if it was selling TPUs as hardware to NVDA [Nvidia] customers," meaning in competition with Nvidia. Conversely, at a time when the cost of AI escalates into multi-hundred-billion-dollar projects such as Stargate, Wall Street analysts believe that Google's TPU could offer the company a way to save money on the cost of AI infrastructure. Also: DeepSeek's new open-source AI model can outperform o1 for a fraction of the cost Although Google has paid chip-maker Broadcom to help it take each new TPU into commercial production, Google might still save money using more TPUs versus the price it has to pay to Intel, AMD, and Nvidia to consume ever-larger fleets of chips for inference. To make the case for Ironwood, Google on Wednesday emphasized the technical advances of Ironwood versus Trillium. Google said Ironwood gets twice the "performance per watt" of Trillium, as measured by 29.3 trillion floating-point math operations per second. The Ironwood part has 192GB of DRAM memory, dubbed HBM, or high-bandwidth memory, six times as much as Trillium. The memory bandwidth transmitted is 4.5 times as much, 7.2 terabits per second. Also: Nvidia dominates in gen AI benchmarks, clobbering 2 rival AI chips Google said those enhancements are supposed to support much greater movement of data in and out of the chip and between systems. "Ironwood is designed to minimize data movement and latency on chip while carrying out massive tensor manipulations," said Google. The memory and bandwidth advances are all part of Google's emphasis on "scaling" its AI infrastructure. The meaning of scaling is to be able to fully use each chip when grouping together hundreds or thousands of chips to work on a problem in parallel. More chips dedicated to the same problem should lead to a concomitant speed-up in performance. Again, scaling has an economic component. By effectively grouping chips, the TPUs can achieve greater "utilization," the amount of a given resource that is actually used versus the amount being left idle. Successful scaling means higher utilization of chips, which is good because it means less waste of an expensive resource. Also: 5 reasons why Google's Trillium could transform AI and cloud computing - and 2 obstacles That's why, in the past, Google has emphasized Trillium's ability to "scale to hundreds of thousands of chips" in a collection of machines. While Google didn't give explicit details on Ironwood's scaling performance on inference tasks, it once again emphasized on Wednesday the ability of "hundreds of thousands of Ironwood chips to be composed together to rapidly advance the frontiers of GenAI computation." Also: Intel's new CEO vows to run chipmaker 'as a startup, on day one' Google's announcement came with a significant software announcement as well, Pathways on Cloud. The Pathways software is code that distributes parts of the AI computing work to different computers. It had been used internally by Google and is now being made available to the public. Get the morning's top stories in your inbox each day with our Tech Today newsletter.
--------------------------------------------------

Title: Veteran fund manager revises Marvell position after big M&A deal
URL: https://www.thestreet.com/investing/veteran-fund-manager-revises-marvell-position-after-big-m-a-deal
Time Published: 2025-04-08T23:33:00Z
Description: The lead portfolio manager for TheStreet Pro Portfolio takes a hard look at the AI-chip maker.
--------------------------------------------------

Title: Deals: New Samsung 3D/OLED Odyssey monitors w/ $300 credits, S25 Ultra $200 off, Galaxy Tab A9+ $158, more
URL: http://9to5google.com/2025/04/08/deals-samsung-3d-and-oled-monitors-galaxy-tab-a9/
Time Published: 2025-04-08T15:59:32Z
Full Content:
Alongside the now live 2025 Samsung AI Vision smart TVs with Galaxy Watch gesture control, the brand has also now debuted its brand-new 2025 3D and OLED Odyssey monitors – they are now up for preorder with up to $300 in FREE credits. The brand also re-stocked all of the colors on the Galaxy S25 Ultra with straight up $200 price drops across all of them while Amazon has dropped its ultra-affordable Galaxy Tab A9+ tablet from $220 down to $158. We also spotted up to $700 in savings on TCL’s 2025 144Hz QD mini-LED smart Google TVs and the launch of the wonderful metal Arc Pulse bumper cases for Samsung’s Galaxy S25 Ultra. All of that and more awaits below in today’s 9to5Toys Lunch Break. Samsung recently expanded its Odyssey gaming monitor lineup by announcing the launch of new 3D and OLED models last month. There are a total of three new models, all of which are now up for pre-orders directly at Samsung with FREE credits. You are looking at two 27-inch and an ultra-wide 49-inch G9 display, and the prices for these start at $999.99 shipped. Folks who took advantage of the limited-time Reserve campaign will also see a bonus $50 off on top of the $300 in savings right now until these monitors officially hit the shelves. Head below for a closer look at the monitors and more details. All three monitors, as previously mentioned are now up for pre-orders. The prices start at $999.99 shipped, and they all come with FREE credits. You’ll also see an additional $50 credit if you reserved the units as a part of the reserve campaign, though the new offers with FREE credits are still worth looking into. Today, Arc is launching its wonderful metal Arc Pulse bumper cases for the new Samsung Galaxy S25 smartphones. Up until now, the Arc’s special brand of smartphone protection has only been available on iPhone models, the latest Google Pixel 9 devices, and previous-generation Galaxy handsets, but it is finally ready to begin taking orders on the new S25 Ultra Arc Pulse Bumper Cases. Head below for more details. We came away thoroughly impressed with the latest iPhone 16 Arc Pulse bumper cases after going hands-on last fall when Apple debuted its latest. Fast forward to today, some two months after Samsung’s latest S25 lineup took the Android world by storm, and Arc is now ready to deliver its unique smartphone case to the latest Galaxy devices. Bumper cases really aren’t for everyone, but if they are for your or you’re considering dabbling in the ultra-minimalist bumper case world, the Arc Pulse lineup is easily one of the best you’ll find anywhere. Made from aerospace-grade metals with a friction-based attachment system that requires no messy adhesives, the two piece case (if you want to call it that), quickly and securely snaps onto either side of your device to provide corner and camera array protection, while also raising your device off the table top like any normal case when you rest it down. Arc Pulse is designed to enhance your phone experience while offering an exceptional blend of aesthetics and phone safeguarding. Crafted from aerospace-grade Aluminum, the smartphone case provides a delightful tactile experience while ensuring phone protection. The outer metal shell distributes the impact while a custom inner layer absorbs the shock. Enhance your phone safety and elevate your style with a statement accessory. The pre-orders offers on the latest Samsung Galaxy S25 and S25 Ultra smartphones were wild, with deep discounts, enhanced trade-in offers, and some seriously notable bundle offers with the add-on gear. While these savings have long since come and gone now, Samsung has now re-stocked all colors of its flagship Galaxy S25 Ultra and is now offering a straight up $200 in savings on all of them with a trade-in requirement. Again, there were some wild deals during the pre-order phase and closer to launch, but if you missed out on those there are now some notable savings to take advantage of with all of the color options on the new S25 Ultra now back in-stock. Considering how hard it is to determine how Trump tariffs might affect both already-released gear and upcoming launches, it might be worth at least considering some of the savings up for grabs right now while you still can. While it appears Samsung hasn’t re-stocked all of the colors on the more affordable S25 and S25+ variants like it has on the S25 Ultra, there are some deals floating around right nonetheless. If you’re looking to sidestep Amazon’s budget-friendly Fire tablet in your search for an ultra-affordable tablet, Samsung’s latest Galaxy Tab A9+ should be on your radar. Regularly $220 and currently marked down to $159.99 directly from Samsung, Amazon actually has it right now for a few bucks less at $157.19 shipped. You’re looking at nearly 30% in savings and the lowest price we can find. While we did see this one drop to $150 for a very brief time for part of the recent Amazon Big Spring Sale last month, today’s deal is otherwise the best price of the year – the only deals better than these were during Black Friday and Prime Day last year. While this is clearly no flagship S10+ or S10 Ultra – the new mid-range Galaxy S10 FE models are coming soon and you can already secure a FREE $50 reserve credit towards your purchase of one (they are set to go live on April 10 and you can get all of the details right here) – the Tab A9+ can make for a notable casual tablet experience. Landing you in the Galaxy family at just over $150 shipped right now, it comes complete with an 11-inch display, a Dolby Atmos “cinema-like audio experience,” the 7,040mAh intenral battery, a “safe place to learn and play with the Samsung Kids app,” and more – all running on the Qualcomm Snapdragon 695 chip. Here’s our launch coverage for more details. The new 2025 TCL QM6K QD mini-LED smart Google TV lineup has arrived and we are now tracking even lower prices than our initial mention to deliver new Amazon all-time low pricing across various sizes. The deepest deal of the lot lands on the 85-inch model at $1,299.99 shipped. This model launched at $2,000 in early February and is now seeing a deep 35% price drop for $700 in savings off the list price. Head below for additional details and price drops on the more modest sizes. The new QM6K model sport TCL’s latest QD mini-LED panels featuring its Halo Control System – “an advanced Technology Suite that includes the New Super High Energy LED Microchip, Condensed Micro Lens, Micro OD Reduced Optical Distance, High Contrast HVA Panel, Enhanced QLED, Zero Delay Transient Response, Bi-direction 23-bit Backlight Controller, and Dynamic Light Algorithm for Stunning “Halo-Free” Images.” That’s on top of the updated Matte HVA Panel, maintaining an “ultra-high native contrast ratio of 7000;1” alongside a 144Hz native refresh rate, up to a “blistering fast 288 VRR” Game Accelerator, Auto Game Mode (ALLM), and AMD Freesync Premium Pro for your new Switch 2 or PS5 Pro. From there, you can also expect everything to run on the Google TV platform, with onboard Bluetooth, Apple AirPlay 2 casting, built-in Google Assistant and Amazon Alexa voice commands, and four HDMI inputs. As part of its spring sale, OnePlus has now upped the ante on the discount for its latest OnePlus Pad 2. You can now land one at $100 off using code SPRING25 at checkout, while also claiming a FREE official Stylo add-on regularly worth $100. Alongside the free shipping here, you’re looking at a total of $200 in savings and a deal on par (in terms of value) with our previous mention – this is $650 worth of gear for $449.99 shipped. Details below. As per usual, all you’ll need to do is navigate to the listing page, and then scroll down to the “Free Gift” section to claim your freebie. You can choose between the regularly $100 OnePlus Stylo stylus or the regularly $40 OnePlus Folio Case 2. And then just add everything to your cart by hitting the “Buy Now” button and apply code SPRING25 at checkout. While our previous mention on this tablet came with a FREE $150 keyboard case, the straight cash deal on the tablet itself wasn’t quite as good ($50 off instead of the $100 off we have today). Depending how much you value the various freebie add-ons here, you’re looking at roughly the same value on both options and one of the best deals we have tracked. This is the latest OnePlus tablet, complete with a CNC aluminum unibody frame and a 12.1-inch 3K LCD display powered by the Snapdragon 8 Gen 3 chip and 12GB of RAM. Everything else you need to know awaits right here and you’ll find the latest from OnePlus below: FTC: We use income earning auto affiliate links. More. Check out 9to5Google on YouTube for more news:
--------------------------------------------------

Title: The Relentless Rise of Jensen Huang
URL: https://www.theatlantic.com/magazine/archive/2025/05/thinking-machine-jensen-huang-nvidia-book-review/682122/
Time Published: 2025-04-08T11:00:00Z
Full Content:
How Jensen Huang built Nvidia into a nearly $3 trillion business Listen to more stories on hark Another day, another new AI large language model that’s supposedly better than all previous ones. When I began writing this story, Elon Musk’s xAI had just released Grok 3, which the company says performs better than its competitors against a wide range of benchmarks. As I was revising the article, Anthropic released Claude 3.7 Sonnet, which it says outperforms Grok 3. And by the time you read this, who knows? Maybe an entirely new LLM will have appeared. In January, after all, the AI world was temporarily rocked by the release of a low-cost, high-performance LLM from China called DeepSeek-R1. A month later, people were already wondering when DeepSeek-R2 would come out. Check out more from this issue and find your next story to read. The competition among LLMs may be hard to keep track of, but for Nvidia, the company that designs the computer chips—or graphics-processing units (GPUs)—that many of these large language models have been trained on, it’s also enormously lucrative. Nvidia, which, as of this writing, is the third-most-valuable company in the world (after Apple and Microsoft), was started three decades ago by engineers who wanted to make graphics cards for gamers. How it evolved into the company that is providing almost all the picks and shovels for the AI gold rush is the story at the core of Stephen Witt’s The Thinking Machine. Framed as a biography of Jensen Huang, the only CEO Nvidia has ever had, the book is also something more interesting and revealing: a window onto the intellectual, cultural, and economic ecosystem that has led to the emergence of superpowerful AI. James Surowiecki: DeepSeek’s chatbot has an important message That ecosystem’s center, of course, is Silicon Valley, where Huang has spent most of his adult life. He was born in Taiwan, the son of a chemical engineer and a teacher. The family moved to Thailand when he was 5, and a few years later, his parents sent him and his older brother to the United States to escape political unrest. Eventually, his parents relocated to the U.S. as well, and Huang grew up in the suburbs of Portland, Oregon. In the early 1980s, after majoring in electrical engineering at Oregon State (which at the time didn’t offer a computer-science major), he got a job at Advanced Micro Devices. The company—then the poor cousin of the chip giant Intel—was headquartered in Sunnyvale, California, near US 101, the highway that runs from San Jose to Stanford. Since then, Huang’s career has unfolded within a five-mile radius of that office. Huang soon left AMD for a firm called LSI Logic Corporation, which built software-design tools for chip architects, and then left LSI in 1993 to start Nvidia with the chip designers Curtis Priem and Chris Malachowsky: He was right on target “to run something by the age of thirty,” as he’d told them he aimed to do. The company was entering a crowded marketplace for developing graphics cards, the computer hardware that’s used to render images and videos. Nvidia didn’t have a real business plan, but Huang’s boss at LSI recommended him to Sequoia Capital. One of the Valley’s most important venture-capital firms, Sequoia helped the company get off the ground. The graphics-card business was built on a perpetual upgrade cycle that forced developers into a never-ending game of performance improvement: A company was only as good as its last card. At various points in those early years, Nvidia was one misstep away from bankruptcy, and its unofficial motto became “Our company is thirty days from going out of business.” One gets the impression that Huang liked it that way. He says his heart rate goes down under pressure, and to call him a relentless worker is to understate matters. “I should make sure that I’m sufficiently exhausted from working that no one can keep me up at night,” he once said. His reading diet features business books (which he devours). He has no obvious politics (or at least never discusses them). He’s not a gaudy philanthropist. Though devoted to his family, he’s also honest: “Lori,” he says of his wife, “did ninety percent of the parenting” of their two children. For the past 30 years, his life has clearly revolved around Nvidia. Huang’s reluctance to talk about himself makes him a challenging subject for Witt to bring to life. But Nvidia’s employees, who almost all refer to Huang by his first name, are effusive. They “worship him—I believe they would follow him out of the window of a skyscraper if he saw a market opportunity there,” Witt writes. He later adds that they see Huang “not just as a leader but as a prophet. Jensen was a prophet who made predictions about things. And then those things came true.” He has a ferocious temper—referred to in the company as “the Wrath of Huang”—and is notorious for publicly reprimanding, at length, workers who have made mistakes or failed to deliver. But he rarely fires people and, in fact, inspires intense devotion. One of his key subordinates says, “I’ve been afraid of Jensen sometimes. But I also know that he loves me.” Read: Jensen Huang is tech’s new alpha dog Huang’s greatest strength as a CEO has been his willingness to make big, risky bets when opportunities present themselves. The first of those came when he changed the architecture of Nvidia’s chips from serial processing to parallel processing. Witt calls this move “a radical gamble,” because up to that point, no company had been able to make selling parallel-processing chips a viable business. Serial computing is the way your computer’s central processing unit works: It executes one instruction at a time, very, very fast. Witt likens it to telling one delivery van to drop off packages in sequence. By contrast, “Nvidia’s parallel GPU acts more like a fleet of motorcycles spreading out across a city,” with the drivers delivering each package at roughly the same time. The coding required to make parallel processing work was much more complex, but if you could do it, you had access to enormous amounts of computing power. Initially, all that power was used mainly to make computer games look and perform better. But then Huang took another big risk, remaking Nvidia’s GPUs so that they could also process massive data sets, of the kind scientists might use. As one Nvidia executive puts it, “You have a video game card on one side, but it has a switch on it. So you flick that switch, and turn the card over, and suddenly the card becomes a supercomputer.” The fascinating thing about this decision was that Huang didn’t know who might want to buy a supercomputer in the guise of a graphics card, or how many such people were out there. He was just betting that if you make powerful tools available to people, they will find a use for them, and at a scale to justify the billions in investment. That use—and it was big—turned out to be artificial intelligence, in particular neural-network technology. As Witt notes, just as parallel processing was revolutionizing computing, a similar revolution was happening in AI research—though no one at Nvidia was paying attention to it. AI had gone through a series of boom-and-bust cycles as researchers tried different techniques, all of which ultimately failed. One of those methods was neural networks, which tried to mimic the human brain and allow the AI to evolve new rules of learning on its own. When you train these networks on massive databases of images and text, they can, over time, identify patterns and become smarter. Neural networks had long been peripheral, partly because they’re black boxes (you can’t explain how the AI is learning, or why it’s doing what it’s doing), and partly because the computing power required to make a high-performance neural network operate was out of reach. Parallel-processing GPUs changed all that. Suddenly, AI researchers, if they could write software well enough to get the most out of the chips, had access to sufficient computing power to allow neural networks to evolve at an extraordinary pace. In 2009, Geoff Hinton, one of the godfathers of AI research, told a conference of machine-learning experts to go buy Nvidia cards. And in 2012, one of Hinton’s students, Alex Krizhevsky, strung together two Nvidia GPUs and built and trained SuperVision (which he later renamed AlexNet). It was an AI model that could, for the first time, identify images with startling accuracy, largely because, in Witt’s words, “the GPU produced in half a minute what would have taken an Intel machine an hour and what would have taken biology a hundred thousand years.” Huang did not immediately recognize the importance of what had happened. When he spoke at Nvidia’s annual GPU Technology Conference in 2013, he never mentioned neural networks, talking instead about weather modeling and computer graphics. But a few months later, after an Nvidia researcher named Bryan Catanzaro made a direct pitch to him about the importance of AI, Huang had what Witt calls a “Damascene epiphany”: He placed another big bet, essentially transforming Nvidia from a graphics company into an AI company over the course of a weekend. This bet was less risky than his earlier ones, because even though Nvidia had competitors who also built GPUs, none of them had really designed theirs to be used as supercomputers. Still, going all in was prescient—developments such as large language models had yet to take off—and is what has turned Nvidia into a nearly $3 trillion company. Read: The lifeblood of the AI boom That weekend feels as if it were the compressed culmination of Nvidia’s story, which isn’t empirically true. The 12 years that followed have been incredibly eventful, and incredibly profitable, as the company has kept improving its chips, servicing the insatiable appetite for computing power created by the emergence of LLMs, and fending off competitors (many of whom are Nvidia’s customers, now building their own chips). But the foundations for that pivot, and all that ensued, were already in place when Huang decided to act on his AI insight. Those foundations, The Thinking Machine makes clear, were not laid by Nvidia alone. Indeed, among Witt’s key contributions is to show that Nvidia’s success can’t be understood apart from the culture and economy of Silicon Valley (and of tech more generally). Take the simple fact of free labor markets. One catalyst of the Valley’s success, as the scholar AnnaLee Saxenian has famously argued, was a freewheeling, risk-taking culture that encouraged workers to leave companies for competitors or to start their own firms. And that depended, in part, on the fact that noncompete clauses were unenforceable in California. Nvidia’s history exemplifies this: not just Huang’s mobility, but that of his early hires as well. Later, one of the company’s favorite tactics was to poach its competitors’ best engineers and coders—bad form, perhaps, but a good business tactic. Nvidia also benefited from the research investments made by the government and universities. One of the crucial breakthroughs in unlocking the power of parallel computing, for instance, was an open-source programming language called Brook, which a gamer and Stanford graduate student named Ian Buck developed with a group of researchers in 2003, relying on a Defense Department grant. Alex Krizhevsky and his partner Ilya Sutskever (who later helped start OpenAI) were grad students at the University of Toronto when Krizhevsky devised AlexNet. The contest in which the model demonstrated its accuracy, the ImageNet challenge, was designed by a Stanford computer scientist named Fei-Fei Li. And as that lineup demonstrates (Krizhevsky and Sutskever were born in the Soviet Union, Li in China), immigration has been central to the history of not just Nvidia but AI generally. Practical economic features of the ecosystem mattered as well. The most important was the rise of independent chip foundries: factories that serve many different companies and make chips on order. Nvidia’s partnership with Taiwan Semiconductor Manufacturing Company, the best-known of these factories, allowed it to become a dominant player by focusing on designing and writing software for its chips; Nvidia didn’t have to invest in actual production, which would have required prohibitive amounts of capital. From the September 2023 issue: Does Sam Altman know what he’s creating? Finally, Nvidia benefited from patience, and its board’s willingness to put long-term thinking ahead of short-term profits. Because of the gaming market, Nvidia was almost always a profitable company, but its stock price dropped nearly 90 percent two different times; it didn’t appreciate for a full 10 years after the dot-com bubble burst, while the company was spending billions turning its graphics cards into supercomputers. One familiar indictment of American capitalism is that it’s too short-term-focused. In the tech industry, at least, the trajectory of Nvidia (and many other companies) suggests that’s a bum rap. To be sure, Huang himself was central to Nvidia’s success: He has run the company essentially on his own (as Witt puts it, he has had “no right-hand man or woman, no majordomo, no second-in-command”), and he’s made the bold moves. What’s more, he seems to have done so without a trace of doubt. Lots of people in the AI industry—including the people training LLMs—have raised concerns about AI’s dangers, but Huang is not one of them. For him, Witt writes, “AI is a pure force for progress.” Huang does not fret that it may eat all of our jobs, or replace artists, or go rogue and decide to wipe out humanity. In fact, when Witt, stricken with existential anxiety about how AI will change the world, asks Huang whether some of these concerns might be worth pondering, he is subjected to one of his legendary tirades: You could write this off as an example of Upton Sinclair’s adage “It is difficult to get a man to understand something, when his salary depends upon his not understanding it!” But the fact that Huang talks about AI in terms of its impact on “marginal costs” shouldn’t be reduced to mere opportunism: It fits right in with the single-minded focus on performance that has driven him from Nvidia’s beginning. Witt at one point calls Huang a “visionary inventor.” The vision Huang has been in thrall to, though, seems to be less about grand future goals, and more about tools—about making the fastest, most powerful chips as efficiently as possible. “Existential risk” has no place in that vision. Huang’s unapologetic stance on AI is bracing in its way, especially in contrast with the public hand-wringing of many AI chieftains, fretting about the dangers of their LLMs while continuing to develop them. But he is in effect making the biggest, riskiest bet ever—not just for Nvidia, but for all of us. Let’s hope he’s right. This article appears in the May 2025 print edition with the headline “The New King of Tech.” ​When you buy a book using a link on this page, we receive a commission. Thank you for supporting The Atlantic.
--------------------------------------------------

Title: Is Nvidia Stock Worth Buying The Dip?
URL: https://www.forbes.com/sites/investor-hub/article/is-nvidia-nvda-stock-worth-buying-the-dip/
Time Published: 2025-04-07T20:29:22Z
Full Content:
ByCatherine Brock ByCatherine Brock, Contributor. Nvidia has continued AI growth, plus opportunities in robotics, PC gaming and autonomous driving ... More that make it an attractive buy at the current valuation.&nbsp; You can buy Nvidia stock on the cheap right now, but should you? That’s the question many investors have been asking since the once-unstoppable NVDA stock fell to its lowest P/E ratio since early 2019. Get your answer now with this look at what happened to Nvidia, factors that could spark a rebound and risks ahead for the company. Nvidia stock gained 168% in the first 10 months of 2024 as investors bought up shares to get their piece of the AI boom. The stock price rally peaked in early November and then turned south when investors got nervous about lingering high interest rates. The negative trend for Nvidia deepened in 2025 as a series of headlines prompted many to question the chip designer’s growth outlook: January through early April, Nvidia stock is down almost 30%. The AI buildout has driven Nvidia's growth in recent years. Nvidia's AI data center sales are still increasing, but the pace has slowed and competition could become a factor soon. A full Nvidia rebound will likely require a different kind of breakthrough. Three opportunity areas to watch are robotics, PC gaming and autonomous driving. Huang recently valued the opportunity in AI-powered robotics and automation at $50 trillion. The CEO says these technologies will "transform manufacturing, logistics, healthcare and other industries." Nvidia has two platforms, Isaac and Cosmos, that support robotics and other types of physical AI. Nvidia’s GeForce Now service uses the cloud to upgrade the gaming experience on any PC. Gamers connect their PC gaming accounts at Steam, Epic Games Store or Ubisoft Connect to GeForce Now for added hardware support—essentially turning any device into a gaming PC. Nvidia announced expanded GeForce Now device support and capabilities at the 2025 CES trade show. Nvidia’s automotive line-up includes a platform for in-vehicle computing plus solutions for training AI driving models and simulating environments for testing. In 2025, the company announced automotive AI partnerships with Toyota, Aurora, Continental, GM, Gatik and Torc. Nvidia expects its automotive business to deliver fiscal 2026 revenue of $5 billion, up from $1.7 billion in fiscal 2025. Alongside these opportunities, Nvidia faces some challenges. Regulatory changes, cheaper AI solutions and in-house chip development top the list. Changing Trump administration policies could pressure Nvidia’s stock price. Reciprocal tariffs, even with semiconductors excluded, could raise input costs and reduce margins. While Huang seems unbothered by the trade war developments, he also said he could move manufacturing to the U.S. if needed. That is a long-term solution. Unfortunately, growth investors are not known for their patience. A transition to domestic manufacturing would impact the NVDA stock price. More concerning than tariffs is the potential for export bans to China and import bans by China. Reports indicate Trump has considered tighter limits on chip sales to China. Meanwhile, Chinese regulators are discouraging sales of Nvidia's H20 chip—a product designed for export to China—because it doesn't meet efficiency standards, according to the Financial Times. The rollout of DeepSeek caused angst for Nvidia shareholders. At issue was the $5.6 million total investment quoted by the AI app’s creator. The price tag seems tiny next to, say, Microsoft's $80 billion AI budget for fiscal 2025. The discrepancy prompted questions about Nvidia's growth outlook. If DeepSeek can do it, why can't big tech spend far less on Nvidia's AI hardware for the same results? Some experts have said the $5.6 million number from DeepSeek is misleading. Still, there will be a time when cost-efficiency enters the AI fray. That could create space for competitors to chip away at Nvidia's AI dominance. Big tech companies don’t want Nvidia to be the only AI chip game in town. It's too risky. They are mitigating that risk by developing in-house solutions. Alphabet (GOOG) recently introduced a collection of lightweight open-source AI models, Meta (META) is testing an AI chip and OpenAI has nearly finalized the design of its AI chip. These solutions will reduce Nvidia's business with existing customers over time. Nvidia will have to improve on performance and price as a result. Nvidia is an attractive, long-term buy right now despite regulatory and competitive challenges. The stock may not return to its pre-Trump growth rates, but it is an innovative, healthy company with a forward-thinking leader—and those qualities usually create shareholder value over time. Like many growth stocks, NVDA is volatile. Its per-share price can fall as quickly as it rises. You can see that dynamic in play for NVDA stock since it went public in 1999. So this stock is best suited for hardy, patient investors and those who already own NVDA and want to reduce their average cost basis. If you don't feel quite ready for NVDA, see this list of best stocks for 2025 for more investing ideas. Bottom Line Nvidia is down nearly 30% this year as investors worry about tariffs, trade bans and lower-cost AI models. But the chip designer is not down for the count. Nvidia has continued AI growth, plus opportunities in robotics, PC gaming and autonomous driving that make it an attractive buy at the current valuation. After the recent downturn, Nvidia’s P/E ratio is about 32. That compares favorably to other semiconductor companies, including Advanced Micro Devices (AMD), Broadcom (AVGO) and Arm Holdings (ARM). Nvidia is a well-run company that can navigate financial downturns and industry shifts. Relative to its history, the company's current valuation is appealing. If you don't mind volatility and believe in the AI promise as well as Nvidia CEO Jensen Huang, now is a good time to buy Nvidia stock. The biggest risks for Nvidia are margin degradation from tariffs, the global impact of tariffs, import and export bans and waning demand if others can replicate DeepSeek's low-power AI model.
--------------------------------------------------

Title: Vulnerability Summary for the Week of March 31, 2025
URL: https://www.cisa.gov/news-events/bulletins/sb25-097
Time Published: 2025-04-07T13:26:40Z
Full Content:
An official website of the United States government Here’s how you know Official websites use .gov A .gov website belongs to an official government organization in the United States. Secure .gov websites use HTTPS A lock (LockA locked padlock) or https:// means you’ve safely connected to the .gov website. Share sensitive information only on official, secure websites. Free Cyber ServicesSecure by design Secure Our WorldShields UpReport A Cyber Issue Search Free Cyber ServicesSecure by design Secure Our WorldShields UpReport A Cyber Issue The CISA Vulnerability Bulletin provides a summary of new vulnerabilities that have been recorded in the past week. In some cases, the vulnerabilities in the bulletin may not yet have assigned CVSS scores. Vulnerabilities are based on the Common Vulnerabilities and Exposures (CVE) vulnerability naming standard and are organized according to severity, determined by the Common Vulnerability Scoring System (CVSS) standard. The division of high, medium, and low severities correspond to the following scores: Entries may include additional information provided by organizations and efforts sponsored by CISA. This information may include identifying information, values, definitions, and related links. Patch information is provided when available. Please note that some of the information in the bulletin is compiled from external, open-source reports and is not a direct result of CISA analysis. Back to top Back to top Back to top Back to top We recently updated our anonymous product survey; we’d welcome your feedback.
--------------------------------------------------