List of news related to Layoffs Amazon:

Title: How To Argue With An AI Booster
URL: https://www.wheresyoured.at/how-to-argue-with-an-ai-booster/
Time Published: 2025-08-27T05:47:01Z
Full Content:
Editor's Note: For those of you reading via email, I recommend opening this in a browser so you can use the Table of Contents. This is my longest newsletter - a 16,000-word-long opus - and if you like it, please subscribe to my premium newsletter. Thanks for reading! In the last two years I've written no less than 500,000 words, with many of them dedicated to breaking both existent and previous myths about the state of technology and the tech industry itself. While I feel no resentment — I really enjoy writing, and feel privileged to be able to write about this and make money doing so — I do feel that there is a massive double standard between those perceived as "skeptics" and "optimists." To be skeptical of AI is to commit yourself to near-constant demands to prove yourself, and endless nags of "but what about?" with each one — no matter how small — presented as a fact that defeats any points you may have. Conversely, being an "optimist" allows you to take things like AI 2027 — which I will fucking get to — seriously to the point that you can write an entire feature about fan fiction in the New York Times and nobody will bat an eyelid. In any case, things are beginning to fall apart. Two of the actual reporters at the New York Times (rather than a "columnist") reported out last week that Meta is yet again "restructuring" its AI department for the fourth time, and that it’s considering "downsizing the A.I. division overall," which sure doesn't seem like something you'd do if you thought AI was the future. Meanwhile, the markets are also thoroughly spooked by an MIT study covered by Fortune that found that 95% of generative AI pilots at companies are failing, and though MIT NANDA has now replaced the link to the study with a Google Form to request access, you can find the full PDF here, in the kind of move that screams "PR firm wants to try and set up interviews." Not for me, thanks! In any case, the report is actually grimmer than Fortune made it sound, saying that "95% of organizations are getting zero return [on generative AI]." The report says that "adoption is high, but transformation is low," adding that "...few industries show the deep structural shifts associated with past general-purpose technologies such as new market leaders, disrupted business models, or measurable changes in customer behavior." Yet the most damning part was the "Five Myths About GenAI in the Enterprise," which is probably the most wilting takedown of this movement I've ever seen: These are brutal, dispassionate points that directly deal with the most common boosterisms. Generative AI isn't transforming anything, AI isn't replacing anyone, enterprises are trying to adopt generative AI but it doesn't fucking work, and the thing holding back AI is the fact it doesn't fucking work. This isn't a case where "the enterprise" is suddenly going to save these companies, because the enterprise already tried, and it isn't working. An incorrect read of the study has been that the "learning gap" that makes these things less useful, when the study actually says that "...the fundamental gap that defines the GenAI divide [is that users resist tools that don't adapt, model quality fails without context, and UX suffers when systems can't remember." This isn't something you learn your way out of. The products don't do what they're meant to do, and people are realizing it. Nevertheless, boosters will still find a way to twist this study to mean something else. They'll claim that AI is still early, that the opportunity is still there, that we "didn't confirm that the internet or smartphones were productivity boosting," or that we're in "the early days" of AI, somehow, three years and hundreds of billions and thousands of articles in. I'm tired of having the same arguments with these people, and I'm sure you are too. No matter how much blindly obvious evidence there is to the contrary they will find ways to ignore it. They continually make smug comments about people "wishing things would be bad" or suggesting you are stupid — and yes, that is their belief! — for not believing generative AI is disruptive. Today, I’m going to give you the tools to fight back against the AI boosters in your life. I’m going to go into the generalities of the booster movement — the way they argue, the tropes they cling to, and the ways in which they use your own self-doubt against you. They’re your buddy, your boss, a man in a gingham shirt at Epic Steakhouse who won't leave you the fuck alone, a Redditor, a writer, a founder or a simple con artist — whoever the booster in your life is, I want you to have the words to fight them with. So, this is my longest newsletter ever, and I built it for quick reference - and, for the first time, gave you a Table of Contents. So, an AI booster is not, in many cases, an actual fan of artificial intelligence. People like Simon Willison or Max Woolf who actually work with LLMs on a daily basis don’t see the need to repeatedly harass everybody, or talk down to them about their unwillingness to pledge allegiance to the graveyard smash of generative AI. In fact, the closer I’ve found somebody to actually building things with LLMs, the less likely they are to emphatically argue that I’m missing out by not doing so myself. No, the AI booster is symbolically aligned with generative AI. They are fans in the same way that somebody is a fan of a sports team, their houses emblazoned with every possible piece of tat they can find, their Sundays living and dying by the success of the team, except even fans of the Dallas Cowboys have a tighter grasp on reality. Kevin Roose and Casey Newton are two of the most notable boosters, and — as I’ll get into later in this piece — neither of them have a consistent or comprehensive knowledge of AI. Nevertheless, they will insist that “everybody is using AI for everything” — a statement that even a booster should realize is incorrect based on the actual abilities of the models. But that’s because it isn’t about what’s actually happening, it’s about allegiance. AI symbolizes something to the AI booster — a way that they’re better than other people, that makes them superior because they (unlike “cynics” and “skeptics”) are able to see the incredible potential in the future of AI, but also how great it is today, though they never seem to be able to explain why outside of “it replaced search for me!” and “I use it to draw connections between articles I write,” which is something I do without AI using my fucking brain. Boosterism is a kind of religion, interested in finding symbolic “proof” that things are getting “better” in some indeterminate way, and that anyone that chooses to believe otherwise is ignorant. I’ll give you an example. Thomas Ptacek’s “My AI Skeptic Friends Are All Nuts” was catnip for boosters — a software engineer using technical terms like “interact with Git” and “MCP,” vague charts, and, of course, an extremely vague statement that says hallucinations aren’t a problem: Is it? Anyway, my favourite part of the blog is this: Nobody projects more than an AI booster. They thrive on the sense they’re oppressed and villainized after years of seemingly every outlet claiming they’re right regardless of whether there’s any proof. They sneer and jeer and cry constantly that people are not showing adequate amounts of awe when an AI lab says “we did something in private, we can’t share it with you, but it’s so cool,” and constantly act as if they’re victims as they spread outright misinformation, either through getting things wrong or never really caring enough to check. Also, none of the booster arguments actually survive a thorough response, as Nik Suresh proved with his hilarious and brutal takedown of Ptacek’s piece. There are, I believe, some people who truly do love using LLMs, yet they are not the ones defending them. Ptacek’s piece drips with condescension, to the point that it feels like he’s trying to convince himself how good LLMs are, and because boosters are eternal victims, he wrote them a piece that they could send around to skeptics and say “heh, see?” without being able to explain why it was such a brutal takedown, mostly because they can’t express why other than “well this guy gets it!” One cannot be the big, smart genius that understands the glory and power of AI while also acting like a scared little puppy every time somebody tells them it sucks. In fact, that’s a great place to start. When you speak to an AI booster, you may get the instinct to shake them vigorously, or respond to their post by saying to do something with your something, or that they’re “stupid.” I understand the temptation, but you want to keep a head on a swivel — they thrive on victimisation. I’m sorry if you are an AI booster and this makes you feel bad. Please reflect on your work and how many times you’ve referred to somebody who didn’t understand AI in a manner that suggested they were ignorant, or tried to gaslight them by saying “AI was powerful” while providing no actionable ways in which it is. You cannot — and should not! — allow these people to act as if they are being victimized or “othered.” First and foremost: there are boosters at pretty much every major think tank, government agency and media outlet. It’s extremely lucrative being a booster. You’re showered with panel invites, access to executives, and are able to get headlines by saying how scared you are of the computer with ease. Being a booster is the easy path! Being a critic requires you to constantly have to explain yourself in a way that boosters never have to. If a booster says this to you, ask them to explain: There is no answer here, because this is not a coherent point of view. Boosters are more successful, get more perks and are in general better-treated than any critic. Fundamentally, these people exist in the land of the vague. They will drag you toward what's just on the horizon, but never quite define what the thing that dazzles you will be, or when it will arrive. Really, their argument comes down to one thought: you must get on board now, because at some point it'll be so good you'll feel stupid for not believing something that kind of sucks wouldn't be really good. If this line sounds familiar, it’s because you’ve heard it a million times before, most notably with crypto. They will make you define what would impress you, which isn't your job, in the same way finding a use case for them isn't your job. In fact, you are the customer! Here’s a great place to start: say “that’s a really weird thing to say!” It is peculiar to suggest that somebody doesn’t get how to use a product, and that we, as the customer, must justify ourselves to our own purchases. Make them justify their attitude. Just like any product, we buy software to serve a need. This is meant to be artificial *intelligence* — why is it so fucking stupid that I have to work out why it's useful? The answer, of course, is that it has no intellect, is not intelligent, and Large Language Models are being pushed up a mountain by a cadre of people who are either easily impressed or invested — either emotionally or financially — in its success due to the company they keep or their intentions for the world. If a booster suggests you “just don’t get it,” ask them to explain: Their use cases will likely be that AI has replaced search for them, that they use it for brainstorming or journaling, proof-reading an article, or looking through a big pile of their notes (or some other corpus of information) and summarizing it or pulling out insights. If a booster refers to AI “being powerful” and getting “more powerful,” ask them: The core of the AI booster’s argument is to make you feel bad. They will suggest you are intentionally not liking A.I. because you're a hater, or a cynic, or a Luddite. They will suggest that you are ignorant for not being amazed by ChatGPT. To be clear, anyone with a compelling argument doesn’t have to make you feel bad to convince you. The iPhone - and to be clear, I am referring to the concept of the smartphone and its utility, I am aware that there was marketing for the iPhone - didn’t need a fucking marketing campaign to explain why one device that can do a bunch of things you already find useful was good. You don't have to be impressed by ANYTHING by default, and any product — especially software — designed to make you feel stupid for "not getting it" is poorly designed. ChatGPT is the ultimate form of Silicon Valley Sociopathy — you must do the work to find the use cases, and thank them for being given the chance to do so. A.I. is not even good, reliable software! It resembles the death of the art of technology — inconsistent and unreliable by definition, inefficient by design, financially ruinous, and ADDS to the cognitive load of the user by requiring them to be ever-vigilant. So, here’s a really easy way to deal with this: if a booster ever suggests you are stupid or ignorant, ask them why it’s necessary to demean you to get their point across! Even if you are unable to argue on a technical level, make them explain why the software itself can’t convince you. Boosters will do everything they can to pull you off course. If you say that none of these companies make money, they’ll say it’s the early days. If you say AI companies burn billions, they’ll say the cost of inference is coming down. If you say the industry is massively overbuilding, they’ll say that this is actually just like the dot com boom and that the infrastructure will be picked up and used in the future. If you say there are no real use cases, they’ll say that ChatGPT has 700 million weekly users. Every time there’s the same god damn arguments, so I’ve sat down and written as many of them as I can think of. Print this and feed it to your local booster today. Anytime a booster says “AI will,” tell them to stop and explain what AI can do, and if they insist, ask them both when to expect the things they’re talking about, and if they say “very soon,” ask them to be more specific. Get them to agree to a date, then call them on that date. There’s that “will” bullshit again. Agents don’t work! They don’t work at all. The term “agent” means, to quote Max Woolf, “a workflow where the LLM can make its own decisions, [such as in the case of] web search [where] the LLM is told “you can search the web if you need to” then can output “I should search the web” and do so.” Yet “agent” has now become a mythical creature that means “totally autonomous AI that can do an entire job.” if anyone tells you “agents are…” you should ask them to point to one. If they say “coding,” please demand that they explain how autonomous these things are, and if they say that they can “refactor entire codebases,” ask them what that means, and also laugh at them. Here’s a comprehensive rundown, but here’s a particularly important part: Long story short, agents are not autonomous, they do not replace jobs, they cannot “replace coders,” they are not going to do so because probabilistic models are a horrible means of taking precise actions, and almost anyone who brings up agents as a booster is either misinformed or in the business of misinformation. Let's start with a really simple question: what does this actually mean? In many cases, I think they're referring to AI as being "like the early days of the internet." "The early days of the internet" can refer to just about anything. Are we talking about dial-up? DSL? Are we talking about the pre-platform days when people accessed it via Compuserve or AOL? Yes, yes, I remember that article from Newsweek, I already explained it here: In any case, one guy saying that the internet won't be big doesn't mean a fucking thing about generative AI and you are a simpleton if you think it does. One guy being wrong in some way is not a response to my work. I will crush you like a bug. If your argument is that the early internet required expensive Sun Microsystems servers to run, Jim Covello of Goldman Sachs addressed that by saying that the costs "pale in comparison," adding that we also didn't need to expand our power grid to build the early Web. This is a straight-up lie. Sorry! Also, as Jim Covello noted, there were hundreds of presentations in the early 2000s that included roadmaps that accurately fit how smartphones rolled out, and that no such roadmap exists for generative AI. The iPhone was also an immediate success as a thing that people paid for, with Apple selling four million units in the space of six months. Hell, in 2006 (the year before the iPhone launch), there was an estimated 17.7 million worldwide smartphone shipments (mostly from BlackBerry and other companies building on Windows Mobile, with Palm vacuuming up the crumbs), though to be generous to the generative AI boosters, I’ll disregard those. The original Attention Is All You Need paper — the one that kicked off the transformer-based Large Language Model era — was published in June 2017. ChatGPT launched in November 2022. Nevertheless, if we're saying "early days" here, we should actually define what that means. As I mentioned above, people paid for the iPhone immediately, despite it being a device that was completely and utterly new. While there was a small group of consumers that might have used similar devices (like the iPAQ), this was a completely new kind of computing, sold at a premium, requiring you to have a contract with a specific carrier (Cingular, now known as AT&T). Conversely, ChatGPT's "annualized" revenue in December 2023 was $1.6 billion (or $133 million a month), for a product that had, by that time, raised over $10 billion, and while we don't know what OpenAI lost in 2023, reports suggest it burned over $5 billion in 2024. Big tech has spent over $500 billion in capital expenditures in the last 18 months, and all told — between investments of cloud credits and infrastructure — will likely sink over $600 billion by year's-end. The "early days" of the internet were defined not by its lack of investment or attention, but by its obscurity. Even in 2000 — around the time of the dot-com bubble — only 52% of US adults used the internet, and it would take another 19 years for 90% of US adults to do so. These early days were also defined by its early functionality. The internet would become so much more because of the things that hyper-connectivity allowed us to do, and both faster internet connections and the ability to host software in the cloud would change, well, everything. We could define what “better” would mean, and make reasonable predictions about what people could do on a “better” internet. Yet even in those early days, it was obvious why you were using the internet, and how it might grow from there. One did not have to struggle to explain why buying a book online might be useful, or why a website might be a quicker reference than having to go to a library, or why downloading a game or a song might be a good idea. While habits might have needed adjusting, it was blatantly obvious what the value of the early internet was. It's also unclear when the early days of the internet ended. Only 44% of US adults had access to broadband internet by 2006. Were those the early days of the internet? The answer is "no," and that this point is brought up by people with a poor grasp of history and a flimsy attachment to reality. The early days of the internet were very, very different to any associated tech boom since, and we need to stop making the comparison. The internet also grew in a vastly different information ecosystem. Generative AI has had the benefit of mass media — driven by the internet! — along with social media (and social pressure) to "adopt AI" for multiple years. According to Pew, as of mid-June 2025, 34% of US adults have used ChatGPT, with 79% saying they had "heard at least a little about it." Furthermore, ChatGPT has always had a free version. On top of that, a study from May 2023 found that over 10,900 news headlines mentioned ChatGPT between November, 2022 and March, 2023, and a BrandWatch report found that in the first five months of its release, ChatGPT received over 9.24 million mentions on social media. Nearly 80% of people have heard of ChatGPT, and over a quarter of Americans have used it. If we're defining "the early days" based on consumer exposure, that ship has sailed. If we're defining "the early days" by the passage of time, it's been 8 years since Attention Is All You Need, and three since ChatGPT came out. While three years might not seem like a lot of time, the whole foundation of an "early days" argument is that in the early days, things do not receive the venture funding, research, attention, infrastructural support or business interest necessary to make them "big." In 2024, nearly 33% of all global venture funding went to artificial intelligence, and according to The Information, AI startups have raised over $40 billion in 2025 alone, with Statista adding that AI absorbed 71% of VC funding in Q1 2025. These numbers also fail to account for the massive infrastructure that companies like OpenAI and Anthropic don't have to pay for. The limitations of the early internet were two-fold: In generative AI's case, Microsoft, Google, and Amazon have built out the "fiber optic cables" for Large Language Models. OpenAI and Anthropic have everything they need. They have (even if they say otherwise) plenty of compute, access to the literal greatest minds in the field, the constant attention of the media and global governments, and effectively no regulations or restrictions stopping them from training their models on the works of millions of people, or destroying our environment. They have already had this support. OpenAI was allowed to burn half a billion dollars on a training run for GPT-4.5 and 5. If anything, the massive amounts of capital have allowed us to massively condense the time in which a bubble goes from "possible" to "bursting and washing out a bunch of people," because the tech industry has such a powerful follower culture that only one or two unique ideas can exist at one time. The "early days" argument hinges on obscurity and limited resources, something that generative AI does not get to whine about. Companies that make effectively no revenue can raise $500 million to do the same AI coding bullshit that everybody else does. In simpler terms, these companies are flush with cash, have all the attention and investment they could possibly need, and are still unable to create a product with a defined, meaningful, mass-market use case. In fact, I believe that thanks to effectively infinite resources, we've speed-run the entire Large Language Model era, and we're nearing the end. These companies got what they wanted. Bonus trick: ask them to tell you what “the fiber boom” was. So, a little history. The "fiber boom" began after the telecommunications act of 1996 deregulated large parts of America's communications infrastructure, creating a massive boom — a $500 billion one to be precise, primarily funded with debt: In one sense, explaining what happened to the telecom sector is very simple: the growth in capacity has vastly outstripped the growth in demand. In the five years since the 1996 bill became law, telecommunications companies poured more than $500 billion into laying fiber optic cable, adding new switches, and building wireless networks. So much long-distance capacity was added in North America, for example, that no more than two percent is currently being used. With the fixed costs of these new networks so high and the marginal costs of sending signals over them so low, it is not a surprise that competition has forced prices down to the point where many firms have lost the ability to service their debts. No wonder we have seen so many bankruptcies and layoffs. This piece, written in 2002, is often cited as a defense against the horrifying capex associated with generative AI, as that fiber optic cable has been useful for delivering high-speed internet. Useful, right? This period was also defined by a gluttony of over-investment, ridiculous valuations and outright fraud. In any case, this is not remotely the same thing and anyone making this point needs to learn the very fucking basics of technology. GPUs are built to shove massive amounts of compute into one specific function again and again, like generating the output of a model (which, remember, mostly boils down to complex maths). Unlike CPUs, a GPU can't easily change tasks, or handle many little distinct operations, meaning that these things aren't going to be adopted for another mass-scale use case. In simpler terms, this was not an infrastructure buildout. The GPU boom is a heavily-centralized, capital expenditure-funded asset bubble where a bunch of chips will sit in warehouses waiting for somebody to make up a use case for them, and if an endearing one existed, we'd already have it because we already have all the fucking GPUs. You are describing fan fiction. AI 2027 is fan fiction. Anyone who believes in it is a mark! It doesn’t matter if all of the people writing the fan fiction are scientists, or that they all have “the right credentials.” They themselves say that AI 2027 is a “guess,” an “extrapolation” (guess) with “expert feedback” (someone editing your fan fiction), and involves “experience at OpenAI” (there are people that worked on the shows they write fan fiction about). I am not going to go line-by-line to cut this apart anymore than I am going to write a lengthy takedown of someone’s erotic Banjo Kazooie story, because both are fictional. The entire premise of this nonsense is that at one point someone invents a self-learning “agent” that teaches itself stuff, and it does a bunch of other stuff as a result, with different agents with different numbers after them. There is no proof this is possible, nobody has done it, nobody will do it. AI 2027 was written specifically to fool people that wanted to be fooled, with big charts and the right technical terms used to lull the credulous into a wet dream and New York Times column where one of the writers folds their hands and looks worried. It was also written to scare people that are already scared. It makes big, scary proclamations, with tons of links to stuff that looks really legitimate but, when you piece it all together, is still fan fiction. My personal favourite part is “Mid 2026: China Wakes Up,” which involves China’s intelligence agencies trying to steal OpenBrain’s agent (no idea who this company could be referring to, I’m stumped!), before the headline of “AI Takes Some Jobs” after OpenBrain released a model oh god I am so bored even writing up this tripe! Sarah Lyons put it well, arguing that AI 2027 (and AI in general) is no different from the spurious “spectral evidence” used to accuse someone of being a witch during the Salem Witch Trials: Anyway, AI 2027 is fan fiction, nothing more, and just because it’s full of fancy words and has five different grifters on its byline doesn’t mean anything. Bonus trick: Ask them to explain whether things have actually got cheaper, and if they say they have, ask them why there are no profitable AI companies. If they say “they’re currently in growth stage,” ask them why there are no profitable AI companies. At this point they should try and kill you. In an interview on a podcast from earlier in the year, journalist Casey Newton said the following about my work: Newton then says — several octaves higher, showing how mad he isn't — that "[he] thought what [he] said was very civil" and that there are "things that are true and there are things that are false, like you can choose which ones you wanna believe." I am not going to be so civil. Other than the fact that Casey refers to "micro-innovations" (?) and "DeepSeek being on a curve that was expected," he makes — as many do — two very big mistakes, ones that I personally would not have said in a sentence that begun with suggesting that I knew how the technology works. Inference — and I've gotten this one wrong in the past too! — is everything that happens from when you put a prompt in to generate an output. It's when an AI, based on your prompt, "infers" meaning. To be more specific, and quoting Google, "...machine learning inference is the process of running data points into a machine learning model to calculate an output such as a single numerical score." Casey will try and weasel out of this one and say this is what he meant. It wasn't. Casey, like many people who talk about stuff without learning about it first, is likely referring to the fact that the price of tokens for some models has gone down in some cases. The problem, however, is that these are raw token costs, not actual expressions or evaluations of token burn in a practical setting. Worse still… Well, the cost of inference actually went up. In an excellent blog for Kilocode, Ewa Szyszka explained: Token consumption per application grew a lot because models allowed for longer context windows and bigger suggestions from the models. The combination of a steady price per token and more token consumption caused app inference costs to grow about 10x over the last two years. To explain in really simple terms, while the costs of old models may have decreased, new models cost about the same, and the "reasoning" that these models do actually burn way, way more tokens. When these new models "reason," they break a user's input and break into component parts, then run inference on each one of those parts. When you plug an LLM into an AI coding environment, it will naturally burn an absolute ton of tokens, in part because of the large amount of information you have to load into the prompt (and the "context window," or the information you load in with your prompt, with token burn increasing with the size of that information), and in part because generating code is inference-intensive. In fact, the inference costs are so severe that Szyszka says that "...combination of a steady price per token and more token consumption caused app inference costs to grow about 10x over the last two years." I refuse to let this point go, because people love to say "the cost of inference is going down" when the cost of inference has increased, and they do so to a national audience, all while suggesting I am wrong somehow. I am not wrong. In fact, software development influencer Theo Browne recently put out a video called "I was wrong about AI costs (they keep going up)," which he breaks down as follows: The price drops have, for the most part, stopped. See the below chart from The Information: You cannot, at this point, fairly evaluate whether a model is "cheaper" just based on its cost-per-tokens, because reasoning models are inherently built to use more tokens to create an output. Reasoning models are also the only way that model developers have been able to improve the efficacy of new models, using something called "test-time compute" to burn extra tokens to complete a task. And in basically anything you're using today, there's gonna be some sort of reasoning model, especially if you're coding. The cost of inference has gone up. Statements otherwise are purely false, and are the opinion of somebody who does not know what he's talking about. ...maybe? It sure isn't trending that way, nor has it gone down yet. I also predict that there's going to be a sudden realization in the media that it's going up, which has kind of already started. The Information had a piece recently about it, where they note that Intuit paid $20 million to Azure last year (primarily for access to OpenAI's models), and is on track to spend $30 million this year, which "outpaces the company's revenue growth in the same period, raising questions about how sustainable the spending is and how much of the cost it can pass along to customers." The problem here is that the architecture underlying Large Language Models is inherently unreliable. I imagine OpenAI's introduction of the router to ChatGPT-5 is an attempt to moderate both the costs of the model chosen and reduce the amount of exposure to reasoning models for simple queries — though Altman was boasting on August 10th about the "significant increase" in both free and paid users' exposure to reasoning models. Worse still, a study written up by VentureBeat found that open-weight models burn between 1.5 to 4 times more tokens, in part due to a lack of token efficiency, and in particular thanks to — you guessed it! — reasoning models: The findings challenge a prevailing assumption in the AI industry that open-source models offer clear economic advantages over proprietary alternatives. While open-source models typically cost less per token to run, the study suggests this advantage can be “easily offset if they require more tokens to reason about a given problem.” And models keep getting bigger and more expensive, too. Because model developers hit a wall of diminishing returns, and the only way to make their models do more was to make them burn more tokens to generate a more accurate response (this is a very simple way of describing reasoning, a thing that OpenAI launched in September 2024 and others followed). As a result, all the "gains" from "powerful new models" come from burning more and more tokens. The cost-per-million-token number is no longer an accurate measure of the actual costs of generative AI, because it's much, much harder to tell how many tokens a reasoning model may burn, and it varies (as Theo Browne noted) from model to model. In any case, there really is no changing this path. They are out of ideas. So, I've heard this argument maybe 50 times in the last year, to the point that I had to talk about it in my July 2024 piece "How Does OpenAI Survive." Nevertheless, people make a few points about Uber and AI that I think are fundamentally incorrect, and I'll break them down for you. I've seen this argument a lot, and it's one that's both ahistorical and alarmingly ignorant of the very basics of society. So, OpenAI got a $200 million defense contract with an "estimated completion date of July 2026," and is selling ChatGPT Enterprise to the US government for a dollar a year (along with Anthropic, which sells access to Claude for the same price, Even Google is undercutting them, selling Gemini access at 47 cents for a year). You're probably reading that and saying "oh no, that means the government has paid them now, they're never going away," and I cannot be clear enough that you believing this is the intention of these deals. These are built specifically to make you feel like these things are never going away. This is also an attempt to get "in" with the government at a rate that makes "trying" these models a no-brainer. ...and??????? "The government is going to have cheap access to AI software" does not mean that "the government relies on AI software." Every member of the government having access to ChatGPT — something that is not even necessarily the case! — does not make this software useful, let alone essential, and if OpenAI burns a bunch of money "making it work for them," it still won't be essential, because Large Language Models are not actually that useful for doing stuff! Uber used lobbyist Bradley Tusk to steamroll local governments into allowing Uber to operate in their cities, but Tusk did not have to convince local governments that Uber was useful or have to train people how to use it. Uber's "too big to fail" moment was that local cabs kind of fucking sucked just about everywhere. Did you ever try and take a yellow cab from Downtown Manhattan to Hoboken New Jersey? Or Brooklyn? Or Queens? Did you ever try to pay with a credit card? How about trying to get a cab outside of a major metropolitan area? Do you remember how bad that was? I am not glorifying Uber the company, but the experience that Uber replaced was very, very bad. As a result, Uber did become too big to fail, because people now rely upon it because the old system sucked. Uber used its masses of venture capital to keep prices low to get people used to it too, but the fundamental experience was better than calling a cab company and hoping that they showed up. I also want to be clear this is not me condoning Uber, take public transport if you can! To be clear, Uber has created a new kind of horrifying, extractive labor practice which deprives people of benefits and dignity, paying off academics to help the media gloss over the horrors of its platform. It is also now having to increase prices. What, exactly, is the "essential" experience of generative AI? What essential experience are we going to miss if ChatGPT disappears tomorrow? And on an enterprise or governmental level: what exactly are these tools doing for governments that would make removing them so painful? What use cases? What outcomes? Uber's "essential" nature is that millions of people use it in place of regular taxis, and it effectively replaced decrepit, exploitative systems like the yellow cab medallions in New York with its own tech-enabled exploitation system that, nevertheless, worked far better for the user. Sidenote: although I acknowledge that the disruption that Uber brought to the medallion system had horrendous consequences for the owners of said medallions — some of whom had paid more than a million dollars for the privilege to drive a New York taxi cab, and were burdened under mountains of debt. There is no such use case with ChatGPT, or any other generative AI system. You cannot point to one use case that is anywhere near as necessary as cabs in cities, and indeed the biggest use cases — things like brainstorming and search — are either easily replaced by any other commoditized LLM or literally already exist with Google Search. Nope! Sorry, this is a really simple one. These data centers are not, in and of themselves, driving much economic growth other than in the costs of building them. As I've discussed again and again, there's maybe $40 billion in revenue and no profit coming out of these companies. There isn't any economic growth! They're not holding up anything! These data centers, once built, also create very little economic activity. They don't create jobs, they take up massive amounts of land and utilities, and they piss off and poison their neighbors. If anything, letting these things die would be a political win. There is no "great loss" associated with the death of the Large Language Model era. Taking away Uber would genuinely affect people's ability to get places. So, the classic (and wrong!) argument about OpenAI and companies like OpenAI is that "Uber burned a bunch of money and is now "cash-flow positive" or "profitable." Let's talk about raw losses, and where people are making this assumption. Uber lost $24.9 billion in the space of four years (2019 to 2022), in part because of the billions it was spending on sales and marketing and R&D — $4.6 billion and $4.8 billion respectively in 2019 alone. It also massively subsidized the cost of rides — which is why prices had to increase — and spent heavily on driver recruitment, burning cash to get scale, the classic Silicon Valley way. This is absolutely nothing like how Large Language Models are growing, and I am tired of defending this point. OpenAI and Anthropic burn money primarily through compute costs and specialized talent. These costs are increasing, especially with the rush to hire every single AI scientist at the most expensive price possible. There are also essential, immovable costs that neither OpenAI nor Anthropic have to shoulder — the construction of the data centers necessary to train and run inference for their models, which I will get to in a little bit. Yes, Uber raised $33.5 billion (through multiple rounds of post-IPO debt, though it raised about $25 billion in actual funding). Yes, Uber burned an absolute ass-ton of money. Yes, Uber has scale. But Uber was not burning money as a means of making its product functional or useful. Furthermore, the costs associated with Uber — and its capital expenditures from 2019 through 2024 were around $2.2 billion! — are miniscule compared to the actual costs of OpenAI and Anthropic. Both OpenAI and Anthropic lost around $5 billion in 2024, but their infrastructure was entirely paid for by either Microsoft, Google or Amazon. While we don't know how much of this infrastructure is specifically for OpenAI or Anthropic, as the largest model developers it's fair to assume that a large chunk — at least 30% — of Amazon and Microsoft's capital expenditures have been to support these loads (I leave out Google as it's unclear whether it’s expanded its infrastructure for Anthropic, but we know Amazon has done so). As a result, the true "cost" of OpenAI and Anthropic is at least ten times what Uber burned. Amazon spent $83 billion in capital expenditures in 2024 and expects to spend $105 billion in 2025. Microsoft spent $55.6 billion in 2024 and expects to spend $80 billion this year. Based on my (conservative) calculations, the true "cost" of OpenAI is around $82 billion, and that only includes capex from 2024 onward, based on 30% of Microsoft's capex (as not everything has been invested yet in 2025, and OpenAI is not necessarily all of the capex) and the $41.4 billion of funding it’s received so far. The true cost of Anthropic is around $77.1 billion, including all its funding and 30% of Amazon's capex from the beginning of 2024. These are inexact comparisons, but the classic argument is that Uber "burned lots of money and worked out okay," when in fact the combined capital expenditures from 2024 onwards that are necessary to make Anthropic and OpenAI work are each — on their own — four times the amount Uber burned in over a decade. I also believe that these numbers are conservative. There's a good chance that Anthropic and OpenAI dominate the capex of Amazon and Microsoft, in part because what the fuck else are they buying all these GPUs for, as their own AI services don't seem to be making that much money at all. Anyway, to put it real simple, AI has burned more in the last two years than Uber burned in ten, Uber didn't burn money in the same way, didn't burn much by way of capital expenditures, didn't require massive amounts of infrastructure, and isn't remotely the same in any way, shape or form, other than it burned a lot of money — and that burning wasn’t because it was trying to build the core product, but rather trying to scale. I covered this in depth in the Hater's Guide To The AI Bubble, but the long and short of it is that AWS is a platform, a necessity with an obvious choice, and has burned about ten percent of what Amazon et. al has burned chasing generative AI, and had proven demand before building it. Also, AWS was break-even in three years. OpenAI was founded in fucking 2015, and even if you start from November 2022, by AWS standards it should be break-even! Amazon Web Services was created out of necessity — Amazon's infrastructure needs were so great that it effectively had to build both the software and hardware necessary to deliver a store that sold theoretically everything to theoretically anywhere, handling both the traffic from customers, delivering the software that runs Amazon.com quickly and reliably, and, well, making sure things ran in a stable way. It didn't need to come up with a reason for people to run web applications — they were already doing so themselves, but in ways that cost a lot, were inflexible, and required specialist skills. AWS took something that people already did, and what there was a proven demand for, and made it better. Eventually, Google and Microsoft would join the fray. As I’ve discussed in the past, this metric is basically “monthx12,” and while it’s a fine measure for high-gross-margin businesses like SaaS companies, it isn’t for AI. It doesn’t account for churn (when people leave). It also is a number intentionally used to make a company sound more successful — so you can say “$200 million annualized revenue” instead of “$16.6 million a month.” Also, if they’re saying this number, it’s likely that number isn’t consistent! Also: Simple answer: why have literally none of them done this yet? Why not one? Why? There’s that “will” bullshit, once again, always about the “will.” We do not know how thinking works in humans and thus cannot extrapolate it to a machine, and at the very least human beings have the ability to re-evaluate things and learn, a thing that LLMs cannot do and will never do. We do not know how to get to AGI. Sam Altman said in June that OpenAI was “now confident [they knew] how to build AGI as we have traditionally understood it.” In August, Altman said that AGI was “not a super useful term,” and that “the point of all this is it doesn’t really matter and it’s just this continuing exponential of model capability that we’ll rely on for more and more things.” So, yeah, total bullshit. Even Meta’s Chief AI Scientist says it isn’t possible with transformer-based models. We don’t know if AGI is possible, anyone claiming they do is lying. This, too, is hogwash, nothing different than your buddy’s friend’s uncle who works at Nintendo that says Mario is coming to PlayStation. Ilya Sutskever and Mira Murati raised billions for companies with no product, let alone a product road map, and they did so because they saw a good opportunity for a grift and to throw a bunch of money at compute. Also: if someone from “deep within the AI industry” has told somebody “big things are coming,” they are doing so to con them or make them think they have privileged information. Ask for specifics. This argument is poised as a comeback to my suggestion that AI isn't particularly useful, a proof point that this movement is not inherently wasteful, or that there are, in fact, use cases for ChatGPT that are lasting, meaningful or important. I disagree. In fact, I believe ChatGPT — and LLMs in general — have been marketed based on lies of inference. Ironic, I know. I also have grander concerns and suspicions about what OpenAI considers a “user” and how it counts revenue, I’ll get into that later in the week on my premium newsletter, which you should subscribe to. Here’s a hint though: 500,000 of OpenAI’s “5 million business customers” are from its $15 million deal with Cal State University, which works out to around $2.50-a-user-a-month. It’s also started doing $1-a-month trials of its $30-a-month “Teams” subscription, and one has to wonder how many of those are counted in that total, and for how long. I do not know the scale of these offers, nor how long OpenAI has been offering them. A Redditor posted about the deal a few months ago, saying that OpenAI was offering up to 5 seats at once. In fact, I've found a few people talking about these deals, and even one adding that they were offered an annual $10-a-month ChatGPT Plus subscription, with one person saying a few weeks ago saying they'd seen people offered this deal for canceling their subscription. Suspicious. But there’s a greater problem at play. So, ChatGPT has 700 million weekly active users. OpenAI has yet to provide a definition — and yes, I've asked! — which means that an "active" user could be defined as somebody who has gone to ChatGPT once in the space of a week. This term is extremely flimsy, and doesn't really tell us much. Similarweb says that in July 2025 ChatGPT.com had 1.287 billion total visits, making it a very popular website. What do these facts actually mean, though? As I said previously, ChatGPT has had probably the most sustained PR campaign for anything outside of a presidency or a pop star. Every single article about AI mentions OpenAI or ChatGPT, every single feature launch — no matter how small — gets a slew of coverage. Every single time you hear "AI" you’re made to think of "ChatGPT” by a tech media that has never stopped to think about their role in hype, or their responsibility to their readers. And as this hype has grown, the publicity compounds, because the natural thing for a journalist to do when everybody is talking about something is to talk about it more. ChatGPT's immediate popularity may have been viral, but the media took the ball and ran with it, and then proceeded to tell people it did stuff it did not. People were pressured to try this service then under false pretenses, something that continues to this day. I'll give you an example. On March 15 2023, Kevin Roose of the New York Times would say that OpenAI's GPT-4 was "exciting and scary," exacerbating (his words!) "...the dizzy and vertiginous feeling I’ve been getting whenever I think about A.I. lately," wondering if he was experiencing "future shock," then described how it was an indeterminate level of "better" and something that immediately sounded ridiculous: In one test, conducted by an A.I. safety research group that hooked GPT-4 up to a number of other systems, GPT-4 was able to hire a human TaskRabbit worker to do a simple online task for it — solving a Captcha test — without alerting the person to the fact that it was a robot. The A.I. even lied to the worker about why it needed the Captcha done, concocting a story about a vision impairment. That doesn't sound remotely real! I went and looked up the paper, and here is the entire extent of what OpenAI shared: This safety card led to the perpetration of one of the earliest falsehoods — and most eagerly-parotted lies — that ChatGPT and generative AI is capable of "agentic" actions. Outlet after outlet — led by Kevin Roose — eagerly interpreted an entire series of events that took place that doesn't remotely make sense, starting with the fact that this is not something you can hire a Taskrabbit to do. Or, at the very least, without a contrived situation where you create an empty task and ask them to complete it. Why not use Mechanical Turk? Or Fiverr? They’ve tons of people offering this service! But I'm a curious little critter, so I went further and followed their citation to a link on METR's research page. It turns out that what actually happened was METR had a researcher copy paste the generated responses from the model and otherwise handle the entire interaction with Taskrabbit, and based on the plurality of "Taskrabbit contractors," it appears to have taken multiple tries. On top of that, it appears that OpenAI/METR were prompting the model on what to say, which kind of defeats the point. Emphases mine, and comments in [brackets]: It took me five whole minutes to find this piece — which is cited on the GPT-4 system card — read it, then write this piece. It did not require any technical knowledge other than the ability to read stuff. It is transparently, blatantly obvious that GPT-4 did not "hire" a Taskrabbit or, indeed, make any of these actions — it was prompted to, and they do not show the prompts they used, likely because they had to use so many of them. Anyone falling for this is a mark, and OpenAI should have gone out of its way to correct people. Instead, they sat back and let people publish outright misinformation. Roose, along with his co-host Casey Newton, would go on to describe this example at length on a podcast that week, describing an entire narrative where “the human actually gets suspicious” and “GPT 4 reasons out loud that it should not reveal that [it is] a robot,” at which point “the TaskRabbit solves the CAPTCHA.” During this conversation, Newton gasps and says “oh my god” twice, and when he asks Roose “how does the model understand that in order to succeed at this task, it has to deceive the human?” Roose responds “we don’t know, that is the unsatisfying answer,” and Newton laughs and states “we need to pull the plug. I mean, again, what?” Credulousness aside, the GPT-4 marketing campaign was incredibly effective, creating an aura that allowed OpenAI to take advantage of the vagueness of its offering as people — including members of the media — willfully filled in the blanks for them. Altman has never had to work to sell this product. Think about it — have you ever heard OpenAI tell you what ChatGPT can do, or gone to great lengths to describe its actual abilities? Even on OpenAI's own page for ChatGPT, the text is extremely vague: Scrolling down, you're told ChatGPT can "write, brainstorm, edit and explore ideas with you." It can "generate and debug code, automate repetitive tasks, and [help you] learn new APIs." With ChatGPT you can "learn something new...dive into a hobby...answer complex questions" and "analyze data and create charts." What repetitive tasks? Who knows. How am I learning? Unclear. It's got thinking built in! What that means is unclear, unexplained, and thus allows a user to incorrectly believe that ChatGPT has a brain. To be clear, I know what reasoning means, but this website does not attempt to explain what "thinking" means. You can also "offload complex tasks from start to finish with an agent," which can, according to OpenAI, "think and act, proactively choosing from a toolbox of agentic skills to complete tasks for you using its own computer." This is an egregious lie, employing the kind of weasel-wording that would be used to torture "I.R. Baboon" for an eternity. Precise in its vagueness, OpenAI's copy is honed to make reporters willing to simply write down whatever they see and interpret it in the most positive light. And thus the lie of inference began. What "ChatGPT" meant was muddied from the very beginning, and thus ChatGPT's actual outcomes have never been fully defined. What ChatGPT "could do" became a form of folklore — a non-specific form of "automation" that could "write code" and "generate copy and images," that can "analyze data," all things that are true but one can infer much greater meaning from. One can infer that "automation" means the automation of anything related to text, or that "write code" means "write the entirety of a computer program." OpenAI's ChatGPT agent is not, by any extension of the word, "already a powerful tool for handling complex tasks," but it has not, in any meaningful sense, committed to any actual outcomes. As a result, potential users — subject to a 24/7 marketing campaign — have been pushed toward a website that can theoretically do anything or nothing, and have otherwise been left to their own devices. The endless gaslighting, societal pressure, media pressure, and pressure from their bosses has pushed hundreds of millions of people to try a product that even its creators can't really describe. As I've said in the past, OpenAI is deliberately using Weekly Active Users so that it doesn't have to publish its monthly active users, which I believe would be higher. Why wouldn't it do this? Well, OpenAI has 20 million paying ChatGPT subscribers and five million "business customers," with no explanation of what the difference might be. This is already a mediocre (3.5%) conversion rate, yet its monthly active users (which are likely either 800 million or 900 million, but these are guesses!) would make that rate lower than 3%, which is pretty terrible considering everybody says this shit is the future. I also am tired of having people claim that "search" or "brainstorm" or "companions" are a lasting, meaningful business models. So, OpenAI announced that it has hit its first $1 billion month on August 20, 2025 on CNBC, which brings it exactly in line with my estimated $5.26 billion in revenue that I believe it has made as of the end of July. However, remember what the MIT study said: enterprise adoption is high but transformation is low. There are tons of companies throwing money at AI, but they are not seeing actual returns. OpenAI's growth as the single-most-prominent company in AI (and if we're honest, one of the most prominent in software writ large) makes sense, but at some point will slow, because the actual returns for the businesses aren't there. If there were, we'd have one article where we could point at a ChatGPT integration that at scale helped a company make or save a bunch of money, written in plain English and not a gobbledygook of "profit improvement." Also… OpenAI is projected to make $12.7 billion in 2025. How exactly will it do that? Is it really making $1.5 billion a month by the end of the year? Even if it does, is the idea that it keeps burning $10 billion or more every year into eternity? What actual revenue potential does OpenAI have long-term? Its products are about as good as everyone else's, cost about the same, and do the same things. ChatGPT is basically the same product as Claude or Grok or any number of different LLMs. The only real advantages that OpenAI has are infrastructure and brand recognition. These models have clearly hit a wall where training is hitting diminishing returns, meaning that its infrastructural advantage is that they can continue providing its service at scale, nothing more. It isn't making its business cheaper, other than the fact that it mostly hasn’t had to pay for it...other than the site in Abilene Texas where it’s promised Oracle $30 billion a year by 2028. I'm sorry, I don't buy it! I don't buy that this company will continue growing forever, and its stinky conversion rate isn't going to change anytime soon. How? Literally…how! How? How! HOW??? Nobody ever answers this question! “Efficiencies”? If you’re going to say GPT-5 — here’s a scoop I have about how it’s less efficient! It's very, very, very common for people to conflate "AI" with "generative AI." Make sure that whatever you're claiming or being told is actually about Large Language Models, as there are all sorts of other kinds of machine learning that people love to bring up. LLMs have nothing to do with Folding@Home, autonomous cars, or most disease research. A lot of people think that they're going to tell me "I use this all the time!" and that'll change my mind. I cannot express enough how irrelevant it is that you have a use case, as every use case I hear is one of the following: This would all be fine and dandy if people weren't talking about this stuff as if it was changing society. None of these use cases come close to explaining why I should be impressed by generative AI. It also doesn't matter if you yourself have kind of a useful thing that AI did for you once. We are so past the point when any of that matters. AI is being sold as a transformational technology, and I am yet to see it transform anything. I am yet to hear one use case that truly impresses me, or even one thing that feels possible now that wasn't possible before. This isn't even me being a cynic — I'm ready to be impressed! I just haven't been in three fucking years and it's getting boring. Also, tell me with a straight face any of this shit is worth the infrastructure. One of the most braindead takes about AI and coding is that "vibe coding" is "allowing anyone to build software." While technically true, in that one can just type "build me a website" into one of many AI coding environments, this does not mean it is functional or useful software. Let's make this really clear: AI cannot "just handle coding." Read this excellent piece by Colton Voege, then read this piece by Nik Suresh. If you contact me about AI and coding without reading these I will send them to you and nothing else, or crush you like a car in a garbage dump, one or the other. Also, show me a vibe coded company. Not a company where someone who can code has quickly spun up some features, a fully-functional, secure, and useful app made entirely by someone who cannot code. You won't be able to find this as it isn't possible. Vibe Coding is a marketing term based on lies, peddled by people who have either a lack of knowledge or morals. Are AI coding environments making people faster? I don't think so! In fact, a recent study suggested they actually make software engineers slower. The reason that nobody is vibe coding an entire company is because software development is not just "put a bunch of code in a pile and hit "go," and oftentimes when you add something it breaks something else. This is all well and good if you actually understand code — it's another thing entirely when you are using Cursor or Claude Code like a kid at an arcade machine turning the wheel repeatedly and pretending they're playing the demo. Vibe coders are also awful for the already negative margins of most AI coding environments, as every single thing they ask the model to do is imprecise, burning tokens in pursuit of a goal they themselves don't understand. "Vibe coding" does not work, it will not work, and pretending otherwise is at best ignorance and at worst supporting a campaign built on lies. If you are an AI booster, please come up with better arguments. And if you truly believe in this stuff, you should have a firmer grasp on why you do so. It's been three years, and the best some of you have is "it's real popular!" or "Uber burned a lot of money!" Your arguments are based on what you wish were true rather than what's actually true, and it's deeply embarrassing. Then again, there are many well-intentioned people who aren't necessarily AI boosters who repeat these arguments, regardless of how thinly-framed they are, in part because we live in a high-information, low-processing society where people tend to put great faith in people who are confident in what they say and sound smart. I also think the media is failing on a very basic level to realize that their fear of missing out or seeming stupid is being used against them. If you don't understand something, it's likely because the person you're reading or hearing it from doesn't either. If a company makes a promise and you don't understand how they'd deliver on it, it's their job to explain how, and your job to suggest it isn't plausible in clear and defined language. This has gone beyond simple "objectivity" into the realm of an outright failure of journalism. I have never seen more misinformation about the capabilities of a product in my entire career, and it's largely peddled by reporters who either don't know or have no interest in knowing what's actually possible, in part because all of their peers are saying the same nonsense. As things begin to collapse — and they sure look like they're collapsing, but I am not making any wild claims about "the bubble bursting" quite yet — it will look increasingly more deranged to bluntly publish everything that these companies say. Never have I seen an act of outright contempt more egregious than Sam Altman saying that GPT-5 was actually bad, and that GPT-6 will be even better. Members of the media: Sam Altman does not respect you. He is not your friend. He is not secretly confiding in you. He thinks you are stupid and easily-manipulated, and will print anything he says, largely in part because many members of the media will print exactly what he says whenever he says it. To be clear, if you wrote about it and actively mocked it, that's fine. But let's close by discussing the very nature of AI skepticism, and the so-called "void" between those who "hate" AI and those who "love" AI, from the perspective of one of the more prominent people in the "skeptic" side. Critics and skeptics are not given the benefit of grace, patience, or, in many cases, hospitality when it comes to their position. While they may receive interviews and opportunities to "give their side," it is always framed as the work of a firebrand, an outlier, somebody with dangerous ideas that they must eternally justify. They are demonized, their points under constant scrutiny, their allegiances and intentions constantly interrogated for some sort of moral or intellectual weakness. "Skeptic" and "critic" are words said with a sneer or trepidation — that the listener should be suspicious that this person isn't agreeing that AI is the most powerful, special thing ever. To not immediately fall in love with something that everybody is talking about is to be framed as a "hater," to have oneself introduced with the words "not everybody agrees..." on 40% of appearances. By comparison, AI boosters are the first to get TV appearances and offers to be on panels, their coverage featured prominently on Techmeme, selling slop-like books called shit like The Future Of Intelligence: Masters Of The Brain featuring 18 interviews with different CEOs that all say the same thing. They do not have to justify their love — they simply have to remember all the right terms, chirping out "test-time compute" and "the cost of inference is going down" enough times to summon Wario Amodei to give them an hour-long interview where he says "the models, they are, in years, going to be the most powerful school teacher ever built." And yeah, I did sell a book, because my shit fucking rocks. I have consistent, deeply-sourced arguments that I've built on over the course of years. I didn't "become a hater" because I'm a "contrarian," I became a hater because the shit that these fucking oafs have done to the computer pisses me off. I wrote The Man Who Killed Google Search because I wanted to know why Google Search sucked. I wrote Sam Altman, Freed because at the time I didn't understand why everybody was so fucking enamoured with this damp sociopath. Everything I do comes from genuine curiosity and an overwhelming frustration with the state of technology. I started writing this newsletter with 300 subscribers and 60 views, and have written it as an exploration of subjects that grows as I write. I do not have it in me to pretend to be anything other than what I am, and if that is strange to you, well, I'm a strange man, but at least I'm an honest one. I do have a chip on my shoulder, in that I really do not like it when people try to make other people feel stupid, especially when they do so as a means of making money for themselves or somebody else. I write this stuff out because I have an intellectual interest, I like writing, and by writing, I am able to learn about and process my complex feelings about technology. I happen to do so in a manner that hundreds of thousands of people enjoy every month, and if you think that I've grown this by "being a hater," you are doing yourself the disservice of underestimating me, which I will use to my advantage by writing deeper, more meaningful, more insightful things than you. I have watched these pigs ruin the computer again and again, and make billions doing so, all while the media celebrates the destruction of things like Google, Facebook, and the fucking environment in pursuit of eternal growth. I cannot manufacture my disgust, nor can I manufacture whatever it is inside me that makes it impossible to keep quiet about the things I see. I don't know if I take this too seriously or not seriously enough, but I am honoured that I am able to do it, and have 72,000 of you subscribed to find out when I do so. Subscribe today. It's free. Please. Great! You’ve successfully signed up. Welcome back! You've successfully signed in. You've successfully subscribed to Ed Zitron's Where's Your Ed At. Your link has expired. Success! Check your email for magic link to sign-in. Success! Your billing info has been updated. Your billing was not updated.
--------------------------------------------------

Title: AI Revolution Will Crush the Blue States
URL: http://www.newgeography.com/content/008643-the-ai-revolution-will-crush-blue-states
Time Published: 2025-08-27T00:28:38Z
Full Content:
“The first step onto the corporate ladder is vanishing for many new graduates,” argued a recent Fortune report. As a result, CEOs are warning that entry-level jobs are on the brink of extinction, with internships and opportunities for college graduates drying up. Of course, not everyone will feel the impact in the same way. At the top of the pyramid are investors, entrepreneurs, and elite programmers who now command salaries on par with professional athletes. But artificial intelligence — fuelled by a huge infusion of Wall Street cash — threatens to eliminate many software jobs, along with high-end professional roles that involve routine analysis. Young people are acutely aware of this. In the class I teach with Marshall Koplansky at Chapman University, several students predicted that the jobs they currently hold will soon disappear. Among them were a game designer, two human-resources executives, and a manufacturing and warehouse manager. My engineering colleagues report a similar trend. While opportunities remain strong for mechanical and chemical engineers, as well as for those designing robotics, the outlook is far less certain for computer science students. Despite soaring profits at the largest tech companies, AI programming tools have enabled sweeping layoffs at firms such as Amazon, Intel, Meta and Microsoft. Today, among college graduates aged 22-27, computer science and computer engineering majors face some of the highest unemployment rates, according to a report from the Federal Reserve Bank of New York. In response, many firms are now focusing on building tangible products rather than merely shifting algorithms for commerce or generating more social media. In the aerospace and defence sectors, for example, AI is seen not as an end but a tool. It is a means to enhance human creativity and productivity rather than replace it. “Software is not in the greatest position with AI,” Delian Asparouhov, who runs a firm focused on in-space manufacturing, told me. “Now people are shifting to hard tech. Designing and building spaceships still needs people.” This could spell trouble for elite universities, but represents a major advantage for schools that teach the practical skills companies actually need. So far, these opportunities are largely concentrated in red and purple states in the Midwest and South — the regions most focused on reshoring manufacturing and other industries from overseas. Attitudes are shifting alongside these economic changes. One recent survey found that roughly 83% of Generation Z feel that learning a skilled trade can be a better pathway to economic security than college — including 90% of those already holding college degrees. Indeed, as college enrolment has dropped between 2020 and 2023, trade school enrolment grew by 10%. These changes suggest that as practical skills gain value, regions offering them are likely to attract both talent and jobs. Read the rest of this piece at: UnHerd. Joel Kotkin is the author of The Coming of Neo-Feudalism: A Warning to the Global Middle Class. He is the Roger Hobbs Presidential Fellow in Urban Futures at Chapman University and and directs the Center for Demographics and Policy there. He is Senior Research Fellow at the Civitas Institute at the University of Texas in Austin. Learn more at joelkotkin.com and follow him on Twitter @joelkotkin. Photo credit: Robot navigating a course during the Defense Advanced Research Projects Agency (DARPA) Robotics Challenge (DRC) June 5 in Pomona, California. Source: Office of Naval Research via Flickr under CC 2.0 License. NewGeography.com is a joint venture of Joel Kotkin and Praxis Strategy Group Joel Kotkin's newest book The Coming of Neo-Feudalism: A Warning to the Global Middle Class is now available to order. Learn more about this title and Joel's other books. Infinite Suburbia is the culmination of the MIT Norman B. Leventhal Center for Advanced Urbanism's yearlong study of the future of suburban development. Find out more. Authored by Aaron Renn, The Urban State of Mind: Meditations on the City is the first Urbanophile e-book, featuring provocative essays on the key issues facing our cities, including innovation, talent attraction and brain drain, global soft power, sustainability, economic development, and localism. Read Michael Lind's new book, Hell to Pay: How the Suppression of Wages is Destroying America. more
--------------------------------------------------

Title: Nearly half of workers feel they’re stagnating, SurveyMonkey data shows
URL: https://www.hrdive.com/news/workers-feel-theyre-stagnating/758655/
Time Published: 2025-08-26T16:30:00Z
Full Content:
Let HR Dive's free newsletter keep you informed, straight from your inbox. Between the need for skilled workers and employee demand for training, HR may want to prioritize L&D and agitate for a bigger budget next year. After several years of an employee-friendly economy, the pivot back to an employer’s market may feel like whiplash for many workers. Anxiety is up, and employees are fumbling for security in a time of widespread layoffs and shrinking opportunity. Benefits and salary budgets generally have stagnated, and employers have seemingly pared back learning and development as well. Respondents to HR Dive’s Identity of HR survey said training had dropped in importance at their organization, with only 5% saying it was a top priority this year, compared to 12% last year. As a result, recruiters have been looking for highly qualified candidates to fill certain roles, while some say overlooked talent can’t access the learning opportunities that might help them rise into those positions — creating a catch-22. AI skills have commanded much of the conversation around training, and reports have shown that few employers are investing in learning programs on the topic. A July report from BambooHR found roughly one-third of employers have offered such training, and a May report from Amazon Web Services showed many employers don’t even know what such training would look like. Between the need for skilled workers and employee demand for training, some HR professionals have shifted from a recruiting focus to one of retention and development; those eyeing a similar path may need to agitate for a bigger L&D budget next year. Concerned about the encroachment of AI on entry-level opportunities, Gen Z has shown it will particularly prioritize employers who can offer skill-building and continuous learning opportunities, with more than half of Gen Z workers telling Flexa as much. Get the free daily newsletter read by industry experts Major issues on the agenda include artificial intelligence in the workplace, skills-based hiring, the ever-evolving employment law landscape and more. Olipop In encouraging all staff to take a week off this summer, Olipop is sending the message “that our unlimited PTO is not just for show,” CEO Ben Goodwin said. Subscribe to HR Dive for top news, trends & analysis Get the free daily newsletter read by industry experts Major issues on the agenda include artificial intelligence in the workplace, skills-based hiring, the ever-evolving employment law landscape and more. Olipop In encouraging all staff to take a week off this summer, Olipop is sending the message “that our unlimited PTO is not just for show,” CEO Ben Goodwin said. The free newsletter covering the top industry headlines
--------------------------------------------------

Title: IROBOT ALERT: Bragar Eagel & Squire, P.C. Reminds Investors in IRobot (IRBT) of the Class Action Lawsuit and Encourages Investors to Inquire About Their Rights
URL: https://www.globenewswire.com/news-release/2025/08/26/3139285/0/en/IROBOT-ALERT-Bragar-Eagel-Squire-P-C-Reminds-Investors-in-IRobot-IRBT-of-the-Class-Action-Lawsuit-and-Encourages-Investors-to-Inquire-About-Their-Rights.html
Time Published: 2025-08-26T12:42:00Z
Full Content:
August 26, 2025 08:42 ET | Source: Bragar Eagel & Squire Bragar Eagel & Squire Bragar Eagel & Squire, P.C. Litigation Attorneys Encourage Investors Who Suffered Losses IRobot (IRBT) To Contact Him Directly To Discuss Their Options If you purchased or acquired securities in IRobot between January 29, 2024 and March 11, 2025 and would like to discuss your legal rights, call Bragar Eagel & Squire partner Brandon Walker or Marion Passmore directly at (212) 355-4648 NEW YORK, Aug. 26, 2025 (GLOBE NEWSWIRE) -- Bragar Eagel & Squire, P.C., a nationally recognized stockholder rights law firm, announces that a class action lawsuit has been filed against iRobot Corporation (“iRobot” or the “Company”) (NASDAQ:IRBT) in the United States District Court for the Southern District of New York on behalf of all persons and entities who purchased or otherwise acquired iRobot securities between January 29, 2024 and March 11, 2025, both dates inclusive (the “Class Period”). Investors have until September 5, 2025 to apply to the Court to be appointed as lead plaintiff in the lawsuit. Click here to participate in the action. The complaint alleges that, throughout the Class Period, Defendants made materially false and misleading statements regarding the Company's business, operations, and prospects. Specifically, Defendants made false and/or misleading statements and/or failed to disclose that: (i) iRobot overstated the extent to which the Restructuring Plan would help the Company maintain stability after the termination of the Amazon Acquisition; (ii) as a result, it was unlikely that iRobot would be able to profitably operate as a standalone company; (iii) accordingly, there was substantial doubt about the Company's ability to continue as a going concern; and (iv) as a result, Defendants' public statements were materially false and misleading at all relevant times. On March 12, 2025, iRobot issued a press release reporting its fourth quarter and full year 2024 financial results. For the quarter, iRobot reported a loss of $2.06 per share on revenue of $172 million, representing a 44% year-over-year decline. iRobot also cautioned investors that "there can be no assurance that [iRobot's] new product launches will be successful due to potential factors, including, but not limited to consumer demand, competition, macroeconomic conditions, and tariff policies." Accordingly, "given these uncertainties and the implication they may have on the Company's financials, there is substantial doubt about the Company's ability to continue as a going concern for a period of at least 12 months from the date of the issuance of its consolidated 2024 financial statements." In addition, the press release stated that, in light of the foregoing developments, iRobot was cancelling its fourth-quarter and full-year 2024 results conference call and webcast, and that the Company would not be providing a 2025 outlook. Market analysts were quick to comment on iRobot's announcement. For example, on March 12, 2025, an analyst from Seeking Alpha downgraded iRobot to a sell rating from a hold rating "due to [a] bleak outlook," stating that "iRobot's business prospects have deteriorated significantly since the Amazon acquisition fell through, leading to massive layoffs and growing losses," "Q4 earnings were disastrous, missing guidance and showing worsening gross margins due to excess inventory and lower sales volumes," "iRobot's future is uncertain, with substantial doubts about its viability within the next 12 months, despite ongoing discussions with its primary lender," and that the Company's "survival hinges on new Roombas being a hit, which seems unlikely." That same day, in an article entitled "Why iRobot Stock Is Crashing Today," The Motley Fool stated, in relevant part, that "iRobot's costly restructuring efforts -- including a 50% workforce reduction -- have yet to yield stability." On this news, iRobot's stock price fell $3.255 per share, or 51.58%, over the following two trading sessions, to close at $3.055 per share on March 13, 2025. If you purchased or otherwise acquired iRobot shares and suffered a loss, are a long-term stockholder, have information, would like to learn more about these claims, or have any questions concerning this announcement or your rights or interests with respect to these matters, please contact Brandon Walker or Marion Passmore by email at investigations@bespc.com, telephone at (212) 355-4648, or by filling out this contact form. There is no cost or obligation to you. About Bragar Eagel & Squire, P.C.: Bragar Eagel & Squire, P.C. is a nationally recognized law firm with offices in New York, California, and South Carolina. The firm represents individual and institutional investors in commercial, securities, derivative, and other complex litigation in state and federal courts across the country. For more information about the firm, please visit www.bespc.com. Attorney advertising. Prior results do not guarantee similar outcomes. Follow us for updates on LinkedIn, X, and Facebook, and keep up with other news by following Brandon Walker, Esq. on LinkedIn and X. Contact Information: Bragar Eagel & Squire, P.C.Brandon Walker, Esq. Marion Passmore, Esq.(212) 355-4648investigations@bespc.comwww.bespc.com The suit claims Hims misled investors by overstating its partnership with Novo Nordisk and access to Wegovy, causing stock losses when the truth emerged. On August 8, 2025, Avita shares fell 21% to $4.25 after revealing unpaid Recell claims caused a demand drop due to CMS pricing and adjudication delays.
--------------------------------------------------

Title: 350,000 Black Women Were Ousted From The Workforce—For Millennials, Where Are They Going Next?
URL: https://www.forbes.com/sites/jasminebrowley/2025/08/25/350000-black-women-were-ousted-from-the-workforce-for-millennials-where-are-they-going-next/
Time Published: 2025-08-25T22:42:24Z
Full Content:
ByJasmine Browley, Contributor. As I’ve stated time and time again, millennials know how to hustle. From the time we reached our formative years, our identities were melded into our professions. So imagine what happens when our merit is systemically attacked at a government level. When the most recent jobs report revealed that 350,000 Black women had left the labor force, I resisted the instinct to call those exits voluntary. “I personally term that as being ousted,” I told the experts I interviewed, “because a lot of those exits were not optional.” Behind that staggering number are women who are losing not only paychecks but also the stability, health access, and professional pathways that keep families and communities afloat. What happens next for them? Some are rebuilding through entrepreneurship, others are demanding systemic reforms, and all are navigating the intersection of workforce inequity and personal resilience. “Job loss directly translates into reduced income and potentially a loss of health insurance,” explained Fade Adetosoye, who helped lead a recent McKinsey study on Black maternal health and workforce participation. “That makes it harder for women to afford prenatal and postpartum care, medications, and services for conditions like gestational diabetes or hypertension—things that disproportionately impact Black women.” The ripple effects are profound. McKinsey’s research estimates that addressing the Black maternal health gap could add $25 billion to the U.S. economy annually and save $385 billion in healthcare costs. Without intervention, Black mothers lose an average of 14–15 healthy days per year for the rest of their lives due to childbirth-related conditions. “We’re talking 350,000 healthy years lost that women could be spending participating in the economy, investing in their families and communities,” Adetosoye told me. Her conclusion is stark: “The gold standard for birthing people should not be survival. It should be thriving. And there is absolutely a tie between Black maternal health disparities and economic mobility.” For Michelda Castro, the ousting wasn’t theoretical. She had climbed through finance and government program management roles, even overseeing a $69 million loan fund for underserved communities. But repeated experiences with bias, systemic barriers, and finally, layoffs tied to shifting federal priorities, left her searching for something more sustainable. “I was stressed, having panic attacks at work, working 60, 70 hours a week,” she recounted to me. “They told me I was doing a great job, but when the director role opened up—the one I’d been running for nine months—they didn’t even give me an interview. Then they asked me to train the white man they hired instead.” After another corporate stint ended in layoffs due to federal grant cuts handed down by the Trump administration, Castro leaned fully into entrepreneurship. More than a decade ago, she founded Versatile Image, a nonprofit that hosts the Unity Block Party in Utah, an annual cultural and economic festival that now generates over $100,000 in revenue. With the layoff, she focused renewed energy and focus into making the once side hustle into her full-time career. Still, she says the uphill climb is relentless: federal grants have been slashed and are continuing to be so, corporate sponsors have pulled out due to DEI alignment concerns, and banks remain reluctant to extend capital. “If I were a white man, the struggle would be glamorized—like starting Amazon in a garage,” she said. “But when it’s a Black woman, people call it complaining.” The broader context, says Jocelyn Frye, president of the National Partnership for Women & Families and co-leader of the 75 Million Campaign, is that today’s job losses are less about performance and more about politics. “When you lose a job not because of your work but because of a political agenda, it can feel demoralizing,” Frye told me. “And for Black women, particularly Black mothers—more than 80% of whom are the primary breadwinners—it threatens the stability of entire families.” She points to the rollback of diversity, equity, and inclusion (DEI) initiatives as both a cause and a consequence. “This administration has deployed a different narrative, but the purpose is the same as always: to erode the pathways that allowed Black women even a chance at the middle class,” Frye told me. Her call to action: don’t internalize the systemic failures. “This is not about individual deficiencies. It’s an ideological agenda. And we must push back—not only to protect jobs, but to protect the progress that got us here in the first place.” Despite the structural barriers, Black women continue to lead in entrepreneurship. According to American Express data, Black women are the fastest-growing demographic of entrepreneurs in the U.S., a trend likely to accelerate as traditional pathways constrict. “I don’t want to just survive pregnancy, or layoffs, or discrimination,” Adetosoye told me. “I want to thrive. And thriving means being able to build wealth, plan estates, and contribute fully to the economy.” Castro echoed the same sentiment, this time through the lens of her community. “When I couldn’t find joy in corporate America, I built it myself. That’s what the Unity Block Party is—it’s our refusal to be erased.” This year, she was sure to lean in on the joy element by tapping headliner Durand Bernard, a popular singer that often weaves messages of self-care in his songs. “I am so excited‚” Castro told me. “We need this happiness more than ever.” The exodus of 350,000 Black women from the labor force is more than a statistic—it’s a referendum on whether the U.S. economy values their contributions. For Adetosoye, Castro, and Frye, the answer is to build new systems: of care, of capital, of narrative. “Black women, especially younger generations, like millennials, are resilient,” Frye said. “But resilience should not be the requirement for survival in this economy. We deserve systems that work for us, not against us.”
--------------------------------------------------

Title: Tradwives and ‘The People That People Come Out Of’
URL: https://msmagazine.com/2025/08/25/tradwife-women-vote-pete-hegseth-christian-nationalist/
Time Published: 2025-08-25T21:56:00Z
Description: For the first time in years, the number of U.S. mothers with young children in the workforce is shrinking—over 212,000 women left between January and June 2025 alone. 
Childcare costs, in-office pressures, and a cultural nudge toward traditional gender roles …
--------------------------------------------------

Title: Pomerantz Law Firm Announces the Filing of a Class Action Against iRobot Corporation and Certain Officers – IRBT
URL: https://www.globenewswire.com/news-release/2025/08/25/3138595/1087/en/Pomerantz-Law-Firm-Announces-the-Filing-of-a-Class-Action-Against-iRobot-Corporation-and-Certain-Officers-IRBT.html
Time Published: 2025-08-25T14:14:00Z
Full Content:
August 25, 2025 10:14 ET | Source: Pomerantz LLP Pomerantz LLP NEW YORK, Aug. 25, 2025 (GLOBE NEWSWIRE) -- Pomerantz LLP announces that a class action lawsuit has been filed against iRobot Corporation (“iRobot” or the “Company”) (NASDAQ: IRBT) and certain officers. The class action, filed in the United States District Court for the Southern District of New York, and docketed under 25-cv-05563, is on behalf of a class consisting of all persons and entities other than Defendants that purchased or otherwise acquired iRobot securities between January 29, 2024 and March 11, 2025, both dates inclusive (the “Class Period”), seeking to recover damages caused by Defendants’ violations of the federal securities laws and to pursue remedies under Sections 10(b) and 20(a) of the Securities Exchange Act of 1934 and Rule 10b-5 promulgated thereunder, against the Company and certain of its top officials. If you are an investor who purchased or otherwise acquired iRobot securities during the Class Period, you have until September 5, 2025, to ask the Court to appoint you as Lead Plaintiff for the class. A copy of the Complaint can be obtained at www.pomerantzlaw.com. To discuss this action, contact Danielle Peyton at newaction@pomlaw.com or 646-581-9980 (or 888.4-POMLAW), toll-free, Ext. 7980. Those who inquire by e-mail are encouraged to include their mailing address, telephone number, and the number of shares purchased. [Click here for information about joining the class action] iRobot designs, builds, and sells robots and home innovation products in the United States, Europe, the Middle East, Africa, Japan, and internationally. The Company’s portfolio of home robots and smart home devices features proprietary technologies for the connected home and advanced concepts in cleaning, mapping and navigation. iRobot is primarily known for its robot vacuum cleaner (“RVC”) products sold under the “Roomba” brand name. While iRobot’s Roomba was the first commercially successful RVC, iRobot’s business has steadily declined over the past decade, apart from a brief sales bump during the COVID-19 pandemic. Competitors from China undercut the luxury-priced Roomba, while other consumer electronics firms like Samsung and SharkNinja introduced their own RVCs. By 2016, iRobot’s market share had dropped to 64% in 2016, and then to only 46% by 2020. Investors believed that iRobot was saved when, in August 2022, iRobot entered into a merger agreement with Amazon.com, Inc. (“Amazon”) pursuant to which Amazon would acquire iRobot for $61 per share in an all-cash transaction valued at approximately $1.7 billion (the “Amazon Acquisition”). In a joint press release announcing the Amazon Acquisition, iRobot’s then Chief Executive Officer (“CEO”) Colin Angle was quoted as stating, in relevant part, “Amazon shares our passion for building thoughtful innovations that empower people to do more at home, and I cannot think of a better place for our team to continue our mission. I’m hugely excited to be a part of Amazon and to see what we can build together for customers in the years ahead.” However, in January 2024, Amazon and iRobot announced that they had mutually agreed to terminate the Amazon Acquisition, citing regulatory concerns. Specifically, the companies stated that there was “no path to regulatory approval in the European Union” and reports circulated that the U.S. Federal Trade Commission was in the process of drafting a lawsuit that would seek to stop the deal. Concurrent with this announcement, iRobot also reported that Colin Angle would step down from his role as CEO, and that the Company would be cutting approximately 350 employees, representing 31% of iRobot’s workforce. Notwithstanding the termination of the Amazon Acquisition and the subsequent job cuts, the Company has consistently maintained that it is “confident in [its] ability to build on [its] legacy of innovation as a standalone company and to navigate this period successfully.” Moreover, in the wake of the Amazon Acquisition’s termination, iRobot touted that it would be implementing an operational restructuring plan (the “Restructuring Plan”)—which the Company has sometimes referred to as “iRobot Elevate”—“designed to position the Company for stabilization in the current environment, while focusing on profitability and advancing key growth initiatives to extend its market share in the mid-tier and premium segments.” The Restructuring Plan, according to the Company, would “enable [it] to chart a new strategic path for sustainable value creation.” The complaint alleges that, throughout the Class Period, Defendants made materially false and misleading statements regarding the Company’s business, operations, and prospects. Specifically, Defendants made false and/or misleading statements and/or failed to disclose that: (i) iRobot overstated the extent to which the Restructuring Plan would help the Company maintain stability after the termination of the Amazon Acquisition; (ii) as a result, it was unlikely that iRobot would be able to profitably operate as a standalone company; (iii) accordingly, there was substantial doubt about the Company’s ability to continue as a going concern; and (iv) as a result, Defendants’ public statements were materially false and misleading at all relevant times. On March 12, 2025, iRobot issued a press release reporting its fourth quarter and full year 2024 financial results. For the quarter, iRobot reported a loss of $2.06 per share on revenue of $172 million, representing a 44% year-over-year decline. iRobot also cautioned investors that “there can be no assurance that [iRobot’s] new product launches will be successful due to potential factors, including, but not limited to consumer demand, competition, macroeconomic conditions, and tariff policies.” Accordingly, “[g]iven these uncertainties and the implication they may have on the Company’s financials, there is substantial doubt about the Company’s ability to continue as a going concern for a period of at least 12 months from the date of the issuance of its consolidated 2024 financial statements.” In addition, the press release stated that, in light of the foregoing developments, iRobot was cancelling its fourth-quarter and full-year 2024 results conference call and webcast, and that the Company would not be providing a 2025 outlook. Market analysts were quick to comment on iRobot’s announcement. For example, on March 12, 2025, an analyst from Seeking Alpha downgraded iRobot to a sell rating from a hold rating “due to [a] bleak outlook,” stating that “iRobot’s business prospects have deteriorated significantly since the Amazon acquisition fell through, leading to massive layoffs and growing losses,” “Q4 earnings were disastrous, missing guidance and showing worsening gross margins due to excess inventory and lower sales volumes,” “iRobot’s future is uncertain, with substantial doubts about its viability within the next 12 months, despite ongoing discussions with its primary lender,” and that the Company’s “survival hinges on new Roombas being a hit, which seems unlikely.” That same day, in an article entitled “Why iRobot Stock Is Crashing Today,” The Motley Fool stated, in relevant part, that “iRobot’s costly restructuring efforts -- including a 50% workforce reduction -- have yet to yield stability.” On this news, iRobot’s stock price fell $3.255 per share, or 51.58%, over the following two trading sessions, to close at $3.055 per share on March 13, 2025. After the end of the Class Period, in May 2025, iRobot experienced a short squeeze—i.e., a rapid increase in the price of a stock owing primarily to an excess of short selling of a stock rather than underlying fundamentals—after it was announced that U.S. tariffs on European Union imports would be delayed until July 2025. However, notwithstanding the increase in the Company’s stock price, market analysts noted that iRobot’s underlying fundamentals remained highly concerning. For example, on May 29, 2025, Seeking Alpha stated that iRobot’s “[t]echnical indicators have turned bullish short-term, but the company’s cash burn and deteriorating financials outweigh these positives” and “Q1 [2025] results revealed falling revenue, worsening losses, shrinking cash reserves, and declining gross margins, signaling severe operational stress.” Pomerantz LLP, with offices in New York, Chicago, Los Angeles, London, Paris, and Tel Aviv, is acknowledged as one of the premier firms in the areas of corporate, securities, and antitrust class litigation. Founded by the late Abraham L. Pomerantz, known as the dean of the class action bar, Pomerantz pioneered the field of securities class actions. Today, more than 85 years later, Pomerantz continues in the tradition he established, fighting for the rights of the victims of securities fraud, breaches of fiduciary duty, and corporate misconduct. The Firm has recovered billions of dollars in damages awards on behalf of class members. See www.pomlaw.com. Attorney advertising. Prior results do not guarantee similar outcomes. CONTACT:Danielle PeytonPomerantz LLPdpeyton@pomlaw.com646-581-9980 ext. 7980 NEW YORK, Aug. 25, 2025 (GLOBE NEWSWIRE) -- Pomerantz LLP is investigating claims on behalf of investors of Ultragenyx Pharmaceutical Inc. (“Ultragenyx” or the “Company”) (NASDAQ: RARE). Such... NEW YORK, Aug. 25, 2025 (GLOBE NEWSWIRE) -- Pomerantz LLP is investigating claims on behalf of investors of Mereo BioPharma Group plc (“Mereo” or the “Company”) (NASDAQ: MREO). Such investors are...
--------------------------------------------------

Title: Business Technology News: What’s In Microsoft Copilot’s GPT-5 Upgrade?
URL: https://www.forbes.com/sites/quickerbettertech/2025/08/24/business-technology-news-whats-in-microsoft-copilots-gpt-5-upgrade/
Time Published: 2025-08-24T11:00:00Z
Full Content:
ByGene Marks, Contributor. Here are five things in tech that happened this week and how they affect your business. Did you miss them? As of August 7, GPT-5 – OpenAI’s most advanced AI model – is now integrated into Microsoft 365 Copilot and Copilot Studio worldwide. Microsoft posted the announcement detailing the following capabilities: -Copilot now uses GPT-5’s real-time router to select the best model for each prompt –either a fast-response engine for simple tasks or a deep-reasoning engine for complex ones. -Its human-like capability mirrors how people think producing incisive feedback when tasks require it. -Copilot can review a project, provide a summary of what worked and what didn’t, and draft a “lessons learned” document. -Copilot can handle large documents up to 100K tokens. Microsoft said this update is available for users who have a Microsoft 365 Copilot license – availability to all users will roll out in the coming weeks. (Source: Microsoft) OpenAI has absorbed a fair amount of abuse for its rollout of GPT-5 in the past few weeks thanks to bugs and hallucinations. But the company has taken steps to resolve these issues, and like any new version of a software product, I’m confident the new platform will be more reliable in the coming months. Microsoft is already doubling down by incorporating into its Copilot offerings. If your company is using Copilot – be it for productivity or internal development – you’ll pretty much have no choice but to use the GPT-5 platform going forward. AI is reshaping the workforce often at the expense of employees according to CNBC’s report that compiled responses from management at companies like Amazon, PayPal and Microsoft. AI is being used to streamline operations, boost productivity, and reduce labor costs. AI tools are replacing roles in customer service, HR, finance, and software development, especially entry-level and repetitive jobs. PayPal’s AI assistant – for example – is cutting down on customer service calls and live interactions. Rising unemployment numbers in the tech sector have been voiced by young workers whose skills overlap with AI capabilities. Executives often frame layoffs as “restructuring” or “optimization,” though experts say these are often euphemisms for AI-driven cuts. AI is quietly becoming a powerful force behind workforce reductions, even as companies tout its benefits for efficiency and innovation. (Source: CNBC) Ever see an employee typing pool? Telephone switchboard operators? Gas station attendants? Blacksmiths? Technology has always replaced people. This is nothing new. In 2025 big companies are doing this, mainly by employing AI in customer service and software development. Ultimately this technology will make its way into operations, sales, marketing, accounting and right down on to the shop floor. Small businesses will follow. Labor will be disrupted. The smartest employees will lean into this stuff and use it well. Others will find themselves out of a job. I’m sure there will be other opportunities, just like there’s always been. Angus Loten of the Wall Street Journal reported on the surge in CEO impersonation scams fueled by AI. Last year alone, over 105,000 deepfake attacks were reported with losses exceeding $200 million. Cybercriminals are using AI-generated deepfakes – hyper-realistic video and audio – to impersonate CEOs and other executives. These scams typically begin with a phone call or video meeting from a fake executive making urgent requests for money transfers, sensitive data, or access credentials. The deepfakes are shockingly convincing, mimicking real voices, facial expressions, and body language in real time. Scammers train AI models using publicly available content – interviews, webinars, earnings calls – to replicate executives’ speech and appearance. The targets of these sophisticated scams are often lower-level employees with privileged access. (Source: Wall Street Journal) Another day, another security threat…this time from AI. Deepfakes are getting better at fooling people and will continue to be a rising threat. But there are some old school ways to defend against this. For example, always call the person back. Require multiple sign-offs on any cash transaction. Require additional documentation, even if it’s from the boss. Get training and have some common sense. DocuSign is evolving far beyond e-signatures with its new AI-powered Intelligent Agreement Management (IAM) platform, unveiled in August 2025. With IAM, DocuSign now helps users manage the entire agreement lifecycle – from request intake to execution – using AI and automation. The AI can flag risky or non-compliant terms based on company playbooks; suggest edits to align with internal policies; and centralize templates and audit trails for consistency and visibility. DocuSign Iris is an AI engine that selects the best model for tasks like contract review, compliance checks, and identity verification. This shift turns contracts from static documents into dynamic, searchable data assets, helping businesses reduce errors, accelerate deal cycles, and improve compliance. (Source: Computer Weekly) As a user of DocuSign, I need to take the advice that I often give to others: lean into your software platforms and get training. We’re still using this platform like we did five years ago. And yet, here are a bunch of ways that we could be doing things better if we only spent the time to learn them. I’m in for some more training. PCWorld’s August 2025 guide offers a treasure trove of tech gear to supercharge your work-from-home setup – 42 products in total, all tested and recommended by their editorial team. Their top categories and standout picks: -Webcams: The Anker PowerConf C200 (2K resolution) helps you look sharp on Zoom without breaking the bank. -Laptop stands: Lamicall’s adjustable aluminum stand improves ergonomics and cooling, while Ugreen’s X-Fit doubles as a USB hub. -Audio gear: USB microphones and noise-canceling headphones are essential for clear communication and focus. -Lighting & ambiance: Ring lights, smart bulbs, and desk lamps enhance visibility and mood. -Furniture upgrades: Sit/stand desks and ergonomic chairs support posture and comfort during long hours. (Source: PCWorld) Great article and great recommendations for any home worker or home-based business. Each week I round up five business technology news stories and explain why they're important for your business. If you have any interesting stories, please post to my X account @genemarks
--------------------------------------------------

Title: Crunchyroll Cuts Staff While Betting Big on India, Brazil & More
URL: https://news.sankakucomplex.com/2025/08/24/crunchyroll-cuts-staff-while-betting-big-on-india-brazil-more/
Time Published: 2025-08-23T22:00:13Z
Description: Crunchyroll is laying off an unspecified number of employees—not because it’s broke, but because it’s refocusing on “booming anime markets” like India, Mexico, Brazil, Southeast Asia, and Europe. Crunchyroll has begun a global restructuring that includes swee…
--------------------------------------------------

Title: 'The dumbest thing I've ever heard': Amazon Web Services CEO lambasts replacing junior employees with AI, but he still loves AI
URL: https://www.pcgamer.com/software/ai/the-dumbest-thing-ive-ever-heard-amazon-web-services-ceo-lambasts-replacing-junior-employees-with-ai-but-he-still-loves-ai/
Time Published: 2025-08-23T17:40:16Z
Full Content:
AWS CEO Matt Garman spoke up for entry-level jobs, but still insists AI will change all jobs. When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works. In an interview with investor and AI evangelist Matthew Berman this week, AWS CEO Matt Garman shut down the idea that junior employees should be replaced by AI, calling it "the dumbest thing I've ever heard." "They're probably the least expensive employees you have, they're the most leaned in to your AI tools, and how's that going to work when you go, like, 10 years in the future and you have no one that has built up or learned anything? "My view is like, you absolutely want to keep hiring kids out of college and teaching them the right ways to go build software and decompose problems and think about it." But overall, Garman still has the optimistic take on AI's impact on the job market you commonly see from tech executives. "I think AI has the potential to transform every single industry, every single company, and every single job," Garman said. "But it doesn't mean they go away. It has transformed them, not replaced them." He added that, "I'm not minimizing that uncertainty that people will have and I know people are worried about [AI] and I think one of the things is, embrace that technology. The more you can embrace that technology, be flexible, understand how it can help you do your job faster and better, [then] I think the better off people are going to be as they make that transition." It's somewhat refreshing to hear a tech CEO defend entry-level jobs, especially with some estimates showing some 50% of all entry-level office jobs could be eliminated by AI in the coming years. Unfortunately, these comments come just a month after hundreds of Amazon Web Services employees were laid off. Amazon explained the layoffs in a statement to Reuters, commenting: "These decisions are necessary as we continue to invest, hire, and optimize resources to deliver innovation for our customers." While that statement doesn't mention AI, Amazon CEO Andy Jassy made comments back in June that seemed to hint that AI will be responsible for job elimination. Jassy said in an employee memo, "We will need fewer people doing some of the jobs that are being done today, and more people doing other types of jobs. It’s hard to know exactly where this nets out over time, but in the next few years, we expect that this will reduce our total corporate workforce as we get efficiency gains from using AI extensively across the company." Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. Stevie Bonifield is a freelance tech journalist specializing in mobile tech, gaming gear, and accessories. Outside of writing, Stevie loves indie games, TTRPGs, and building way too many custom keyboards. You must confirm your public display name before commenting Please logout and then login again, you will then be prompted to enter your display name. PC Gamer is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site. © Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York, NY 10036. Please login or signup to comment Please wait...
--------------------------------------------------

Title: A Microsoft veteran says declining morale and a 'culture shift' drove him to resign: 'The great flattening was definitely happening'
URL: https://www.businessinsider.com/microsoft-layoffs-culture-change-employee-resigned-found-new-job-2025-8
Time Published: 2025-08-23T07:13:01Z
Full Content:
Phil Coachman loved working at Microsoft for most of his nearly decadelong tenure. But as the culture began to shift and layoffs piled up, he decided it was time to move on. He started looking for a new role in July 2024 when he was still at Microsoft, but struggled to get much traction. In January, he resigned from his role as a senior cloud solution architect to focus on his job search. "I just didn't feel happy there anymore," said the 44-year-old, who lives in Pennsylvania. "I wanted to just continue making cool stuff and not have this constant fear of losing my job every week." Coachman said company layoffs — including the elimination of about 10,000 roles in 2023 — took a toll on his morale and that of his co-workers. He said he knew several people who were let go, including one teammate he regarded as a "top performer." "Every team that I worked with was just down," he said. "So now you go to work and everybody's depressed every day." Coachman is among the current and former Microsoft employees who have been affected by workforce reductions — either by losing a job or being left to adjust to coworker departures. After cutting about 6,000 jobs in May, the company laid off roughly 9,000 more in July. A Microsoft spokesperson previously told Business Insider that the company was focused on reducing management layers and streamlining processes. The cuts have also included many individual contributor roles. Microsoft isn't alone. Google, Intel, Amazon, and Walmart are among the companies that have also announced plans to reduce the number of managers in a trend dubbed the "Great Flattening." Layoffs remain low by historical standards, but tech workers have been hit hard — just as white-collar hiring has slowed. That's made it more difficult for workers like Coachman to switch jobs — or find new ones after they resign or are laid off. "It just got to a point where morale was no longer up to par," Coachman said, "and ultimately it got to a place where it was time to make a change." Business Insider has heard from dozens of tech workers about how corporate strategy shifts, layoffs, and hiring slowdowns have affected their careers. If you have a story to share, contact this reporter via email at jzinkula@businessinsider.com or Signal at jzinkula.29. Use a personal email address, a nonwork WiFi network, and a nonwork device; here’s our guide to sharing information securely. Read more on the topic: After working as a Microsoft contractor for several years, Coachman joined the company full-time in 2015. He said he's extremely grateful for his time at the company, and that before joining, he was earning far less and just trying to get by. He'd be open to returning in the future, he said, if he saw signs of a positive culture shift. During his final years at Microsoft, Coachman said there appeared to be a push to reduce the number of managers in an effort to increase what the company calls "span of control" — or the number of employees who report to each manager. While his own manager's number of direct reports didn't change during his time on the team, he said they are now working in an individual contributor role. He also said he saw the number of reports per manager increase on other teams. "I saw managers leave and other managers absorb that head count, so you go from managing 10 people to now maybe 18 people, which when you talk to those managers, becomes really hard," he said. "The great flattening was definitely happening." Beyond layoffs and shifts in management structure, Coachman said he also began being asked to focus more on performance metrics, which he felt came at the cost of flexibility and meaningful customer work. In recent years, Big Tech firms, including Microsoft, Google, Meta, and Amazon, have revamped their performance review and compensation structures to better reward top performers and weed out underperformers in pursuit of smaller, higher-performing teams. Another key factor in his decision to resign was the uptick in business travel. He said he was on the road about three times a month — a return to pre-pandemic norms. "It just got to a point where I was missing too much of my kids' lives," he said, adding that his work frustrations made travel less tolerable. "It was different when I was traveling and the job was awesome." When Coachman started his job search in July 2024, he thought it was going to be fairly easy. There seemed to be plenty of job postings on platforms like LinkedIn and Indeed, so after pinpointing a few roles he felt qualified for, he figured he'd be able to get interviews and eventually land an offer. But he quickly realized it wouldn't be that simple. After all, US businesses were hiring at nearly the slowest pace in more than a decade. "It was six months of basically just being ghosted," he said. "It was a completely different world than when I had applied to a job decades ago." Coachman said he went through several rounds of résumé tweaks — thinking the format might be the issue — but still got little response. As his job search dragged on, Coachman said he was hesitant to resign from his Microsoft job before having another role lined up because it would mean giving up a steady paycheck and unvested company stock. However, he said the "rainy day fund" he'd built over the years helped him feel more comfortable. "I had enough savings that even if it would take me a year to find a job, I would be fine," he said. "So it was just getting the courage to take that jump." In early 2024, Coachman said he hired a life coach — in part to help him navigate the changes he was experiencing at work. He said the life coach helped him get confident in his decision to resign, adding that hiring them was maybe the "best investment" he'd ever made. After resigning, Coachman said he adjusted his job search strategy. Rather than simply applying for jobs, he spent more time tapping into the network he'd built over the years. He targeted roles at companies where he knew someone, then reached out to ask whether they thought he'd be a good fit — and if they'd be willing to refer him. In one instance, he saw a few open roles at the data analytics and AI startup Databricks, where a former Microsoft colleague of his worked. After reaching out, Coachman said they recommended he focus on one specific role that they'd be willing to refer him for. The next day, Coachman received a call from a company recruiter. After the interview process, he received an offer and started working for the company in April. "Finding my next gig was 100% through my network," he said. Coachman said his pay is comparable to what he earned at Microsoft, and that the signing bonus helped make up for what he left behind in unvested stock. He said his travel is also now limited to no more than once a month, which was a big factor in his decision to accept the offer. Over the course of his roughly nine-month job search, Coachman said he applied to hundreds of jobs and received two offers — one for his current role and another from a startup he turned down due to concerns about job security. His top advice for job seekers: build your network and lean on it. Rather than just connecting on LinkedIn, he said, it's better to have real conversations that help foster relationships. He said this could boost your chances of landing a referral down the road — giving your application the "personal touch" it might need to get past applicant tracking system scanners. "It's real connections with people that I think make all the difference," he said. Jump to
--------------------------------------------------