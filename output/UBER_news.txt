List of news related to Uber stock price UBER:

Title: Year-End Rally Begins
URL: https://realinvestmentadvice.com/resources/blog/year-end-rally-begins/
Time Published: 2025-11-29T09:46:21Z
Description:  At a Glance


<ul><!-- wp:list-item -->

<li>Rally Into Year-End As Dip Buyers Emerge?</li>




<li>Year-End Rally Begins</li>




<li>Portfolio Tactics For Next Week</li>




<li>From Lance's Desk: Market Bubbles: A Rational Guide To An Irrational …
--------------------------------------------------

Title: Mass. health insurance premium hikes worsen affordability crisis
URL: https://www.bostonglobe.com/2025/11/28/business/health-insurance-premium-massachusetts/
Time Published: 2025-11-28T10:55:28Z
Full Content:
Each time one of Tamara Modig’s children needs a telehealth appointment, she boots up a fragile 13-year-old laptop and prays it’ll hold a charge. Then Modig is forced to fret with another concern: the cost of health care. The Fitchburg Public Schools employee and her husband pay more than $9,300 annually in health insurance premiums for their family of five. It’s already a substantial chunk of their combined $160,000 income. Modig knows it’s almost certainly going to rise next year, though she won’t learn how much until spring. Already, her out-of-pocket maximum this past year increased by $1,000, to $4,000. The family doesn’t take vacations, drives only used cars, and lives on a strict budget — which is about to get even stricter. “You wonder if you can make that pair of shoes last longer,” Modig said. “Does last year’s coat still fit the kids?” FEATURED VIDEO The rising price of health insurance has become top of mind in many households across the state as families like the Modigs take stock during traditional open enrollment periods. Many employers’ premiums have risen by 10 percent or more, insurers and brokers say, straining families’ budgets at a time when overall health care costs are on the rise and expected to grow even more in the coming years. “This is the hardest health insurance market I’ve seen in two decades, with no end in sight,” said David Shore, executive vice president at Borislow Insurance, which helps companies across the East Coast, including in Massachusetts, navigate insurance and funding options. “It’s only going to get worse.” The costs aren’t just growing — they are exacerbating an affordability crisis for state residents and businesses, experts say. The Massachusetts Taxpayers Foundation, a business-backed policy group, recently cited health care costs as a top concern, with Massachusetts having among the highest employer health care costs in the country. Advertisement Rising premiums, plus out-of-pocket health care costs, are all outpacing increases in household income, inflation, and other indicators of economic growth. As a result, Massachusetts is increasingly unaffordable to live in, said David Seltz, executive director of the state Health Policy Commission, during a recent hearing. Massachusetts had the highest insurance premiums in the US last year. And the years of consistent, rising premium costs in the state have long surpassed national trends. Families in Massachusetts spent, on average, $28,151 for employer-based health insurance last year, according to data presented by the Health Policy Commission, a state agency. Nationally, that figure was $24,540. And that’s just premiums. Consumers face other health care costs, including co-pays and deductibles. All told, the average annual cost of health care coverage for a family in Massachusetts was $32,469 in 2024. Since 2024, insurance costs have grown. Family premiums for employer-sponsored health insurance grew 6 percent nationally from 2024 to 2025, according to a Kaiser Family Foundation survey of employers. Because official data sources lag, it’s difficult to assess private insurance prices planned for 2026 in real time. But regardless of how a person gets commercial insurance — whether purchasing for themselves on the state’s insurance marketplace, going through a broker, getting it directly from an insurer, or receiving insurance through an employer — costs are seemingly on the rise for everyone next year. The state’s insurance exchange, where people and small businesses can buy their own insurance, has reported its premiums will increase by 11.5 percent, on average, in 2026. Those premium increases range from 7.2 percent by Fallon Health to 13.6 percent by Boston Medical Center Health Plan, which also goes by WellSense Health Plan. Advertisement These hikes and others mean small businesses are maxed out on health care costs, said Jon Hurst, president of the Retailers Association of Massachusetts. “We’re at a breaking point,” Hurst said. Beyond pending retirements, profitability is flat and health care and energy costs have skyrocketed, meaning about half of small businesses in Massachusetts predicted they would have to sell or close within five years, a UMass Donahue Institute study showed in February. For businesses, premium increases can depend on a number of factors, including an employer’s size, health care usage trends, and how the employer purchases insurance. Less than half of Massachusetts employers are considered “fully insured,” meaning they buy insurance in which the insurer takes on the financial responsibility for paying out medical claims. Most of the employers in this group who have 50 to 100 employees are seeing whopping premium increases that range from 10 to 30 percent, said Mark Gaunya, principal at Borislow Insurance, which works with hundreds of companies of varying sizes. Larger employers in this bucket are more likely to see smaller price increases, he said, though things vary. Most employers in Massachusetts, however, are considered “self-insured” as they pay for the health care claims of their employees more directly. These types of workplaces are also seeing premium increases, from low single digit to low double digits. “I haven’t seen rate increases like this in a long, long time,” Gaunya said of the industry at large. “The market is really, really hard right now for a lot of people.” The state’s two largest insurers cited figures that were similar. At Blue Cross Blue Shield of Massachusetts, “fully-insured” employers are seeing high single to low double-digit increases on average, though numbers vary widely depending on other factors, the insurer said. For its own employees, Blue Cross passed on an average 20 percent premium increase. Advertisement At Point32Health, premiums for employers with over 50 employees are increasing by about 15 to 20 percent on average in 2026, again with considerable variability. When confronted with ever-rising costs, employers have to get more creative with the way they offer insurance, or change what is covered. Beyond passing on the entirety of the increase to their employees as higher premiums, employers can redesign the plan, or shift more costs onto the employee. That can mean higher co-pays and deductibles, in addition to higher premiums. “We’re seeing more 20s- and 30-percent increases than we ever have before, in premiums. It’s quite insane,” said Gregory Puig, partner with Sentinel Group and the president of the Massachusetts chapter of the National Association of Benefits and Insurance Professionals. Shifting costs onto employees is not a new strategy. Nearly half of Massachusetts residents with commercial insurance had a high deductible health plan in 2023 — up from 19 percent in 2014, state data show. Average deductibles are also on the rise — from about $2,300 in 2014 to over $3,100 in 2023. Rob DiMase, a partner at Sentinel Group, an employee benefits administration and consulting firm, said these strategies have helped some employers he works with mitigate a 20 percent cost increase for next year, so the employees only see their premiums rise by 10 to 12 percent. Still, some employers are seeing huge increases. In the large group business, which includes employers with 50 to 500 employees, the best initial increase DiMase saw was 7 percent. The worst was around 57 percent. Escalating costs have patients worried about the future. Joe D’Eramo, 61, who has a broker, paid just over $615 monthly this year for coverage through WellSense. Beginning Jan. 1, that premium will increase by $90 per month and his deductible will rise by $500, to $3,500. Advertisement D’Eramo works as a freelance copy editor and public relations specialist, and drives for Uber on the side. But a $1,000 settlement with the ride share company will be wiped out by his health insurance increase. D’Eramo, who lives in Millis, was recently diagnosed with diabetes. He is counting down the years until he qualifies for Medicare, the government insurer that generally serves people 65 and older, so that he can save more money. “I just want to be able to make it to that [while still] healthy,” he said. Jessica Bartlett can be reached at jessica.bartlett@globe.com. Follow her @ByJessBartlett. Marin Wolf can be reached at marin.wolf@globe.com. Digital Access Home Delivery Gift Subscriptions Log In Manage My Account Customer Service Delivery Issues Feedback News Tips Help & FAQs Staff List Advertise Newsletters View the ePaper Order Back Issues News in Education Search the Archives Privacy PolicyYour Privacy Choices Terms of Service Terms of Purchase Career Opportunities Internship Program Co-op Program Do Not Sell My Personal Information Boston Globe Media
--------------------------------------------------

Title: Delivery Hero Investors Said to Push for Sale, Divestments
URL: https://finance.yahoo.com/news/delivery-hero-investors-said-push-082245746.html
Time Published: 2025-11-28T10:55:24Z
Description: Investors including Hong Kong hedge fund Aspex Management, which is Delivery Hero’s second-biggest shareholder with a stake of more than 5%, are pushing...
--------------------------------------------------

Title: Black Friday is triggering global ‘jet brag’ with these 10+ travel deals
URL: https://nypost.com/shopping/best-black-friday-travel-deals-sales-2025/
Time Published: 2025-11-28T10:00:00Z
Full Content:
Jet brag: A post–Black Friday side-effect marked by the urge to mention that your all-inclusive beach vacation cost less than your buddy’s Uber to Jersey. If you’ve been vying for a getaway, now’s the moment to pounce because the window for extended Black Friday and Cyber Weekend travel deals is slowly closing. This year, travel brands like Hilton, Skyscanner, and JetBlue are already rolling out offers that may not make it through Cyber Monday. I’m talking flights that won’t make your bank account cancel itself, resort stays at dreamed-of destinations at their lowest prices, and full-package deals that bundle everything so you just show up and relax. VRAI: Major savings on VRAI created diamonds + free giftsAudien Hearing: Save up to $250 + free consultationMERIT Beauty: 20% off sitewide + free giftOmre: 40% off all beauty supplementsWavytalk: Up to 45% off Big Ass Luxuries: 20% off the entire siteRitual: 40% off sitewide — the best sale of the year!B.T.R. Nation: 35% off sitewide Babbel: Up to 60% off your lifetime subscription!Nutrafol: 25% off sitewide for the first time ever!InsideTracker: 50% off all blood testsEpic Pass: Save up to 65% for 40+ ski resorts compared to lift tickets Whether you’ve started hunting for the best extended Black Friday hotel deals, all-inclusive deals, or flight and cruise deals, stop. We’ve already done it for you. Your move? Click to go to a specific category of Cyber Weekend deals and make it quick, because these offers are hyper-limited (and often have specific dates, terms, and eligibility). Black Friday Flight Deals | Black Friday All-Inclusive Deals | Black Friday Hotel Deals | Black Friday Travel Package Deals | Black Friday Luggage Deals Rentalcars.com is running Black Friday discounts up to 30% off rental cars from major brands like Hertz, Enterprise, Budget, and Avis. Discounts vary by country and pickup date but apply to many 2025–2026 bookings. Travel in style with La Compagnie, where every journey feels exclusive. Begin your experience in the private Newark lounge, then step onboard to enjoy fully flat business-class seats, sparkling Champagne at takeoff, and thoughtfully prepared in-flight dining. Offering unbeatable value, La Compagnie delivers elegant, business class round-trip flights to Paris, Nice, and Milan, proving that luxury can be both refined and accessible. I had a chance to fly La Compagnie to Paris, and my experience with this boutique luxury airline felt nothing short of VIP. Book you flight today and discover why La Compagnie is simply the best deal in business-class travel to Europe this Cyber Monday. Select a flight plus hotel vacation package and save up to $450 when you book by November 25, 2025, for travel between January 1, 2026, and October 26, 2026 (plus, you can lock in your trip with deposits starting at just $99 per person). Just use the code PRESALE300 (for $300 off vacations to Nassau, Bahamas, and Grenada) or PRESALE450 (for $450 off vacations to Atlantis Paradise Island, Bahamas) at checkout. Skyscanner is running travel deals all week, leading up to Black Friday, with “up to 30% off” select airfare and hotel bookings highlighted across their live deals hub. The Post Wanted-loved platform publishes price drops from hundreds of airlines and booking sites live as they happen, so you’ll see rolling offers appear throughout the week (not just on Black Friday itself) with some routes dipping below their typical seasonal averages. The well-loved Euro airline has kicked off its Black Friday promo with special fares for flights booked in November 2025, offering passengers exclusive savings when you act before each deal’s provided deadline. Fly to Ireland, the UK and Europe for less using Aer Lingus’s own Flight Finder, which makes finding deals and planning trips extremely easy (you can even input your budget maximum). Or, you can simply scroll the pre-made list of flight deals, such as New York to Dublin between January 23 and 26, 2026, for $348.37, as it’s updated live. Prices are constantly in flux, so lock down any good ones you see ASAP! Their Black Friday Sale (Nov. 11 through Dec. 2, 2025) offers up to 65% off stays, up to $2,000 in booking credits, as well as a free night at select resorts. The travel window extends through December 25, 2027, giving you lots of flexibility for future getaways. The brand’s Black Friday 2025 offer features up to 50% off stays at all-inclusive resorts in Mexico and the Dominican Republic, with pricing from as low as about $100–$172 per night in select cases. Expedia’s Black Friday sale is live with up to 30% off select hotels plus member-exclusive discounts on flights and packages. Travelers can book stays through the first half of 2026, and rewards members earn boosted points. Right now, Hilton’s sale lets you save up to 25% on holiday stays at hundreds of hotels across the U.S., Canada, Mexico, and beyond, with travel valid through April 6, 2026. On top of that, they’re offering perks like earning double Hilton Honors points on select stays, and “Stay Longer in Paradise” packages that offer up to 20% off stays in destinations like the Caribbean and Hawaii. Barceló Hotel Group’s Black Friday sale touts “up to 55% off” reservations worldwide, including all-inclusive resorts, with added bonus savings for members. Some of these deals include an extra percentage off with a promotional code (found on the sale site) and apply to bookings made now. Marriott is offering up to 20% off room rates at participating MGM Collection with Marriott Bonvoy destinations in Las Vegas when you book between November 4 and November 18, 2025. You can lock in the discounted rate now and stay anytime through October 31, 2026, giving you nearly a full year to use the deal. Just enter code F4639 at checkout to score the perfect deal on a Vegas getaway with plenty of flexibility. Trafalgar’s Black Friday 2025 campaign is offering 15% off international land/cruise tours around the world, which is helpful if you’re planning multi-country trips. Plus, if you’ve booked with them before, you get an additional 5% off your booking. On select guided tours, from “Best of Germany” to “Country Roads of Scandinavia,” you can score a BOGO 50% off (or similar global vacation package) deal now through Black Friday, giving you twice the value when a second traveler joins. Amazon is offering huge discounts up to 50% on all of the top luggage brands. Shop American Tourister, Samsonite, and more at all-time low prices. Right now, you can save 25% on everything Away has to offer, from hard-sided carry on rollers to softside checked bags (which Post Wanted reporter Angela Tricarico highly recommends). The 25% savings also apply to all of Away’s products on Amazon. Solgaard, known for their Carry On Closet packing system, is offering 20-40% off everything, from carry ons and checked bags to backpacks and packing cubes. For a limited time, everything on BÉIS is 30% off, including the cult-favorite Weekender and rolling luggage. Monos is currently offering 25% off everything, including luggage, totes, and more, with the code BLACKFRIDAY. Good news! Many of the best Black Friday sales have been extended through the weekend. Don’t wait in the cold for stores like Walmart to open and fight the crowds to score the most popular gifts — we found all the deals you can shop from the comfort of your own home. Kick off December with the online shopping sale of the year on Monday, December 1, 2025. It depends! Black Friday shoppers may benefit from higher inventory on sell-out-risk items, while Cyber Monday may bring steeper discounts and incentives — depending on where you shop. We recommend making a strategic plan to maximize your savings without missing out on the items you want the most. Jet-setters rejoice: Travel Tuesday, with its great deals on flights, hotels, cruises, all-inclusives and more, will take place on Tuesday, December 2, 2025. Absolutely! Black Friday and Cyber Monday bring some of the biggest sales of the year on big-ticket tech items such as AirPods, popular gifts like the Dyson Airwrap, clothing and shoes, top kids toys, and so much more. Right here! Follow Post Wanted shopping insiders for updates on all the Black Friday, Cyber Monday and Travel Tuesday deals worth shopping in 2025. For over 200 years, the New York Post has been America’s go-to source for bold news, engaging stories, in-depth reporting, and now, insightful shopping guidance. We’re not just thorough reporters – we sift through mountains of information, test and compare products, and consult experts on any topics we aren’t already schooled specialists in to deliver useful, realistic product recommendations based on our extensive and hands-on analysis. Here at The Post, we’re known for being brutally honest – we clearly label partnership content, and whether we receive anything from affiliate links, so you always know where we stand. We routinely update content to reflect current research and expert advice, provide context (and wit) and ensure our links work. Please note that deals can expire, and all prices are subject to change. Looking for a headline-worthy haul? Keep shopping Post Wanted.
--------------------------------------------------

Title: Presentation: Reliable Data Flows and Scalable Platforms: Tackling Key Data Challenges
URL: https://www.infoq.com/presentations/data-architectures-challenges/
Time Published: 2025-11-28T09:45:00Z
Full Content:
A monthly overview of things you need to know as an architect or aspiring architect. View an example We protect your privacy. Facilitating the Spread of Knowledge and Innovation in Professional Software Development Unlock the full InfoQ experience by logging in! Stay updated with your favorite authors and topics, engage with content, and download exclusive resources. Vivek Yadav, an engineering manager from Stripe, shares his experience in building a testing system based on multi-year worth of data. He shares insights into why Apache Spark was the choice for creating such a system and how it fits in the "traditional" engineering practices. In this podcast, Michael Stiefel spoke with David Blank-Edelman about the relationship between software architecture and site reliability engineering. Site reliability engineering can give architecture vital feedback about how the system actually behaves in production. Architects and designers can then learn from their failures to improve their ability to build systems that can evolve. Matthias Niehoff discusses how to bridge the gap between application development and data engineering. Learn to apply software engineering best practices to data pipelines, embrace boring technologies, and simplify architecture to maximize business value and data reliability. In this podcast Shane Hastie, Lead Editor for Culture & Methods spoke to Joe Cassavaugh about building a sustainable solopreneur game development business, the importance of authenticity over conventional wisdom, and learning from both successes and failures in the digital content marketplace. Emma Yuan Fang explains the Zero Trust mindset required to combat modern software supply chain attacks. She details security controls for dependency management, including SBOM (Software Bill of Materials), artifact signing, Git commit signing, and CI/CD hardening. Learn how to implement security gating, enforce policies as code, and manage secrets across your build and runtime environments. Go from AI demos to real engineering impact. Learn to embed LLMs, govern & scale securely. Early Bird ends Dec 9. Learn what works in AI, architecture, data, security & FinTech. Early Bird ends Dec 9. Learn how leading engineering teams run AI in production—reliably, securely, and at scale. Launch pricing ends Dec 9. InfoQ Homepage Presentations Reliable Data Flows and Scalable Platforms: Tackling Key Data Challenges Matthias Niehoff discusses how to bridge the gap between application development and data engineering. Learn to apply software engineering best practices to data pipelines, embrace boring technologies, and simplify architecture to maximize business value and data reliability. Matthias Niehoff works as Head of Data and Data Architect for codecentric AG and supports customers in the design and implementation of data architectures. His focus is on the necessary infrastructure and organization to help data and ML projects succeed. Software is changing the world. QCon London empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams. Matthias Niehoff: I'm Matthias. I work for codecentric, a small consultancy in Germany. Before I joined codecentric, I made my professional education in an insurance company. When I finished this education, I came back to the company and they said, congratulations, and yes, we want you to stay with us. We have a new job for you. Back then I was in the architecture team, whatever that meant. The job is going to be in the data warehouse team. I was like, what? Because for me, data is that which the others are doing. From a software engineering perspective, I didn't care about data warehouses. This is where the data goes and they get it some magical way. Turns out it was a joke. I stayed in the architecture team. After joining codecentric, I moved into the data space. I started with Apache Spark 10 years ago, and then evolved into getting all the data stuff, but I have a software engineering background turning data. These challenges remain. From a software engineering perspective, it's often still like data, data, data for analytics, it's not that that we care about. I'll give you an example. Thinking about, we have some data that is stored, order data and order id and quantity, on the right side, we have the analytics side. We just get the order id and the quantity. Then the application thing, we want to track if the order was unfulfilled. The analytics side still doesn't select order id and quantity, and wonders, we have more orders than we had in stock because they didn't know there was an unfulfilled field that they had to track and that has to be selected as well. They're eventually coming up to select this as well, but they want to measure now the unfulfilled items and then they change the data type from a Boolean to an integer. What happens? Either the consuming side converts it implicitly to a Boolean and it basically fails silently, or they just fail implicit because the data type has changed. This is a quite common scenario we have when providing data. What is the reason? The problem is that most of the time only the technical schema is exposed towards the consumers of the data. The consumers just get that schema and try to reverse engineer what is actually happening here. What is the business process that creates that data? What are the constraints here? What is the data model underneath? Also, is this source really trustable? Can I trust it? How do I access it? Is it stable? Is it durable? All these questions remain. It might just be a dump on a file system somewhere more often than not. On the other side, the producers, if they even care who uses the data, they don't often know who uses the data. For what reason? For what purpose? They're most of the time blind on this side. To summarize, most of the time, data is still a second-class citizen, if we're talking about software architectures. It's just a database where we store the data in. This leads to quick one-off solution if data is to provide it, we do dumps. We just say, here, you have a user for our database. You can access it. Nobody would do that normally, but, yes, that's done. The data team is left on its own. I'm going to show you some examples of what we did with clients to manage this. The first example is a small FinTech, which is in the realms of a larger bank in Germany. They were doing B2C stuff, and they wanted to do analysis on marketing campaigns on how the customers behave, what kind of customers they have. They want to check their online sales process. Nothing fancy, just gathering some data from different systems. There was a small team of data analysts who were really curious about getting results out of data, not the really technical guys, all your Python, all your SQL and know how to start a Jupyter Notebook, but they were more interested in, what is the data to say. They had minimal support from the org. The org was really focusing on the application. It was DevOps and software engineers and all this stuff, but they want to just build online applications. They didn't really see the case for data. As a data team, what did we do? We got files, in this case, dumped over at an SFTP server. The first thing we do, we did tests on the source data. In this case we used dbt. We wrote tests to test what is in the data. Does the data change? Is it what we expected, to get notified early if there would be changes in the data? For instance, there would be all null columns, because suddenly would likely mean there was some problem in the export of the data from the source system. We made it visible. We built dashboards. This is elementary build on dbt. It most of the time didn't look like this. This is in the best case. We want to make it really visible. Having this really as a chart where we looked on each day and also using it to communicate with our stakeholders, we don't have the data we needed. Data is broken and so on. We started documenting what we learned about the data. How is data created? What is the business process behind the data? What are the constraints? We learned on the fly, basically, while working with the data. We adjusted the test to actually have the knowledge we had in the test. We extended this. This also was part of dbt in the end. This was all just basically mitigation of the problems. We had mitigation of the problems that the producer actually wanted on board. We wanted to get to the source. We wanted to involve the producers to actually help us getting better data. Maybe it's getting a step back. If you have an application data model where you store your data of the application, you have defined it everywhere. Might be in normal form. Might be normalized or whatever. You have another application wanting to access this. No software engineer on earth would say, here's the application model, just do the access on it. You would always build some API on it. Might be a REST API, might be a stream. You might be putting on messages on AMQP, whatever. You would always use an abstraction. From analytics, it's more often than not like this. You just dump out the internal application model. What we're thinking about is we want to use an API as well. Maybe not the REST API that is optimized for single record access, but maybe some more efficient. Or it might even be a separate data model that has been created for sharing the data towards the outside. All of this should actually be the responsibility of the team. We should care about how those interfaces look like. How should those interfaces look like where we share those data from the team towards the analytics part. The one thing is, how is this sharing data model created? It needs some pipeline. It has to be some process, something where this sharing data model is created. The sharing data model itself, from a technology perspective, could be a view in the same database that they are using for the application. It might be a topic in some broker. It might be files in Blob storage. It might be Iceberg tables, data tables, CSV, whatever. It might be something in a data warehouse or data lake. With the MCP servers now giving LLMs access to data, the first idea is popping up to actually use MCP servers to access those data. I'm not sure if it's a good idea still, but at least something one can have in mind. We need those pipelines so that software developers can easily share this data. These pipelines should be part of the whole application, so owned by the development team. The development team is responsible for creating this data model for sharing the data. First of all, it means more responsibilities, but also it gives you chances. I think a lot of you who work in data know that CI/CD tests and so on are not as widespread as in software engineering. It might be a chance for software development teams to actually build and use their software stack, to use their best practices or good practices that they have, writing tests, doing CI/CD from this. Taking ownership of it means not only responsibility, but also having the freedom to do it in the way they want to implement it. In the same tech stack, possibly. It's just another API that they are building next to maybe an existing REST API. Still, those pipelines need to be supported. We want to support those people with blueprints, with libraries creating this. It might be that the application tech might be Java something or Kotlin or so on. On the data side and on the consumer side where you want to provide the data might be something like a Delta Lake managed by Databricks. I happen to have an example afterwards. You need to do more support for this. You need to do support for execution and retry, something that is not that usual in a normal web application. You have to build frameworks and things for this. You have to enable monitoring with them. Yes, they can use the monitoring that they use for applications, but it might be something different. Schema evolution, write to data-specific infrastructure? Also, things in business, like data changes have to be supported. Those are some of the things they are not used to when writing a normal application. Next example. We have a Java application writing to a Databricks-powered data lake. Most of the developers that would implement this application don't know what Databricks is or know how to use it or how to implement it, and how to do stuff in it. They might be able to do this, writing data to a Blob storage, maybe in Parquet file and not in JSON, or something a little bit more efficient. There are two options, basically. You can register this as an external table in Unity Catalog, or you can load it to Databricks. I won't discuss which one is better, but as a software developer, I just don't care. I don't want to know how to do this. The pipeline should support, actually, this library. We built libraries that made it possible to just say, I have a large amount of data here in some file or in a database, and I want to publish it. The library takes care, with the pipeline behind it, to put the data into a Blob storage, load it to Databricks, make it available there. The SDK or the library had some input fields, so you can do a description in it, you can put information to the data in it, so that it gets published to the Unity Catalog as well. This was just the beginning of the interface we were describing. We started collecting also not only the data, but also information and metadata about it. We took another step forward after this. One thing we used were data contracts. I think most of you have heard of data mesh, a few years old by now. Huge impact, huge hype back then, a lot of different concepts. I think the most tangible concept that was there was the data products, and out of this came the data contracts. Contracts basically describe the data that you're sharing between a data provider that has some data product, in this case it would be the application team, and the data consumer, what data they share, and how it looks like, and what guarantees that gives you. The data contract is then described often as a YAML file, and you can use it for enforcement. I'll come to it later. The most important part is you make it explicit. You explicitly describe, we share or provide this data, and we use it, and they talk to each other, which most of the time didn't happen before. What would a data contract look like? I have here the high-level overview of a data contract on the right side. You have servers. Where's the data stored? Might be in a database, might be in S3 bucket, might be anything, might be a topic. You have the schema of this. You can put price tags on it. If you say consumption costs like this, so you can enforce it, informational only. You give information about who's the team. You give roles. You give SLA properties. We give you the guarantee that this data is up to date, or is updated every day, at maximum lags behind one day after the event has occurred. We give you guarantees that we will do failure resolution of data within whatever days. This is really something that you argue about, and more often than not, coming to points where you're saying, you expect this, now we only can deliver this, because we have this and this constraint on this. Then you find a solution together. We created those contracts data first, so we had the data, and then we started creating a contract, all of the data. You can just say, here's data, just build a contract out of it. We versioned those things with semantic versioning. We put it into a central Git repository. What we used was the Open Data Contract Standard. There are multiple standards, currently it seems like this is the one that evolves from all of those standards, and it's going to be the main standard, until there's another one. We were lucky and did the right choice at this point. I see that we can also enforce those contracts. There's a CLI, datacontract-cli as it's called, where you can test those contracts. You have defined this contract in a YAML file, all the information inside, where's the data stored, what kind of schema do we expect, and we're doing tests on it. We can also do like, do we have breaking changes? If we do another version of the data contract, we can just give the YAML file of the old contract, of the new contract, and say, is this breaking or is it forward compatible? Can we use it again? What we did is, we created source tests. You remember right at the beginning, we manually created source tests, we can create dbt source tests, based on the information we have in our contract, and use those to enforce the data, and test the data as a consumer. Where were we enforcing the contracts in the end? Once on the publisher side, in CI/CD. We changed an API. We created a pull request. Within the pull request, we checked the changes against the contract. We checked for the format, for the schema, for the data quality, is it still what we expected? If all these checks passed, then that was one of the necessary requirements to get those pull requests merged. On the consumer side, on the other side, we also retrieved the data, and the first thing we do, we're checking the data. Checking for schema. Checking for quality. Checking if those SLAs we had agreed on were true. This is what we did there. Where and how did we check for reliability? Because one of the topics is reliability. We did unit tests and tested contracts in the app development. The pipeline execution, we tested against data, and we integrated it into application monitoring, having one view, and not the application monitoring with Grafana and whatever, and some other view somewhere else. We did source tests and tests in further stages along the processing as well. Didn't go into details here. Reliable and trusted data is important to scale the data usage. When I'm talking about scale, I'm not talking about having billions and billions and more data, but with scale, I mean getting more out of the data we have, empowering more use cases, getting more use cases done, creating more value for the business. This is, for me, scale. I don't need the tech scale. It's interesting, but it's not necessarily the most important part. Next to this trusted data, what is the role of tech and platform in this case? We talked about the pipeline getting data, but what is the role of tech and platform? A few people know this. What does it mean when you see this? For me, it's sometimes really a cognitive overload. This is all the tooling and things one should know, or at least have an idea about what they do when it comes to data, and it's getting bigger and bigger. Also, GenAI is included, which is a big part of it, but it's really a lot. For me, sometimes it's really causing constant stress. I just go through LinkedIn or whatever, or Hacker News, and see there's a new tool, and there's something cool, and there's something I want to have to dig in, and it's really like constant stress. What do you do then if you're coming in this thing? There's multiple strategies of what people do. You try to start, I'm looking everywhere. I'm trying to get there. I get an idea there, and so you know nothing about everything, basically. Or another strategy is, that's too much. I just focus on this one and go really deep. I know everything about Apache Spark. I don't care what's left and right, but in Apache Spark, I'm the absolute expert. Or you just say, no, back then, we were just using Apache NiFi example. We can do everything with it. You're just clinging to the past, also a strategy that people are doing. Or you get dogmatic, or you just give up and say, what else? I just don't care. It's definitely this complexity and all these things that might be there, and people are telling you, and market telling you, you need this to get actually data platform, and this is needed, and this is needed. It's unhealthy for people, and it's unhealthy for organizations. A reason why I might come back to something that is essential complexity versus accidental complexity. Essential complexity is the things that you need to fulfill your functional requirements, your non-functional requirement, and there might be some boundary conditions. A lot of you might be in banking, so there are boundary conditions from regulations or whatever that you have to comply to. These are essential complexity. You cannot do this without those complexity. It's inherent to the problem and the essence of the problem that you are solving. All the rest is the accidental complexity. Is using Kubernetes over kubectl really essential to the problem you're solving? Might be, might not be. I tend to say in data, more often than not, it's not the problem. For me, there's a lot of accidental complexity we introduce by using those tooling. Therefore, it's really important to embrace simple and effective solutions to really build things that solve the problems at hand, to scale things. A good strategy to do this is choose boring technologies. Technologies that are well understood, both in what they can do, what they can't do, and what failures might occur. You might find all failures in Stack Overflow, or ChatGPT has learned about them. It's pretty good to understand. You have a community. You have people to talk about. It might be sometimes that a new problem comes up where existing and boring technology doesn't work. One of the most boring technologies, which is pretty good at data, is actually Postgres. It's not as sexy as Snowflake. It's not as fast as DuckDB. Postgres is actually a really good solution for a lot of data cases we have. Also, there's the concept of innovation tokens. A really good idea. There's the idea of, as a team, you have two or three or one innovation token per year that you can spend on new technologies. You try to manage and not to lose yourself into new stuff as well. Next to boring technologies and also Postgres, is you build on well-understood technical foundation and open standards. Open standards give you the flexibility to argue with vendors, to switch things. It's good understanding. There's a foundation to use this. One of the open standards next to Postgres is all the things that come towards data storage right now. Think data storage in the cloud is a lot of getting commodities. If you're using Delta or Iceberg or Parquet, it's open standards. Also, query engines get open standards, and you can build on top of this. Another thing to keep things simple is leveraging the key concept of the cloud. Doing automation. Doing scaling to zero, and reduce the vertical integration. I said, I'm not a big fan of Kubernetes in data because I think there's a lot of good PaaS and SaaS platforms that actually go higher level and enable you to do more with less in the end. This might be something that is very familiar to a lot of software engineers, but you see in the data place, there's a gap between what software engineering knows and what software engineers knows is a good thing, and what data engineering does. Another case, a pretty conservative bank, slow, not that large, regional bank, a cooperative, they want to do more data-driven decisions. Which management doesn't want to do more data-driven decisions? The existing solutions didn't scale, not in terms of data, but scale in terms of what decisions we can make, what kind of insights do we get? They built a cloud data platform. They use a central team for the platform, no data mesh, but they have different data user teams that actually use those platforms. What we built is we used open standards. A platform in the cloud, in this case, it's Azure, but it could be any cloud, using Delta Lake as an open standard for software, using PySpark, which is an open-source product to run their data. Nobody wants to run PySpark on its cluster on its own. You never want to do this. We used Databricks. We used the Unity Catalog, which is proprietary stuff, yes, but it gives us things that we didn't get about any standards, gives us things about permissions, management. It's not the business data catalog, it's a technical data catalog. We're using Databricks Serverless SQL to access that data. Those are proprietary things, but the foundations are open source and open standards. We used also the Unity Catalog features to provide the data towards, I said, we had a core team and multiple single teams as well. We had the core team having catalogs, and those can be shared within other teams. We're using the Unity Catalog features of Databricks with it. If you're building things like these, on Kubernetes, you have to find a solution where you can do the cataloging. The Unity Catalog is open source. I've not seen a lot of companies actually running it. You want to get it managed because you won't get paid for running in catalog most of the time. We use the catalogs for data sharing between those departments. This was proprietary stuff, yes, but built on open foundation. Those open foundations enabled us, for instance, to use different query engines. Databricks is a great platform. I really like it. Other platforms might be good as well. I'm not that experienced with it. If you are coming to data that is not huge, but it's big, like 30 gigs, 40 gigs, or whatever, Spark is actually really slow. It's a lot of overhead to actually process the data. Thanks to the open standard, we could use DuckDB to connect that data. We can get a token from Unity Catalog and read the data from the Delta Lake. It's an open catalog. It's an open standard, nothing proprietary. Because one thing, data is smaller than you might think. This is an analysis of Amazon Redshift. The team behind Amazon Redshift did it. It's a few months old already. This is the title, Why TPC Is Not Enough. TPC is a benchmark used in analytics to measure the speed of warehouses and SQL engines. The paper was about, is TPC still the right benchmark? They analyzed all the data they have in their system and they found out, actually most of the data, so we have the rowcount buckets up to 10 million rows over here. Ten million rows isn't that much on data, but most of the data actually is about 10 million rows. It's not that big. Sometimes it's even enough to run on a single machine. You don't need distributed systems at all, making it simpler, easier, more cost-effective, and more reliable to build data platforms. Another example, we built a more small stack, FinTech. We want to do everything on-prem. There were some reasons to do this. We just built things on Postgres with dbt and Airflow to orchestrate. One can argue if Airflow is a bit of overkill, yes. We use Python to ingest the data. We use the contracts before to actually synchronize on the data and have a clear interface between this. It was really simple tech, nothing fancy, but it gets the job done. One thing I want to give you, apply software engineering best practices. I'm on a more or less software engineering conference for you as clients, writing tests for code and data, monitoring and observability, infrastructure as code. It's all a given, but it isn't too much in the data world, actually. It's a hard way to say, tests are a benefit. You also have to understand, yes, some things are definitely different. Data gives you another dimension. The data itself is another dimension you have to test as well, using separate environments. Also thinking that software engineers say, of course I use separate environments. In data, it more often than not comes to this. Separate environments is often like, yes, we have prod. Bringing it together, reducing the complexity of the platform also reduces the error surface. Reducing complexity reduces costs. All in all, this improves reliability of your data platform, data pipelines. Then using standards, improving practices, simplifies the operation, also less complexity, less overhead, and sometimes a single node system can bring you a long way. We talked about having the pipelines and contracts basically in the platform and say it's simplifying. Maybe take one step further here. Another idea I just want to present and give out. Another case, an industrial IoT startup. They collected sensor data in their electronic grid and they visualized that data. Eventually, they also want to control the grid based on the data. What they did is they had a document database, put all the data into the document database. The team was consisting mostly of full stack developers and mostly heavily leaning to the TypeScript and the ecosystem around this. Then, a document database is fine, because if you're building a frontend with TypeScript, all the data is in JSON, perfect. At some point, they started wanting to do data analysis. What we had were like this. We had an ingest. The data was put into a document database. There was a UI and APIs connected to the document database, and there was a relational database for some master data. What stations did we have? What centers did we have out in the field? Pretty common. Document database was also using the time series mode of those databases. Then they wanted to add aggregations on this, because they wanted to do hourly and daily values, so they did a Python script that runs on Document DB, then they wanted to have alerts running on the Document DB, getting the data out of it. The relational database was extended somehow because they wanted to do analytics. The center of the data was actually Document DB. To summarize this, this was actually a web application that is a data platform with a UI. It's not a web application, but the center of this application is data. We built a UI on it, but it's not a UI and data is somewhere. We redesigned it. We have the ingest, and the first place where the data lands is the data platform. We don't have any application doing anything, but we have the data platform, the one that is responsible for storing, analyzing the data. We do all the aggregation and the alerting on this data platform. Then we have the Document DB that is fed from the data platform with the data that is needed to visualize the data. It might be aggregated. It might be filtered. It might be optimized to actually fit the needs of visualization. We had data analytics directly on the data platform. What we did basically is a shift left. Shift left is like everywhere. Shift left in security is more about having it earlier in the process, in designing as well. We have to shift left in the data flows, not too much in the process of creating the software. There might be ideas as well, but in the shift left in data flow. We had from data source to application to doing some ETL and ELT stuff to doing it somewhere where we do analytics. We moved it to, we're having a data platform. Some optional ETL, ELT might be, might not be. Depends on what data platform you have. You don't have to load the data. You might access the data platform directly. Then connected some analytics stuff, some web application where a user can do something. I'm not saying this should be done for all, definitely not. I think especially for applications where the data is in the center, and I see it a lot in IoT and industrial applications where you get a lot of sensor data or also in marketing where you get tracking data. This might actually make sense to move data at the center and not treating this like an application. We moved the data lake. When we think about this, we can even take it one step further. Why do we actually build different platforms for data and applications? There was a talk about platform engineer basically and developer experience. It was a lot about building applications for software engineers with all the CI/CD and the golden path and improving the developer experience, making it easy to deploy stuff. It's all good and all fine. What we end up with, we have an application platform where we run all the applications on it. We have a data platform, which is storing the data and powering ML and AI use cases, BI use cases. Taking observability, for instance. Observability is a huge thing in application platforms. Monitoring, logs, all the stuff is built into it. We would definitely need more of this in data platforms. There is a lot of things that can be used. Most of the time, the data platform solves it itself. I take an example, Databricks. Getting data out of Databricks for monitoring and observability reasons is possible, but definitely not easy to do. It's completely two separate subsystems with different approaches, with different technologies. I don't think it has to be like this. This is actually also widening the gap between those systems, because we're building on different platforms and different paradigms, and try to avoid this. We use the data platform, getting the data from the data management. Why don't we build application and data platforms? Platforms that actually have in mind from the beginning, we want to support application development, running applications, and using the data within this application for analytical purposes. We have things like storage and runtime. The storage might be different for data than for applications. The runtime might be different, but it comes from one vision, might not be one team, but from one platform building this. Everything that is CI/CD means there are proven patterns on how to do CI/CD in every software company, most likely. I think 80% of the time those can be reused for data. Why not provide it to data within a data platform as well? All the stuff like secret management. Secret management, there's a lot of tooling around it, but it's reinvented in data as well. Test data management, and the contracts themselves. The contracts are in between application and data. This is what is common to both, and of course should be managed within this platform itself. You might be asking, what is the tech? There isn't the one technology that powers all of this. You might say, yes, we built everything on Kubernetes because we built our developer platform and our data platform on Kubernetes. One might say Databricks or Snowflake or whatever might be your thing. I would say, coming back to what I said was platforms, we did think about what is the essential complexity? What is the accidental complexity? What complexity is actually needed to solve this? It might be that Kubernetes is one thing. It might be that Kubernetes powers all of your cases and all the things that you have also with data. It might be that you have two different technologies and just creating good bridges in between and good integrations in between. It might be that there's even another solution based on this. There isn't the one answer. I promised I'm coming back to DuckDB as well. It's an idea of DuckDB not seeing this, but I think it's a good thing to think about is going beyond two-tier data architectures. The two-tier data architecture is basically this. We have data in the application. We have data in the analytics part. This is an idea of DuckDB. DuckDB, pretty fast embedded analytical query engine and database, which can basically run everywhere on a server or in a browser with Wasm, or in a Lambda function, and can read basically everywhere. Having the central data lake, in this case it's more like a swamp maybe, or between a lake and a swamp, as a central datastore, both powering applications and analytics on the same way. It's an idea. We haven't seen this in a project. I think it's an idea worth thinking about, maybe having in mind when designing this. Because, in my opinion, it would be a good thing when data and applications became one. We would have less friction between operational and analytics: common ground, common platform, common ideas maybe to build stuff. We could actually define and encapsulate responsibilities more clearly. Remember the example from the IoT startup? There was a long discussion. Who should do the aggregation? Who is responsible for doing the other thing? Is it the application or is it the data team? Where is the knowledge about when to do the other thing? If you're thinking this is one, then it's a common ground you find and you can decide together where it is. You might have optimized data representations for different cases. A document database isn't that good actually for analytics. There might be better settings, but it's really good for UIs, for example. The same is a Blob storage isn't that good for actually real-time requests from a web UI, but a document database would be. It's really important to use the tooling on both sides. Not saying, we have Kubernetes and Postgres and everything, so just get started with this and do this, but to know the stringencies, the differences that there are. Also, use the engineering best practices and good practices, and apply them to data. What do we take away from this? The first thing, validate and monitor your data early on. Having contracts both from the provider side, and do it on the consumer side as well. Start at the source. Really take the devs into this. Don't say, we just get the data and we have to be on our own. Try to get the devs on board by building solutions and abstractions that actually have them, that make it easy for them. Don't say to them, here's Databricks, publish it there. You can build workflows there. You just need to write Python and write it to Parquet. No, that's not going to work. Really get to the devs and get them where they are, and address those pains. Of course, the organization has to prioritize data as well, otherwise every product owner of every team would just say data, data, I don't care. Simplify the architecture. Don't make it more complex. Really make it handleable, also for smaller teams, for teams that are not that experienced. It doesn't have to be the latest, newest technology everywhere. Apply the good practices you know from software engineering. It might be a thing to think data and application platform together, not separate those worlds from the beginning, from the foundation. It's getting split up in the application anyway and in the usage, but maybe having a common ground to actually build on them. We talk a lot about tech now. Actually, data modeling is really a thing. Only write tables and having everything joined and having one large table might not be sufficient all the time. Things like star schema, dimensional modeling, all this stuff isn't the sexy part, but it's actually really helpful. Besides all the tech, there's also some things to learn from data and good classic data warehouse. Although, sometimes it's really dry, but it's helpful. Nardon: Your take on using the data for application and for the operational system using the same platform. Usually, in companies, especially when you have several departments, you have teams that need data from other departments. Usually, that's why there's a data platform where you can advertise your data and share data with other teams. If you're using the same platform for data and applications, this gets a little more complex, because you have the teams developing the data and having to share. How are you doing a data mesh architecture and sharing data between departments if you're using the same architecture? Matthias Niehoff: Even when you build the same platform for application and data, I would still have those typical data platform capabilities like having a catalog and having access management to different data, and a point where data is shared towards others. It would be still a capability that should be built into this application and data platform, definitely. It's not like we don't have the traditional data platform anymore with all the functions. We still have those functions, just moved to one platform that is developed under one common vision with one common ground and one common goal. I'm pretty sure in a large organization, it won't be one team building this platform as well. What's more interesting is, often it's like the data department, especially in classical environments, somewhere in finance and the CFO might be the boss and the application platform is in CDO, CTO, CIO department, and it's completely different areas of the organization with different cultures as well. This is more like a tricky thing I see that would hinder adoption of this. Yes, definitely. Participant 1: How do you know when you should start buying a tool instead of building on your own? Because a lot of the other talks so far have said that you should buy before you build, but you say to choose boring tech. At what point do you know that you should switch to buying existing software? Matthias Niehoff: Boring tech doesn't mean you shouldn't buy. Postgres, ok, you don't buy Postgres, but you could buy Databricks with PySpark, and PySpark is boring tech. It depends. When you're doing stuff that you have the feeling of, it may mean you could take Wardley Maps or something to get out what is commodity, what is actually new, in genesis. If you're building stuff that is undifferentiated heavy lifting, things you do that don't provide really value to you, running a Kubernetes cluster as a data team, I would take as an example. Then you think about, might there be something that would reduce the vertical integration that gives us more intermediate ways to do things with data, work with data. I don't have one person that actually only is there to run the Kubernetes cluster. I think this would be a thing. It's not like there's this one idea to do this. This is more like, depends on your situation. Participant 2: I think my question is around the complexities of having a platform that has already been adopted or consumed by multiple data teams. How easy is it actually to change technology? I think I'm asking this because we're moving in a world where technologies are released every day. We see that there's a Microsoft Fabric that's been introduced now to say that it's also removing the complexities around infrastructure. How do you actually manage that? Because we are also in a distributed type of mesh, and data teams are actually responsible for their own tech. How do you ensure that all those principles can actually be a central component? Matthias Niehoff: When you were changing tech, what would be the motivation, would be the question? Why would you choose changing the tech? Just to standardize, and all of them having the same technology? That wouldn't be enough as an argument to just change tech. If you say, we have to standardize because we have increasing license costs because everybody has bought their own tooling and so on, yes, then it's more or less a classical migration and modernization project with a clear path on how to move each of those teams to a central technology or a central platform. You still want to have distributed responsibilities, I understand, so teams are responsible for the data, but they should use common infrastructure. It's more like a normal modernization project. Participant 3: You commented that running the same data platform for applications as well as, let's say, analytics is not something you've seen much of. I'm aware of a few that run, Uber, us, and usually the complexity is around non-functionals. There are two patterns I know, which is running multiple concurrent versions of the same platform optimized for different non-functionals, and the one we're attempting, which is to use the same platform overlaying the tooling on the same source. You keep talking about DuckDB and these tools. Given those two paradigms, do you have a thought as which one you think would be more efficient, profitable, functional? Matthias Niehoff: You mean by having one platform or having separate platforms? Participant 3: One platform with three different copies of the data optimized for the different non-functional use cases or one version of the data platform that then has to support optimizing for different use cases. Matthias Niehoff: Yes, either optimizing the data or optimizing the processing, is basically the question. Participant 3: For me, the question is usually around the idea of overlaying tooling, so different interfaces, query products. Matthias Niehoff: It depends a bit. For instance, if you're taking a database as your storage, then you have the query engine baked in. Then you can't change too much. If you say my data is stored in a Blob storage underneath and I just use a different engine, then you would be flexible, of course. I think then flexibility actually pays off and it would go that way. It depends a lot on how you store it, but you made one good point. This distinction between having storage and query engine directly coupled goes more and more away. We have the storage that is often pretty cheap as a Blob storage, and we have query engines that optimize for different use cases, definitely. See more presentations with transcripts Recorded at: Nov 28, 2025 by Matthias Niehoff A round-up of last week’s content on InfoQ sent out every Tuesday. Join a community of over 250,000 senior developers. View an example We protect your privacy. Get production-proven patterns from the senior architects and engineers building what's next in software: Validate your roadmap, de-risk your next technical bets, and build what's next. InfoQ.com and all content copyright © 2006-2025 C4Media Inc. Privacy Notice, Terms And Conditions, Cookie Policy
--------------------------------------------------

Title: Dublin Airport warns of delays as taxi drivers to stage protest this evening
URL: https://www.independent.ie/regionals/dublin/dublin-news/dublin-airport-warns-of-delays-as-taxi-drivers-to-stage-protest-this-evening/a1349713719.html
Time Published: 2025-11-27T15:10:40Z
Full Content:
Dublin Stock image Dublin Airport has warned passengers of possible delays this evening as taxi drivers are set to stage a protest across the city. In a passenger advisory issued this afternoon, the airport said the demonstration due to take place between 4.30pm and 6.30pm, may lead to “heavier than usual” congestion on roads approaching Dublin Airport. “Some delays are possible,” it warned, advising those with flights to give themselves “plenty of time” for their journey. Airport Police will work alongside gardaí and local authority traffic management teams in an effort to minimise disruption for passengers and staff. The protest comes amid an escalating dispute between taxi drivers and Uber over the company’s recently introduced fixed-fare option, which drivers claim undermines the regulated fare structure set by the National Transport Authority. Uber recently introduced the optional fixed-price model, guaranteeing passengers a maximum fare upfront instead of the usual estimated range that can rise with traffic or route changes. Organisers have said that they “sincerely apologise” for disruption but insisted the protest is necessary. Some 1,500 taxi drivers are set to engage in a ‘slow drive’ at three points including Dublin Airport, Clontarf, and Phoenix Park, from 4.30pm this evening, before converging on Government Buildings. Stock image Today's News in 90 Seconds - Thursday, November 27 It marks a second round of protests following demonstrations outside Leinster House last Saturday, with today’s protests sparked by the reported failure of the Government to meet a talks ultimatum by Monday evening. The company says it will reduce “meter anxiety”, and passengers will still pay the lower amount if the meter comes in under the fixed price. Taxi drivers are also asking Dublin Airport to stop facilitating Uber at the airport, saying that the introduction of fixed fares will have a “detrimental” impact on their livelihoods. Driver representatives say the model leaves many earning less during heavy traffic and have warned that further action is likely unless the issue is addressed. Taxi drivers fear other app operators will follow Uber’s lead in introducing fixed fares, ultimately weakening the role of the meter and increasing dependency on platform-controlled pricing. The NTA has been criticised by drivers for not intervening in the dispute to date. The authority has previously said that taxi fares are set under its regulatory framework and that operators must comply with those rules. The NTA has been contacted for comment. Ireland will convey its “steadfast support” and “unwavering commitment” to Ukraine against Russia’s illegal war, when its president, Volodymyr Zelensky, visits Dublin tomorrow.
--------------------------------------------------

Title: Uber Technologies (UBER) Partners With Starship Technologies For Robot Deliveries in The UK
URL: https://finance.yahoo.com/news/uber-technologies-uber-partners-starship-105202171.html
Time Published: 2025-11-27T10:52:02Z
Description: ​Uber Technologies, Inc. (NYSE:UBER) is one of the Best Very Cheap Stocks to Invest In. On November 20, Reuters reported a strategic partnership between Uber...
--------------------------------------------------

Title: Is Uber Technologies Stock Outperforming the S&P 500?
URL: https://www.barchart.com/story/news/36338560/is-uber-technologies-stock-outperforming-the-s-p-500
Time Published: 2025-11-27T06:58:30Z
Description: With Uber Technologies outperforming the S&P 500 over the past year, analysts continue to hold a decidedly bullish view on the company’s future prospects.
--------------------------------------------------

Title: Bank of America Securities Reiterates a Buy Rating on Uber Technologies (UBER), Sets a $119 PT
URL: https://finance.yahoo.com/news/bank-america-securities-reiterates-buy-203037238.html
Time Published: 2025-11-26T20:30:37Z
Description: Uber Technologies, Inc. (NYSE:UBER) is one of the best large cap stocks to invest in for the long term. On November 23, Bank of America Securities analyst...
--------------------------------------------------

Title: ‘These Chips Will Profoundly Change the World’ and ‘Save Lives.’ Elon Musk Doubles Down on AI Chips as TSLA Stock Stagnates YTD.
URL: https://www.barchart.com/story/news/36329726/these-chips-will-profoundly-change-the-world-and-save-lives-elon-musk-doubles-down-on-ai-chips-as-tsla-stock-stagnates-ytd
Time Published: 2025-11-26T16:32:45Z
Description: Elon Musk posted on social media that the company has bold visions for its AI chips and is planning to build at higher volumes than all other AI chip...
--------------------------------------------------