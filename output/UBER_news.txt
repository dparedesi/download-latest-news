List of news related to Uber stock price UBER:

Title: Prices in the Machine
URL: http://prospect.org/2025/12/02/prices-in-the-machine-ai/
Time Published: 2025-12-02T10:30:00Z
Full Content:
This article appears in the December 2025 issue of The American Prospect magazine. Subscribe here. Earlier this year, a slightly balding man in spectacles, a black T-shirt, and bright high-top sneakers gave a presentation about how his computer can predict what you want to buy. His name is Dr. Uri Yerushalmi, co-founder and chief artificial intelligence officer of Fetcherr, an Israeli-based pricing consultant used by a half dozen airlines around the world. “While most of us in the AI community have been focusing on building models that are generating either text or image, in Fetcherr we have been focusing during the last five years on building large market models,” Yerushalmi said. Trained on years of market data—prices, orders, competitors, regulation, stock prices, even the weather—Fetcherr’s “business agents” aim to simulate market dynamics, assist pricing analysts, and even automatically price tickets. More from David Dayen One presentation slide stuck out. It showed price fluctuations before the use of Fetcherr’s system and after. In the before graph, prices are relatively static and straight. After Fetcherr, the jagged lines pulse with staccato rhythms. “The dynamic is much more similar to dynamics in NASDAQ or capital markets where the prices change much more frequently,” said Yerushalmi, “because every time something in the market changes, there is an immediate response.” Yerushalmi once ran AI for a high-speed stock trading firm. He approaches pricing like a science experiment, engaging in constant real-time testing and tweaking to maximize corporate profits. Fetcherr boasts of delivering annual “revenue uplift” of over 10 percent. The guinea pigs for these tests, the ones being separated from their cash, are you and me. AI can depict you as an anime character. It can respond half-intelligently to questions about the Franco-Prussian War or concentrations of sulfur in the upper atmosphere. It can delight and distract and maybe help you get work done. But none of that is as prized by corporate America as its data-driven approach to the previously conjectural world of pricing. Supply and demand no longer solely determines price, as in textbook economics. “That’s the use case they don’t want you to talk about. That’s why we’re building all these data centers,” said Lee Hepner, senior legal counsel for the American Economic Liberties Project. “As we have built social media platforms that shape the flow of information across society, now we are building the platforms that control the flow of money.” Technology-fueled pricing is more widespread than once thought, presenting serious policy challenges in an age where affordability is on everyone’s minds. AI bots are colluding with one another, anticipating consumer choices, and accumulating surplus—that is, transferring wealth—for the businesses that employ them. It’s part of why corporate profits hit record highs after the pandemic and have stayed there. The public recoils at the thought of seeing prices tick up based on when they like to get lunch or what device they’re using. When Delta, which employs Fetcherr as a pricing agent, announced on an earnings call this summer that up to 20 percent of its flights would be priced using AI by the end of the year, the outcry was so intense that the company claimed its critics were peddling “misinformation.” Policymakers and advocates have united to crack down on some tactics, winning real legislative victories from coast to coast. But every step government takes can be countered by AI price-setters. How can consumers keep up? “What worries me is that it will become hyperefficient to price discriminate and to charge more in ways that ordinary people will not be able to combat,” said Doha Mekki, the top deputy at the Justice Department’s Antitrust Division under Joe Biden. “And it will make a very bad cost-of-living crisis even worse.” A YEAR AGO, I WROTE ABOUT WHAT I TERMED surveillance pricing: companies offering individualized prices based on personal data. The idea was that a business equipped with granular information about demographics, purchase history, social and financial interactions, or even medical status could exploit a customer’s willingness to pay. Uber could charge more when a rider booked on a company credit card; people aren’t as price-sensitive when someone else is paying. Delta could jack up fares after learning that a traveler needs to attend a funeral; desperation could translate into opportunity. I couldn’t be certain how many businesses used surveillance pricing. The proliferation of third-party pricing consultants touting “digital pricing transformations” seemed like a strong indicator. But concealment was key to the strategy, to minimize the anger generated by charging different prices to different people. For instance, when a reporter at SFGate logged in to hotel booking platforms using his regular IP address in high-wage San Francisco, he received a quote of $829 a night. But when he used a virtual private network set up to originate from Phoenix or Kansas City, prices were more than $500 less. Without a public price, it takes research to understand if someone is being ripped off. A month after my report, Lina Khan’s Federal Trade Commission announced an investigation into surveillance pricing, seeking information from eight third-party consultants. The agency only had a short time to conduct the study before the changeover of power in Washington. But days before Donald Trump’s inauguration, it issued “research summaries” of the work done thus far. (Trump’s FTC has yet to finish the study.) Stephanie Nguyen, the FTC’s chief technologist under Khan, told me those eight pricing consultants were working with over 250 clients, suggesting broad reach across the economy. Services included individually targeted pricing, segmentation of customers based on their profiles, and ranking tools that alter what products people see atop a web page or search. A customer clicking on fast shipping, for example, suggests an urgency that would lead them to tolerate higher prices. The key to surveillance pricing is data. Nguyen and Sam Levine, the FTC’s former head of the Bureau of Consumer Protection, recently wrote a paper about how customers deliver that data when they sign up for loyalty programs. Enticed by promised discounts and concierge treatment, customers consent to data collection that allows companies to build intricate social graphs; one customer profile created by the grocery chain Kroger stretched to 62 pages. The discounts often aren’t maintained or are curtailed, loyalty card fees expand over time, and the more loyal a customer, the more data is collected and the more they pay over time, according to the report. One case study is the McDonald’s app, which has 185 million users and provides seamless access on smartphones, our personal data-spewing machines. According to a recent earnings call, after downloading the app, a customer goes from 10.5 annual McDonald’s visits on average to 26. McDonald’s clearly wants its customers on the app. Its popular Monopoly game gives out stickers on the packaging of items like Big Macs and fries; you collect the right ones to win big prizes. But this year, instead of filling out a physical game board, Monopoly pieces now must be scanned into the app. “This is about taxing people who don’t turn over their data, and manipulating people who do,” Levine said. “Do we really want a world in which you will have to pay a premium if you want to shop anonymously?” Surveillance pricing is just one technology that companies use to set prices. Surge pricing accelerates supply and demand to raise prices when more people want the product or service. Subscription pricing offers products for a monthly fee, a steady stream of revenue from absent-minded customers that can feature deceptive sign-ups and impossible cancellation policies. Trump’s FTC sued Uber for a subscription service that takes as many as 23 screens and 32 actions to cancel; Amazon Prime’s cancellation policy, the subject of a separate FTC settlement that cost the company $2.5 billion, was internally nicknamed “Iliad Flow,” like the tortuous war Odysseus waged in the Greek epic. Electronic shelf tags at Walmart and other grocers have triggered fears of endlessly changing prices while you shop. FIFA has confirmed use of differential pricing for World Cup events based on demand, something New York City Mayor-elect Zohran Mamdani has criticized. “They can become a tool for pure extraction and leave everybody paying what they shouldn’t,” said Kevin Erickson of the Future of Music Coalition, which monitors event ticketing. One of the most insidious uses of technology is on the wage side. An issue brief from the Washington Center for Equitable Growth focused on how companies are using personal data to set worker pay, something first seen in the ride-hailing industry. Twenty AI consultants offer surveillance wage services to clients in logistics, manufacturing, retail, finance, education, transportation, technology, and health care, which is “the new Uber in a lot of ways,” said Veena Dubal, a law professor at the University of California and co-author of the report. The AI systems can adjust worker pay in real time, and offer different wages to people doing the same work. They can adjust wages based on performance data, customer feedback, and even factors off the job, such as “predictive analytics that attempt to determine a worker’s potential tolerance for low pay,” the report notes. Most of these wage-setting practices are opaque to workers or their bargaining representatives. “This is becoming normalized,” Dubal said. “It fundamentally undermines the well-worn and very American idea that hard work should result in higher pay.” The result of these strategies is that supply and demand no longer solely determines price, as in textbook economics. AI-based pricing has become more critical than sale volume or product quality. Customers seeking a fair deal are simply outworked, unable to avoid being targeted. “There used to be moments where you really blew it, you had to buy a last-minute airline ticket and they’ve got you,” said Tim Wu, former competition policy chief in the Biden White House. “That’s really daily life now.” Consumers used to benefit from what economists would call imperfect information. The uncertainty of defining the optimal price, and the ability in open markets for new businesses to undercut competitors, gave consumers a fighting chance to get a deal, or just to manage their life without being tracked and prodded. Technology eliminates that information inefficiency. “What these technologies are about is eliminating all risk for the shareholder,” Hepner said. “There’s no more error. It is a well-oiled extraction machine.” EVEN ADVOCATES HAVE BEEN SURPRISED by the furiousness of the response to technology-aided pricing. As post-pandemic inflation swelled, AI trickery was a tangible, easy-to-understand depiction of an economy rigged against ordinary people, and the lack of transparency and unpredictability vexed consumers. “It was hiding in plain sight and everyone has found it and you just can’t unsee it,” said Lindsay Owens, executive director of the Groundwork Collaborative. Politicians looking for anti-inflation messaging rode the wave of constituent anger. Sen. Ruben Gallego (D-AZ) challenged Delta’s palpable glee over surveillance pricing in an earnings call, and when Delta responded that they wouldn’t actually target customers using their personal data, Gallego didn’t let up. After all, Delta’s president Glen Hauenstein said on the earnings call that “to get [travelers] the right offer in your hand at the right time” is the “Holy Grail,” which sure sounds like surveillance pricing. “Delta is telling their investors one thing, and then turning around and telling the public another,” Gallego said in a statement. House Democrats proposed a bill to ban surveillance pricing and surveillance wage-setting. So-called “drip pricing,” where junk fees are added during a sale, was effectively banned in event ticketing and hotel stays by an FTC rule finalized in May. But with GOP control of Washington, the real action has moved to the states. Surveillance pricing bans were introduced in California, Colorado, Georgia, Illinois, and Minnesota; surge pricing bans for grocery stores and restaurants were introduced in New York and Maine. By July, 24 states had seen bills introduced over some form of technology-aided pricing, according to a tracker from Consumer Reports. In a major win this year, the nation’s two largest blue states explicitly banned algorithmic price-fixing, a tactic famously used by RealPage, which aggregated data from landlords across a city and recommended coordinated rent hikes. Joe Biden’s Justice Department sued RealPage, reaching settlements with corporate landlords vowing to end the practice. But data aggregators like Agri Stats and PotatoTrac already do this for meat and produce, and states wanted to prevent tech-enabled collusion from expanding. Subscribe for analysis that goes beyond the noise. Advocates were pushing on an open door; housing is the biggest monthly expense people have. “When RealPage came out, it quickly mapped onto something people feel viscerally,” said Samantha Gordon, chief advocacy officer at TechEquity, which lobbied on the California tech pricing bills. Moreover, since price-fixing is illegal when human beings enter a smoke-filled room and collude, extending that to algorithmic collusion made sense to lawmakers. That was the concept behind California’s AB 325, which states that “common pricing algorithms,” whether informed through public or nonpublic competitor data, violate the Cartwright Act, California’s antitrust law. The law lowers the evidence standard to prove an algorithmic pricing conspiracy. It also prohibits anyone from distributing a pricing tool that coerces the setting of a recommended price, protecting small businesses along with consumers, explained Hepner, who was active in drafting the bill. “We saw mom-and-pop businesses swept up into these pricing schemes,” he said. “If you want to get the buy box on Amazon, you have to use their smart pricing tool. You have to give up independent decision-making authority to participate in the economy.” After seven rounds of amendments, AB 325 passed and Gov. Gavin Newsom (D-CA) signed it in October. Days later, Gov. Kathy Hochul (D-NY) signed S.7882, another prohibition on coordinated price-fixing via software, though limited to rental housing. The New York law also blocks landlords from using algorithms to set “lease renewal terms, ideal occupancy levels, or other lease terms and conditions.” Cities like Jersey City, Philadelphia, Minneapolis, and Seattle have enacted similar citywide collusion bans, and 51 algorithmic price-fixing bills were introduced nationwide in just the 2025 legislative session. It’s “a potentially very powerful first step in tilting some of this economy back in favor of consumers,” Hepner said. The best way to get at surveillance pricing might not be to go after the data, but rather those who receive, share, and process it. Surveillance pricing bills have not seen the same success, though their failure revealed critical information. In California, AB 446, which would have banned pricing based on personal data, was held back, giving sponsors time to build support next year. That’s because businesses across the economy, even ones not suspected of surveillance pricing, swarmed Sacramento to defend their practices. “It was fascinating over the course of that legislative process to see the opposition come out and openly say, ‘Hey, we’re actually doing this, you mean we can’t do this anymore?’” Hepner told me. Some economists allege that tech pricing crackdowns threaten discounts and other potential benefits for consumers. But it’s hard to believe that companies would hire the most sophisticated engineers to figure out how to reduce their revenues. As Nguyen told me, the argument to preserve discounts boils down to this: “You can have privacy or low prices, but you can’t have both.” A modest breakthrough came in New York, which passed a disclosure law requiring that relevant transactions include a pop-up stating: “This price was set by an algorithm using your personal data.” The National Retail Federation sued to block the law, but in October, a federal judge dismissed the case, ruling that the disclaimer “is plainly factual.” Advocates believe that the legal victory will give policymakers permission to revisit the topic. But the fierce pushback showed that AI-based pricing was no longer a speculative harm. “So many companies and industries trying to kill [surveillance pricing bans] told us everything we need to know on why everything is getting more and more expensive,” Gordon concluded. RealPage got the message as well. As the price-fixing bills worked their way through statehouses, the company announced it was acquiring Livble, a property management software tool. With Livble integrated into their platform, RealPage said, residents could subdivide rental payments into installments based on real-time cash flow landlords can see. In other words, RealPage was trying out surveillance pricing. THESE TECHNOLOGICAL SHIFTS FROM ONE SCHEME to the next make it difficult for policymakers to stay ahead of the curve. For example, Lyft, one of the early adopters of surge pricing, reacted to rider anger about the service being more expensive precisely when they needed it most by moving to something called Price Lock, a subscription-based pricing tool that caps fares on targeted routes—for a monthly fee. Another tactic involves redefining basic terms. When Delta claimed it would not use personal data, it added that it was merely “enhanc[ing] our existing fare pricing processes” with AI. “The lack of regulations about this and the way people’s data can be collected and used allows them to play around with what they are calling personal data and use,” said Ben Winters, director of AI and privacy at the Consumer Federation of America. This forces policymakers to think more broadly, even on the outer edges of the possible. “I do worry that we’re always going to be chasing the companies,” said Sam Levine. “I think companies need to advertise a price publicly that’s available to all consumers … there should be a basic principle to advertise a price and it’s deceptive to charge more than that.” Levine’s testimony to Congress this summer hits on another option: limiting collection of personal data, rather than just its usage. “‘Surveillance pricing’ is only possible because of how companies collect, share, and weaponize our personal data,” he told lawmakers, while arguing that relying on outdated tools to safeguard privacy risked further abuse. Consumer protection and anti-monopoly experts are warming to this idea of cutting the data off at the source. Companies, after all, provide a full road map of what they collect in the privacy policies almost nobody reads. Levine and Nguyen’s loyalty program paper documents privacy policies, finding that rental car giant Hertz admits to collecting demographic and behavioral data, Home Depot tracks Wi-Fi usage inside their stores, and Macy’s captures customer driver’s licenses. Much of this data is sold to third-party data brokers, who share it with other businesses. A data minimization standard could prevent use of personal information for pricing, Winters suggested. Mekki added that antitrust law recognizes that antitrust remedies often call for disgorgement of whatever gives companies an unfair advantage in the marketplace. Applied to surveillance pricing, that would argue for destroying the data allowing them to price-set. “If you can be forced to give up money or property, it stands to reason that you need to give up data,” she said. The bipartisan American Privacy Rights Act introduced last year would have limited collection of certain kinds of data and given people the ability to opt out of having their data used to target them or sold to third parties. But this is at least the third major comprehensive privacy bill in the internet era that’s gone nowhere in Congress; too many companies are invested in the surveillance economy to let it be legislated out of existence. “This is a textbook example of people not getting what they want,” said Wu, whose recent book The Age of Extraction highlights Big Tech platform-ization multiplying across the economy. “Is there anyone in America who wants less privacy or thinks it’s fine, or really wants targeted ads? The constituency is zero … In 20 years of privacy laws, there hasn’t been a single vote. This is not even a conspiracy. The tech industry lobbyists specialize in one thing: killing privacy laws.” Some states have fared better, with a dozen passing limitations on data collection or letting consumers opt out of targeted ads or sale of personal data. California updated its policy this legislative session by passing AB 566, which allows internet users to opt out of data collection at the browser level, rather than having to go website by website. But practically all privacy legislation has carve-outs for “bona fide loyalty programs,” giving companies an escape hatch under the guise of offering discounts. Even given that, however, loyalty programs are not exempt from “excessive” collection under these statutes that is not reasonably necessary. Maryland recently finalized a privacy law that prevents companies from conditioning loyalty programs on the sale of data, despite industry pushback. The best way to get at this activity might not be to go after the data, but rather those who receive, share, and process it. That includes companies acting as data brokers by selling and trading data, and the third-party pricing consultants they share the data with. Data sales could be considered unfair or deceptive under state consumer protection laws. And even when customers consent to having their data collected, they may not be consenting to transferring that data to third parties. “State enforcers should be thinking, under what circumstances is it lawful for companies to share personal data with pricing consultants?” Levine said. Under Lina Khan, the FTC made headway by looking at these agglomerations of data as unfair practices, and subsequently prohibiting the sale of sensitive data, like when someone visits an abortion clinic. Today’s FTC has walked away from this enforcement, but their past work outlines a path for states to argue that targeted pricing is similarly unfair. AI PRICING WON’T LOOK THE SAME IN FIVE YEARS as it does now, but we have hints of where it’s going. That’s why the role of Fetcherr is more interesting than Delta’s meandering explanations for how it uses AI. Airlines have been at the forefront of every pricing innovation of the past 30 years, from junk fees to differential pricing to loyalty programs. What’s the next frontier? Fetcherr, which closed $90 million in funding in 2024 and another $42 million this year, tells you right on its website: “hyper-personalization at scale.” (Or at least, it used to tell you that, before it scrubbed this section.) “We’re talking about understanding each customer as an individual, optimizing every interaction for maximum value,” the now-redacted section states, detailing how its AI model uses data points like “customer lifetime value, past purchase behaviors, and the real-time context of each booking inquiry” to give each passenger their own product bundle at a bespoke price. The goal isn’t just a pleasant customer experience—it’s “revenue growth.” Robby Nissan, a Fetcherr co-founder, has said publicly that its AI systems can “manipulat[e] the market in order to gain more profit.” There’s not a lot of mystery here. There are other airline pricing consultants, like Peter Thiel–backed FLYR and PROS and Air Price IQ. But the pitch is the same: maximizing travelers’ willingness to pay by analyzing reams of data. If you want to see the dystopia of perfect information accurately predicting exactly how much you’ll pay for a service, just book a flight this Christmas. “It’s a very real example of how AI-based pricing schemes put consumers into economic silos,” Hepner said. “You cannot comparison shop anymore. You cannot predict what a price is supposed to be anymore.” This is facilitated by a concentrated economy where markets aren’t as open to alternatives to AI-optimized schemes. But AI pricing may shrink choice even further. For example, Delta recently stopped flying from LAX to London Heathrow Airport, despite the popularity of the route. Virgin Atlantic still makes the LAX-Heathrow trip; it’s a joint venture partner with Delta, and perhaps as important, both of them use Fetcherr. Was canceling the route an AI-enabled suggestion to benefit another client? We will never know. Large market models running millions of price tests with thousands of input signals could short-circuit restrictions on surveillance pricing or data minimization. If it’s all just simulations, after all, is it really using personal data? Another future is glimpsed in the rise of AI agents that shop for you. ChatGPT users will soon be able to link their PayPal accounts for instant purchases based on chatbot recommendations. Walmart has partnered with OpenAI for e-commerce as well. Autonomous bots with a linked bank account could even shop on their own, something a new AI agent lobby is trying to facilitate in Washington. There’s little transparency on how products are recommended, how prices are derived, or whether AI agents will act in the interest of consumers or the retailers they’re partnering with. “You’re delegating purchasing power to one algorithm to interface with another,” said Ben Winters. “You’re getting people signed up into these systems where they have no control.” But there’s also a brighter future possible, one where the backlash to nonstop data collection and pricing subterfuge accelerates, and people simply demand a fair price. Polling shows that the public is deeply skeptical that AI will work in their favor. Consumers may not have much control, but they do hold the dollars companies need to thrive, and they can withhold them from companies that treat them poorly. “I think it’s possible that this is headed back to cost-based pricing,” said Owens, whose book Gouged about the new era of pricing releases next year. “It’s the way companies priced for decades … There is a world where the outcome is transparent, public, predictable pricing.” First, I want to thank you for reading the Prospect. At the end of the day, you’re helping build a more just and fair world by taking the time to read independent and fearless journalism. We hope that when you read our work it isn’t just idle entertainment or cynical ragebait, but something that informs and inspires you. If you agree with that, and I hope you do, we’re asking that you take the next step and consider supporting our work. We don’t have corporate sponsors or billionaire backers – we have you, our readers. A significant portion of the money it takes to send reporters into the field to write the hard-hitting stories you expect from the Prospect comes from readers just like you. Can you chip in today? Your support funds deeply reported investigations into power, how it works, and what it means for you. Mitchell GrummonPublisher This article appears in Dec 2025 Issue. David Dayen is the executive editor of The American Prospect. He is the author of Monopolized: Life in the Age of Corporate Power (2020) and Chain of Title: How Three Ordinary Americans Uncovered... More by David Dayen
--------------------------------------------------

Title: How Campbell’s leaked audio turned a pantry staple into a PR crisis
URL: https://fortune.com/article/campbell-leaked-audio-pr-crisis-comms-soup-meat/
Time Published: 2025-12-01T12:17:05Z
Full Content:
Ruth Umoh is the Next to Lead editor at Fortune, covering the next generation of C-Suite leaders. She also authors Fortune’s Next to Lead newsletter. Just as families across the U.S. were heading into Thanksgiving weekend and stocking up on Campbell’s staples, a leaked audio recording landed online of a company vice president mocking Indian workers, calling customers “poor people,” and claiming its soups contained “bioengineered” or “3D” meat. The timing was combustible. A brand associated with holiday comfort was suddenly overshadowed by a senior leader’s contemptuous voice, directed at the very people who buy and make its products. Campbell confirmed the recording’s believed authenticity and removed the executive from his role. Several elements of the story emerged through a lawsuit filed by cybersecurity analyst Robert Garza, who alleges he told his manager about the comments in January, was discouraged from reporting them, and was later terminated. Campbell disputes that timeline, saying it first learned of the lawsuit months later and heard only “segments” of the audio on November 20. Regardless of how those details are adjudicated, once the recording went public, the question shifted from one executive’s conduct to the organization’s ability to confront issues before they escalate. These alleged events surfaced three essential truths about leadership that extend far beyond this single episode. The blowback was swift because Campbell is not merely a food manufacturer. Its products occupy a symbolic place on holiday tables, and the remarks felt like a direct insult to the households that rely on them. Consumers interpreted the comments as a reflection of how a leader with influence viewed the people who sustain the brand. Culture is judged in moments like this—not by corporate messaging but by private conduct that contradicts it. The allegations about internal handling reveal a more profound leadership truth: Trust within companies is fragile. Reporting systems work only when employees believe their concerns will be taken seriously rather than met with indifference or retaliation. When people think the organization is unwilling or unable to act, the crisis becomes bigger than the initial misconduct. The episode also highlights the modern fact that private comments are never entirely private. Power attracts scrutiny, and the tools to record unguarded moments are everywhere. The answer is to ensure that even one’s private behavior is consistent with the values the company claims to uphold. The final takeaway: What happened at Campbell ultimately reinforces why leadership is judged at its edges. The quiet moments and the offhand comments reveal far more about a leader’s integrity and C-suite readiness than any formal performance review ever could. Editor’s note: The deadline to apply for the Fortune Next to Lead list is today, Monday, Dec.1, 2025. For more information or to submit a nomination, apply here. Ruth Umohruth.umoh@fortune.com Feedback engine. How Xbox is turning its loudest fans into a roadmap for its biggest transformation yet Mood economics. Harvard professor says leaders have a responsibility to be happy at work because it can affect your stock price Soda supremacy. How Pepsi trounced Coca-Cola in the Middle East Stress shield. The youngest self-made billionaire hasn’t rested in 3 years but says this keeps burnout at bay Jim Farley, Ford CEO, on why he pushed to raise pay after hearing how younger workers are struggling to make ends meet: “None of the young people want to work here…you pay $17 an hour, and they are so stressed.” DoorDash has grown into a $13 billion giant and is now twice the size of its next rival, Uber Eats. Fortune Trump commuted a 7-year prison sentence for a private-equity executive convicted in a scheme to defraud more than 10,000 investors. Fortune "PayPal Mafia" luminary David Sacks is reportedly using his role in the Trump administration to advance AI and crypto policies that benefit his Silicon Valley allies and his own investments. NYT NATO is exploring a more assertive posture against Russia’s hybrid warfare, while Ukraine described its latest peace plan talks with the U.S. as challenging but productive. FT Trump said he’s selected the next Fed chair but isn’t naming the pick and signaled that he expects rate cuts. Bloomberg Northwestern Mutual's CEO said the touted $124 trillion Great Wealth Transfer will be slower and more complicated than many expect. Fortune Looking attractive is fast becoming an unofficial part of the job description. BI © 2025 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.
--------------------------------------------------

Title: DoorDash CEO Tony Xu outmaneuvered meal delivery rivals by obsessing over his customer
URL: https://fortune.com/2025/12/01/doordash-ceo-tony-xu-delivery-wars-obsessing-over-customer/
Time Published: 2025-12-01T10:39:57Z
Full Content:
Good morning. If Americans treated their post-Thanksgiving Day exhaustion with take-out this weekend, there’s a good chance they ordered from DoorDash. The meal delivery giant now controls 60% of the market in the U.S and is more than twice the size of its closest competitor, Uber Eats. It wasn’t always that way. In a new Fortune feature, tech correspondent Jason Del Rey rode along with DoorDash CEO Tony Xu on a delivery run and got a front-seat look at how Xu built the scrappy upstart into an unexpected powerhouse in the cutthroat meal delivery industry. The startup founded in 2013 by Xu and three fellow Stanford students once had less than $30,000 left in its bank account. Now, thanks to grit, ingenuity, and a heaping serving of luck, it’s on track to generate more than $13 billion this year. Here a few lessons other leaders can learn from DoorDash and CEO Xu:Zig when others zag: Early on, DoorDash distinguished itself in the crowded meal delivery space by enlisting gig workers to pick up—and sometimes even order—food, whereas then-rivals Grubhub and Seamless only partnered with restaurants that had their own delivery drivers. Obsess over your customer: Xu prides himself on sweating the small stuff to improve service. For instance, DoorDash now highlights desserts on orders because they are the most likely to be forgotten. Its app also advises “Dashers” on where to park and which building entrance to use. Stay in touch with the front-line experience: Every corporate DoorDash employee in the U.S. must do four delivery shifts a year. On Jason’s ride-along, CEO Xu attempted to deliver four orders in San Francisco. He managed to drop off three and earned $19 for the hour’s worth of work. (DoorDash would top up that amount.) Xu’s hands-on experience stands out in a world in which the ultra-wealthy are living increasingly private lives. Strike when there’s blood: When Uber Eats’ parent was rehabilitating its culture and imposing financial discipline in the late 2010s, DoorDash went on a spending blitz. It poached several UberEats executives and expanded from 1,500 locations to 6,000, including into mid-tier cities and suburban towns that rivals had neglected. Be ready when opportunity hits: DoorDash’s investment in the suburbs paid off when the COVID pandemic pushed convenience-obsessed consumers out of cities and turned sit-down restaurants on to delivery. DoorDash’s business more than tripled in 2020. Jason cautions that DoorDash’s lead isn’t safe. It could “be displaced by a competitor like Uber—or an AI-native company that may not yet even exist.” But for now it’s won the meal delivery wars and is moving into new markets, like grocery delivery, where the battle for dominance may be just as fierce. You can read the full feature here. —Claire ZillmanContact CEO Daily via Diane Brady at diane.brady@fortune.com Trump to announce Fed chair pick U.S. President Donald Trump says he’s decided who will succeed Jerome Powell as Fed chair. “We’ll be announcing it,” he said Sunday. White House National Economic Council Director Kevin Hassett is reportedly the frontrunner for the job but declined to address whether he was the top candidate in an interview Sunday. Gen Z penny-pinching Gen Z shoppers, those in their teens to early 20s, plan on spending less this holiday season as economic uncertainty takes hold. A Deloitte survey shows they plan to cut spending 34% this year, steeper than any other cohort. Only Gen Xers, aged 45 to 60, plan to spend more. Frozen salaries Major consulting firms like McKinsey and Boston Consulting Group have frozen starting salaries for the third straight year as AI imposes more cautious hiring practices in the industry. AI is making junior employees more productive, putting downward pressure on their pay. Smartphone risks for kids A new study in the journal Pediatrics finds that children who had a smartphone by age 12 were at higher risk of depression, obesity, and inadequate sleep compared to their phoneless peers. The research included 10,500 kids in the U.S., making it the largest long-term look at children’s brain development in the country to date. AI job displacement An MIT study published last week found that nearly 12% of the U.S. workforce could already be replaced by AI systems. The report notably found that AI models can complete tasks in finance and health care, not just in technology or coding. Great Wealth Transfer won’t be a ‘big bang’ Northwestern Mutual CEO Tim Gerend told Fortune that the Great Wealth Transfer between Baby Boomers and younger generations won’t be a “big bang” like many expect but instead will come gradually. For young people who are “really anxious” about the economy, per Gerend, waiting for that inheritance only adds to their unease. S&P 500 futures are down 0.55% this morning. The last session closed up 0.54%. STOXX Europe 600 was down 0.44% in early trading. The U.K.’s FTSE 100 was up 0.03% in earning trading. Japan’s Nikkei 225 was down 1.89%. China’s CSI 300 was up 1.10%. The South Korea KOSPI was down 0.16%. India’s NIFTY 50 is down 0.10%. Bitcoin was down at $87K. Ford workers told their CEO ‘none of the young people want to work here.’ So Jim Farley took a page out of the founder’s playbook by Sasha Rogelberg Harvard professor says leaders have a responsibility to be happy at work because it can affect your stock price by Dave Smith From Martha Stewart to Dockers to Toys “R” Us, brand managers are raking in billions betting on classic American names by Amanda Gerut McDonald’s promoted its new $8 nugget combo meal, then got blasted online with complaints about affordability, quality and service by Nino Paoli CEO Daily is compiled and edited by Joey Abrams and Claire Zillman.
--------------------------------------------------

Title: Sources of America’s Hidden Inflation
URL: http://prospect.org/2025/12/01/sources-of-americas-hidden-inflation/
Time Published: 2025-12-01T10:30:00Z
Full Content:
This article appears in the December 2025 issue of The American Prospect magazine. Subscribe here. Prices have been rising across the economy in ways that are both visible and opaque. There are short-term drivers of inflation due to President Trump’s mismanagement of the economy. But the deeper drivers result from the degradation of capitalism. For example, the lethal combination of digital technology and tech monopolies picks your pocket in countless ways. Instead of technical advances leading to greater convenience and lower cost, as they logically should, they create strategies for opportunistic price hikes. More from Robert Kuttner When Amazon uses its deep knowledge of consumer preferences to rig markets and undermine competitors, higher prices are passed along. When HP makes it illegal or impossible for consumers to use cheaper non-HP cartridges in their printers, it can charge exorbitant prices. If you are prohibited from repairing your own car or your own iPhone, or as a farmer, prohibited from saving seed for next year’s planting, that invites monopoly profits built on higher prices. Costs rise because the rules are rigged. It isn’t just the increasing cost of health insurance, but the tax on your time when a health system of byzantine complexity requires you to waste hours to get a simple referral or get a claim paid. Middlemen and algorithms, both in the business of denying claims, are a direct cost to the system and a source of rising out-of-pocket prices to patients. If insurance doesn’t pay, you do. These middlemen also function as a drain on doctor time and thus a tax on doctors’ incomes, as well as a debasement of medical services In this special issue of the Prospect, we take stock of several hidden drivers of rising costs. David Dayen explores all the ways that technology allows sellers of any product that uses the internet to take advantage of surveillance capitalism to personalize prices and charge more than the market price that would be produced by ordinary supply and demand. It’s another case of monopoly pricing power, facilitated by invasive technology. The remedy for opportunistic price hikes in so many sectors is the resurrection of antitrust. As Emma Janssen writes, plutocracy is a driver of the affordability crisis. The top 10 percent of income earners now do close to half of the spending, by some estimates. This has led to “premiumization,” where airline seats, concert tickets, and even staple goods are priced for wealthier people because they are the primary buyers. That in turn drives prices higher for everyone else, as anyone who buys a ticket to a concert or a sporting event appreciates. Antitrust policy has thus far failed to curb obvious abuses by middlemen ticket sellers. Paul Starr writes in a sidebar that these trends invert the 20th-century pattern of affordable culture, beginning with nickel movie tickets and cheap bleacher seats. As the concert ticket example shows, middlemen are drivers of the affordability crisis. As Whitney Curry Wimbish writes, across the economy, from pharmaceuticals to real estate to meatpacking and more, middlemen sit in between suppliers and customers and squeeze out a layer of profit for themselves, taking advantage of their knowledge of industries and typically concentrated control of the supply chain. Regulatory capture, as James Baratta points out, is another source of price increases. Baratta’s focus is on electricity rates, and the failure of state public utility commissions to protect ratepayers from extractive price hikes based on dubious economic models used by utility companies. There is an even more subtle and insidious version of regulatory capture, in which regulations themselves are used to enforce industry’s anti-competitive strategies. As Cory Doctorow observes in his indispensable book Enshittification (reviewed in our October issue), far more sinister than traditional regulatory capture is regulatory fusion, in which industry designs laws and rules to thwart competition and raise profits. Using provisions of the industry-written 1998 Digital Millennium Copyright Act, Apple makes it a copyright violation for technicians to use non-Apple parts for most repairs to Apple products. The preposterous doctrine is that if an inferior non-Apple part caused the product to fail, that could reduce consumer confidence in Apple’s brand. There is even a name for this doctrine: tarnishment. TRUMP’S CONTRIBUTION TO RISING PRICES adds to these long-term abuses. Eric Winograd, a senior vice president at AllianceBernstein, calculates that Trump’s tariffs have added about one percentage point to the core inflation rate, though other estimates that account for Trump’s numerous exemptions of various products and industries put the number at 0.6 to 0.7 percent. But tariffs are only the beginning of Trump’s impact on prices. The remedy for opportunistic price hikes in so many sectors is the resurrection of antitrust. Internet innovators grew from purveyors of ingenuity and user convenience into extractive monopolies largely because antitrust enforcement ceased to exist from late in the Carter administration until Joe Biden’s appointments of some of the smartest students of abuses and their remedies: Lina Khan at the FTC, Jonathan Kanter at the Justice Department, Rohit Chopra at the CFPB, and Tim Wu (whose latest book is reviewed in these pages) as competition policy chief at the White House. These agencies had just gotten serious about reversing four decades of corporate market power when the antitrust revolution was reversed by Trump. As antitrust enforcement has been aborted, private equity companies have been buying up traditionally local firms in order to use market power to raise prices, in sectors as diverse as pest control operations, HVAC companies, and veterinary practices In addition, Trump’s shutting down of Biden’s industrial policies has raised prices. Electric power costs have lately risen far faster than the general rate of inflation. Trump’s policy of killing renewable-energy projects, in order to raise profits for oil and gas companies, contributes to this price rise, by taking away power that might otherwise come onto the grid and alleviate demand. So does the failure to regulate the extensive use of electric power by data centers used by tech companies. There was a time when we needed to shift to solar and wind in order to help address climate change. But now that renewables are cheaper than most fossil fuels, we need to make that shift because it saves money. Instead, Trump promotes more costly oil and gas. It is hard to quantify precisely how much all of Trump’s policies taken together have added to price inflation, but a very conservative estimate would be about two percentage points. That may not sound like a lot, but it is the difference between a baseline inflation rate of 2.5 percent and a rate of 4.5 percent. And that difference is not just a hidden tax on consumers. It has knock-on effects. With inflation at 2.5 percent, the Federal Reserve is willing to reduce interest rates in order to reduce unemployment. But when price increases approach 4 percent, the Fed worries more about inflation. At a time when unemployment rates are rising, policies that raise prices tie the Fed’s hands. This paradoxically could make things more unaffordable for low-income workers, if a loose job market keeps wages from rising. It’s not clear whether there will be any more rate cuts in the near future, and there may even be rate increases. A Fed policy of tighter money means that people pay more for their mortgage loans, their credit card debt, their student loans, and consumers pay more when small businesses pay more for credit. Trump’s cuts in programs such as SNAP and Medicaid are another form of indirect price hikes; even worse, they are price hikes that hit the poor. If your food stamp allotment is reduced, you have to pay more out of pocket in order to eat. If your Medicaid is cut, you pay out of pocket or do without. Subscribe for analysis that goes beyond the noise. The failure to address climate change, and Trump’s reversal of long-standing policies aimed at preserving the environment, also add to costs. As destructive storms become more severe and frequent, the costs of recovery and of homeowner insurance relentlessly rise, some borne by consumers directly and some by taxpayers. This natural-world aspect of hidden inflation is addressed in another of our special articles, by Gabrielle Gurley. SOME YEARS AGO, THERE WAS A DEBATE among economists and policymakers about whether the Consumer Price Index should be adjusted downward to reflect improvements in quality. A 2020 car was simply a better car than a 1980 car, so some of the nominal price inflation actually represented a superior product. Likewise coffee: In 1970, a cup of coffee only cost a dime, but the swill was undrinkable. Coffee today is a lot more expensive, but it is palatable. In practice, the Bureau of Labor Statistics did some minor revisions to adjust for quality improvements. Today, we might need to do the reverse. We are paying more money for products and services whose quality has deteriorated. And in many cases, the deterioration has been deliberate. Indeed, the business model of the tech giants is to systematically degrade the quality of products and services. Products are needlessly complex from the perspective of the consumer, but the complexity adds to the surveillance. Same with tech services. This is the essence of what Doctorow calls enshittification. Doctorow tells the story of how Google deliberately slowed down its search process in order to make more time for its system to festoon the search results with annoying ads that embody everything that Google knows about the user’s buying habits. In conventional economics, wages and prices are two different aspects of the economy. But whether people can afford to live decently is a function of what they earn applied to what they have to pay to live. The same technology that allows tech monopolists to gouge consumers enables them to treat workers like serfs and evade the protections of labor law. As Harold Meyerson writes in his piece, the cost of living is higher because earnings are lower. And earnings are lower because of both the weakening of the labor movement and the sidelining of the National Labor Relations Board. In addition, tech allows employers to double down on the old practice of denying that workers are payroll employees protected by labor laws. From Uber drivers and UPS contractors to chicken farmers and fast-food workers, the company controls and monitors every aspect of the work, but the employer pretends that the workers are independent contractors. This represents a toxic marriage of app technology and market power. The effect is to batter down earnings. And as Naomi Bethune writes, all of the predations tend to hit racial and ethnic minorities harder, because African Americans and Latinos are more economically vulnerable to begin with. Trump’s war on DEI only increases the vulnerability. EVEN APART FROM THE PREDATIONS of extractive and invasive capitalism, other sources of the relentless rise in prices are the result of policy failures that predate the abuses of surveillance capitalism and Donald Trump by decades. Housing is out of reach for so many Americans because America has failed to build enough houses. Houses were cheap after World War II mainly because cheap farmland was being converted to suburbia, with the help of federal highway and mortgage policies. The cheap land is gone, and homebuilder consolidation has distorted markets. Trump’s tariffs have caused a rise in building costs, and his proposal to privatize the secondary mortgage market would make financing more expensive. Housing tax breaks are tilted toward more expensive homes, and toward homeowners over renters. The sheer purchasing power of the oligarchy also bids up prices. Local regulatory barriers also raise costs, but many zoning restrictions also reflect the political power of oligarchs, who don’t want riffraff in their neighborhoods. The bipartisan failure to regulate new financial scams such as subprime mortgages added to the pressure on housing. Millions of homeowners, disproportionately Black and Hispanic, lost their homes in the wake of the subprime collapse. Congress passed legislation to allow for mortgage refinancing for duped homeowners, but the Obama administration failed to carry it out. You might think that scads of vacant and foreclosed houses would lead to lower prices via the law of supply and demand. But the same regulatory failure led private equity companies to swoop in, buy houses, and jack up prices and rents, while making homebuilders reluctant to build the additional units we need. Public policy has also failed to produce an adequate supply of affordable rental housing. The rental housing that we do subsidize is done in wildly inefficient ways, for the convenience and profit of landlords and developers. Our failure to have a true social-housing sector reflects power imbalances between oligarchs and ordinary citizens. The cost of higher education is out of sight because legislators, Republican and Democrat alike, permitted universities to inflate costs via the student loan program, and then piled debt onto generations of students and graduates. State legislatures compounded the damage by reducing direct appropriations to public universities that were free to state residents until a generation ago, substituting tuition and fees. It now costs more to attend a state university than what a private university cost when I was a student. Health care costs also keep rising because the United States has failed to adopt an efficient program of universal public health insurance, as every other major country has done. To compound the damage, policymakers have allowed a plague of for-profit middlemen to take over hospitals, nursing homes, and doctors’ practices. In many hospitals, people of means hire private nurses because the hospital’s own nursing staff is so stretched thin and overworked by cost-cutting aimed at siphoning profits. A hidden price hike, if you can afford it. If the U.S. had a universal nonprofit insurance system, it would save about 6 percent of GDP. That’s $1.2 trillion a year. THERE WAS A TIME WHEN THE ABUSES of capitalism were relatively straightforward and transparent. Large corporations sought to combine into monopolies. The remedy was enactment and enforcement of antitrust laws. Corporations tried to pay their workers as little as possible and to break their unions. The remedy was the Wagner Act, guaranteeing workers the right to organize and join unions. Bankers tried to swindle investors for their own enrichment. The remedy was tougher banking and securities laws, prohibiting conflicts of interest. Corporations tried to cheat consumers with tricky terms and shoddy products. The remedy was the pro-consumer legislation of the 1960s and 1970s. But today’s corporate and financial abuses are hidden in a web of algorithms and apps, reinforced by regulations that actually help tech companies cheat consumers rather than protect them. It takes a complex, deeply knowledgeable, and radical critique of capitalism as currently practiced to get serious about remedies. That does not describe about half of the current Democratic Party, which is on the take from these same industries. One of the many tragedies of Joe Biden was that he was a feeble spokesman for his own good policies. He hired the most knowledgeable people in the country to revive antitrust and apply it to new abuses. But Biden did poorly at narrating the significance of these policies to ordinary Americans coping with rising prices. As I’ve written elsewhere, many of these policies and people reflected the work of Sen. Elizabeth Warren, who was a superb narrator of what they meant and why we needed them. But we ended up with the soul of Elizabeth Warren in the feckless persona of Joe Biden. If we can make the heroic assumption that Trump will not be president forever, and that there will be an opportunity to pick up where Biden and Warren left off, America’s next leaders will need to be even more radical in their understanding and remediation of capitalism, which has become even more debased since Trump took office. First, I want to thank you for reading the Prospect. At the end of the day, you’re helping build a more just and fair world by taking the time to read independent and fearless journalism. We hope that when you read our work it isn’t just idle entertainment or cynical ragebait, but something that informs and inspires you. If you agree with that, and I hope you do, we’re asking that you take the next step and consider supporting our work. We don’t have corporate sponsors or billionaire backers – we have you, our readers. A significant portion of the money it takes to send reporters into the field to write the hard-hitting stories you expect from the Prospect comes from readers just like you. Can you chip in today? Your support funds deeply reported investigations into power, how it works, and what it means for you. Mitchell GrummonPublisher This article appears in Dec 2025 Issue. Robert Kuttner is co-founder and co-editor of The American Prospect, and professor at Brandeis University’s Heller School. His latest book is Going Big: FDR’s Legacy, Biden’s New Deal, and the Struggle... More by Robert Kuttner
--------------------------------------------------

Title: Ready for takeoff? The 19 best Cyber Monday travel deals of 2025 just landed
URL: https://nypost.com/shopping/best-cyber-monday-travel-deals-sales-2025/
Time Published: 2025-11-30T19:27:26Z
Full Content:
Jet brag: The Cyber Monday condition where you casually mention your all-inclusive beach vacation cost less than your friend’s Uber ride to Jersey. If you’ve been vying for a getaway, now is the moment to strike. Cyber Monday travel deals are live, but they’re not sticking around much longer. Major players like Hilton, Skyscanner, and JetBlue are continuing their discounts through Cyber Monday — but many of these offers will close tonight. LELO: Up to 50% OFF at LELO.comVRAI: Major savings on VRAI created diamonds + free giftsTineco: Up to 59% off the brand’s smart cleaning productsJEMMA: 30% off + a free toiletry bagEvery Body Eat: 25% off the brand’s sweet and savory treatsRitual: 40% off sitewide — the best sale of the year!B.T.R. Nation: 35% off sitewide Babbel: Up to 60% off your lifetime subscription!Nutrafol: 25% off sitewide for the first time ever!Whisker Litter-Robot: Up to $150 off bundles & 35% off subscriptionsecoNugenics: 25% off sitewide, including best-selling detox supplements I’m talking flights that won’t bankrupt you, resorts at fantasy-level pricing, and full-package bundles that practically book themselves. If you’ve been waiting for the best Cyber Monday travel deals, your time is just about up. We’ve rounded everything up in one place so you can book fast, because these offers are limited, time-sensitive, and competitive. Cyber Monday Rental Car Deals | Cyber Monday Flight Deals | Cyber Monday All-Inclusive Deals | Cyber Monday Hotel Deals | Cyber Monday Travel Package Deals | Cyber Monday Luggage Deals Rentalcars.com is running up to 30% off Black Friday/Cyber Monday pricing on rentals from Hertz, Enterprise, Budget, Avis, and more. Discounts vary by destination and travel window, and many apply to 2025–2026 bookings. Travel in style with La Compagnie, where every journey feels exclusive. Begin your experience in the private Newark lounge, then step onboard to enjoy fully flat business-class seats, sparkling Champagne at takeoff, and thoughtfully prepared in-flight dining. Offering unbeatable value, La Compagnie delivers elegant, business class round-trip flights to Paris, Nice, and Milan, proving that luxury can be both refined and accessible. I had a chance to fly La Compagnie to Paris, and my experience with this boutique luxury airline felt nothing short of VIP. Book your flight today and discover why La Compagnie is simply the best deal in business-class travel to Europe this Cyber Monday. JetBlue Vacations is still offering up to $450 off select flight + hotel packages with the codes: Travel dates span January 1–October 26, 2026, and deposits start at just $99 per person. Skyscanner’s up to 30% off travel offers are live through Cyber Monday, with rolling fare drops and nightly-updated price listings from hundreds of airlines. As usual, the platform highlights routes that dip below seasonal averages, many of which are still populating live. Lufthansa’s Black Friday sale continues into Cyber Monday with special promotional fares on flights booked this month. These vary by route and sell out quickly, but the deal window remains active for Cyber Monday. Fly to Ireland, the UK and Europe for less using Aer Lingus’s own Flight Finder, which makes finding deals and planning trips extremely easy (you can even input your budget maximum). Or, you can simply scroll the pre-made list of flight deals, such as New York to Dublin between January 23 and 26, 2026, for $348.37, as it’s updated live. Prices are constantly in flux, so lock down any good ones you see ASAP! Sandals’ Cyber Monday Sale (running through Dec. 2 on travel dates that extend through December 25, 2027, giving you massive flexibility) includes: The brand’s extended Black Friday/Cyber Monday event includes up to 50% off all-inclusive stays across Mexico and the Dominican Republic, with rates dipping as low as $100–$172 per night depending on dates and availability. Expedia is running up to 30% off select hotels for Cyber Monday, plus member-exclusive savings on flights and packages. Bookings span much of 2025 and early 2026. Hilton’s holiday sale is live for Cyber Monday with up to 25% off stays across the U.S., Canada, Mexico, the Caribbean, and more. Travel is valid through April 6, 2026. Many properties also qualify for double Hilton Honors points or “Stay Longer in Paradise” deals. Barceló Hotel Group’s Cyber Monday sale touts “up to 55% off” reservations worldwide, including all-inclusive resorts, with added bonus savings for members. Some of these deals include an extra percentage off with a promotional code (found on the sale site) and apply to bookings made now. Marriott is offering up to 20% off room rates at participating MGM Collection with Marriott Bonvoy destinations in Las Vegas when you book between November 4 and November 18, 2025. You can lock in the discounted rate now and stay anytime through October 31, 2026, giving you nearly a full year to use the deal. Just enter code F4639 at checkout to score the perfect deal on a Vegas getaway with plenty of flexibility. Trafalgar’s Cyber Monday 2025 campaign is offering 15% off international land/cruise tours around the world, which is helpful if you’re planning multi-country trips. Plus, if you’ve booked with them before, you get an additional 5% off your booking. On select guided tours, from “Best of Germany” to “Country Roads of Scandinavia,” you can score a BOGO 50% off (or similar global vacation package) deal now through Cyber Monday, giving you twice the value when a second traveler joins. If you want “buy it once” luggage, this 3-piece Omni 2 set is it. The 100% polycarbonate shell has a scratch-resistant texture, the 20/24/28-inch spinners glide on 360° wheels, and all three expand and lock with TSA-approved locks for extra security. Away’s cult-favorite carry-on earns the hype with a lightweight 100% polycarbonate shell, ultra-smooth 360° spinner wheels and an interior compression system that helps you squeeze in up to a long weekend’s worth of outfits. It’s sized to fit in most overhead bins but still feels luxe. This Samsonite duo covers weekend escapes and long-haul trips with a lightweight, scratch-resistant polypropylene shell, smooth dual spinner wheels and built-in TSA locks. You get a carry-on and a large checked bag that both expand, so “one more outfit” is always an option. This is the “toss it in the cargo hold and don’t worry about it” bag. The Boren’s impact-absorbing, water-resistant hardshell, 360° dual spinner wheels and expandable gusset make it a rugged checked option, while the lined interior, divider and straps keep everything from sloshing around mid-flight. For the price of some single carry-ons, you get an entire travel lineup: a 26-inch checked spinner, 20-inch carry-on, boarding tote and toiletry bag. The hardside shells, 4-wheel spinners and matching rose-gold finish make it a budget set that still looks pulled-together at the airport. Good news! Many of the best Black Friday sales have been extended through the weekend. Don’t wait in the cold for stores like Walmart to open and fight the crowds to score the most popular gifts — we found all the deals you can shop from the comfort of your own home. Kick off December with the online shopping sale of the year on Monday, December 1, 2025. It depends! Black Friday shoppers may benefit from higher inventory on sell-out-risk items, while Cyber Monday may bring steeper discounts and incentives — depending on where you shop. We recommend making a strategic plan to maximize your savings without missing out on the items you want the most. Jet-setters rejoice: Travel Tuesday, with its great deals on flights, hotels, cruises, all-inclusives and more, will take place on Tuesday, December 2, 2025. Absolutely! Black Friday and Cyber Monday bring some of the biggest sales of the year on big-ticket tech items such as AirPods, popular gifts like the Dyson Airwrap, clothing and shoes, top kids toys, and so much more. Right here! Follow Post Wanted shopping insiders for updates on all the Black Friday, Cyber Monday and Travel Tuesday deals worth shopping in 2025. For over 200 years, the New York Post has been America’s go-to source for bold news, engaging stories, in-depth reporting, and now, insightful shopping guidance. We’re not just thorough reporters – we sift through mountains of information, test and compare products, and consult experts on any topics we aren’t already schooled specialists in to deliver useful, realistic product recommendations based on our extensive and hands-on analysis. Here at The Post, we’re known for being brutally honest – we clearly label partnership content, and whether we receive anything from affiliate links, so you always know where we stand. We routinely update content to reflect current research and expert advice, provide context (and wit) and ensure our links work. Please note that deals can expire, and all prices are subject to change. Looking for a headline-worthy haul? Keep shopping Post Wanted.
--------------------------------------------------

Title: Year-End Rally Begins
URL: https://realinvestmentadvice.com/resources/blog/year-end-rally-begins/
Time Published: 2025-11-29T09:46:21Z
Description:  At a Glance


<ul><!-- wp:list-item -->

<li>Rally Into Year-End As Dip Buyers Emerge?</li>




<li>Year-End Rally Begins</li>




<li>Portfolio Tactics For Next Week</li>




<li>From Lance's Desk: Market Bubbles: A Rational Guide To An Irrational …
--------------------------------------------------

Title: Mass. health insurance premium hikes worsen affordability crisis
URL: https://www.bostonglobe.com/2025/11/28/business/health-insurance-premium-massachusetts/
Time Published: 2025-11-28T10:55:28Z
Full Content:
Each time one of Tamara Modig’s children needs a telehealth appointment, she boots up a fragile 13-year-old laptop and prays it’ll hold a charge. Then Modig is forced to fret with another concern: the cost of health care. The Fitchburg Public Schools employee and her husband pay more than $9,300 annually in health insurance premiums for their family of five. It’s already a substantial chunk of their combined $160,000 income. Modig knows it’s almost certainly going to rise next year, though she won’t learn how much until spring. Already, her out-of-pocket maximum this past year increased by $1,000, to $4,000. The family doesn’t take vacations, drives only used cars, and lives on a strict budget — which is about to get even stricter. “You wonder if you can make that pair of shoes last longer,” Modig said. “Does last year’s coat still fit the kids?” FEATURED VIDEO The rising price of health insurance has become top of mind in many households across the state as families like the Modigs take stock during traditional open enrollment periods. Many employers’ premiums have risen by 10 percent or more, insurers and brokers say, straining families’ budgets at a time when overall health care costs are on the rise and expected to grow even more in the coming years. “This is the hardest health insurance market I’ve seen in two decades, with no end in sight,” said David Shore, executive vice president at Borislow Insurance, which helps companies across the East Coast, including in Massachusetts, navigate insurance and funding options. “It’s only going to get worse.” The costs aren’t just growing — they are exacerbating an affordability crisis for state residents and businesses, experts say. The Massachusetts Taxpayers Foundation, a business-backed policy group, recently cited health care costs as a top concern, with Massachusetts having among the highest employer health care costs in the country. Advertisement Rising premiums, plus out-of-pocket health care costs, are all outpacing increases in household income, inflation, and other indicators of economic growth. As a result, Massachusetts is increasingly unaffordable to live in, said David Seltz, executive director of the state Health Policy Commission, during a recent hearing. Massachusetts had the highest insurance premiums in the US last year. And the years of consistent, rising premium costs in the state have long surpassed national trends. Families in Massachusetts spent, on average, $28,151 for employer-based health insurance last year, according to data presented by the Health Policy Commission, a state agency. Nationally, that figure was $24,540. And that’s just premiums. Consumers face other health care costs, including co-pays and deductibles. All told, the average annual cost of health care coverage for a family in Massachusetts was $32,469 in 2024. Since 2024, insurance costs have grown. Family premiums for employer-sponsored health insurance grew 6 percent nationally from 2024 to 2025, according to a Kaiser Family Foundation survey of employers. Because official data sources lag, it’s difficult to assess private insurance prices planned for 2026 in real time. But regardless of how a person gets commercial insurance — whether purchasing for themselves on the state’s insurance marketplace, going through a broker, getting it directly from an insurer, or receiving insurance through an employer — costs are seemingly on the rise for everyone next year. The state’s insurance exchange, where people and small businesses can buy their own insurance, has reported its premiums will increase by 11.5 percent, on average, in 2026. Those premium increases range from 7.2 percent by Fallon Health to 13.6 percent by Boston Medical Center Health Plan, which also goes by WellSense Health Plan. Advertisement These hikes and others mean small businesses are maxed out on health care costs, said Jon Hurst, president of the Retailers Association of Massachusetts. “We’re at a breaking point,” Hurst said. Beyond pending retirements, profitability is flat and health care and energy costs have skyrocketed, meaning about half of small businesses in Massachusetts predicted they would have to sell or close within five years, a UMass Donahue Institute study showed in February. For businesses, premium increases can depend on a number of factors, including an employer’s size, health care usage trends, and how the employer purchases insurance. Less than half of Massachusetts employers are considered “fully insured,” meaning they buy insurance in which the insurer takes on the financial responsibility for paying out medical claims. Most of the employers in this group who have 50 to 100 employees are seeing whopping premium increases that range from 10 to 30 percent, said Mark Gaunya, principal at Borislow Insurance, which works with hundreds of companies of varying sizes. Larger employers in this bucket are more likely to see smaller price increases, he said, though things vary. Most employers in Massachusetts, however, are considered “self-insured” as they pay for the health care claims of their employees more directly. These types of workplaces are also seeing premium increases, from low single digit to low double digits. “I haven’t seen rate increases like this in a long, long time,” Gaunya said of the industry at large. “The market is really, really hard right now for a lot of people.” The state’s two largest insurers cited figures that were similar. At Blue Cross Blue Shield of Massachusetts, “fully-insured” employers are seeing high single to low double-digit increases on average, though numbers vary widely depending on other factors, the insurer said. For its own employees, Blue Cross passed on an average 20 percent premium increase. Advertisement At Point32Health, premiums for employers with over 50 employees are increasing by about 15 to 20 percent on average in 2026, again with considerable variability. When confronted with ever-rising costs, employers have to get more creative with the way they offer insurance, or change what is covered. Beyond passing on the entirety of the increase to their employees as higher premiums, employers can redesign the plan, or shift more costs onto the employee. That can mean higher co-pays and deductibles, in addition to higher premiums. “We’re seeing more 20s- and 30-percent increases than we ever have before, in premiums. It’s quite insane,” said Gregory Puig, partner with Sentinel Group and the president of the Massachusetts chapter of the National Association of Benefits and Insurance Professionals. Shifting costs onto employees is not a new strategy. Nearly half of Massachusetts residents with commercial insurance had a high deductible health plan in 2023 — up from 19 percent in 2014, state data show. Average deductibles are also on the rise — from about $2,300 in 2014 to over $3,100 in 2023. Rob DiMase, a partner at Sentinel Group, an employee benefits administration and consulting firm, said these strategies have helped some employers he works with mitigate a 20 percent cost increase for next year, so the employees only see their premiums rise by 10 to 12 percent. Still, some employers are seeing huge increases. In the large group business, which includes employers with 50 to 500 employees, the best initial increase DiMase saw was 7 percent. The worst was around 57 percent. Escalating costs have patients worried about the future. Joe D’Eramo, 61, who has a broker, paid just over $615 monthly this year for coverage through WellSense. Beginning Jan. 1, that premium will increase by $90 per month and his deductible will rise by $500, to $3,500. Advertisement D’Eramo works as a freelance copy editor and public relations specialist, and drives for Uber on the side. But a $1,000 settlement with the ride share company will be wiped out by his health insurance increase. D’Eramo, who lives in Millis, was recently diagnosed with diabetes. He is counting down the years until he qualifies for Medicare, the government insurer that generally serves people 65 and older, so that he can save more money. “I just want to be able to make it to that [while still] healthy,” he said. Jessica Bartlett can be reached at jessica.bartlett@globe.com. Follow her @ByJessBartlett. Marin Wolf can be reached at marin.wolf@globe.com. Digital Access Home Delivery Gift Subscriptions Log In Manage My Account Customer Service Delivery Issues Feedback News Tips Help & FAQs Staff List Advertise Newsletters View the ePaper Order Back Issues News in Education Search the Archives Privacy PolicyYour Privacy Choices Terms of Service Terms of Purchase Career Opportunities Internship Program Co-op Program Do Not Sell My Personal Information Boston Globe Media
--------------------------------------------------

Title: Delivery Hero Investors Said to Push for Sale, Divestments
URL: https://finance.yahoo.com/news/delivery-hero-investors-said-push-082245746.html
Time Published: 2025-11-28T10:55:24Z
Description: Investors including Hong Kong hedge fund Aspex Management, which is Delivery Hero’s second-biggest shareholder with a stake of more than 5%, are pushing...
--------------------------------------------------

Title: Black Friday is triggering global ‘jet brag’ with these 10+ travel deals
URL: https://nypost.com/shopping/best-black-friday-travel-deals-sales-2025/
Time Published: 2025-11-28T10:00:00Z
Full Content:
Jet brag: A post–Black Friday side-effect marked by the urge to mention that your all-inclusive beach vacation cost less than your buddy’s Uber to Jersey. If you’ve been vying for a getaway, now’s the moment to pounce because the window for extended Black Friday and Cyber Weekend travel deals is slowly closing. This year, travel brands like Hilton, Skyscanner, and JetBlue are already rolling out offers that may not make it through Cyber Monday. I’m talking flights that won’t make your bank account cancel itself, resort stays at dreamed-of destinations at their lowest prices, and full-package deals that bundle everything so you just show up and relax. LELO: Up to 50% OFF at LELO.comVRAI: Major savings on VRAI created diamonds + free giftsTineco: Up to 59% off the brand’s smart cleaning productsJEMMA: 30% off + a free toiletry bagEvery Body Eat: 25% off the brand’s sweet and savory treatsRitual: 40% off sitewide — the best sale of the year!B.T.R. Nation: 35% off sitewide Babbel: Up to 60% off your lifetime subscription!Nutrafol: 25% off sitewide for the first time ever!Whisker Litter-Robot: Up to $150 off bundles & 35% off subscriptionsecoNugenics: 25% off sitewide, including best-selling detox supplements Whether you’ve started hunting for the best extended Black Friday hotel deals, all-inclusive deals, or flight and cruise deals, stop. We’ve already done it for you. Your move? Click to go to a specific category of Cyber Weekend deals and make it quick, because these offers are hyper-limited (and often have specific dates, terms, and eligibility). Black Friday Flight Deals | Black Friday All-Inclusive Deals | Black Friday Hotel Deals | Black Friday Travel Package Deals | Black Friday Luggage Deals Rentalcars.com is running Black Friday discounts up to 30% off rental cars from major brands like Hertz, Enterprise, Budget, and Avis. Discounts vary by country and pickup date but apply to many 2025–2026 bookings. Travel in style with La Compagnie, where every journey feels exclusive. Begin your experience in the private Newark lounge, then step onboard to enjoy fully flat business-class seats, sparkling Champagne at takeoff, and thoughtfully prepared in-flight dining. Offering unbeatable value, La Compagnie delivers elegant, business class round-trip flights to Paris, Nice, and Milan, proving that luxury can be both refined and accessible. I had a chance to fly La Compagnie to Paris, and my experience with this boutique luxury airline felt nothing short of VIP. Book you flight today and discover why La Compagnie is simply the best deal in business-class travel to Europe this Cyber Monday. Select a flight plus hotel vacation package and save up to $450 when you book by November 25, 2025, for travel between January 1, 2026, and October 26, 2026 (plus, you can lock in your trip with deposits starting at just $99 per person). Just use the code PRESALE300 (for $300 off vacations to Nassau, Bahamas, and Grenada) or PRESALE450 (for $450 off vacations to Atlantis Paradise Island, Bahamas) at checkout. Skyscanner is running travel deals all week, leading up to Black Friday, with “up to 30% off” select airfare and hotel bookings highlighted across their live deals hub. The Post Wanted-loved platform publishes price drops from hundreds of airlines and booking sites live as they happen, so you’ll see rolling offers appear throughout the week (not just on Black Friday itself) with some routes dipping below their typical seasonal averages. The well-loved Euro airline has kicked off its Black Friday promo with special fares for flights booked in November 2025, offering passengers exclusive savings when you act before each deal’s provided deadline. Fly to Ireland, the UK and Europe for less using Aer Lingus’s own Flight Finder, which makes finding deals and planning trips extremely easy (you can even input your budget maximum). Or, you can simply scroll the pre-made list of flight deals, such as New York to Dublin between January 23 and 26, 2026, for $348.37, as it’s updated live. Prices are constantly in flux, so lock down any good ones you see ASAP! Their Black Friday Sale (Nov. 11 through Dec. 2, 2025) offers up to 65% off stays, up to $2,000 in booking credits, as well as a free night at select resorts. The travel window extends through December 25, 2027, giving you lots of flexibility for future getaways. The brand’s Black Friday 2025 offer features up to 50% off stays at all-inclusive resorts in Mexico and the Dominican Republic, with pricing from as low as about $100–$172 per night in select cases. Expedia’s Black Friday sale is live with up to 30% off select hotels plus member-exclusive discounts on flights and packages. Travelers can book stays through the first half of 2026, and rewards members earn boosted points. Right now, Hilton’s sale lets you save up to 25% on holiday stays at hundreds of hotels across the U.S., Canada, Mexico, and beyond, with travel valid through April 6, 2026. On top of that, they’re offering perks like earning double Hilton Honors points on select stays, and “Stay Longer in Paradise” packages that offer up to 20% off stays in destinations like the Caribbean and Hawaii. Barceló Hotel Group’s Black Friday sale touts “up to 55% off” reservations worldwide, including all-inclusive resorts, with added bonus savings for members. Some of these deals include an extra percentage off with a promotional code (found on the sale site) and apply to bookings made now. Marriott is offering up to 20% off room rates at participating MGM Collection with Marriott Bonvoy destinations in Las Vegas when you book between November 4 and November 18, 2025. You can lock in the discounted rate now and stay anytime through October 31, 2026, giving you nearly a full year to use the deal. Just enter code F4639 at checkout to score the perfect deal on a Vegas getaway with plenty of flexibility. Trafalgar’s Black Friday 2025 campaign is offering 15% off international land/cruise tours around the world, which is helpful if you’re planning multi-country trips. Plus, if you’ve booked with them before, you get an additional 5% off your booking. On select guided tours, from “Best of Germany” to “Country Roads of Scandinavia,” you can score a BOGO 50% off (or similar global vacation package) deal now through Black Friday, giving you twice the value when a second traveler joins. Amazon is offering huge discounts up to 50% on all of the top luggage brands. Shop American Tourister, Samsonite, and more at all-time low prices. Right now, you can save 25% on everything Away has to offer, from hard-sided carry on rollers to softside checked bags (which Post Wanted reporter Angela Tricarico highly recommends). The 25% savings also apply to all of Away’s products on Amazon. Solgaard, known for their Carry On Closet packing system, is offering 20-40% off everything, from carry ons and checked bags to backpacks and packing cubes. For a limited time, everything on BÉIS is 30% off, including the cult-favorite Weekender and rolling luggage. Monos is currently offering 25% off everything, including luggage, totes, and more, with the code BLACKFRIDAY. Good news! Many of the best Black Friday sales have been extended through the weekend. Don’t wait in the cold for stores like Walmart to open and fight the crowds to score the most popular gifts — we found all the deals you can shop from the comfort of your own home. Kick off December with the online shopping sale of the year on Monday, December 1, 2025. It depends! Black Friday shoppers may benefit from higher inventory on sell-out-risk items, while Cyber Monday may bring steeper discounts and incentives — depending on where you shop. We recommend making a strategic plan to maximize your savings without missing out on the items you want the most. Jet-setters rejoice: Travel Tuesday, with its great deals on flights, hotels, cruises, all-inclusives and more, will take place on Tuesday, December 2, 2025. Absolutely! Black Friday and Cyber Monday bring some of the biggest sales of the year on big-ticket tech items such as AirPods, popular gifts like the Dyson Airwrap, clothing and shoes, top kids toys, and so much more. Right here! Follow Post Wanted shopping insiders for updates on all the Black Friday, Cyber Monday and Travel Tuesday deals worth shopping in 2025. For over 200 years, the New York Post has been America’s go-to source for bold news, engaging stories, in-depth reporting, and now, insightful shopping guidance. We’re not just thorough reporters – we sift through mountains of information, test and compare products, and consult experts on any topics we aren’t already schooled specialists in to deliver useful, realistic product recommendations based on our extensive and hands-on analysis. Here at The Post, we’re known for being brutally honest – we clearly label partnership content, and whether we receive anything from affiliate links, so you always know where we stand. We routinely update content to reflect current research and expert advice, provide context (and wit) and ensure our links work. Please note that deals can expire, and all prices are subject to change. Looking for a headline-worthy haul? Keep shopping Post Wanted.
--------------------------------------------------

Title: Presentation: Reliable Data Flows and Scalable Platforms: Tackling Key Data Challenges
URL: https://www.infoq.com/presentations/data-architectures-challenges/
Time Published: 2025-11-28T09:45:00Z
Full Content:
A monthly overview of things you need to know as an architect or aspiring architect. View an example We protect your privacy. Facilitating the Spread of Knowledge and Innovation in Professional Software Development Unlock the full InfoQ experience by logging in! Stay updated with your favorite authors and topics, engage with content, and download exclusive resources. Vivek Yadav, an engineering manager from Stripe, shares his experience in building a testing system based on multi-year worth of data. He shares insights into why Apache Spark was the choice for creating such a system and how it fits in the "traditional" engineering practices. In this podcast, Michael Stiefel spoke with David Blank-Edelman about the relationship between software architecture and site reliability engineering. Site reliability engineering can give architecture vital feedback about how the system actually behaves in production. Architects and designers can then learn from their failures to improve their ability to build systems that can evolve. In this episode, QCon AI New York 2025 Chair Wes Reisz speaks with Reken CEO and Google Trust & Safety founder Shuman Ghosemajumder about the erosion of digital trust. They explore how deepfakes and automated social engineering are scaling cybercrime and argues defenders must move beyond default trust, utilizing behavioral telemetry and game theory to counter attacks that simulate human behavior. Erin Doyle explains the evolution from siloed IT Ops to the Platform Team model, revealing why the "You Build It, You Run It" principle created new cognitive load. She shares the Empathy-Driven Platforms strategy - the ultimate attack against engineering roadblocks. Discover ways platform teams can build empathy, foster psychological safety, and adopt a product mindset. Emma Yuan Fang explains the Zero Trust mindset required to combat modern software supply chain attacks. She details security controls for dependency management, including SBOM (Software Bill of Materials), artifact signing, Git commit signing, and CI/CD hardening. Learn how to implement security gating, enforce policies as code, and manage secrets across your build and runtime environments. Go from AI demos to real engineering impact. Learn to embed LLMs, govern & scale securely. SOLD OUT! Learn what works in AI, architecture, data, security & FinTech. Early Bird ends Dec 9. Learn how leading engineering teams run AI in production—reliably, securely, and at scale. Launch pricing ends Dec 9. InfoQ Homepage Presentations Reliable Data Flows and Scalable Platforms: Tackling Key Data Challenges Matthias Niehoff discusses how to bridge the gap between application development and data engineering. Learn to apply software engineering best practices to data pipelines, embrace boring technologies, and simplify architecture to maximize business value and data reliability. Matthias Niehoff works as Head of Data and Data Architect for codecentric AG and supports customers in the design and implementation of data architectures. His focus is on the necessary infrastructure and organization to help data and ML projects succeed. Software is changing the world. QCon London empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams. Matthias Niehoff: I'm Matthias. I work for codecentric, a small consultancy in Germany. Before I joined codecentric, I made my professional education in an insurance company. When I finished this education, I came back to the company and they said, congratulations, and yes, we want you to stay with us. We have a new job for you. Back then I was in the architecture team, whatever that meant. The job is going to be in the data warehouse team. I was like, what? Because for me, data is that which the others are doing. From a software engineering perspective, I didn't care about data warehouses. This is where the data goes and they get it some magical way. Turns out it was a joke. I stayed in the architecture team. After joining codecentric, I moved into the data space. I started with Apache Spark 10 years ago, and then evolved into getting all the data stuff, but I have a software engineering background turning data. These challenges remain. From a software engineering perspective, it's often still like data, data, data for analytics, it's not that that we care about. I'll give you an example. Thinking about, we have some data that is stored, order data and order id and quantity, on the right side, we have the analytics side. We just get the order id and the quantity. Then the application thing, we want to track if the order was unfulfilled. The analytics side still doesn't select order id and quantity, and wonders, we have more orders than we had in stock because they didn't know there was an unfulfilled field that they had to track and that has to be selected as well. They're eventually coming up to select this as well, but they want to measure now the unfulfilled items and then they change the data type from a Boolean to an integer. What happens? Either the consuming side converts it implicitly to a Boolean and it basically fails silently, or they just fail implicit because the data type has changed. This is a quite common scenario we have when providing data. What is the reason? The problem is that most of the time only the technical schema is exposed towards the consumers of the data. The consumers just get that schema and try to reverse engineer what is actually happening here. What is the business process that creates that data? What are the constraints here? What is the data model underneath? Also, is this source really trustable? Can I trust it? How do I access it? Is it stable? Is it durable? All these questions remain. It might just be a dump on a file system somewhere more often than not. On the other side, the producers, if they even care who uses the data, they don't often know who uses the data. For what reason? For what purpose? They're most of the time blind on this side. To summarize, most of the time, data is still a second-class citizen, if we're talking about software architectures. It's just a database where we store the data in. This leads to quick one-off solution if data is to provide it, we do dumps. We just say, here, you have a user for our database. You can access it. Nobody would do that normally, but, yes, that's done. The data team is left on its own. I'm going to show you some examples of what we did with clients to manage this. The first example is a small FinTech, which is in the realms of a larger bank in Germany. They were doing B2C stuff, and they wanted to do analysis on marketing campaigns on how the customers behave, what kind of customers they have. They want to check their online sales process. Nothing fancy, just gathering some data from different systems. There was a small team of data analysts who were really curious about getting results out of data, not the really technical guys, all your Python, all your SQL and know how to start a Jupyter Notebook, but they were more interested in, what is the data to say. They had minimal support from the org. The org was really focusing on the application. It was DevOps and software engineers and all this stuff, but they want to just build online applications. They didn't really see the case for data. As a data team, what did we do? We got files, in this case, dumped over at an SFTP server. The first thing we do, we did tests on the source data. In this case we used dbt. We wrote tests to test what is in the data. Does the data change? Is it what we expected, to get notified early if there would be changes in the data? For instance, there would be all null columns, because suddenly would likely mean there was some problem in the export of the data from the source system. We made it visible. We built dashboards. This is elementary build on dbt. It most of the time didn't look like this. This is in the best case. We want to make it really visible. Having this really as a chart where we looked on each day and also using it to communicate with our stakeholders, we don't have the data we needed. Data is broken and so on. We started documenting what we learned about the data. How is data created? What is the business process behind the data? What are the constraints? We learned on the fly, basically, while working with the data. We adjusted the test to actually have the knowledge we had in the test. We extended this. This also was part of dbt in the end. This was all just basically mitigation of the problems. We had mitigation of the problems that the producer actually wanted on board. We wanted to get to the source. We wanted to involve the producers to actually help us getting better data. Maybe it's getting a step back. If you have an application data model where you store your data of the application, you have defined it everywhere. Might be in normal form. Might be normalized or whatever. You have another application wanting to access this. No software engineer on earth would say, here's the application model, just do the access on it. You would always build some API on it. Might be a REST API, might be a stream. You might be putting on messages on AMQP, whatever. You would always use an abstraction. From analytics, it's more often than not like this. You just dump out the internal application model. What we're thinking about is we want to use an API as well. Maybe not the REST API that is optimized for single record access, but maybe some more efficient. Or it might even be a separate data model that has been created for sharing the data towards the outside. All of this should actually be the responsibility of the team. We should care about how those interfaces look like. How should those interfaces look like where we share those data from the team towards the analytics part. The one thing is, how is this sharing data model created? It needs some pipeline. It has to be some process, something where this sharing data model is created. The sharing data model itself, from a technology perspective, could be a view in the same database that they are using for the application. It might be a topic in some broker. It might be files in Blob storage. It might be Iceberg tables, data tables, CSV, whatever. It might be something in a data warehouse or data lake. With the MCP servers now giving LLMs access to data, the first idea is popping up to actually use MCP servers to access those data. I'm not sure if it's a good idea still, but at least something one can have in mind. We need those pipelines so that software developers can easily share this data. These pipelines should be part of the whole application, so owned by the development team. The development team is responsible for creating this data model for sharing the data. First of all, it means more responsibilities, but also it gives you chances. I think a lot of you who work in data know that CI/CD tests and so on are not as widespread as in software engineering. It might be a chance for software development teams to actually build and use their software stack, to use their best practices or good practices that they have, writing tests, doing CI/CD from this. Taking ownership of it means not only responsibility, but also having the freedom to do it in the way they want to implement it. In the same tech stack, possibly. It's just another API that they are building next to maybe an existing REST API. Still, those pipelines need to be supported. We want to support those people with blueprints, with libraries creating this. It might be that the application tech might be Java something or Kotlin or so on. On the data side and on the consumer side where you want to provide the data might be something like a Delta Lake managed by Databricks. I happen to have an example afterwards. You need to do more support for this. You need to do support for execution and retry, something that is not that usual in a normal web application. You have to build frameworks and things for this. You have to enable monitoring with them. Yes, they can use the monitoring that they use for applications, but it might be something different. Schema evolution, write to data-specific infrastructure? Also, things in business, like data changes have to be supported. Those are some of the things they are not used to when writing a normal application. Next example. We have a Java application writing to a Databricks-powered data lake. Most of the developers that would implement this application don't know what Databricks is or know how to use it or how to implement it, and how to do stuff in it. They might be able to do this, writing data to a Blob storage, maybe in Parquet file and not in JSON, or something a little bit more efficient. There are two options, basically. You can register this as an external table in Unity Catalog, or you can load it to Databricks. I won't discuss which one is better, but as a software developer, I just don't care. I don't want to know how to do this. The pipeline should support, actually, this library. We built libraries that made it possible to just say, I have a large amount of data here in some file or in a database, and I want to publish it. The library takes care, with the pipeline behind it, to put the data into a Blob storage, load it to Databricks, make it available there. The SDK or the library had some input fields, so you can do a description in it, you can put information to the data in it, so that it gets published to the Unity Catalog as well. This was just the beginning of the interface we were describing. We started collecting also not only the data, but also information and metadata about it. We took another step forward after this. One thing we used were data contracts. I think most of you have heard of data mesh, a few years old by now. Huge impact, huge hype back then, a lot of different concepts. I think the most tangible concept that was there was the data products, and out of this came the data contracts. Contracts basically describe the data that you're sharing between a data provider that has some data product, in this case it would be the application team, and the data consumer, what data they share, and how it looks like, and what guarantees that gives you. The data contract is then described often as a YAML file, and you can use it for enforcement. I'll come to it later. The most important part is you make it explicit. You explicitly describe, we share or provide this data, and we use it, and they talk to each other, which most of the time didn't happen before. What would a data contract look like? I have here the high-level overview of a data contract on the right side. You have servers. Where's the data stored? Might be in a database, might be in S3 bucket, might be anything, might be a topic. You have the schema of this. You can put price tags on it. If you say consumption costs like this, so you can enforce it, informational only. You give information about who's the team. You give roles. You give SLA properties. We give you the guarantee that this data is up to date, or is updated every day, at maximum lags behind one day after the event has occurred. We give you guarantees that we will do failure resolution of data within whatever days. This is really something that you argue about, and more often than not, coming to points where you're saying, you expect this, now we only can deliver this, because we have this and this constraint on this. Then you find a solution together. We created those contracts data first, so we had the data, and then we started creating a contract, all of the data. You can just say, here's data, just build a contract out of it. We versioned those things with semantic versioning. We put it into a central Git repository. What we used was the Open Data Contract Standard. There are multiple standards, currently it seems like this is the one that evolves from all of those standards, and it's going to be the main standard, until there's another one. We were lucky and did the right choice at this point. I see that we can also enforce those contracts. There's a CLI, datacontract-cli as it's called, where you can test those contracts. You have defined this contract in a YAML file, all the information inside, where's the data stored, what kind of schema do we expect, and we're doing tests on it. We can also do like, do we have breaking changes? If we do another version of the data contract, we can just give the YAML file of the old contract, of the new contract, and say, is this breaking or is it forward compatible? Can we use it again? What we did is, we created source tests. You remember right at the beginning, we manually created source tests, we can create dbt source tests, based on the information we have in our contract, and use those to enforce the data, and test the data as a consumer. Where were we enforcing the contracts in the end? Once on the publisher side, in CI/CD. We changed an API. We created a pull request. Within the pull request, we checked the changes against the contract. We checked for the format, for the schema, for the data quality, is it still what we expected? If all these checks passed, then that was one of the necessary requirements to get those pull requests merged. On the consumer side, on the other side, we also retrieved the data, and the first thing we do, we're checking the data. Checking for schema. Checking for quality. Checking if those SLAs we had agreed on were true. This is what we did there. Where and how did we check for reliability? Because one of the topics is reliability. We did unit tests and tested contracts in the app development. The pipeline execution, we tested against data, and we integrated it into application monitoring, having one view, and not the application monitoring with Grafana and whatever, and some other view somewhere else. We did source tests and tests in further stages along the processing as well. Didn't go into details here. Reliable and trusted data is important to scale the data usage. When I'm talking about scale, I'm not talking about having billions and billions and more data, but with scale, I mean getting more out of the data we have, empowering more use cases, getting more use cases done, creating more value for the business. This is, for me, scale. I don't need the tech scale. It's interesting, but it's not necessarily the most important part. Next to this trusted data, what is the role of tech and platform in this case? We talked about the pipeline getting data, but what is the role of tech and platform? A few people know this. What does it mean when you see this? For me, it's sometimes really a cognitive overload. This is all the tooling and things one should know, or at least have an idea about what they do when it comes to data, and it's getting bigger and bigger. Also, GenAI is included, which is a big part of it, but it's really a lot. For me, sometimes it's really causing constant stress. I just go through LinkedIn or whatever, or Hacker News, and see there's a new tool, and there's something cool, and there's something I want to have to dig in, and it's really like constant stress. What do you do then if you're coming in this thing? There's multiple strategies of what people do. You try to start, I'm looking everywhere. I'm trying to get there. I get an idea there, and so you know nothing about everything, basically. Or another strategy is, that's too much. I just focus on this one and go really deep. I know everything about Apache Spark. I don't care what's left and right, but in Apache Spark, I'm the absolute expert. Or you just say, no, back then, we were just using Apache NiFi example. We can do everything with it. You're just clinging to the past, also a strategy that people are doing. Or you get dogmatic, or you just give up and say, what else? I just don't care. It's definitely this complexity and all these things that might be there, and people are telling you, and market telling you, you need this to get actually data platform, and this is needed, and this is needed. It's unhealthy for people, and it's unhealthy for organizations. A reason why I might come back to something that is essential complexity versus accidental complexity. Essential complexity is the things that you need to fulfill your functional requirements, your non-functional requirement, and there might be some boundary conditions. A lot of you might be in banking, so there are boundary conditions from regulations or whatever that you have to comply to. These are essential complexity. You cannot do this without those complexity. It's inherent to the problem and the essence of the problem that you are solving. All the rest is the accidental complexity. Is using Kubernetes over kubectl really essential to the problem you're solving? Might be, might not be. I tend to say in data, more often than not, it's not the problem. For me, there's a lot of accidental complexity we introduce by using those tooling. Therefore, it's really important to embrace simple and effective solutions to really build things that solve the problems at hand, to scale things. A good strategy to do this is choose boring technologies. Technologies that are well understood, both in what they can do, what they can't do, and what failures might occur. You might find all failures in Stack Overflow, or ChatGPT has learned about them. It's pretty good to understand. You have a community. You have people to talk about. It might be sometimes that a new problem comes up where existing and boring technology doesn't work. One of the most boring technologies, which is pretty good at data, is actually Postgres. It's not as sexy as Snowflake. It's not as fast as DuckDB. Postgres is actually a really good solution for a lot of data cases we have. Also, there's the concept of innovation tokens. A really good idea. There's the idea of, as a team, you have two or three or one innovation token per year that you can spend on new technologies. You try to manage and not to lose yourself into new stuff as well. Next to boring technologies and also Postgres, is you build on well-understood technical foundation and open standards. Open standards give you the flexibility to argue with vendors, to switch things. It's good understanding. There's a foundation to use this. One of the open standards next to Postgres is all the things that come towards data storage right now. Think data storage in the cloud is a lot of getting commodities. If you're using Delta or Iceberg or Parquet, it's open standards. Also, query engines get open standards, and you can build on top of this. Another thing to keep things simple is leveraging the key concept of the cloud. Doing automation. Doing scaling to zero, and reduce the vertical integration. I said, I'm not a big fan of Kubernetes in data because I think there's a lot of good PaaS and SaaS platforms that actually go higher level and enable you to do more with less in the end. This might be something that is very familiar to a lot of software engineers, but you see in the data place, there's a gap between what software engineering knows and what software engineers knows is a good thing, and what data engineering does. Another case, a pretty conservative bank, slow, not that large, regional bank, a cooperative, they want to do more data-driven decisions. Which management doesn't want to do more data-driven decisions? The existing solutions didn't scale, not in terms of data, but scale in terms of what decisions we can make, what kind of insights do we get? They built a cloud data platform. They use a central team for the platform, no data mesh, but they have different data user teams that actually use those platforms. What we built is we used open standards. A platform in the cloud, in this case, it's Azure, but it could be any cloud, using Delta Lake as an open standard for software, using PySpark, which is an open-source product to run their data. Nobody wants to run PySpark on its cluster on its own. You never want to do this. We used Databricks. We used the Unity Catalog, which is proprietary stuff, yes, but it gives us things that we didn't get about any standards, gives us things about permissions, management. It's not the business data catalog, it's a technical data catalog. We're using Databricks Serverless SQL to access that data. Those are proprietary things, but the foundations are open source and open standards. We used also the Unity Catalog features to provide the data towards, I said, we had a core team and multiple single teams as well. We had the core team having catalogs, and those can be shared within other teams. We're using the Unity Catalog features of Databricks with it. If you're building things like these, on Kubernetes, you have to find a solution where you can do the cataloging. The Unity Catalog is open source. I've not seen a lot of companies actually running it. You want to get it managed because you won't get paid for running in catalog most of the time. We use the catalogs for data sharing between those departments. This was proprietary stuff, yes, but built on open foundation. Those open foundations enabled us, for instance, to use different query engines. Databricks is a great platform. I really like it. Other platforms might be good as well. I'm not that experienced with it. If you are coming to data that is not huge, but it's big, like 30 gigs, 40 gigs, or whatever, Spark is actually really slow. It's a lot of overhead to actually process the data. Thanks to the open standard, we could use DuckDB to connect that data. We can get a token from Unity Catalog and read the data from the Delta Lake. It's an open catalog. It's an open standard, nothing proprietary. Because one thing, data is smaller than you might think. This is an analysis of Amazon Redshift. The team behind Amazon Redshift did it. It's a few months old already. This is the title, Why TPC Is Not Enough. TPC is a benchmark used in analytics to measure the speed of warehouses and SQL engines. The paper was about, is TPC still the right benchmark? They analyzed all the data they have in their system and they found out, actually most of the data, so we have the rowcount buckets up to 10 million rows over here. Ten million rows isn't that much on data, but most of the data actually is about 10 million rows. It's not that big. Sometimes it's even enough to run on a single machine. You don't need distributed systems at all, making it simpler, easier, more cost-effective, and more reliable to build data platforms. Another example, we built a more small stack, FinTech. We want to do everything on-prem. There were some reasons to do this. We just built things on Postgres with dbt and Airflow to orchestrate. One can argue if Airflow is a bit of overkill, yes. We use Python to ingest the data. We use the contracts before to actually synchronize on the data and have a clear interface between this. It was really simple tech, nothing fancy, but it gets the job done. One thing I want to give you, apply software engineering best practices. I'm on a more or less software engineering conference for you as clients, writing tests for code and data, monitoring and observability, infrastructure as code. It's all a given, but it isn't too much in the data world, actually. It's a hard way to say, tests are a benefit. You also have to understand, yes, some things are definitely different. Data gives you another dimension. The data itself is another dimension you have to test as well, using separate environments. Also thinking that software engineers say, of course I use separate environments. In data, it more often than not comes to this. Separate environments is often like, yes, we have prod. Bringing it together, reducing the complexity of the platform also reduces the error surface. Reducing complexity reduces costs. All in all, this improves reliability of your data platform, data pipelines. Then using standards, improving practices, simplifies the operation, also less complexity, less overhead, and sometimes a single node system can bring you a long way. We talked about having the pipelines and contracts basically in the platform and say it's simplifying. Maybe take one step further here. Another idea I just want to present and give out. Another case, an industrial IoT startup. They collected sensor data in their electronic grid and they visualized that data. Eventually, they also want to control the grid based on the data. What they did is they had a document database, put all the data into the document database. The team was consisting mostly of full stack developers and mostly heavily leaning to the TypeScript and the ecosystem around this. Then, a document database is fine, because if you're building a frontend with TypeScript, all the data is in JSON, perfect. At some point, they started wanting to do data analysis. What we had were like this. We had an ingest. The data was put into a document database. There was a UI and APIs connected to the document database, and there was a relational database for some master data. What stations did we have? What centers did we have out in the field? Pretty common. Document database was also using the time series mode of those databases. Then they wanted to add aggregations on this, because they wanted to do hourly and daily values, so they did a Python script that runs on Document DB, then they wanted to have alerts running on the Document DB, getting the data out of it. The relational database was extended somehow because they wanted to do analytics. The center of the data was actually Document DB. To summarize this, this was actually a web application that is a data platform with a UI. It's not a web application, but the center of this application is data. We built a UI on it, but it's not a UI and data is somewhere. We redesigned it. We have the ingest, and the first place where the data lands is the data platform. We don't have any application doing anything, but we have the data platform, the one that is responsible for storing, analyzing the data. We do all the aggregation and the alerting on this data platform. Then we have the Document DB that is fed from the data platform with the data that is needed to visualize the data. It might be aggregated. It might be filtered. It might be optimized to actually fit the needs of visualization. We had data analytics directly on the data platform. What we did basically is a shift left. Shift left is like everywhere. Shift left in security is more about having it earlier in the process, in designing as well. We have to shift left in the data flows, not too much in the process of creating the software. There might be ideas as well, but in the shift left in data flow. We had from data source to application to doing some ETL and ELT stuff to doing it somewhere where we do analytics. We moved it to, we're having a data platform. Some optional ETL, ELT might be, might not be. Depends on what data platform you have. You don't have to load the data. You might access the data platform directly. Then connected some analytics stuff, some web application where a user can do something. I'm not saying this should be done for all, definitely not. I think especially for applications where the data is in the center, and I see it a lot in IoT and industrial applications where you get a lot of sensor data or also in marketing where you get tracking data. This might actually make sense to move data at the center and not treating this like an application. We moved the data lake. When we think about this, we can even take it one step further. Why do we actually build different platforms for data and applications? There was a talk about platform engineer basically and developer experience. It was a lot about building applications for software engineers with all the CI/CD and the golden path and improving the developer experience, making it easy to deploy stuff. It's all good and all fine. What we end up with, we have an application platform where we run all the applications on it. We have a data platform, which is storing the data and powering ML and AI use cases, BI use cases. Taking observability, for instance. Observability is a huge thing in application platforms. Monitoring, logs, all the stuff is built into it. We would definitely need more of this in data platforms. There is a lot of things that can be used. Most of the time, the data platform solves it itself. I take an example, Databricks. Getting data out of Databricks for monitoring and observability reasons is possible, but definitely not easy to do. It's completely two separate subsystems with different approaches, with different technologies. I don't think it has to be like this. This is actually also widening the gap between those systems, because we're building on different platforms and different paradigms, and try to avoid this. We use the data platform, getting the data from the data management. Why don't we build application and data platforms? Platforms that actually have in mind from the beginning, we want to support application development, running applications, and using the data within this application for analytical purposes. We have things like storage and runtime. The storage might be different for data than for applications. The runtime might be different, but it comes from one vision, might not be one team, but from one platform building this. Everything that is CI/CD means there are proven patterns on how to do CI/CD in every software company, most likely. I think 80% of the time those can be reused for data. Why not provide it to data within a data platform as well? All the stuff like secret management. Secret management, there's a lot of tooling around it, but it's reinvented in data as well. Test data management, and the contracts themselves. The contracts are in between application and data. This is what is common to both, and of course should be managed within this platform itself. You might be asking, what is the tech? There isn't the one technology that powers all of this. You might say, yes, we built everything on Kubernetes because we built our developer platform and our data platform on Kubernetes. One might say Databricks or Snowflake or whatever might be your thing. I would say, coming back to what I said was platforms, we did think about what is the essential complexity? What is the accidental complexity? What complexity is actually needed to solve this? It might be that Kubernetes is one thing. It might be that Kubernetes powers all of your cases and all the things that you have also with data. It might be that you have two different technologies and just creating good bridges in between and good integrations in between. It might be that there's even another solution based on this. There isn't the one answer. I promised I'm coming back to DuckDB as well. It's an idea of DuckDB not seeing this, but I think it's a good thing to think about is going beyond two-tier data architectures. The two-tier data architecture is basically this. We have data in the application. We have data in the analytics part. This is an idea of DuckDB. DuckDB, pretty fast embedded analytical query engine and database, which can basically run everywhere on a server or in a browser with Wasm, or in a Lambda function, and can read basically everywhere. Having the central data lake, in this case it's more like a swamp maybe, or between a lake and a swamp, as a central datastore, both powering applications and analytics on the same way. It's an idea. We haven't seen this in a project. I think it's an idea worth thinking about, maybe having in mind when designing this. Because, in my opinion, it would be a good thing when data and applications became one. We would have less friction between operational and analytics: common ground, common platform, common ideas maybe to build stuff. We could actually define and encapsulate responsibilities more clearly. Remember the example from the IoT startup? There was a long discussion. Who should do the aggregation? Who is responsible for doing the other thing? Is it the application or is it the data team? Where is the knowledge about when to do the other thing? If you're thinking this is one, then it's a common ground you find and you can decide together where it is. You might have optimized data representations for different cases. A document database isn't that good actually for analytics. There might be better settings, but it's really good for UIs, for example. The same is a Blob storage isn't that good for actually real-time requests from a web UI, but a document database would be. It's really important to use the tooling on both sides. Not saying, we have Kubernetes and Postgres and everything, so just get started with this and do this, but to know the stringencies, the differences that there are. Also, use the engineering best practices and good practices, and apply them to data. What do we take away from this? The first thing, validate and monitor your data early on. Having contracts both from the provider side, and do it on the consumer side as well. Start at the source. Really take the devs into this. Don't say, we just get the data and we have to be on our own. Try to get the devs on board by building solutions and abstractions that actually have them, that make it easy for them. Don't say to them, here's Databricks, publish it there. You can build workflows there. You just need to write Python and write it to Parquet. No, that's not going to work. Really get to the devs and get them where they are, and address those pains. Of course, the organization has to prioritize data as well, otherwise every product owner of every team would just say data, data, I don't care. Simplify the architecture. Don't make it more complex. Really make it handleable, also for smaller teams, for teams that are not that experienced. It doesn't have to be the latest, newest technology everywhere. Apply the good practices you know from software engineering. It might be a thing to think data and application platform together, not separate those worlds from the beginning, from the foundation. It's getting split up in the application anyway and in the usage, but maybe having a common ground to actually build on them. We talk a lot about tech now. Actually, data modeling is really a thing. Only write tables and having everything joined and having one large table might not be sufficient all the time. Things like star schema, dimensional modeling, all this stuff isn't the sexy part, but it's actually really helpful. Besides all the tech, there's also some things to learn from data and good classic data warehouse. Although, sometimes it's really dry, but it's helpful. Nardon: Your take on using the data for application and for the operational system using the same platform. Usually, in companies, especially when you have several departments, you have teams that need data from other departments. Usually, that's why there's a data platform where you can advertise your data and share data with other teams. If you're using the same platform for data and applications, this gets a little more complex, because you have the teams developing the data and having to share. How are you doing a data mesh architecture and sharing data between departments if you're using the same architecture? Matthias Niehoff: Even when you build the same platform for application and data, I would still have those typical data platform capabilities like having a catalog and having access management to different data, and a point where data is shared towards others. It would be still a capability that should be built into this application and data platform, definitely. It's not like we don't have the traditional data platform anymore with all the functions. We still have those functions, just moved to one platform that is developed under one common vision with one common ground and one common goal. I'm pretty sure in a large organization, it won't be one team building this platform as well. What's more interesting is, often it's like the data department, especially in classical environments, somewhere in finance and the CFO might be the boss and the application platform is in CDO, CTO, CIO department, and it's completely different areas of the organization with different cultures as well. This is more like a tricky thing I see that would hinder adoption of this. Yes, definitely. Participant 1: How do you know when you should start buying a tool instead of building on your own? Because a lot of the other talks so far have said that you should buy before you build, but you say to choose boring tech. At what point do you know that you should switch to buying existing software? Matthias Niehoff: Boring tech doesn't mean you shouldn't buy. Postgres, ok, you don't buy Postgres, but you could buy Databricks with PySpark, and PySpark is boring tech. It depends. When you're doing stuff that you have the feeling of, it may mean you could take Wardley Maps or something to get out what is commodity, what is actually new, in genesis. If you're building stuff that is undifferentiated heavy lifting, things you do that don't provide really value to you, running a Kubernetes cluster as a data team, I would take as an example. Then you think about, might there be something that would reduce the vertical integration that gives us more intermediate ways to do things with data, work with data. I don't have one person that actually only is there to run the Kubernetes cluster. I think this would be a thing. It's not like there's this one idea to do this. This is more like, depends on your situation. Participant 2: I think my question is around the complexities of having a platform that has already been adopted or consumed by multiple data teams. How easy is it actually to change technology? I think I'm asking this because we're moving in a world where technologies are released every day. We see that there's a Microsoft Fabric that's been introduced now to say that it's also removing the complexities around infrastructure. How do you actually manage that? Because we are also in a distributed type of mesh, and data teams are actually responsible for their own tech. How do you ensure that all those principles can actually be a central component? Matthias Niehoff: When you were changing tech, what would be the motivation, would be the question? Why would you choose changing the tech? Just to standardize, and all of them having the same technology? That wouldn't be enough as an argument to just change tech. If you say, we have to standardize because we have increasing license costs because everybody has bought their own tooling and so on, yes, then it's more or less a classical migration and modernization project with a clear path on how to move each of those teams to a central technology or a central platform. You still want to have distributed responsibilities, I understand, so teams are responsible for the data, but they should use common infrastructure. It's more like a normal modernization project. Participant 3: You commented that running the same data platform for applications as well as, let's say, analytics is not something you've seen much of. I'm aware of a few that run, Uber, us, and usually the complexity is around non-functionals. There are two patterns I know, which is running multiple concurrent versions of the same platform optimized for different non-functionals, and the one we're attempting, which is to use the same platform overlaying the tooling on the same source. You keep talking about DuckDB and these tools. Given those two paradigms, do you have a thought as which one you think would be more efficient, profitable, functional? Matthias Niehoff: You mean by having one platform or having separate platforms? Participant 3: One platform with three different copies of the data optimized for the different non-functional use cases or one version of the data platform that then has to support optimizing for different use cases. Matthias Niehoff: Yes, either optimizing the data or optimizing the processing, is basically the question. Participant 3: For me, the question is usually around the idea of overlaying tooling, so different interfaces, query products. Matthias Niehoff: It depends a bit. For instance, if you're taking a database as your storage, then you have the query engine baked in. Then you can't change too much. If you say my data is stored in a Blob storage underneath and I just use a different engine, then you would be flexible, of course. I think then flexibility actually pays off and it would go that way. It depends a lot on how you store it, but you made one good point. This distinction between having storage and query engine directly coupled goes more and more away. We have the storage that is often pretty cheap as a Blob storage, and we have query engines that optimize for different use cases, definitely. See more presentations with transcripts Recorded at: Nov 28, 2025 by Matthias Niehoff A round-up of last week’s content on InfoQ sent out every Tuesday. Join a community of over 250,000 senior developers. View an example We protect your privacy. Reliability rules have changed. At QCon London 2026, unlearn legacy patterns and get the blueprints from senior engineers scaling production AI today. Join senior peers from high-scale orgs as they share how they are: InfoQ.com and all content copyright © 2006-2025 C4Media Inc. Privacy Notice, Terms And Conditions, Cookie Policy
--------------------------------------------------