List of news related to NVIDIA NVDA:

Title: Google TPUv7: “perf per TCO advantage of TPUs is so strong that you already get the gains from adopting TPUs even before turning one on”
URL: https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the
Time Published: 2025-11-30T05:47:02Z
Full Content:
The two best models in the world, Anthropic’s Claude 4.5 Opus and Google’s Gemini 3 have the majority of their training and inference infrastructure on Google’s TPUs and Amazon’s Trainium. Now Google is selling TPUs physically to multiple firms. Is this the end of Nvidia’s dominance? The dawn of the AI era is here, and it is crucial to understand that the cost structure of AI-driven software deviates considerably from traditional software. Chip microarchitecture and system architecture play a vital role in the development and scalability of these innovative new forms of software. The hardware infrastructure on which AI software runs has a notably larger impact on Capex and Opex, and subsequently the gross margins, in contrast to earlier generations of software, where developer costs were relatively larger. Consequently, it is even more crucial to devote considerable attention to optimizing your AI infrastructure to be able to deploy AI software. Firms that have an advantage in infrastructure will also have an advantage in the ability to deploy and scale applications with AI. Google had peddled the idea of building AI-specific infrastructure as far back as 2006, but the problem came to a boiling point in 2013. They realized they needed to double the number of datacenters they had if they wanted to deploy AI at any scale. As such, they started laying the groundwork for their TPU chips which were put into production in 2016. It’s interesting to compare this to Amazon, who in the same year, realized they needed to build custom silicon too. In 2013, they started the Nitro Program, which was focused on developing silicon to optimize general-purpose CPU computing and storage. Two very different companies optimized their efforts for infrastructure for different eras of computing and software paradigms. We’ve long believed that the TPU is among the world’s best systems for AI training and inference, neck and neck with king of the jungle Nvidia. 2.5 years ago we wrote about TPU supremacy, and this thesis has proven to be very correct. TPU’s results speak for themselves: Gemini 3 is one of the best models in the world and was trained entirely on TPUs. In this report, we will talk about the huge changes in Google’s strategy to properly commercialize the TPU for external customers, becoming the newest and most threatening merchant silicon challenger to Nvidia. We plan to: (Re-)Educate our clients and new readers about the rapidly growing commercial success of external TPU customers, starting with Anthropic and extending to Meta, SSI, xAI and even potentially OpenAI... Show that: The more (TPU) you buy, the more (NVIDIA GPU capex) you save! OpenAI hasn’t even deployed TPU yet and already increased perf per TCO by getting ~30% off their compute fleet due to competitive threats Explain the circular economy deals for AI Infrastructure. Revisit our original TPU deep dive with a refresher on the TPU hardware stack from silicon down to the software layer. Cover the positive developments on the open software ecosystem front as well as the critical missing ingredient for Google to make the TPU ecosystem a viable challenger to the CUDA moat: open source their XLA:TPU compiler, runtime, and multi-pod “MegaScaler” code. In the paywall, we will discuss the implications for Nvidia’s moat and compare Vera Rubin to the next gen TPUv8AX/8X (aka Sunfish/Zebrafish) Also cover the long term threat to Nvidia. First, let’s talk about the impact this news has had on the ecosystem. TPU performance has clearly caught the attention of its rivals. Sam Altman has acknowledged “rough vibes” ahead for OpenAI as Gemini has stolen the thunder from OpenAI. Nvidia even put out a reassuring PR telling everyone to keep calm and carry on — we are well ahead of the competition. We understand why. These past few months have been win after win for the Google Deepmind, GCP, and TPU complex. The huge upwards revisions to TPU production volumes, Anthropic’s >1GW TPU buildout, SOTA models Gemini 3 and Opus 4.5 trained on TPU, and now an expanding list of clients being targeted (Meta, SSI, xAI, OAI) lining up for TPUs. This has driven a huge re-rating of the Google and TPU supply chain at the expense of the Nvidia GPU-focused supply chain. While the “sudden” emergence of Google and the TPU supply chain has caught many by surprise, SemiAnalysis institutional product subscribers have been anticipating this for the last year. Another reason Nvidia has been on the defensive is a growing chorus of skeptics who argue the company is propping up a “circular economy” by funding cash-burning AI startups, essentially moving money from one pocket to another with extra steps. We think this view is misplaced, but it has clearly struck a nerve inside Nvidia. The finance team issued a detailed response, reproduced below. We think a more realistic explanation is that Nvidia aims to protect its dominant position at the foundation labs by offering equity investment rather than cutting prices, which would lower Gross margins and cause widespread investor panic. Below, we outline the OpenAI and Anthropic arrangements to show how frontier labs can lower GPU TCO by buying, or threatening to buy TPUs. OpenAI hasn’t even deployed TPUs yet and they’ve already saved ~30% on their entire lab wide NVIDIA fleet. This demonstrates how the perf per TCO advantage of TPUs is so strong that you already get the gains from adopting TPUs even before turning one on. Our Accelerator Industry Model, Datacenter Industry Model and Core Research subscribers saw the industry implications well before this was announced and became market consensus. In early August, we shared with our Accelerator Model clients that we saw massive upward revisions for Broadcom / Google TPU orders in the supply chain for 2026. We also revealed that the reason for these order increases was the fact that Google would begin selling systems externally to multiple customers. In early September, we revealed that one of the big external customers will be Anthropic, with demand of at least 1 million TPUs. This was officially confirmed by Anthropic and Google in October. We also called out Meta as a big TPU customer on the 7th of November, weeks before others. In addition we have discussed other customers as well. As a result, our institutional clients have had plenty of heads up on one of the largest performance dispersions in the AI Trade to date. SemiAnalysis was the first to break all these insights because no other research firm can connect the dots from the fabs to supply chain through the datacenters to the labs. To get access to these insights as and stay ahead of the curve: sales@semianalysis.com Onto the deal. The TPU stack has long rivaled Nvidia’s AI hardware, yet it has mostly supported Google’s internal workloads. In typical Google fashion, it never fully commercialized the TPU even after making it available to GCP customers in 2018. That is starting to change. Over the past few months, Google has mobilized efforts across the whole stack to bring TPUs to external customers through GCP or by selling complete TPU systems as a merchant vendor. The search giant is leveraging its strong in-house silicon design capabilities to become a truly differentiated cloud provider. Furthermore, it aligns with marquis customer Anthropic’s continued push to diversify away from its dependence on NVDA. The Anthropic deal marks a major milestone in this push. We understand that GCP CEO Thomas Kurian played a central role in the negotiations. Google committed early by investing aggressively in Anthropic’s funding rounds, even agreeing to no voting rights and a 15% cap on their ownership to expand the use of TPUs beyond internal Google. This strategy was eased by the presence of former DeepMind TPU talent within the foundation lab, resulting in Anthropic training Sonnet and Opus 4.5 on multiple types of hardware including TPUs. Google has already built a substantial facility for Anthropic, as shown below as part of our building-by-building tracker of AI labs. Beyond renting capacity in Google datacenters through GCP, Anthropic will deploy TPUs in its own facilities, positioning Google to compete directly with Nvidia as a true merchant hardware vendor. In terms of the split of the 1M TPUs: The first phase of the deal covers 400k TPUv7 Ironwoods, worth ~$10 billion in finished racks that Broadcom will sell directly to Anthropic. Anthropic is the fourth customer referenced in Broadcom’s most recent earnings call. Fluidstack, a gold-rated ClusterMax Neocloud provider, will handle on-site setup, cabling, burn-in, acceptance testing, and remote hands work as Anthropic offloads managing physical servers. DC infrastructure will be supplied by TeraWulf (WULF) and Cipher Mining (CIFR). The remaining 600k TPUv7 units will be rented through GCP in a deal we estimate at $42 billion of RPO, accounting for most of the $49 billion increase in GCP’s backlog reported in the third quarter. We believe additional deals with Meta, OAI, SSI, and xAI could provide additional RPO + direct hardware sales for GCP in coming quarters. Despite heavy internal and external demand, Google has not been able to deploy TPUs at the pace it wants. Even though it has more control over its hardware supply than other hyperscalers that still need curry favor with Jensen, Google’s main bottleneck is power. While other hyperscalers have expanded their own sites and secured significant colocation capacity, Google has moved more slowly. We believe the core issue is contractual and administrative. Each new datacenter vendor requires a Master Services Agreement, and these are multibillion-dollar, multiyear commitments that naturally involve some bureaucracy. Yet Google’s process is especially slow, often taking up to three years from initial discussions to a signed MSA. Google’s workaround carries major implications for Neocloud providers and cryptominers looking to pivot to AI DC infrastructure. Instead of leasing directly, Google offers a credit backstop, an off-balance-sheet “IOU” to step in if Fluidstack cannot pay its datacenter rent. Neoclouds like Fluidstack are nimble and flexible, making it easier for them to deal with new datacenter vendors like reformed cryptominers. This mechanic has been key to our bullish views on the cryptomining industry – notably we were calling out numerous including IREN and Applied Digital at the beginning of the year when stock prices were materially lower. The opportunity for miners rests on a simple dynamic: the datacenter industry faces acute power constraints power, and cryptominers already control capacity through their PPAs and existing electrical infrastructure. We expect many more agreements to follow in the coming weeks and quarters. Prior to the Google/Fluidstack/TeraWulf deal, we had not seen any deal with a mere off-balance-sheet “IOU” in the Neocloud market. After the deal, we believe it has become the new de-facto standard financing template. This solves a key headache for Neoclouds looking to secure datacenter capacity and grow their business: A GPU cluster has a useful and economic life of 4-5years A large datacenter lease is typically 15+ years, with a typical payback period of ~8 years. This duration mismatch has made it very complicated for both Neoclouds and Datacenter vendors to secure financing for projects. But with the rise of the “hyperscaler backstop”, we believe the financing issues are solved. We expect a new wave of growth for the NeoCloud industry. Check out our Accelerator and Datacenter models to understand the key beneficiaries. These are the how’s and why’s behind the Anthropic deal, now let’s get into the hardware. Furthermore, Neoclouds who count Jensen as an investor such as CoreWeave, Nebius, Crusoe, Together, Lambda, Firmus, and Nscale all have a notable incentive to not adopt any competing technology in their datacenter: TPUs, AMD GPUs, or even Arista switches are off limits! This leaves a gaping hole in the market for TPU hosting that is currently filled by a combination of crypto miners + Fluidstack. In the coming months, we expect to see more Neoclouds make the tough decision between pursuing a growing TPU hosting opportunity and securing allocations of the latest and greatest Nvidia Rubin systems. The answer is simple. It is a strong chip inside an excellent system, and that combination offers compelling performance and TCO for Anthropic. 2.5 years ago, we wrote about Google’s compute infrastructure advantage. Even with silicon that lagged Nvidia’s on paper, Google’s system-level engineering allowed the TPU stack to match Nvidia in both performance and cost efficiency. We argued then that “systems matter more than microarchitecture,” and the past two years have reinforced that view. Anthropic’s massive TPU orders are a direct validation of the platform’s technical strength. The GPU ecosystem has shifted forward as well. Nvidia’s GB200 represents a major leap forward, pushing Nvidia toward becoming a true systems company that designs full servers rather than only the chip package inside. While we are on the topic of the GB200’s huge innovation in rack-scale interconnect, one underappreciated point is that Google has been scaling up TPUs within and across racks since TPU v2 back in 2017! Further down in the report, we feature a deep dive on Google’s ICI scale-up networking, the only real rival to Nvidia’s NVLink. Google’s recent Gemini 3 model is now viewed as the state of the art frontier LLM. Like all earlier versions of Gemini, it was trained entirely on TPUs. That result offers concrete proof of both TPU capability and Google’s broader infrastructure advantage. Today’s attention often centers on hardware for inference and post-training, yet pre-training a frontier model remains the hardest and most resource-intensive challenge in AI hardware. The TPU platform has passed that test decisively. This stands in sharp contrast to rivals: OpenAI’s leading researchers have not completed a successful full-scale pre-training run that was broadly deployed for a new frontier model since GPT-4o in May 2024, highlighting the significant technical hurdle that Google’s TPU fleet has managed to overcome. One of the key highlights of the new model include noticeable gains in tool calling and agentic capability, especially on longer-horizon tasks for economically valuable tasks. Vending Bench is an evaluation that aims to measure how well models would run a business over a long period of time by placing them as the owner of a simulated vending machine business and Gemini 3 destroyed the competition. This launch brought not just improved capabilities but new products. Antigravity, a product born from the acqui-hire of former Windsurf CEO Varun Mohan and team, is Google’s answer to OpenAI’s Codex, officially entering Gemini into the vibe coding token guzzling wars. For Google to quietly muscle in and establish a performance lead in one of the most challenging hardware problems is a truly impressive feat for a company whose core business isn’t, or should we say, wasn’t in hardware business. The corollary to “Systems matter more than Microarchitecture”, is that while Google has been pushing the boundary on system and networking design, TPU silicon itself wasn’t too ground-breaking. Since then, TPU silicon has made massive strides with the latest generations. From the outset, Google’s design philosophy has been more conservative on silicon relative to Nvidia. Historically, TPUs have shipped with significantly fewer peak theoretical FLOPs, and lower memory specs than corresponding Nvidia GPUs. There are 3 reasons for this. First, Google places a high internal emphasis on ‘RAS’ (Reliability, Availability, and Serviceability) for their infrastructure. Google prefers to sacrifice absolute performance in exchange for greater hardware uptime. Running things to the limit means higher instances of hardware mortality which has a real TCO impact in terms of system downtime and hot spares. After all, the hardware you cannot use has infinite TCO relative to performance. The second reason is that up until 2023, Google’s primary AI workload was recommendation system models to power their core Search and Ad properties. RecSys workloads carry much lower arithmetic intensity compared to LLM workloads which means fewer FLOPs are required relative to every bit of data that is transferred. The third comes down to the utility of “peak theoretical FLOPs” numbers that are marketed and how they can be manipulated. Merchant GPU providers like Nvidia and AMD want to market the best performance specifications possible for their chips. This incentivizes them to stretch marketed FLOPs to the highest number possible. In practice, these numbers are unable to be sustained. On the other hand, the TPU has primarily been internal facing, with much less pressure to inflate these specifications externally. This has important implications that we’ll discuss further. The generous way to look at it would be Nvidia is better at DVFS therefore happy to report peak specs only. After we ushered in the LLM era, there has been a clear shift in Google’s TPU design philosophy. We can see that with the 2 most recent TPU generations that were designed-post LLM: TPUv6 Trillium (Ghostlite) and TPUv7 Ironwood (Ghostfish) reflect that change. We can see in the chart below that for TPUv4 and v5, compute throughput was much lower than the Nvidia flagship at the time. TPUv6 came very close to the H100/H200 on FLOPs, but it came 2 years later than the H100. With TPU v7, the gap narrows further with servers available only a few quarters later, while delivering almost the same level of peak theoretical FLOPs. What drove these performance gains? Partially it’s that Google started announcing TPUs as they ramp into production rather then after the next generation was being deployed. Furthermore, TPU v6 Trillium is manufactured on the same N5 node as TPU v5p with similar silicon area but was able to deliver a whopping 2x increase in peak theoretical FLOPs with significantly less power! For Trillium, Google quadrupled the size of each systolic array to 256 x 256 tiles from 128 x 128, and this increase in array size is what has delivered the increase in compute. Trillium was also the last of the “E” (lite) SKUs which meant it was equipped with only 2 sites of HBM3. While Trillium closed the gap to Hopper on compute, it fell far short of the H100/H200 on memory capacity and bandwidth, with only 2 stacks of HBM3 vs 5 and 6 stacks of HBM3 and HBM3E respectively. This made it painful to use for novices, but the performance TCO achieved for Trillium is unbeatable if you get shard your model properly and utilize all those cheap FLOPS. TPU v7 Ironwood is the next iteration where Google nearly completely closes the gap to the corresponding Nvidia flagship GPU on FLOPs, memory, and bandwidth albeit with general availability 1 year later than Blackwell. Compared to the GB200, FLOPs and memory bandwidth only have a slight shortfall, with capacity being the same with 8-Hi HBM3E, which is of course a significant shortfall to GB300 which has 288GB of 12-Hi HBM3E. Theoretical absolute performance is one thing but what matters is real world performance per Total Cost of Ownership (TCO). While Google procures TPUs through Broadcom and pays a hefty margin, it is significantly less than the margin Nvidia earns on not only the GPUs they sell but entire the whole system including CPUs, Switches, NICs, system memory, cabling and connectors. From Google’s perspective this results in the all-in TCO per Ironwood chip for the full 3D Torus configuration being ~44% lower than the TCO of a GB200 server. This more than makes up for the ~10% shortfall on peak FLOPs and peak memory bandwidth. This is from the perspective of Google and the price they procure TPU servers at. What about Google’s external customers when Google adds their margin on top? We assume that in the case where Google earns a margin on leasing TPU 7 to external customers that the TCO per hour can still be up to ~30% lower than the cost of the GB200 and ~41% lower than the cost of the GB300. This is what we believe is reflective of Anthropic’s pricing via GCP. Comparing theoretical FLOPs tells only part of the story. What matters is effective FLOPs, since peak numbers are almost never reached in real-world workloads. In practice, Nvidia GPUs typically achieve only about a small portion of their theoretical peak once communication overhead, memory stalls, power limits, and other system effects are factored in. A good rule of thumb for training is 30%, but utilization also varies heavily by workload. A large share of the gap comes down to software and compiler efficiency. Nvidia’s advantage here stems from the CUDA moat and the wide set of open source libraries that come out of the box, helping workloads run efficiently with high realized FLOPs and memory bandwidth. The TPU software stack is not as easy to use, though this is beginning to change. Inside Google, TPUs benefit from excellent internal tooling that is not exposed to external customers, which makes out of the box performance weaker. However, this only applies to small and/or lazy users, and Anthropic is neither of those. Anthropic has strong engineering resources and ex-Google compiler experts who know both the TPU stack and understand their own model architecture well. They can invest in custom kernels to drive high TPU efficiency. As a result, they can reach substantially higher MFU and much better $/PFLOP performance. We believe that despite lower marketed peak FLOPs, TPUs can reach higher realized Model FLOP Utilization (MFU) than Blackwell, which translates into higher effective FLOPs for Ironwood. A major reason is that marketed GPU FLOPs from Nvidia and AMD are significantly inflated. Even in tests designed to maximize throughput through GEMMs shaped far from real workloads, Hopper only reached about ~80% of peak, Blackwell landed in the 70s, and AMD’s MI300 series in the 50s-60s. The limiting factor is power delivery. These chips cannot sustain the clock speeds used in the peak math. Nvidia and AMD implement Dynamic Voltage and Frequency Scaling which means that the chip’s clock frequency is dynamically adjusted based on power consumption and thermals rather than a stable clock frequency that can actually be sustained. Nvidia and AMD then select the highest clock frequency that could possibly be delivered, even if very intermittently, to be used in the calculation of peak theoretical FLOPs (operations per cycle per ALU x number of ALUs x cycles per second i.e. clock frequency). There are other tricks that are employed, like running GEMMs on tensors filled with zeroes, as 0x0=0, the transistors don’t need to switch state from 0 to 1, therefore reducing the power draw of each operation. Of course, in the real world, zero-filled tensors are not multiplied together. When we put together much lower TCO and higher effective FLOPs utilization, from the perspective of Google the $ per effective FLOP becomes much cheaper, with ~15% MFU being the breakeven with GB300 at 30% MFU. This means if Google (or Anthropic) manages to hit half the FLOPs utilization of GB300, they still come out even. Of course, with Google’s elite compiler engineer team and deep understanding of their own models, the MFU they can realize on TPUs could be 40% potentially. That would be a whopping ~62% reduction in cost per effective training FLOP! However, when looking at the 600K rented TPUs, when we incorporate the higher TCO that Anthropic pays (ie inclusive of Google’s margin stacking) into this analysis, we estimate the cost to Anthropic to be $1.60 per TPU-hour from GCP, narrowing the TCO advantage. We believe that Anthropic can realize 40% MFU on TPUs due to both their focus on performance optimization as well as the TPU’s marketed FLOPs inherently being more realistic. This provides Anthropic with a staggering ~52% lower TCO per effective PFLOP compared to GB300 NVL72. The equilibrium where TCO per effective FLOP compared to the GB300 baseline is the same is at a much lower 19% extracted MFU for Anthropic. This means that Anthropic can suffer a sizeable performance shortfall relative to the baseline GB300 and the perf/TCO for training FLOPs still ends up being the same as the baseline Nvidia system. FLOPs are not the end all and be all for performance, memory bandwidth is super important for inference, especially on the bandwidth intensive decode step. It should be no surprise that $ per memory bandwidth for the TPU also ends up being much cheaper than GB300. There is significant evidence that at small message sizes such as 16MB to 64MB (loading an expert of a single layer), TPU’s even achieve higher memory bandwidth utilization then GPUs. All of this translates into more efficient compute to train and serve a model. Anthropic’s release of Opus 4.5 continued the usual focus on coding, setting a new SWE-Bench record. The main surprise was a ~67% price cut on the API. This price cut paired with the lower verbosity and higher token efficiency of the model compared to Sonnet (76% fewer tokens to match Sonnet’s best score, and 45% fewer to exceed it by 4 points) means Opus 4.5 is the best model for coding use cases and could effectively raise Anthropic’s realized token pricing as Sonnet is over 90% of token mix today. When it comes to pricing for external customers, Google needs to thread the needle to balance their own profitability whilst offering customers a competitive proposition. Our estimate for Anthropic pricing is on the lower end of ranges we’ve heard for external pricing. For a flagship customer such as Anthropic, who will provide valuable input into both the software and hardware roadmap whilst ordering a huge amount of volumes, we’d expect sweetheart pricing. While Nvidia’s eye-watering 4x markup (~75% gross margin) offers a lot of room for pricing flexibility, a good amount of oxygen is sucked away by Broadcom. Broadcom, as the TPU’s co-designer, earns a high margin on the silicon which is the largest component of system BOM. Still, this leaves a lot of room for Google to earn very good acceptable margins. We can see this from comparing the GCP Anthropic deal to other large GPU-based cloud deals. Note that this is looking at the 600k TPUs that is being rented with the remaining 400k TPU v7 chips being bought upfront by Anthropic. Under these assumptions, the TPU v7 economics show superior EBIT margins than the other large GPU-based cloud deals we have observed, with only OCI-OpenAI coming close. Even with Broadcom’s margin stack on the chip-level BOM, Google can still eke out far superior margins and returns than much more commoditized GPU deals. This is where the TPU stack allows GCP to be a truly differentiated CSP. Meanwhile someone like Microsoft Azure, whose ASIC program is struggling, is confined to earning more mediocre returns in the mere business of leasing merchant hardware. We’ve so far discussed how TPUs compare to Nvidia GPUs, focusing on per chip specs and the shortcomings. Now, let’s get back to the system discussion which is where TPU’s capabilities really start to diverge. One of the most distinctive features of the TPU is its extremely large scale up world size through the ICI protocol. The world size of a TPU pod reaches 9216 Ironwood TPUs, with large pod sizes being a feature of TPUs as early as TPUv2 back in 2017 scaling up to a full 256 1024-chip cluster size. Let’s start at the rack level, the basic building block of each TPU superpod. The TPU rack has a similar design over the last couple of generations. Each rack consists of 16 TPU Trays, 16 or 8 Host CPU Trays depending on the cooling configuration, a ToR Switch, power supply units, and BBUs. Each TPU tray consists of 1 TPU board with 4 TPU chip packages mounted. Each Ironwood TPU will have 4 OSFP cages for ICI connections and 1 CDFP PCIe cage for the connection to the Host CPU. Google has been implementing liquid cooled TPU racks since TPU v3 in 2018, but there are still some TPU generations in between that were designed to be air-cooled. The main difference between the liquid cooled and the air-cooled rack is that the air-cooled rack has 2 TPU trays to 1 host CPU tray ratio while the liquid cooled rack has a 1 to 1 ratio instead. An innovative design of TPU’s liquid cooling is that the flow rate of the coolant is actively controlled by the valves. This enables much more efficient cooling as the flow can be adjusted depending on the amount of workload that each chip has at any given time. Google’s TPU has also long adopted vertical power delivery, in which the VRM modules of the TPUs are on the other side of the PCB board. These VRM modules also require a cold plate for cooling. Overall, the TPU rack design is much simpler than that of the Nvidia Oberon NVL72 design, which has a much higher density and utilizes a backplane to connect GPUs to scale up switches. The scale up connections between the TPU trays are all over external copper cables or optics, which will be explained in the ICI section below. The connection between the TPU tray and the CPU tray is also over PCIe DAC cable. The building block of Google’s ICI scale-up network for TPUv7 is a 4x4x4 3D torus consisting of 64 TPUs. Each 4x4x4 cube of 64 TPUs maps to one physical rack of 64 TPUs. This is an ideal dimension as all 64 TPUs can be connected electrically to each other and still fit in a physical rack. The TPUs are connected to each other in a 3D torus configuration, with each TPU connecting to 6 neighbors total – 2 logically adjacent neighboring TPUs for each of the X, Y and Z axes. Each TPU is always connected to 2 other TPUs via PCB traces within the compute tray but depending on where the TPU is located within the 4x4x4 cube, it will connect to 4 other neighbors either via Direct Attach Copper (DAC) cables or via an Optical Transceiver. Connections within the interior of the 4x4x4 cube happen over copper, while connections outside of the 4x4x4 cube (including wrap-around connections back to the other side of the cube as well as connections to neighboring 4x4x4 cubes) will use optical transceivers and OCSs. In the below diagram, we see that as this is a 3D Torus network: TPU 2,3,4 (on the Z+ face) has a wraparound connection back to the opposite Z-axis face to TPU 2,3,1 (on the Z- face) using an 800G optical transceiver and routing through an OCS. As mentioned above, in addition to the 2 neighboring TPUs that are always connected via PCB traces, TPUs will connect to 4 other neighbors using DACs, transceivers or a mix of both depending on where in the 4x4x4 cube they are. TPUs in the interior of the 4x4x4 cube will connect to the 4 other neighbors exclusively using DACs, TPUs on the face of the cube will connect via 3 DACs and 1 optical transceiver, TPUs on the edge of the cube will connect via 2 optical transceivers and 2 DACs, while TPUs on the corners will connect via 1 DAC and 3 optical transceivers. You can remember how many transceivers a given TPU will use by looking at how many of the TPU’s sides are facing the “outside” of the cube. The diagram above, as well as the table below, summarizes the number of respective location types for TPUs and can be used to derive the attach ratio of 1.5 Optical Transceivers per TPU v7. These transceivers connect to Optical Circuit Switches (OCSs) which enable connections between 4x4x4 cubes – more on that in the next section. Google adopts a software-defined networking approach to manage network routes through Optical Circuit Switches (OCSs). An NxN OCS is basically a massive train station with N tracks in and N tracks out. Any train coming in can be transferred to any train coming out, but this has to be reconfigured at the station. Trains cannot be “looped back” or sent back on another N track in, they must be routed only to one of the N tracks out. The benefit of this approach is that the network can assemble smaller logical TPU slices – for different workloads from the theoretical maximum of 9,216 chips in the ICI network layer. By rerouting ICI paths around faults in the network through slicing a larger cluster, cluster availability improves. Unlike Electronic Packet Switching (EPS) switches such as an Arista Tomahawk 5 where there is a fixed total bandwidth that is further split into several ports of smaller bandwidth sizes, OCSs allow any bandwidth of optical fiber to be connected to its ports. OCSs is also low latency compared to EPSs because optical signals entering an OCS simply bounce from the input port to the output port. For EPSs, optical signals must be converted to electrical signals when entering the switch – one key reason why an OCS is typically more power efficient than an EPS. An EPS also allows routing of packets from any port to any port, while an OCS only allows you to route an “in” port to any other “out” port only. OCS ports only route individual fiber strands. This becomes a challenge for standard duplex transceivers because bandwidth is transmitted over multiple fiber strands, which reduces the effective radix and bandwidth of the OCS. To solve this problem, an FR optical transceiver is used to consolidate all wavelengths onto a single fiber strand to be connected to 1 OCS port. The Apollo Project innovatively achieved this in two steps. First, the 8 wavelengths – 1 wavelength for each 100G lane – are multiplexed through Coarse Wave Division Multiplexing (CWDM8) to transmit 800G over a single fiber pair, instead of 8 fiber pairs. Second, an optical circulator is integrated on the wave division multiplexing (WDM) transceiver to enable full duplex data flow, reducing requirements from 1 fiber pair to only 1 fiber strand. The circulator forms a bi-directional link by combining the Tx and Rx fiber strands at the transceiver onto a single fiber strand that is sent to the OCS switch. Google’s ICI scale-up network is unique in that it allows the connection of multiple 64 TPU 4x4x4 Cubes together in a 3D torus configuration to create massive world sizes. The TPUv7 has a stated maximum world size of 9,216 TPUs but today, Google supports the configuration of TPUs into multiple different slice sizes of between 4 TPUs all the way up to 2,048 TPUs. While Google can innovatively achieve an impressive scale-up cluster of 9,216 TPUs, the benefit of running training workloads on incrementally larger block sizes of up to approximately 8,000 TPUs at any point in time decreases. This is because larger block sizes are more prone to failures and disruption, therefore decreasing slice availability, which is defined by the fraction of time in which the ICI cluster is able to form a contiguous 3D torus slice. For slices that can fit entirely within a 4x4x4 Cube, we can simply carve these slices out of that cube using the copper interconnects within the rack as well as the optical transceivers on the face/edge/corner of the cube to wrap around and complete the 3D Torus if needed.To see how wraparound and inter-cube connections are made, let’s start by looking at how we would create a 64 TPU slice in a 4x4x4 topography. We can use the unit 4x4x4 cube of 64 TPUs corresponding to one physical 64 TPU rack to build up this topography. All 8 TPUs in the interior of the 4x4x4 cube can fully connect to all 6 neighbors using copper. If a TPU does not have an interior neighbor along a given axis, it will wrap around and connect to a TPU on the opposite side of the cube. For example, TPU 4,1,4 has no interior neighbor in the Z+ direction, so it will use one 800G optical transceiver to connect to an OCS assigned to the Z-axis, with the OCS configured to direct this connection to the Z- side of the cube, connecting to TPU 4,1,1. In the Y- direction, TPU 1,1,1 will use an optical transceiver to connect to a Y-axis OCS to link to the Y+ side of TPU 1,4,1 and so on. Each face of the 4x4x4 cube will connect via 16 different OCSs – one OCS for each TPU on each face. For example, in the diagram below, on the X+ face, TPU 4,3,2 connects to the input side of OCS X,3,2. OCS X,3,2’s input side will also connect to the same TPU Index (4,3,2) on the X+ face of all 144 4x4x4 Cubes in the 9,216 TPU cluster. OCS X,3,2’s output side will then connect to the same TPU Index for every single cube in the cluster except this time on the X- face – so it will connect to TPU 1,3,2 on all 144 cubes of the cluster. The diagram below illustrates how all 16 TPUs on the X+ face of Cube A connect via 16 OCSs to 16 TPUs on the X- of Cube B. These connections allow any “+” face of any cube to connect to the “-“ face of any other cube, enabling complete fungibility of cubes when forming slices. There are two constraints to briefly point out. First, TPUs of one Index on a given face can never connect directly to a different index – so TPU 4,3,2 could never be configured to connect to TPU 1,2,3. Second, as the OCS essentially acts as a patch panel – TPUs connected on the input side cannot “loop back” to connect to any other TPU that is also connected on the input side of the OCS - as an example, TPU 4,3,2 can never connect to TPU 4,3,3. So – any TPU on the “+” face can never connect to the “+” face of any other cube, and any TPU on the “-” face can never connect to the “-” face of any other cube. Let’s go larger and see how a 4x4x8 Topography could be set up. In this configuration, we extend the slice by connecting two 64 TPU 4x4x4 cubes along the Z-axis. In this case, the OCS will reconfigure the optical port that TPU 4,1,4 is connected to so that it now connects to TPU 4,1,5 instead of wrapping around back to TPU 4,1,1 as was the case for a standalone 4x4x4 topography. Extending this, we will have 16 optical connections extending from the Z- and Z+ faces of each of the two 4x4x4 TPU cubes, for a total of 64 Fiber strands connected into 16 Z-Axis OCSs. It is important to remind readers that Cube A and Cube B depicted below are not necessarily physically located next to each other. Instead, they are connected via OCSs and they could each be in completely different locations in the datacenter. We will now move to a much larger topology – the 16x16x16 topology, which brings us up to 4,096 TPUs. In this topology, we use a total of 48 OCSs to connect 64 Cubes of 64 TPUs each. In the diagram below, each multi-colored cube represents one 64 TPU 4x4x4 cube. Taking the bottom right 4x4x4 cube as an example – this cube is connected to adjacent cubes along the Y-axis via OCSs. The maximum world size of 9,216 TPUs is built up using 144 4x4x4 cubes requiring 96 optical connections each amounting to a total requirement of 13,824 ports. Dividing this total port requirement by 288 (144 input and 144 output ports on each OCS) means we need 48 144x144 OCSs to support this maximum world size. But what is so great about Google’s unique ICI scale-up network – other than all the fancy cube diagrams one can spend countless hours drawing? World Size: The most obvious benefit is the very large 9,216 TPU maximum world size that the TPUv7 Ironwood supports. Even though the maximum slice size of 9,216 may rarely be used due to the drawback of diminished goodput, slices of thousands of TPUs can and are commonly used. This is far larger than the 64 or 72 GPU world size that is common in the merchant accelerator market and for other custom silicon providers. Reconfigurable and Fungibility: The use of OCSs mean that the network topology inherently supports the reconfiguration of network connections to support a high number of different topologies – in theory thousands of topologies. Google’s documentation site lists out 10 different combinations (image earlier in this section), but these are only the most common 3D slice shapes – there are many more available. Even slices of the same size can be reconfigured differently. In the simple example of a Twisted 2D Torus diagrammed below, we see how looping across to an index of a different X coordinate instead of an index of the same X coordinate can reduce the worst-case number of hops and the worst-case bisection bandwidth. This can help improve all to all collective throughput. A TPUv7 cluster will twist at the 4x4x4 cube level. Reconfigurability also opens the door to a broad diversity of parallelisms. In a 64 or 72 GPU world size, different parallelism combinations are generally limited to the factors of 64. When it comes to the ICI scale-up network, the possibilities for implementing topologies to precisely match the combination of data parallelism, tensor parallelism and pipeline parallelism desired are plentiful. The fact that OCSs allow one to connect any “+” face of any cube to the “-“ face of any other cube means that there is complete fungibility of cubes. Slices can be formed out of any set of cubes. So if there are any faults or change in user demands or usage, this will not obstruct the formation of new topology slices. Lower Cost: Google’s ICI network has a lower cost than most switched scale-up networks. Though the FR optics used can be slightly expensive due to the use of circulators, the mesh network reduces the overall number of switches and ports that are needed and eliminates cost arising from connections between switches. Low Latency and Better Locality: The use of direct links between TPUs means that it is possible to achieve much lower latency for TPUs that are physically located close to one another or are reconfigured to connect directly to each other. TPUs that are close to each other also have better data locality. The Datacenter Network (DCN) is a network separate to ICI that serves the role of both a typical backend and front-end network. It connects across an even larger domain – 147k TPUs in the case of TPUv7 clusters. As discussed in our earlier post on Mission Apollo, where Google proposed replacing the Electronic Packet Switch (EPS)-containing spine layer of the traditional “Clos” architecture with Paloma Optical Circuit Switches (OCS), Google’s DCN consists of an optically switched Datacenter Network Interconnect (DCNI) layer that combines several aggregation blocks, each of which connects several 9,216 TPU ICI clusters. In 2022, Google’s Apollo project proposed a DCN architecture that described using 136x136 OCS switches for TPUv4 pods with a pod size of 4,096 TPUs. OCS switches at the DCNI layer were organized into 4 Apollo zones, each containing a maximum of 8 racks of 8 OCS switches for a total of 256 OCS switches. When it comes to Ironwood, to support up to 147k TPUv7s on the same network, we hypothesize that the number of ports on the OCS will nearly double as opposed to increasing the maximum number of OCS switches.The diagram below illustrates what an Ironwood DCN network using 32 racks holding 256 300x300 OCS switches could look like. Assuming that there is no oversubscription between the spine layers of each aggregation block, a maximum of 16 ICI pods can be connected in the DCN with 4 aggregation blocks connecting 4 ICI pods each – a total of 147,456 TPUs. The DCNI layer connects the 4 aggregation blocks – depicted as the top layer in the diagram below. As with ICI, FR Optics are used to connect to the OCSs in order to maximize bandwidth per port on each OCS. While existing Ironwood clusters may only have 1 or 2 aggregation blocks, Google DCN’s unique architecture allows for new aggregation blocks of TPUs to be added to the network without significant rewiring. By using OCSs for the DCNI layer, the size of the DCN fabric can be incrementally expanded and the network can be re-striped to support new aggregation blocks. Furthermore, the bandwidth of aggregation blocks can be upgraded without having to change the make-up of the DCN layer. This allows the link speeds of existing aggregation blocks to be refreshed without changing the fundamental architecture of the network itself. The process of fabric expansion cannot go on indefinitely – at significant scale, it becomes unmanageable to rewire the network. Traditionally, TPU software and hardware teams have been internal-facing. This comes with advantages such as the absence of pressure by marketing teams to inflate stated theoretical FLOPs. Another advantage of being only internal facing is that the TPU teams heavily prioritized internal feature requests & optimizing internal workloads. The disadvantage is that they did not care much about external customers or workloads. The number of external developers in the TPU ecosystem is way lower than in the CUDA ecosystem. This is one of the main weaknesses of the TPU as it is with all non-Nvidia accelerators. Google has since revised their software strategy for externally-facing customers and has already made major changes to their TPU team’s KPIs and how they approach contributing to the AI/ML ecosystem. There are 2 major changes that we will discuss: Massive engineering effort on PyTorch TPU “native” support Massive engineering effort on vLLM/SGLang TPU support The externalization strategy is clear to see by looking at the number of contributions from various TPU software repos by Google. We can see a noticeable increase in vLLM contributions starting from March. Then from May, the “tpu-inference” repo was created which is the official vLLM TPU unified back-end, and since then there has been a flurry of activity. Traditionally, Google only had first class support on the Jax/XLA:TPU stack (and TensorFlow/TF-Mesh RIP), but treated PyTorch on TPU as second class citizen. It relied on lazy tensor graph capture through PyTorch/XLA instead of having a first-class eager execution mode. Furthermore, it did not support PyTorch native distributed APIs (torch.distributed.*) or support PyTorch native parallelism APIs (DTensor, FSDP2, DDP, etc), but relied on weird out of tree XLA SPMD APIs (torch_xla.experimental.spmd_fsdp, torch_xla.distributed.spmd, etc.). This has led to a subpar non-native experience for external users that are used to the native PyTorch CUDA backend on GPUs and trying to switch to TPUs. In October, Google’s “Captain Awesome” Robert Hundt quietly announced in the XLA repo that they will be moving away from a non-native lazy tensor backend towards a “native” TPU PyTorch backend that will support eager execution by default & integration with torch.compile & DTensor & torch.distributed APIs, etc. They will be doing this through the use of PrivateUse1 TorchDispatch key. This will mainly be done for Meta who has renewed interest in buying TPUs & does not want to move to JAX. It will also make it for people that enjoy PyTorch and do not like JAX to use TPUs too. Previously from 2020 to 2023, heavily used by a couple of teams at Meta FAIR used PyTorch XLA on TPUs but it was not widely adopted thus Meta leadership ended up cancelling the contracts in 2023. PyTorch XLA on TPUs is not a fun experience. The Meta FAIR GCP TPUs back then were even run using SLURM and not anything typical you would find on TPU stack like GKE/Xmanager/borg/etc. This new PyTorch <> TPU will create a smoother transition for ML scientists that are used to PyTorch on GPUs to switch to PyTorch on TPUs and take advantage of the higher performance per TCO on TPUs. Pallas is the kernel authoring language for writing custom kernels for TPU (similar to cuTile or Triton or CuTe-DSL). Meta & Google have also started work on supporting Pallas kernels as a codegen target for the Torch Dynamo/Inductor compile stack. This will allow for native TPU integration with PyTorch’s native torch.compile API & allow for end users to register custom pallas ops into PyTorch. In addition to the core in tree PyTorch native APIs, there is also work behind the scenes on integrating TPU pallas kernel language as a codegen target for Helion. You can think of Helion as a higher-level language for writing decently performing kernels in a high level language. Users can think about Helion as a low level Aten operators rather than as high level Triton/Pallas due to its similarity matching much closer to the Native PyTorch Aten ops. Another area where the CUDA ecosystem is supreme is for open ecosystem inference. Historically, vLLM & SGLang support CUDA as first class (as ROCm as 2nd class citizen). Now Google wants in to the vLLM & SGlang open inference ecosystem and have announced beta TPU v5p/v6e support for vLLM & SGLang through a very “unique” integration. vLLM& SGLang currently does this by lowering the PyTorch modelling code into JAX and taking advantage of the existing mature JAX TPU compilation flow. In the future once PyTorch XLA RFC #9684 (aka native TPU PyTorch backend) gets implemented, vLLM & SGLang plan on evaluating whether to switch to using that instead of translating modelling from PyTorch to JAX through TorchAX. Google & vLLM claim that this lowering to jax path does not require any changes to the PyTorch modelling code but given how few models vLLM TPU supports so far, we doubt this is true. Furthermore, Google has open-sourced & integrated some of their TPU kernels into vLLM such as a TPU optimized paged attention kernels, compute-comms overlapped GEMM kernels & a couple other quantized matmul kernels. They do not yet have MLA-friendly TPU kernels. It would be interesting to see once Inductor Pallas TPU codegen integration is more mature, whether it is possible to integrate kernel fusion & pattern matching into the existing vLLM PassManager. SGLang is also looking into implementing an torch.compile PassManager to make managing kernel fusions for many models more maintainable. For Ragged Paged Attention v3, TPU handles it quite differently from vLLM GPU. vLLM manages KV cache with a technique similar to virtual memory and paging. However, this technique requires fetching dynamic addresses and performing scatter operations, something TPUs don’t support well. As a result, TPU kernels leverage fine-grained operation pipelining. Specifically, TPU’s page attention kernel prefetches query and KV blocks for the next sequence, so the memory loading is overlapped with computation. In the existing vLLM MoE kernel, we sort tokens by expert ID, dispatch tokens to the devices with the corresponding experts, perform group matrix multiplication, and combine tokens from experts back to original devices. However, the kernel performs poorly for two reasons: TPUs are slow at performing sorting operations, and the kernel is unable to overlap communication with computation. To work around this issue, Google developers designed all-fused MoE. All-fused MoE dispatches tokens for one expert per device at a time while overlapping MoE dispatch & MoE combine communications & avoiding sorting tokens by expert ID. With all-fused MoE, the Google engineer reported 3 - 4x speedup over existing kernels. Furthermore, another hardware unit in TPUs is the SparseCore (SC) used to accelerate embedding lookups and updates. SC comes with a scalar subcore SparseCore Sequencer (SCS) and multiple vector subcores SparseCore Tiles (SCT). SCT supports local and remote direct memory access at a more fine-grained 4-byte or 32-byte granularity, compared to TPU TensorCore’s 512-byte loads. This enables SC to perform gather/scatter operations and ICI communications while overlapping with TensorCore operations. At JAX DevLabs, we learned that programmability of SparseCore is a work in progress. We can expect Mosaic, the TPU custom kernel compiler, to compile in an MPMD fashion, where SCS and SCT executes different kernels, and different SparseCores can run different programs. We suspect once the programmability catches up, TPU MoE kernels would be able to perform dispatch and combine operations in a similar way as GPUs, instead of dispatching by expert IDs. In terms of disaggregated prefill decode, which we described in depth in our AMD 2.0 post, Google has experimental support on vLLM for single host disagg PD, not they do not support multi-host wideEP disagg prefill or MTP yet. These inference optimizations are critical to lower the TCO per million tokens and increase the perf per dollar and perf per watt. Furthermore, they have not yet integrated TPU vLLM inference support into popular RL frameworks like VERL, etc. Google is slowly moving in the correct direction in terms of how they approach the open AI/ML ecosystem especially for their “native” TPU backend. This week, there was an new inference benchmark on TPUv6e that dropped claiming that TPUv6e has 5x worst performance per dollar than NVIDIA GPUs. We disagree mainly due to 2 reasons. First of all, this is benchmark is on vLLM on TPUs which was only released an couple month ago thus does not yet have optimized performance. Google internal Gemini workloads & Anthropic workloads work on an internal custom inference stack that has better perf per TCO than NVIDIA GPUs. Secondly, Aritifical Analysis’s cost per million tokens is using the list price of $2.7/hr/chip for TPUv6e. No major customers of TPUs is paying anywhere close to that much for TPUv6e given the BOM is a tiny fraction of the H100. As everyone knows, most clouds have an high ball list price such that their account sales executives can do “car salesman” tactics and give massive discounts so that the customer thinks they are getting a good deal. The SemiAnalysis AI TCO Model tracks the acutal market rental price of TPUs across all the various contract lengths (1 month, 1 year, 3 years, etc). One part where Google is still approaching their software strategy incorrectly is with their XLA graph compiler & networking libraries & TPU runtime is still not open sourced nor well documented. This has led to frustrated users across the spectrum from advanced users to the average user of not being able to debug what is going wrong with their code. Furthermore their MegaScale codebase for multi-pod training is not open source either. We strongly believe that in order to accelerate the adoption, Google should open source it and the increased user adoption will outweigh all the software IP they will make public & free. Just like how PyTorch or Linux being open sourced rapidly increased adoption, open sourcing XLA:TPU & TPU runtime & networking libs will rapidly accelerated this too. Now that Google has gotten their act together on TPU and is selling them externally for people to put in their own datacenters, what are the implications on Nvidia’s business? Does Nvidia finally have a legitimate competitor that will put its market share and margins at threat? Behind the paywall, we will share our thoughts on what this means for Nvidia as well as reveal more about the TPU roadmap.
--------------------------------------------------

Title: Wildly Violent (and Profitable)
URL: https://dailyreckoning.com/wildly-violent-and-profitable/
Time Published: 2025-11-29T23:00:03Z
Full Content:
By Adam Sharp PostedNovember 29, 2025 The story of the world’s first IPO is a wild one. The Dutch East India Company (VOC) was founded in Amsterdam in 1602. Over the next hundred years, the VOC would dominate trade with Asia, wage multiple wars, commit genocide, and build monopolies. Along the way they sold gobs of spices, silks, and opium. But before all of that could happen, the VOC’s founders needed to raise money. So in 1602 the Dutch East India Company conducted the first initial public offering, giving all Dutchmen the chance to invest in this new venture. VOC Co-Founder Dirck Van Os | Source: Wikipedia The VOC IPO was unique because it was open to anyone, not just wealthy merchants and royalty. In total, 1,143 investors participated. Co-founder Dirck Van Os’ maid, Neeltgen Cornelis, was the second-to-last investor on the registry, having subscribed 100 guilders, which was likely her life’s savings. If Mrs. Cornelis the maid held onto her shares for the long run, she did quite well. In some years, VOC investors received a dividend equivalent to 75% of the capital they initially committed. Now, that dividend occasionally came in the form of spices (cloves and mace in particular). But hey, profit is profit… Dutch historian Lodewijk Petram has calculated the return of VOC shares (with dividends reinvested), and from its founding in 1602 through 1698, each 100 guilders invested turned into about 65,000. Source: The World’s First Stock Exchange VOC shares were groundbreaking because they were tradable on what would become the world’s first stock exchange. In addition, the Dutch East India Company introduced the concept of limited liability for shareholders. In other words, investors could only lose the amount they invested. They wouldn’t be responsible for the company’s debts or losses if the venture failed. The VOC set a new standard for how companies could raise capital and distribute profits. But the business model was downright diabolical… The VOC quickly became a publicly-traded empire. By 1637 it was worth the equivalent of around $6.9 trillion in today’s money. That’s double the value of the largest modern companies such as Apple (AAPL), Microsoft (MSFT), and Nvidia (NVDA). The Dutch East India Company maintained its own navy with around 40 warships and 200 trading vessels at any given time. It fought wars against Spain, Portugal, Britain, and various Asian powers. These great powers clashed over lucrative trade routes and assets in Asia. They fought over territories such as the Spice Islands and the Philippines, and sold opium to China. In one notorious incident, the VOC invaded the Banda Islands, the world’s only source of nutmeg and mace at the time. Back then these spices were more valuable than gold. Using mercenaries which included Japanese samurai, VOC Governor-General Jan Pieterszoon Coen brutally massacred the Bandanese, publicly executing its leaders. Of around 15,000 natives, only a few hundred are believed to have survived. The VOC brought in slaves and other workers to take over Banda’s nutmeg and mace operations. These newly-acquired plantations were wildly profitable for shareholders. The company now had a total monopoly on two of the most valuable spices, which they sold into European markets at 1,000% markups. The Dutch East India Company was basically a publicly-traded evil empire. But for many years it did succeed in its mission of rewarding shareholders. Over time, however, the company lost its monopoly on spices and trade with Asia. Nutmeg and mace were smuggled out of Banda and cultivated elsewhere. Eventually the price of these once-rare goods collapsed. Artificial monopolies can only last so long. The VOC became increasingly corrupt and bureaucratic. It took on too much debt, and in time it essentially became a ponzi scheme. Ever-increasing debt loads were required to pay dividends. That’s a classic sign of the end approaching. The British East India Company eventually became a fierce competitor and seized many of the VOC’s prime territories. In 1799 the Dutch government dissolved the VOC and absorbed its assets and debts. The company had lasted almost 200 years. Despite the evil deeds committed by the Dutch East India Company, the firm played a critical role in the history of finance. It set a new standard for how companies could raise money and distribute profits to shareholders.
--------------------------------------------------

Title: Jim Cramer Explains Why He Says “Own, Don’t Trade the Stock of NVIDIA”
URL: https://finance.yahoo.com/news/jim-cramer-explains-why-says-175304790.html
Time Published: 2025-11-29T17:53:04Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the stocks Jim Cramer recently talked about. Cramer explained why he “won’t give up” on the stock, as he commented...
--------------------------------------------------

Title: Dan Ives Says These Are the Top 3 Stocks to Buy Right Now
URL: https://www.barchart.com/story/news/36371180/dan-ives-says-these-are-the-top-3-stocks-to-buy-right-now
Time Published: 2025-11-29T17:00:02Z
Description: The Wedbush analyst reckons these stocks to be table pounders at the moment.
--------------------------------------------------

Title: Meta Platforms May Ditch NVIDIA Chips—Here’s Why Investors Care
URL: https://www.marketbeat.com/originals/meta-to-buy-google-chips-why-it-could-shift-demand-from-nvidia/?utm_source=yahoofinance&amp;utm_medium=yahoofinance
Time Published: 2025-11-29T15:12:00Z
Description: Meta Platforms may be looking to alter where it spends its AI dollars. See what benefits the firm could reap from Google and Broadcom's chips.
--------------------------------------------------

Title: Nvidia (NVDA) Responds to Competition Fears as Meta Explores Google’s TPUs
URL: https://finance.yahoo.com/news/nvidia-nvda-responds-competition-fears-110654850.html
Time Published: 2025-11-29T11:06:54Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the AI Stocks Making Headlines on Wall Street. On November 25, Bank of America maintained a positive outlook on...
--------------------------------------------------

Title: ChatGPT was unveiled 3 years ago, kicking off the AI revolution. For investors, it did even more.
URL: https://finance.yahoo.com/news/chatgpt-was-unveiled-3-years-ago-kicking-off-the-ai-revolution-for-investors-it-did-even-more-110014912.html
Time Published: 2025-11-29T11:00:14Z
Description: OpenAI released ChatGPT on Nov. 30, 2022, not only kicking off the AI revolution but turning around one of the worst market environments of this century.
--------------------------------------------------

Title: Jim Cramer on NVIDIA: “Anything That Allows the Company to Sell in China Would Be Huge”
URL: https://finance.yahoo.com/news/jim-cramer-nvidia-anything-allows-064250979.html
Time Published: 2025-11-29T06:42:50Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the stocks that received Jim Cramer’s latest comments. Cramer noted the recent chatter around the President...
--------------------------------------------------

Title: Is NVIDIA (NVDA) One of the Best Semiconductor Stocks to Buy Heading into 2026?
URL: https://finance.yahoo.com/news/nvidia-nvda-one-best-semiconductor-061117049.html
Time Published: 2025-11-29T06:11:17Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the best Semiconductor Stocks to Buy Heading into 2026. On November 20, Raymond James analyst Simon Leopold...
--------------------------------------------------

Title: Alphabet: The AI Leader Best Positioned to Dominate 2026
URL: https://www.marketbeat.com/originals/alphabet-the-ai-leader-best-positioned-to-dominate-2026/?utm_source=yahoofinance&amp;utm_medium=yahoofinance
Time Published: 2025-11-28T20:07:00Z
Description: Alphabet has flipped its H1 sentiment to overwhelmingly bullish, boosted by accelerating growth, the release of Gemini 3, and Berkshire’s recent stake.
--------------------------------------------------

Title: Intel's Stock Pops as Rumors Swirl About a Big New Customer
URL: https://www.investopedia.com/intel-s-stock-pops-as-rumors-swirl-about-a-big-new-customer-apple-intc-11858735
Time Published: 2025-11-28T20:06:12Z
Full Content:
Could Intel be closer to scoring a deal with iPhone maker Apple? The chipmaker's shares surged Friday as an analyst added fuel to rumors that the iPhone maker could become a new customer for the chipmaker. Shares of Intel (INTC) popped over 10% during Friday's shortened trading session to lead gains on the S&P 500 and Nasdaq. (Read Investopedia's daily markets coverage here.) The likelihood of Apple (AAPL) becoming a new customer for Intel "has recently improved significantly," TF International Securities analyst Ming-Chi Kuo, citing industry surveys, posted on X Friday, suggesting that Intel could start shipping Apple processors as soon as 2027. A deal with Apple could boost confidence in a turnaround for Intel, which still faces persistent worries about its ability to secure long-term commitments to its manufacturing business. Apple and Intel did not respond to Investopedia's requests for comment in time for publication. Shares of Intel have roughly doubled in value this year after a flurry of recent deals, including a partnership with AI chip leader Nvidia (NVDA). Still, they remain well off their historical highs as the company is still working to convince investors regarding a sustainable turnaround in its business. One of those challenges continues to be securing new customers for its manufacturing operation. The deal with Nvidia did not include commitments to Intel's foundry, raising speculation about whether the relationship could expand later or if the lack of foundry commitments could point to troubles convincing customers. A deal with former Intel customer Apple could go a long way in helping assuage those concerns. Shares of Apple rose a bit less than 0.5% on Friday.
--------------------------------------------------

Title: Stock market today: S&P 500, Dow rise to end a rocky month, Nasdaq snaps 7-month win streak
URL: https://finance.yahoo.com/news/live/stock-market-today-sp-500-dow-rise-to-end-a-rocky-month-nasdaq-snaps-7-month-win-streak-180355656.html
Time Published: 2025-11-28T18:03:55Z
Description: US stocks rose for the fifth day in a row on Friday, but the Nasdaq snapped its monthly winning streak.
--------------------------------------------------

Title: Michael Burry just started a massive group chat on Substack, and it's as chaotic as you'd expect
URL: https://www.businessinsider.com/michael-burry-substack-group-chat-big-short-subscribers-ai-stocks-2025-11
Time Published: 2025-11-28T17:39:20Z
Full Content:
Every time Theron publishes a story, you’ll get an alert straight to your inbox! Enter your email By clicking “Sign up”, you agree to receive emails from Business Insider. In addition, you accept Insider’s Terms of Service and Privacy Policy. Michael Burry's group chat is going off. The investor of "The Big Short" fame invited all of his paid Substack subscribers to a group chat on Friday, and it quickly led to hundreds of replies ranging from memes to questions for Burry. This month, Burry pivoted from running a hedge fund to publishing a newsletter named "Cassandra Unchained." It has amassed more than 97,000 subscribers since it launched on Sunday. "This is a conversation space exclusively for paid subscribers—kind of like a group chat or live hangout," reads Burry's introductory post. "I plan to post updates that come my way, and you can jump into the discussion." The first reply on the chat reads: "I think Dr. Burry just broke Substack." Another early response jokes about Burry's disclosure this week that he owns bearish put options on Nvidia and Palantir stock: "I think Dr Burry is going to make more money from Substack than his NVDA and pltr puts 🤣" A third poked fun at a potential spike in traffic to Substack. "Someone pray for substacks backend engineers." "It's gonna be legendburry!!" one subscriber wrote, while another noted: "This chat is gonna be nuts." "Bro don't allow anyone to start threads. This a spam fest," one concerned poster added. Other subscribers rushed to post memes, videos, and even photos of Black Friday crowds. The questions to Burry ranged from who the next chair of the Federal Reserve might be, to how an 80-year-old should invest to prepare for a crash, to how the dollar stacks up against other currencies. Burry resurfaced on X in late October after more than two years of silence, and has wasted no time issuing numerous warnings of an AI bubble and taking aim at key players such as Nvidia and Palantir. The investor, who has 1.6 million X followers, is best known for predicting and profiting from the collapse of the US housing bubble that triggered a global financial crisis, and for issuing dire pronouncements of crashes and recessions. He became famous in financial circles after his bet against the subprime mortgage market was featured in author Michael Lewis' book "The Big Short," and actor Christian Bale played him in the movie adaptation. Jump to
--------------------------------------------------

Title: AI Stocks You Should Buy to Boost and Reenergize Your Portfolio
URL: https://finance.yahoo.com/news/ai-stocks-buy-boost-reenergize-163800128.html
Time Published: 2025-11-28T16:38:00Z
Description: Here, we have picked three AI stocks, NVDA, MU and ADI, which are well-poised to benefit from AI's growing use and ability to solve complex problems.
--------------------------------------------------

Title: Earnings live: S&P 500 on track for solid Q3 season, with reports from Macy's, C3.ai, Salesforce on deck
URL: https://finance.yahoo.com/news/live/earnings-live-sp-500-on-track-for-solid-q3-season-with-reports-from-macys-c3ai-salesforce-on-deck-151055563.html
Time Published: 2025-11-28T15:10:55Z
Description: The third quarter earnings season has been mostly positive, with most of the reports in the rearview mirror.
--------------------------------------------------

Title: Dow, S&P 500, Nasdaq futures muted as rocky month draws to an end, CME restores trading
URL: https://finance.yahoo.com/news/live/dow-sp-500-nasdaq-futures-muted-as-rocky-month-draws-to-an-end-cme-restores-trading-140109707.html
Time Published: 2025-11-28T14:01:09Z
Description: CME is gradually resuming operations after a futures outage that halted trading in US stock indexes.
--------------------------------------------------

Title: Dow, S&P 500, Nasdaq open muted as rocky month draws to an end, CME restores trading
URL: https://finance.yahoo.com/news/live/dow-sp-500-nasdaq-open-muted-as-rocky-month-draws-to-an-end-cme-restores-trading-140109595.html
Time Published: 2025-11-28T14:01:09Z
Description: CME is gradually resuming operations after a futures outage that halted trading in US stock indexes.
--------------------------------------------------

Title: Stock market today: Nasdaq, S&P 500, Dow rise toward a 5th straight day of gains to cap a rocky month
URL: https://finance.yahoo.com/news/live/stock-market-today-nasdaq-sp-500-dow-rise-toward-a-5th-straight-day-of-gains-to-cap-a-rocky-month-140109029.html
Time Published: 2025-11-28T14:01:09Z
Description: CME is gradually resuming operations after a futures outage that halted trading in US stock indexes.
--------------------------------------------------

Title: Wall Street Likes Server Stocks After Nvidia’s Q3. Is DELL or HPE Stock a Better Buy Here?
URL: https://www.barchart.com/story/news/36356183/wall-street-likes-server-stocks-after-nvidias-q3-is-dell-or-hpe-stock-a-better-buy-here
Time Published: 2025-11-28T12:30:02Z
Description: NVDA results and commentary from CEO Jensen Huang support a continued bull market for AI stocks. Dell and HPE are both expected to benefit.
--------------------------------------------------

Title: US stock futures rose today as Dow, S&P 500 and Nasdaq all in green – here's top pre-market gainers
URL: https://economictimes.indiatimes.com/news/international/us/us-stock-futures-rose-today-as-dow-sp-500-and-nasdaq-all-in-green-heres-top-pre-market-gainers/articleshow/125635580.cms
Time Published: 2025-11-28T10:34:00Z
Full Content:
U.S. stock futures rose on Friday. Dow Futures hit 47,542, up 0.11%. S&P 500 Futures touched 6,835, up 0.10%. Nasdaq Futures moved to 25,347.75, up 0.18%. Pre-market gainers were strong. SMX jumped 78.16%. CDT rose 15.46%. Micron gained 3.24%. Crypto-linked stocks also advanced. The market showed steady buying ahead of new economic data. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Explore More Stories What is Canada’s new colour-coded alert system and how it will ensure public safety during extreme weather Canada launches new alert system for extreme weather; Colour-coded system marks shift to impact-based forecasting Canadian Olympic swimming champion Penny Oleksiak suspended for two years over anti-doping whereabouts failures Who is Colleen Jones? Two-time World champion curler and veteran Canadian broadcaster dies at 65 What exactly is the Canada Pension Plan? New CPP payments rolling out nationwide on November 26; Check all the payment dates of 2025-26 - Why is Quebec not part of CPP? Madeleine Poulin, Radio-Canada’s first female correspondent in Ottawa and Paris, dies at 87 Yoplait recalls 'Yop yogurt' drinks in Canada over plastic contamination fears under Class 1 category - Check out which flavor and best-before dates beverage are subject to recall CMA Awards: Vince Gill honored with prestigious Willie Nelson Lifetime Achievement Award at 59th CMA Awards in Tennessee MGK rocks Canadian fans with explosive Grey Cup halftime show; Are Megan Fox, MGK really working on their relationship post daughter’s birth? Roughriders end 12-year drought, beat Alouettes for 5th Grey Cup title Who is Dr Sanjeev Sirpal? Trouble mounts for New Brunswick doctor accused of sexual assault in hospitals as he faces additional charges; what do we know so far Russian humanoid robot 'AIDOL' shockingly faceplants during much-hyped public debut - WATCH Who is Ben Watsa, the future chairman of Fairfax financial and heir to Prem Watsa’s $100-billion empire? 'Kumbh Shahi Snan in Haridwar on…': CM Dhami announces key dates Deaths, ICU cases and FIRs trigger outrage during voter-list revision drive Air Marshal Dixit explains Integrated Theatre Command Assam Polygamy Ban sparks controversy Imran Khan’s Sister speaks out - full interview PM Modi at Sri Krishna Matha, Udupi, Karnataka ‘Suspect worked with CIA in Afghanistan’: Kash Patel reveals... Sambit Patra Slams Rahul Gandhi over 'Toolkit' controversy Ex‑CJI Gavai comments on Justice Varma cash‑recovery scandal ‘Faith, not merit?’: BJP's demand for Hindu quota at Vaishno Devi med college 'Kumbh Shahi Snan in Haridwar on…': CM Dhami announces key dates Deaths, ICU cases and FIRs trigger outrage during voter-list revision drive Air Marshal Dixit explains Integrated Theatre Command Assam Polygamy Ban sparks controversy Imran Khan’s Sister speaks out - full interview PM Modi at Sri Krishna Matha, Udupi, Karnataka ‘Suspect worked with CIA in Afghanistan’: Kash Patel reveals... Sambit Patra Slams Rahul Gandhi over 'Toolkit' controversy Ex‑CJI Gavai comments on Justice Varma cash‑recovery scandal ‘Faith, not merit?’: BJP's demand for Hindu quota at Vaishno Devi med college Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Slideshow Top Prime Articles Private Companies Top Story Listing Top Definitions Top Market Pages Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Nvidia (NVDA) Memo Calms Investors as Bernstein Reaffirms Outperform
URL: https://finance.yahoo.com/news/nvidia-nvda-memo-calms-investors-062120029.html
Time Published: 2025-11-28T06:21:20Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the AI stocks analysts are betting on. On November 26, Bernstein reaffirmed its Outperform rating on Nvidia and...
--------------------------------------------------

Title: This AI Dividend Stock Is a Buy Even as the S&P 500’s Yield Falls to Dot-Com Lows
URL: https://www.barchart.com/story/news/36349747/this-ai-dividend-stock-is-a-buy-even-as-the-s-p-500s-yield-falls-to-dot-com-lows
Time Published: 2025-11-28T00:30:02Z
Description: The S&P 500 Index’s dividend yield is now at levels last seen during the dot-com boom. For tech investors, though, Microsoft looks like a good buy given its ...
--------------------------------------------------

Title: Dow, S&P 500, Nasdaq futures muted as rocky month draws to an end, before CME glitch halts trading
URL: https://finance.yahoo.com/news/live/dow-sp-500-nasdaq-futures-muted-as-rocky-month-draws-to-an-end-before-cme-glitch-halts-trading-000737050.html
Time Published: 2025-11-28T00:07:37Z
Description: CME is gradually resuming operations after a futures outage that halted trading in US stock indexes.
--------------------------------------------------

Title: Analyst revisits Nvidia stock after Google-Meta news
URL: https://www.thestreet.com/investing/analyst-revisits-nvidia-stock-after-google-meta-news
Time Published: 2025-11-27T18:17:00Z
Description: Nvidia (NVDA) shares were volatile this week after news that Meta (META) is in talks to buy billions of dollars of Google (GOOG) chips starting in 2027. The ...
--------------------------------------------------

Title: ACM Research, Inc. (ACMR): A Bull Case Theory
URL: https://finance.yahoo.com/news/acm-research-inc-acmr-bull-181311084.html
Time Published: 2025-11-27T18:13:11Z
Description: We came across a bullish thesis on ACM Research, Inc. on FluentInQuality’s Substack. In this article, we will summarize the bulls’ thesis on ACMR. ACM...
--------------------------------------------------

Title: The easy set and forget ASX share portfolio I'd build today
URL: https://www.fool.com.au/2025/11/28/the-easy-set-and-forget-asx-share-portfolio-id-build-today/
Time Published: 2025-11-27T16:13:00Z
Description: It is easier than you think to build a winning portfolio.
The post The easy set and forget ASX share portfolio I'd build today appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: Using Probability Density to Extract a Huge Payout from Microchip’s Potential Breakout
URL: https://www.barchart.com/story/news/36344520/using-probability-density-to-extract-a-huge-payout-from-microchips-potential-breakout
Time Published: 2025-11-27T14:15:01Z
Description: You’re not going to find informational arbitrage opportunities with middle school math. Instead, MCHP stock requires a quantitative approach.
--------------------------------------------------

Title: TPUs vs. GPUs and why Google is positioned to win AI race in the long term
URL: https://www.uncoveralpha.com/p/the-chip-made-for-the-ai-inference
Time Published: 2025-11-27T13:28:34Z
Full Content:
Hey everyone, As I find the topic of Google TPUs extremely important, I am publishing a comprehensive deep dive, not just a technical overview, but also strategic and financial coverage of the Google TPU. Topics covered: The history of the TPU and why it all even started? The difference between a TPU and a GPU? Performance numbers TPU vs GPU? Where are the problems for the wider adoption of TPUs Google’s TPU is the biggest competitive advantage of its cloud business for the next 10 years How many TPUs does Google produce today, and how big can that get? Gemini 3 and the aftermath of Gemini 3 on the whole chip industry Let’s dive into it. The history of the TPU and why it all even started? The story of the Google Tensor Processing Unit (TPU) begins not with a breakthrough in chip manufacturing, but with a realization about math and logistics. Around 2013, Google’s leadership—specifically Jeff Dean, Jonathan Ross (the CEO of Groq), and the Google Brain team—ran a projection that alarmed them. They calculated that if every Android user utilized Google’s new voice search feature for just three minutes a day, the company would need to double its global data center capacity just to handle the compute load. At the time, Google was relying on standard CPUs and GPUs for these tasks. While powerful, these general-purpose chips were inefficient for the specific heavy lifting required by Deep Learning: massive matrix multiplications. Scaling up with existing hardware would have been a financial and logistical nightmare. This sparked a new project. Google decided to do something rare for a software company: build its own custom silicon. The goal was to create an ASIC (Application-Specific Integrated Circuit) designed for one job only: running TensorFlow neural networks. Key Historical Milestones: 2013-2014: The project moved really fast as Google both hired a very capable team and, to be honest, had some luck in their first steps. The team went from design concept to deploying silicon in data centers in just 15 months—a very short cycle for hardware engineering. 2015: Before the world knew they existed, TPUs were already powering Google’s most popular products. They were silently accelerating Google Maps navigation, Google Photos, and Google Translate. 2016: Google officially unveiled the TPU at Google I/O 2016. This urgency to solve the “data center doubling” problem is why the TPU exists. It wasn’t built to sell to gamers or render video; it was built to save Google from its own AI success. With that in mind, Google has been thinking about the »costly« AI inference problems for over a decade now. This is also one of the main reasons why the TPU is so good today compared to other ASIC projects. The difference between a TPU and a GPU? To understand the difference, it helps to look at what each chip was originally built to do. A GPU is a “general-purpose” parallel processor, while a TPU is a “domain-specific” architecture. The GPUs were designed for graphics. They excel at parallel processing (doing many things at once), which is great for AI. However, because they are designed to handle everything from video game textures to scientific simulations, they carry “architectural baggage.” They spend significant energy and chip area on complex tasks like caching, branch prediction, and managing independent threads. A TPU, on the other hand, strips away all that baggage. It has no hardware for rasterization or texture mapping. Instead, it uses a unique architecture called a Systolic Array. The “Systolic Array” is the key differentiator. In a standard CPU or GPU, the chip moves data back and forth between the memory and the computing units for every calculation. This constant shuffling creates a bottleneck (the Von Neumann bottleneck). In a TPU’s systolic array, data flows through the chip like blood through a heart (hence “systolic”). It loads data (weights) once. It passes inputs through a massive grid of multipliers. The data is passed directly to the next unit in the array without writing back to memory. What this means, in essence, is that a TPU, because of its systolic array, drastically reduces the number of memory reads and writes required from HBM. As a result, the TPU can spend its cycles computing rather than waiting for data. Google’s new TPU design, also called Ironwood also addressed some of the key areas where a TPU was lacking: They enhanced the SparseCore for efficiently handling large embeddings (good for recommendation systems and LLMs) It increased HBM capacity and bandwidth (up to 192 GB per chip). For a better understanding, Nvidia’s Blackwell B200 has 192GB per chip, while Blackwell Ultra, also known as the B300, has 288 GB per chip. Improved the Inter-Chip Interconnect (ICI) for linking thousands of chips into massive clusters, also called TPU Pods (needed for AI training as well as some time test compute inference workloads). When it comes to ICI, it is important to note that it is very performant with a Peak Bandwidth of 1.2 TB/s vs Blackwell NVLink 5 at 1.8 TB/s. But Google’s ICI, together with its specialized compiler and software stack, still delivers superior performance on some specific AI tasks. The key thing to understand is that because the TPU doesn’t need to decode complex instructions or constantly access memory, it can deliver significantly higher Operations Per Joule. For scale-out, Google uses Optical Circuit Switch (OCS) and its 3D torus network, which compete with Nvidia’s InfiniBand and Spectrum-X Ethernet. The main difference is that OCS is extremely cost-effective and power-efficient as it eliminates electrical switches and O-E-O conversions, but because of this, it is not as flexible as the other two. So again, the Google stack is extremely specialized for the task at hand and doesn’t offer the flexibility that GPUs do. Performance numbers TPU vs GPU? As we defined the differences, let’s look at real numbers showing how the TPU performs compared to the GPU. Since Google isn’t revealing these numbers, it is really hard to get details on performance. I studied many articles and alternative data sources, including interviews with industry insiders, and here are some of the key takeaways. The first important thing is that there is very limited information on Google’s newest TPUv7 (Ironwood), as Google introduced it in April 2025 and is just now starting to become available to external clients (internally, it is said that Google has already been using Ironwood since April, possibly even for Gemini 3.0.). And why is this important if we, for example, compare TPUv7 with an older but still widely used version of TPUv5p based on Semianalysis data: TPUv7 produces 4,614 TFLOPS(BF16) vs 459 TFLOPS for TPUv5p TPUv7 has 192GB of memory capacity vs TPUv5p 96GB TPUv7 memory Bandwidth is 7,370 GB/s vs 2,765 for v5p We can see that the performance leaps between v5 and v7 are very significant. To put that in context, most of the comments that we will look at are more focused on TPUv6 or TPUv5 than v7. Based on analyzing a ton of interviews with Former Google employees, customers, and competitors (people from AMD, NVDA & others), the summary of the results is as follows. Most agree that TPUs are more cost-effective compared to Nvidia GPUs, and most agree that the performance per watt for TPUs is better. This view is not applicable across all use cases tho. A Former Google Cloud employee: »If it is the right application, then they can deliver much better performance per dollar compared to GPUs. They also require much lesser energy and produces less heat compared to GPUs. They’re also more energy efficient and have a smaller environmental footprint, which is what makes them a desired outcome. The use cases are slightly limited to a GPU, they’re not as generic, but for a specific application, they can offer as much as 1.4X better performance per dollar, which is pretty significant saving for a customer that might be trying to use GPU versus TPUs.« source: AlphaSense Similarly, a very insightful comment from a Former Unit Head at Google around TPUs materially lowering AI-search cost per query vs GPUs: »TPU v6 is 60-65% more efficient than GPUs, prior generations 40-45%« This interview was in November 2024, so the expert is probably comparing the v6 TPU with the Nvidia Hopper. Today, we already have Blackwell vs V7. Many experts also mention the speed benefit that TPUs offer, with a Former Google Head saying that TPUs are 5x faster than GPUs for training dynamic models (like search-like workloads). There was also a very eye-opening interview with a client who used both Nvidia GPUs and Google TPUs as he describes the economics in great detail: »If I were to use eight H100s versus using one v5e pod, I would spend a lot less money on one v5e pod. In terms of price point money, performance per dollar, you will get more bang for TPU. If I already have a code, because of Google’s help or because of our own work, if I know it already is going to work on a TPU, then at that point it is beneficial for me to just stick with the TPU usage. In the long run, if I am thinking I need to write a new code base, I need to do a lot more work, then it depends on how long I’m going to train. I would say there is still some, for example, of the workload we have already done on TPUs that in the future because as Google will add newer generation of TPU, they make older ones much cheaper.For example, when they came out with v4, I remember the price of v2 came down so low that it was practically free to use compared to any NVIDIA GPUs. Google has got a good promise so they keep supporting older TPUs and they’re making it a lot cheaper. If you don’t really need your model trained right away, if you’re willing to say, “I can wait one week,” even though the training is only three days, then you can reduce your cost 1/5.« source: AlphaSense Another valuable interview was with a current AMD employee, acknowledging the benefits of ASICs: »I would expect that an AI accelerator could do about probably typically what we see in the industry. I’m using my experience at FPGAs. I could see a 30% reduction in size and maybe a 50% reduction in power vs a GPU.« We also got some numbers from a Former Google employee who worked in the chip segment: »When I look at the published numbers, they (TPUs) are anywhere from 25%-30% better to close to 2x better, depending on the use cases compared to Nvidia. Essentially, there’s a difference between a very custom design built to do one task perfectly versus a more general purpose design.« What is also known is that the real edge of TPUs lies not in the hardware but in the software and in the way Google has optimized its ecosystem for the TPU. A lot of people mention the problem that every Nvidia »competitor« like the TPU faces, which is the fast development of Nvidia and the constant »catching up« to Nvidia problem. This month a former Google Cloud employee addressed that concern head-on as he believes the rate at which TPUs are improving is faster than the rate at Nvidia: »The amount of performance per dollar that a TPU can generate from a new generation versus the old generation is a much significant jump than Nvidia« In addition, the recent data from Google’s presentation at the Hot Chips 2025 event backs that up, as Google stated that the TPUv7 is 100% better in performance per watt than their TPUv6e (Trillium). Even for hard Nvidia advocates, TPUs are not to be shrugged off easily, as even Jensen thinks very highly of Google’s TPUs. In a podcast with Brad Gerstner, he mentioned that when it comes to ASICs, Google with TPUs is a »special case«. A few months ago, we also got an article from the WSJ saying that after the news publication The Information published a report that stated that OpenAI had begun renting Google TPUs for ChatGPT, Jensen called Altman, asking him if it was true, and signaled that he was open to getting the talks back on track (investment talks). Also worth noting was that Nvidia’s official X account posted a screenshot of an article in which OpenAI denied plans to use Google’s in-house chips. To say the least, Nvidia is watching TPUs very closely. Ok, but after looking at some of these numbers, one might think, why aren’t more clients using TPUs? Where are the problems for the wider adoption of TPUs The main problem for TPUs adoption is the ecosystem. Nvidia’s CUDA is engraved in the minds of most AI engineers, as they have been learning CUDA in universities. Google has developed its ecosystem internally but not externally, as it has used TPUs only for its internal workloads until now. TPUs use a combination of JAX and TensorFlow, while the industry skews to CUDA and PyTorch (although TPUs also support PyTorch now). While Google is working hard to make its ecosystem more supportive and convertible with other stacks, it is also a matter of libraries and ecosystem formation that takes years to develop. It is also important to note that, until recently, the GenAI industry’s focus has largely been on training workloads. In training workloads, CUDA is very important, but when it comes to inference, even reasoning inference, CUDA is not that important, so the chances of expanding the TPU footprint in inference are much higher than those in training (although TPUs do really well in training as well – Gemini 3 the prime example). The fact that most clients are multi-cloud also poses a challenge for TPU adoption, as AI workloads are closely tied to data and its location (cloud data transfer is costly). Nvidia is accessible via all three hyperscalers, while TPUs are available only at GCP so far. A client who uses TPUs and Nvidia GPUs explains it well: »Right now, the one biggest advantage of NVIDIA, and this has been true for past three companies I worked on is because AWS, Google Cloud and Microsoft Azure, these are the three major cloud companies. Every company, every corporate, every customer we have will have data in one of these three. All these three clouds have NVIDIA GPUs. Sometimes the data is so big and in a different cloud that it is a lot cheaper to run our workload in whatever cloud the customer has data in. I don’t know if you know about the egress cost that is moving data out of one cloud is one of the bigger cost. In that case, if you have NVIDIA workload, if you have a CUDA workload, we can just go to Microsoft Azure, get a VM that has NVIDIA GPU, same GPU in fact, no code change is required and just run it there. With TPUs, once you are all relied on TPU and Google says, “You know what? Now you have to pay 10X more,” then we would be screwed, because then we’ll have to go back and rewrite everything. That’s why. That’s the only reason people are afraid of committing too much on TPUs. The same reason is for Amazon’s Trainium and Inferentia.« source: AlphaSense These problems are well known at Google, so it is no surprise that internally, the debate over keeping TPUs inside Google or starting to sell them externally is a constant topic. When keeping them internally, it enhances the GCP moat, but at the same time, many former Google employees believe that at some point, Google will start offering TPUs externally as well, maybe through some neoclouds, not necessarily with the biggest two competitors, Microsoft and Amazon. Opening up the ecosystem, providing support, etc., and making it more widely usable are the first steps toward making that possible. A former Google employee also mentioned that Google last year formed a more sales-oriented team to push and sell TPUs, so it’s not like they have been pushing hard to sell TPUs for years; it is a fairly new dynamic in the organization. Google’s TPU is the biggest competitive advantage of its cloud business for the next 10 years The most valuable thing for me about TPUs is their impact on GCP. As we witness the transformation of cloud businesses from the pre-AI era to the AI era, the biggest takeaway is that the industry has gone from an oligopoly of AWS, Azure, and GCP to a more commoditized landscape, with Oracle, Coreweave, and many other neoclouds competing for AI workloads. The problem with AI workloads is the competition and Nvidia’s 75% gross margin, which also results in low margins for AI workloads. The cloud industry is moving from a 50-70% gross margin industry to a 20-35% gross margin industry. For cloud investors, this should be concerning, as the future profile of some of these companies is more like that of a utility than an attractive, high-margin business. But there is a solution to avoiding that future and returning to a normal margin: the ASIC. The cloud providers who can control the hardware and are not beholden to Nvidia and its 75% gross margin will be able to return to the world of 50% gross margins. And there is no surprise that all three AWS, Azure, and GCP are developing their own ASICs. The most mature by far is Google’s TPU, followed by Amazon’s Trainum, and lastly Microsoft’s MAIA (although Microsoft owns the full IP of OpenAI’s custom ASICs, which could help them in the future). While even with ASICs you are not 100% independent, as you still have to work with someone like Broadcom or Marvell, whose margins are lower than Nvidia’s but still not negligible, Google is again in a very good position. Over the years of developing TPUs, Google has managed to control much of the chip design process in-house. According to a current AMD employee, Broadcom no longer knows everything about the chip. At this point, Google is the front-end designer (the actual RTL of the design) while Broadcom is only the backend physical design partner. Google, on top of that, also, of course, owns the entire software optimization stack for the chip, which makes it as performant as it is. According to the AMD employee, based on this work split, he thinks Broadcom is lucky if it gets a 50-point gross margin on its part. Without having to pay Nvidia for the accelerator, a cloud provider can either price its compute similarly to others and maintain a better margin profile or lower costs and gain market share. Of course, all of this depends on having a very capable ASIC that can compete with Nvidia. Unfortunately, it looks like Google is the only one that has achieved that, as the number one-performing model is Gemini 3 trained on TPUs. According to some former Google employees, internally, Google is also using TPUs for inference across its entire AI stack, including Gemini and models like Veo. Google buys Nvidia GPUs for GCP, as clients want them because they are familiar with them and the ecosystem, but internally, Google is full-on with TPUs. As the complexity of each generation of ASICs increases, similar to the complexity and pace of Nvidia, I predict that not all ASIC programs will make it. I believe outside of TPUs, the only real hyperscaler shot right now is AWS Trainium, but even that faces much bigger uncertainties than the TPU. With that in mind, Google and its cloud business can come out of this AI era as a major beneficiary and market-share gainer. Recently, we even got comments from the SemiAnalysis team praising the TPU: »Google’s silicon supremacy among hyperscalers is unmatched, with their TPU 7th Gen arguably on par with Nvidia Blackwell. TPU powers the Gemini family of models which are improving in capability and sit close to the pareto frontier of $ per intelligence in some tasks« source: SemiAnalysis How many TPUs does Google produce today, and how big can that get? Here are the numbers that I researched:
--------------------------------------------------

Title: As Founder Ray Dalio Warns the Market Is in a Bubble, Bridgewater Associates Just Bought CoreWeave Stock
URL: https://www.barchart.com/story/news/36343272/as-founder-ray-dalio-warns-the-market-is-in-a-bubble-bridgewater-associates-just-bought-coreweave-stock
Time Published: 2025-11-27T13:00:02Z
Description: Down by over 60% from all-time highs, CoreWeave stock offers significant upside potential for shareholders, despite concerns about an AI bubble.
--------------------------------------------------

Title: Sands Capital Select Growth Strategy’s Top Absolute Individual Contributor: NVIDIA Corporation (NVDA)
URL: https://finance.yahoo.com/news/sands-capital-select-growth-strategy-125514597.html
Time Published: 2025-11-27T12:55:14Z
Description: Sands Capital, an investment management company, released its “Sands Capital Select Growth Strategy” Q3 2025 investor letter. A copy of the letter can be...
--------------------------------------------------

Title: Will Michael Burry’s Big Nvidia Bet Pay Off? NVDA Stock Tumbles As Polymarket Rakes It In
URL: https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_e3f583e2-6ef1-4dc7-a603-897b8f620cb0
Time Published: 2025-11-27T11:48:47Z
Description: None
--------------------------------------------------

Title: Google's Gemini 3 Sends Broadcom Soaring: TPUs Take Center Stage
URL: https://www.marketbeat.com/originals/googles-gemini-3-sends-broadcom-soaring-tpus-take-center-stage/?utm_source=yahoofinance&amp;utm_medium=yahoofinance
Time Published: 2025-11-26T22:58:00Z
Description: Broadcom's virtuous relationship with Google is leading to gains. AVGO is now directly attached to what many view as the best AI model family: Gemini 3.
--------------------------------------------------

Title: Earnings live: Zscaler stock tanks, Dell rises, Deere slides in last major earnings day this week
URL: https://finance.yahoo.com/news/live/earnings-live-zscaler-stock-tanks-dell-rises-deere-slides-in-last-major-earnings-day-this-week-215532658.html
Time Published: 2025-11-26T21:55:32Z
Description: The third quarter earnings season has been mostly positive, with most of the reports in the rearview mirror.
--------------------------------------------------

Title: Palantir Isn’t Just Riding the AI Boom—It’s Orchestrating It
URL: https://www.marketbeat.com/originals/palantir-isnt-done-growing-did-we-just-see-a-floor/?utm_source=yahoofinance&amp;utm_medium=yahoofinance
Time Published: 2025-11-26T21:38:00Z
Description: Palantir’s earnings and commercial growth remain strong as AI demand accelerates. See why recent weakness may signal a floor and a new buying opportunity
--------------------------------------------------

Title: Apple shares hit new all-time closing high
URL: https://macdailynews.com/2025/11/26/apple-shares-hit-new-all-time-closing-high-31/
Time Published: 2025-11-26T21:32:21Z
Full Content:
Update font size. Reset In Nasdaq trading today, shares of Apple Inc. (AAPL) rose $0.58 to $277.55, a new all-time closing high. Apple’s all-time intraday high was set yesterday, November 25, 2025, at $280.38. Apple’s 52-week low stands at $169.21 set on April 8, 2025, the very date on which we wrote, “Sub-$170 AAPL seems like an absolute gift to us…” Today’s trading volume for AAPL shares was 31,046,299 versus Apple’s average trading volume of 51,490,889 shares. Apple’s PE Ratio currently stands at 37.26. Apple currently has a market value of $4.119 trillion, making it the world’s second-most valuable company. The top five U.S. publicly-traded companies, based on market value: Alphabet (GOOGL) – $3.875T Microsoft (MSFT) – $3.609T Amazon (AMZN) – $2.450T Selected companies’ current market values: • Meta Platforms (META) – $1.597T • Taiwan Semi (TSM) – $1.504T • Tesla (TSLA) – $1.419T • Berkshire Hathaway (BRK-A) – $1.105T • Walmart (WMT) – $870.664B • Netflix (NFLX) – $449.749B • Advanced Micro Devices (AMD) – $348.792B • Cisco (CSCO) – $300.560B • IBM (IBM) – $283.421B • Disney (DIS) – $185.960B • Intel (INTC) – $175.584B • Sony (SONY) – $173.487B • SoftBank (SFTBF) – $155.763B • Adobe (ADBE) – $134.692B • Spotify (SPOT) – $122.031B • Dell (DELL) – $89.600B • Nokia (NOK) – $33.717B • Hewlett-Packard (HPQ) – $22.524B • SiriusXM (SIRI) – $7.041B • BlackBerry (BB) – $2.430B • Sonos (SONO) – $2.252B • RealNetworks (RNWK) was delisted from U.S. exchanges on December 21, 2022 and is no longer publicly traded.🍩 Apple all-time high (AAPL) via NASDAQ here. MacDailyNews Take: Onward and upward, Cupertino soldiers! Please help support MacDailyNews — and enjoy subscriber-only articles, comments, chat, and more — by subscribing to our Substack: macdailynews.substack.com. Thank you! Support MacDailyNews at no extra cost to you by using this link to shop at Amazon. Warren Buffet takes the L here. Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Notify me of follow-up comments by email. Notify me of new posts by email. Δ This site uses Akismet to reduce spam. Learn how your comment data is processed. In the whimsical woods where holiday magic meets tech wizardry, a band of furry friends stumbles upon a shiny lost iPhone 17 Pro in… Nearly two years after its ambitious debut, Apple’s Vision Pro headset has received a subtle yet significant refresh with the M5 chip… Apple has filed a petition at the Delhi High Court challenging India’s updated antitrust regulations, which could expose the company to… It’s April 1, 1976 — April Fool’s Day, no les — and two Steves, Jobs and Wozniak, along with a third partner named Ron Wayne, are scribbling… A group has sued Apple alleging that the company uses conflict minerals tied to human rights abuses in the Democratic Republic of Congo… Enter your email address to follow MacDailyNews and receive notifications of new posts by email. Email Address Follow :-)
--------------------------------------------------

Title: European Fintech Draws in Investors Seeking Refuge From Wall Street’s AI Bubble (Dmytro Spilka)
URL: https://www.finextra.com/blogposting/30228/european-fintech-draws-in-investors-seeking-refuge-from-wall-streets-ai-bubble
Time Published: 2025-11-26T20:52:23Z
Full Content:
As fears on Wall Street mount over the sustainability of the AI boom, Europe’s fintechs are showing their worth. Recently, WIRED author Brian Merchant spoke to University of Maryland economists Brent Goldfarb and David A. Kirsch, who devised their own scale for assessing whether certain innovations led to a market bubble. The economists analyzed 58 historical examples of tech bubbles and broke them down to their core components to create a scale from one to eight, where eight is the most likely to predict a bubble. Looking at factors like uncertainty, pure plays, novice investors, and narratives around commercial innovations, Goldfarb concluded that the AI bull run is a fully fledged eight. While this doesn’t confirm that we’re in a bubble, it shows that all the hallmarks are present. Billionaire Peter Thiel recently offloaded his entire stake in leading AI stock Nvidia (NASDAQ: NVDA), according to 13F filings by Thiel Macro LLC. The total sell-off amounted to 537,742 shares in the chipmaker in Q3. Michael Burry, the investor who inspired The Big Short, also opened short positions against Nvidia and Palantir, betting that each stock is set to decline in value. The outlook for Wall Street AI firms is becoming increasingly clouded as concerns over a growing bubble mount. But in Europe, there’s a more optimistic tone surrounding the continent’s fintech sector. Could European fintech be an antidote to the artificial intelligence bubble? Let’s take a deeper look at a market that’s gathering momentum: European fintech is growing at a rapid pace. Forecasts suggest that the continent’s Buy Now Pay Later (BNPL) market size is set to expand from a value of $2.46 billion in 2024 to $18.32 billion by 2033, representing a CAGR of 25.2%. This expected growth comes despite the recent news that leading UK-based fintechs Revolut and Wise are reportedly eyeing listings in the United States. “The jewel in the crown of European fintech is Adyen (AMS: ADYEN), which has a capitalization of €43.04 billion ($49.9bn) and provides flexible cross-border payment solutions for merchants at scale,” said Iván Marchena, Senior Economist at global brokerage brand Just2Trade. “Crucially, Adyen’s client list spans a wide range of sectors and features names like Uber, Netflix, and Spotify. At a time when there are tangible concerns over the self-funding cycle of AI giants on Wall Street, we’re seeing more fintechs build a sustainable client pool that could help to ensure their longevity.” Platforms like Klarna, which serve more than 100 million customers in 17 countries, are an example of the immediate success that many European fintechs have found in immediately accessing major international markets with their services. There are signs that Europe’s fintech ecosystem is gathering momentum. Flatpay, which facilitates card payments for SMBs, recently reached a value of $1 billion. The startup has already sought to take on its larger rivals by introducing a flat transaction rate for merchants to use its card terminals and point-of-sale systems. Impressively, the Danish startup has claimed that it has grown from 7,000 customers in April 2024 to 60,000 today. Flatpay’s own valuation has also grown exponentially and is now valued at €1.5 billion ($1.75bn). Given its high growth rate, CEO and co-founder Sander Janca-Jensen is now targeting annual recurring revenue (ARR) to solidify Flatpay’s emergence into a profitable European fintech platform. Europe is also becoming a key location for blockchain innovations in the fintech landscape. MQube, a platform that tokenizes mortgage debt by converting financial assets into digital tokens and recording them on a shared ledger, has now tokenized £1.3 billion ($1.7bn), underlining its position as a driving force for securing mortgages using blockchain technology. As blockchain and digital currency technology become increasingly intertwined with fintech, Europe has an opportunity to use its leading position in the ecosystem to introduce cutting-edge financial tools and services to drive further growth throughout the ecosystem. This helps to ensure a pipeline for innovation, even as leading domestic players like Revolut and Wise remain attracted to the prospect of US listings. At a time when suspicions over the longevity of the AI boom are continually coming into question, Europe’s melting pot of innovative fintechs can be a major draw for investors who are seeking exposure to stocks with higher growth potential as AI investments receive mixed levels of sentiment. Given that fintech solutions are finding adoption in a more frictionless manner, it’s reasonable to expect European firms to achieve sustainable growth that can help them find profitability and scalability in the future. Whether the artificial intelligence boom is in a bubble or not, Wall Street investors can find plenty of opportunities throughout Europe’s melting pot of fintech innovators, and these tech firms could offer some resilience in the face of uncertainty in AI.
--------------------------------------------------

Title: Evercore ISI Raises PT on NVIDIA (NVDA) Stock
URL: https://finance.yahoo.com/news/evercore-isi-raises-pt-nvidia-195920291.html
Time Published: 2025-11-26T19:59:20Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the Best Quantum Computing Stocks to Invest In Right Now. On November 20, analyst Mark Lipacis at Evercore ISI...
--------------------------------------------------

Title: Coffee Break: Nvidia’s Narrative Breaking Down?
URL: https://www.nakedcapitalism.com/2025/11/nvidias-narrative-cramer-burry-alphabet-tpus-druckenmiller.html
Time Published: 2025-11-26T19:00:49Z
Full Content:
Nvidia’s narrative took a major hit this week due to multiple factors including the emergence of a credible rival, OpenAI’s struggles, and a trade war pincer that has them caught between Trump and China. Is a Government Backstop the Bull Case? Nvidia’s narrative, which it kicked off with the launch of Ampere architecture and A100 chip in 2020, subsumed any competing story-lines in the post-pandemic American stock markets in 2022, and engulfed the entire American economy under Trump. Nvidia’s narrative that Large Language Models (LLMs) like OpenAI’s ChatGPT, Anthropic’s Claude, and Google’s Gemini ARE the future of technology and the global economy has made them the world’s largest corporation by market cap. The Trump administration may be all-in with Nvidia’s narrative and may be signaling its willingness to backstop the industry to prevent the AI bubble popping. Paypal mafioso and Trump tech czar David Sacks had previously indicated the government would not be backstopping OpenAI earlier this month. I argued elsewhere that Sacks’ tweet contributed to the downturn in AI stocks we saw in the first weeks of this month. But on the 24th, Sacks seemed to signal a change in his (very influential) thinking, in response to a Wall Street Journal piece headlined, “How the U.S. Economy Became Hooked on AI Spending and subtitled, “Growth has been bolstered by data-center investment and stock-market wealth. A reversal could raise the risk of recession.” According to today’s WSJ, AI-related investment accounts for half of GDP growth. A reversal would risk recession. We can’t afford to go backwards. — David Sacks (@DavidSacks) November 24, 2025 Paired with the White House’s executive order titled “Launching the Genesis Mission” released the same day, many took this as bullish news for Nvidia’s narrative and the larger AI bubble. The closest thing I could find to an action word in the EO was this promise to: …build an integrated AI platform to harness Federal scientific datasets — the world’s largest collection of such datasets, developed over decades of Federal investments — to train scientific foundation models and create AI agents to test new hypotheses, automate research workflows, and accelerate scientific breakthroughs. Critics of the Nvidia narrative will of course point out that when a government backstop is the bull case, the bubble is in trouble. Maybe that’s why the a handful of naysayers who’ve been fighting the tide, are now being joined by some major investors who are putting their money where the critics’ keyboards have been. Team Bear Beefs Up Pioneering Nvidia bears like AI scientist Gary Marcus and journalist Ed Zitron have now been joined by notable investors including the “Big Short” contrarian Michael Burry and billionaire Stanley “I killed the Pound” Druckenmiller who sold all 214,060 of his funds’ Nvidia shares. Marcus and Zitron remain opinion leaders in the space, however. Marcus is currently dealing with the emergence of a class of rival AI experts who’ve been on the AGI (Artificial General Intelligence) bandwagon and are now getting off. AGI is the patent nonsense that LLMs are just a few months away from creating super-intelligent, self-replicating machines. Naturally, belief in AGI has been the conventional wisdom in Silicon Vally for the last couple of years and continues to be a big part of the bulls’ case for the Nvidia narrative. Marcus has taken lots of heat for calling bullshit on LLMs as the road to AGI from the get go, and is now expressing mixed feelings about the big names who are joining him on the critical side. Those names include Meta’s Chief AI Scientist Yann LeCun and OpenAI co-founder Ilya Sutskever. As for Ed Zitron, his latest “The Hater’s Guide To NVIDIA” is well worth the subscription price and the estimated 54 minute reading time. It includes an excellent, concise explanation of why Nvidia has become the hero of the Nvidia narrative which has reshaped the U.S. economy: Back in 2006, NVIDIA launched CUDA, a software layer that lets you run (some) software on (specifically) NVIDIA graphics cards, and over time this has grown into a massive advantage for the company. The thing is, GPUs are great for parallel processing – essentially spreading a task across multiple, by which I mean thousands, of processor cores at the same time – which means that certain tasks run faster than they would on, say, a CPU. While not every task benefits from parallel processing, or from having several thousand cores available at the same time, the kind of math that underpins LLMs is one such example. CUDA is proprietary to NVIDIA, and while there are alternatives (both closed- and open-source), none of them have the same maturity and breadth. Pair that with the fact that Nvidia’s been focused on the data center market for longer than, say, AMD, and it’s easy to understand why it makes so much money. There really isn’t anyone who can do the same thing as NVIDIA, both in terms of software and hardware, and certainly not at the scale necessary to feed the hungry tech firms that demand these GPUs. Anyway, back in 2019 NVIDIA acquired a company called Mellanox for $6.9 billion, beating off other would-be suitors, including Microsoft and Intel. Mellanox was a manufacturer of high-performance networking gear, and this acquisition would give NVIDIA a stronger value proposition for data center customers. It wanted to sell GPUs — lots of them — to data center customers, and now it could also sell the high-speed networking technology required to make them work in tandem. This is relevant because it created the terms under which NVIDIA could start selling billions (and eventually tens of billions) of specialized GPUs for AI workloads. … Because nobody else has really caught up with CUDA, NVIDIA has a functional monopoly… NVIDIA has been printing money, quarter after quarter, going from a meager $7.192 billion in total revenue in the third (calendar year) quarter of 2023 to an astonishing $50 billion in just data center revenue (that’s where the GPUs are) in its most recent quarter, for a total of $57 billion in revenue, and the company projects to make $63 billion to $67 billion in the next quarter. But never fear, Ziton also puts a stake through the heart of the Nvidia narrative over the course of several thousand words. Here’s a key point: NVIDIA makes so much money, and it makes it from a much smaller customer base than most companies, because there are only so many entities that can buy thousands of chips that cost $50,000 or more each. Zitron cites pseudonymous finance poster “Just Dario” as someone who’s provided key insights into the workings of Nvidia and Dario’s latest piece on the company is worth reading in full, but the TL;DR explanation of Dario’s role in the larger Nvidia narrative wars can be grasped from glancing at these tweets about whether or not Enron is a valid comparison point for Nvidia: Nvidia says it’s not Enron. I actually agree with them, they are Envidia https://t.co/8zN308lZKK pic.twitter.com/IeahPNSd4Q — JustDario 🏊‍♂️ (@DarioCpx) November 25, 2025 I should note that this pretty much unknown (and I suspect AI-using Substack) writer Shanaka Anslem Perera got the credit from Yahoo Finance with triggering Nvidia’s now infamous “we’re not Enron” memo. Yahoo also quoted “Jim Chanos, who is famous for predicting the fall of Enron, (who) thinks the comparison between Nvidia and Lucent bears weight.” “They’re [Nvidia is] putting money into money-losing companies in order for those companies to order their chips,” Chanos said. As for “Big Short” Burry, his new Substack is a bit rich for my blood, although serious investors will likely find it a bargain, but his latest contribution to Nvidia’s narrative involves comparing Nvidia to Cisco before the dot.com bust: Michael Burry's case against Nvidia: pic.twitter.com/XafV3SLYVT — Nat Wilson Turner (@natwilsonturner) November 26, 2025 This adds to Burry’s previous X.com post alleging that AI industry accounting practices of “understating depreciation by extending useful life of assets artificially boosts earnings (are) one of the more common frauds of the modern era.” And of course, Burry’s close to a billion-dollar bet against Nvidia’s narrative has impacted our story as well. There’s also been a spate of bad news for Nvidia in the tech business press. Two More Problems for the Nvidia Narrative Two headlines from The Information neatly summarize the latest issues Nvidia is facing in the marketplace, unfortunately, it’s an expensive subscription only newsletter and we’ll have to look elsewhere for explanations: The talks also involve Meta renting chips from Google Cloud as early as next year and are part of Google’s broader push to get customers to adopt its tensor processing units (TPUs) – used for AI workloads – in their own data centers, the report said, citing people involved in the talks.” Nvidia responded with a statement that is getting mocked online. ByteDance bought more Nvidia chips than ‌any other Chinese firm in 2025 as it raced to secure computing power for its billion-plus ‌users amid concerns Washington could curb supply, according to the report. The reported ban underscores Beijing’s efforts to reduce reliance on U.S. technology, a campaign that has intensified as Washington tightens curbs on exports of advanced semiconductors to China. The Mid-Wits Weigh In No debate in 2025 would be complete without one of the Abundance bros weighing in. Naturally Ezra Klein’s “Abundance” co-author Derek Thomas (co-writing with Understanding AI founder Timothy B. Lee is coming down in the middle with “Six reasons to think there’s an AI bubble — and six reasons not to” and shrewdly saves the bull case for its paying customers. Talk about knowing your audience. The Real Bulls Include Jim Cramer and AGI Crazytown’s Finest But I’ll leave the real bull case to the legendary CNBC commentator Jim Cramer (I should note that the Inverse Jim Cramer ETF has been getting killed lately). His case boils down to growth: I am a huge believer in growth stocks, and …growth stocks are what draws me to the hyperscalers for my travel trust. Now we own many, many stocks from other universes drug stocks, aerospace materials, data center builders. They have long term growth stories, but they’re not turbocharged with the resources of these gigantic companies. That’s what always brings me back to the Mag-7. These stocks are prominent because of their success. The companies they represent have bountiful profits, which is why they could rise to their lofty trillionaire status in the first place. It’s why I don’t kick them out when they’re down in fact. Or in fact, it’s why I might buy them for the trust. But the far more entertaining bull case for the Nvidia narrative is made by Utopia believers like Tomas Pueyo who is helping his 119,000 Substack subscribers prepare for a post-scarcity Utopia brought about by “super-intelligence.” Admittedly, I have an immediate and utter disdain for anyone pitching imminent Utopia but a couple of gummies and Pueyo’s stuff becomes quite entertaining. Here’s a taste from his latest, “AI: How Do We Avoid Societal Collapse and Build a Utopia?“: In the previous article, we saw that we’ll eventually live in a utopia, a world of full abundance and without the conflicts over scarce resources that have plagued humanity throughout history. Well then. But lest readers think Mr. Pueyo isn’t concerned with the obstacles in our path, there’s more: intelligence will not be infinite until there’s infinite energy and infinite compute, which will also need plenty of raw materials and land. So the scarcity of intelligence might not even be completely eliminated until more of these inputs are sufficiently abundant. Assuming AIs still serve humans, how will they prioritize? They will need a signal of what matters most to humans. How will humans convey that? Through money. … So it’s very unlikely that we’re going to get rid of capitalism. We need the price signal to convey the optimal allocation of capital. But in that world, where humans are not working anymore because they’ve been fully automated by AIs more intelligent than themselves, how do you decide how much money each person should have? I’ll leave it up to readers who are really dying to know just how many angels are dancing on the top of his pinhead to read more of his work. Just be aware that the Nvidia narrative bears are up against a whole lot of people who really really really want to believe and are clapping as hard as they can to keep Tinkerbell alive the AI bubble inflated. Unfortunately for the bulls, and Nvidia’s narrative. AI slop is having very real world impacts. The kind people notice. pic.twitter.com/bnATXesA3D — Nat Wilson Turner (@natwilsonturner) November 26, 2025 Will Americans take comfort in their 401K wealth effect if they can’t even go into a happy turkey coma after Thanksgiving dinner because Nvidia’s narrative led to AI slop getting served to the table? Posts Related to Nvidia’s Narrative: Veteran political operative and corporate media professional. RE: …intelligence will not be infinite until there’s infinite energy and infinite compute… It sounds like Mr. Pueyo could use a refresher on the concept of infinity. These AI humpers really are whacked out of their gourds, aren’t they? Krell machine or bust (h/t Dr. Morbius). The Krell data center. Pretty prescient. Morbius to Ostrow and Adams underground, pointing off into the distance: “Twenty billions,” points in the opposite direction, “twenty billions.” Adams, “Listen. Trades opening and closing.” Morbius, in reply, “And they never rest. This is one of their funding vehicles. You can feel the extracted rents rising. Look down here. Look down gentlemen, are you afraid of the deplorables?” Morbius points upwards: “Seventy-eight hundred tranches. And four hundred other funding vehicles like this one.” Later Morbius is to sum it up as: “The harnessed power of an exploited planetary system.” Beware the Monsters From the App! Lol. Gotta say: that is one of my fave scenes in the whole film. I remember thinking when I really looked at those Krell doorways that the set designer for the film had read Lovecraft. Those doors are shaped like Lovecraft described the unnamed aliens from “At the Mountains of Madness.” They too suffered a terrible doom due to their hubris and overconfidence in creating the shoggoths. Monsters from the Id indeed. It’s superb. Every time I see it, and listen to it, I marvel. One thing about tensor processing units is they use way less energy on inference. At least that might help make money on AI. ..Unless we all end up paying for openAI oracle etc. electricity bills. Well, ok, it’ll bring down, but certainly not eliminate, the overhead going to the utility cos. Won’t do anything about paying for the silicon, the optical networking gear, the programming, the training, the $1M+ salaries, and most importantly, the profit margin needed to pay back the gigantic pile of capital they’re in the process of lighting on fire. And of course, one only trains the model once (in principle) and when it’s ready tens, hundreds or even thousands of copies are made to distribute the load of inference (for there may be tens of thousands of request per second). Maybe it’s just me, but even WITH infinite compute and infinite energy, I don’t think we’d be capable of hitting infinite intelligence. I don’t think we have it in us. We’ll probably hit infinite bullshit first, if we’re not almost there already. The universe may not contain infinite energy and we cannot have infinite compute unless compute is absolutley efficient. As to BS, well, that is open ended and without physical limits Based on Zitron and Darion’s summaries it appears Nvidia and AI are headed for the mother of all busts. NVIDIA’s scheme is: Nvidia invests in some company, say two billion dollars. That company then buys a couple billion dollars of Nvidia GPU’s. Now using the GPU’s as collateral, the company borrows 20 billion. It then buys 15 billion of Nvidia GPUS. Now it goes back and borrows another 20 billion using the last twenty billion worth of gpus. or it signs a contract with OpenAI for $50 billion in compute services, and then uses that contract to borrow 40 billion to buy another 30 billion in Nvidia gpus. So, number always goes up for everybody as long as somebody can still borrow money. Some VC said at the current rate, all the VC money will be gone in six quarters ($200 billion ? I forget exactly). So when that day comes and the whole thing collapses, the bankruptcies will be phenomenal. I bet Nvidia drops by half in value, and the NYSE goes down by 30%. OpenAI has announced 1.4 trillion in spending in the next 15 years, when it makes maybe 12 billion a year currently. I think in 2027, if it lives that long, it needs a 100 billion just for that year. One interesting link I’ve read that I don’t think was mentioned was this article: https://www.theverge.com/ai-artificial-intelligence/822011/coreweave-debt-data-center-ai aka (“I looked into CoreWeave and the abyss gazed back: Meet the company Nvidia is propping up”) This is an excellent case study for one of the companies Nvidia is propping up. Possibly the best quote that guarantees a bust is as follows: “These [Nvidia] investments [in CoreWeave] are not circular; they are complementary,” CoreWeave’s Davis wrote. “This is about an entire ecosystem all rowing in the same direction to accelerate the AI economy. There’s nothing circular about it. Rather, these partnerships are about accelerating innovation and adoption. We are, collectively, defining the next-generation operating system for civilization.” Lemmings one and all… Another interesting point mentioned was that at the end of July, Nvidia owned $3.9B in non-marketable securities–probably investments in a number of companies like CoreWeave. So I thought, I wonder where Nvidia is now on these securitiesthat they just released a new quarter of data?… $8.2B or over 100% growth in 3 months. My son is in intro accounting and a major AI user. I asked him if they looked at many corporations’ balance sheets in his class. He said not many. I said, go look at NVDA and tell me if you think they’re a $5T company. Seems like a big bust in on the horizon although I easily could see another “socialism for the rich” scenario in the offing. The companies building these data centers for companies like OpenAI might be the first to fail. They are taking on huge debt levels to build these things, with often nothing more than the contracted promise to buy compute services as the collateral behind that debt. Like Oracle, who signed a $300 billion dollar contract with OpenAI to supply 4.5 GigaWatts of computing power, deliverable Jan 1, 2027. Oracle hired Crusoe to build the Stargate Abilene data center, which I believe last month the first of eight buildings was turned over to Oracle with 200 MW of power. Another seven have to be built, equipped and made operational in the next 12 months, lol. The 4GW power substation hasn’t even really started construction yet, Oracle needs to come up with $40 billion just for the GPUs, and has to start paying Crusoe $1 billion dollars a month for the next 15 years for the construction. Good question as to where Larry is going to find the money. The other thing is these contracts are worded such that if the data center is not completed to the specs, the counter-party can walk away. So if, as expected, Oracle fails to build out all the gpu power OpenAI contracted, and Altman walks, who is Oracle going to be able to find to buy the compute power ? Nobody. Then there’s the problem that OpenAI won’t have the money to pay Oracle anyway. Hundreds of billions of dollars are being incinerated, and when the flames go out, the cold will be unreal. Maybe the IDF is helping him out with some of the jewelry they stole from Palestinians. I know that gold has risen a lot but not enough to cover this hole completely. Maybe the precious gems? A good friend who has been working in the US power utility sector recently left that industry for headhunting. He is specialising in recruiting the people required to design and project manage the power connections for AI datacentres. He said over coffee, I might need another specialism. I told him he did…. There really isn’t anyone who can do the same thing as NVIDIA, both in terms of software and hardware, and certainly not at the scale necessary to feed the hungry tech firms that demand these GPUs. Expect that Nvidia doesn’t make the GPUs themselves. They are a (great) hardware and (very poor) software design company. Their “assets” are the engineers, tools, knowhow and of course IP surrounding these chips. So, if the topic of a massive bailout comes up, wouldn’t it just be cheaper to liquidate the company, and sell the tools and re-hire everyone into a new venture? Nvidia’s real lock is CUDA, the software that you use to drive the gpu’s. It is of course proprietary and nobody has anything remotely close. I’m sure anyone who even thinks of trying will run into a thicket of patents surrounded by a moat guarded by an army of 10,000 patent lawyers. https://www.eetimes.com/after-three-years-modulars-cuda-alternative-is-ready/ CUDA alternatives. One of the most successful has been the open-source project ApacheTVM. TVM’s main aim is to enable AI to run efficiently on diverse hardware by automating kernel fusion, but generative AI proved to be a technical challenge, since algorithms are larger and more complex than for older computer vision applications. Generative AI algorithms are also more hardware-specific (like FlashAttention). TVM’s core contributors formed a company called OctoAI, which developed a generative AI inference stack for enterprise clusters, but it was acquired recently by Nvidia, casting some doubt on the project’s future. Another widely known technology, OpenCL, is a standard designed to enable code portability between GPUs and other hardware types. It has been broadly adopted in mobile and embedded devices. However, critics of this standard (including Lattner) point to its lack of agility to keep up with fast-moving AI technology, in part because it is driven by a “co-opetition” of competing companies who generally decline to share anything about future hardware features. Other commercial projects of this nature are still in the early stages, Lattner said. Worth noting that CUDA is not for AI only. To give a real life usage egzample, my brother is a CAD specialist contractor, models architecture and interiors for developers and furniture producers etc. He bought his third Nvidia GPU now because there is a lot of software that requires it for speeding up or automating certain operations like cloth physics (very useful in interior modeling). The GPUs return their price manifold saving him a lot of time. This is true. I currently am involved in high performance computing (undergraduate research) and we often discuss gpu vs cpu for the calculations we are doing. Any time you have a parallelizable problem, ur gonna wanna consider gpu. They originally became big in gaming, because most image processing is inherently parallel. And now that we cant just design smaller faster chips, theres a motivation to find more ways to speedup like parallel processing. We also optimize for memory in my lab. One of the grad students was talking about how cuda doesnt have any way to parallelize sparse or banded matrices. They actually just pass everything to cpu. Which is something they could work on. NVDA chip buyers need more years (if not decades) to pay off their notes that capitalized their “hyper scaling” than NVDA PUs will be state of art. There are efficiencies…. The short term answer to AI is reduce the cost to train your llModles. Then reduce the run cost for compute /inference. Low case “l” not typo. LLM are often too expensive for the answer they provide. DeepSeek does this with SW. The other efficiency is in the PU’s. Which TPU may be just one. OpenAI won’t make any money ever, many NVDA customers go broke and will default a bunch of bonds. The Fed can keep solvent a lot of CDS resulting from a lot of analysts failing to question Huang and Altman. While they ignored macro economics and new disruptive product profit cycles. Bond underwriting. There are all kinds of disrupters chasing tech that will bring down the TBTF. No govt bail out, which tech firm will be first Bear Sterns, ENRON? To put a 2025 technology LLM AI into a humanoid android, approximately 4.3 m³ would be required for the hardware (Computing: 1.5 m³, Cooling: 2.0 m³, Power Supply: 0.5 m³, Connectivity: 0.3 m³). This AI must be pre-trained in data centers that require about 1600 m³ for the hardware, and between two and five of these data centers are needed for the training phase. Once trained, several of these data centers will perform the task of fine-tuning and improving the LLM model, and others will handle storage and backups. Additional data centers will surely be required for this latter task. This AI could perform local inference and fine-tuning, but it would still require connectivity to data centers for model updates (weekly/monthly), data synchronization (optional), specific tasks requiring the cloud, internet searches, complex non-time-sensitive analysis, data backups, among other things. In other words, self-replicating machines and Terminator-style robots are still a long way from becoming a reality. Great compendium as always, Nat. What a time to be alive. In a YouTube video I heard the argument that this certainly looks like a bubble, and there are lots of warning flags, but those firms mostly use their giant pile of cash to do the investing, rather than using (leveraged) debt. I can’t assess it that is totally true, but if it is, then it is a very risky bet, and given that the US economy is driven by those “Magnificent Seven”, it may not hurt the mainstream economy all that much? Of course, there are many investors who will get hurt, so that’s bad. Is this a reasonable point of contention with the bubble-story or not? It’s the pension funds and retail investors who will lose everything when the bubble pops. As a horrible old Man who was licensed to run a collection agency for more than a decade ( I actually specialized in second and third placement debts, including corporate debt) I look at the Money flows. And while NVIDIA is not as blatant a scam as WeWork was, the financial relationships between NVIDIA and its “Customers” are Peculiar, to say the least. I’m reminded of Keyne’s saying that the market can stay irrational longer than I can stay solvent, regarding the short sellers. Since Sam Altman’s ultimate contribution to society and business is going to be individualized sex porn, the only real question for the business world is, what is the monetary value as a percentage of GDP? I assume we can take OpenAI being worth $500B to some investors based on his penultimate contribution of individualized non-sex porn for all ChatGPT users’ needs for companionship and flattery as a starting point for computing the value of his ultimate contribution. Your email address will not be published. Required fields are marked * Comment * Name * Email * SUBSCRIPTIONS
--------------------------------------------------

Title: 10 Datapoints for Thanksgiving
URL: https://ritholtz.com/2025/11/10-datapoints-for-thanksgiving/
Time Published: 2025-11-26T19:00:14Z
Full Content:
November 26, 2025 2:00pm by Barry Ritholtz It’s that time of year again when families gather to feast on bountiful harvests and to give thanks for all of our blessings. This year, skip the “Vibes” and instead focus on market data. Don’t lose sight of nuances and shades of grey; they don’t make for great memes, but they do lead to a better understanding of what’s going on. Here are ten nuanced, slightly contrarian ideas for you to chew over: 1. ARTIFICIAL INTELLIGENCE: Perhaps we are in the late stages of an AI-driven bubble; we could just as easily be in a once-in-a-generation transformational technology boom that will drive both the economy and the stock market higher for years to come. Too many people fail to recognize how challenging it is to identify these generational market turning points in real time. My favorite takes on AI have come from Derek Thompson and Timothy Lee, who looked into the 12 main arguments Pro & Con, and Benjamin Riley, who aims to “help people understand human cognition and artificial intelligence.” 2. INFLATION: Everything costs more this year — except for the Turkey. The largest fiscal stimulus since World War 2 led to the largest inflation surge since the 1970s. The rate of price increases rose by 9% (peaking June 2022) before falling back to 3% nearly as quickly. There were numerous causes of inflation, but the top of the list was the pandemic supply issues and the huge fiscal stimulus. People confuse the rate of price change with prices. We had high inflation; today, we have low(ish) inflation, but we still retain higher prices. Everything is much more expensive today, even with inflation way down. Low Inflation and High Prices are not mutually exclusive. CPI Inflation is in the 2-3% range today, but it is ticking upwards, creating difficulties for those on the FOMC who want to cut rates. 3. SUPPLY & DEMAND: We may not have structural inflation as we did in the 1970s, but we do have a structural imbalance in supply and demand of many critical goods and services. A few significant examples: Until supply catches up with demand, those prices will remain high. And that is before we get to health care and education costs. 4. ENERGY: The inflation of the 1970s was structural, caused in large part by the Arab Oil Embargo. In contrast, the United States is a net energy exporter today. In the 1970s, energy accounted for about 10% of the average household budget; the Chicago Fed found it peaked at nearly 14% in the early 1980s. Household energy costs are about half of those levels today (5-6%), even as energy consumption has increased significantly. Every power-hungry device, from automobiles to HVAC systems to appliances, is now many times more efficient than in the past. The wildcard is increased demand from power-hungry data centers… 5. CRYPTO CRASH: Given the embrace of crypto by the President (and POTUS’s family), much of Bitcoin 2025 gains can be attributed to this administration’s policies. We should not be surprised by the correlation between the President’s political fortunes / approval ratings, and the price of Bitcoin. The President has had a terrible month; from the election thumping to the fallout with MTG to losing multiple legal cases (Tariffs at SCOTUS, Comey / James case dismissals), it’s no surprise that Bitcoin has suffered a 30% crash this month as well: 6. TARIFFS: Are fascinating: They cause temporary inflation spikes and permanent higher prices. There is no getting around it – any additional tax on imported goods is a source of increased prices. And as we have seen before, even domestic producers will raise prices (Greedflation) if they believe consumers won’t balk. The good news: If the Supreme Court arguments were anything to go on, many of the Tariffs are likely to get struck down. 7. RATE CUTS: You can make a solid case either way – inflation remains stubborn at (or over) 3%, but there are signs of labor market softness, slowing consumer sales, and mediocre sentiment. Expectations had fallen to a ~20% chance of a rate cut – until yesterday’s poor data. Now, we are back to an 80% chance of a December cut. Beyond that is anyone’s guess… 8. BUBBLES: By definition, it takes a crowd to create a bubble. Can you recall the public, the media, or even the Fed identifying a bubble on a timely basis? (Me neither). Asked differently, can investors rationally believe that prices are not entirely irrational? If your answer is yes, then it’s likely not a bubble. Perhaps the most interesting aspect of the AI bubble debate is Alphabet (GOOGL) passing Nvidia (NVDA) YTD returns: : 9. RECESSION: People hate inflation, but the alternative was a deep and long-lasting pandemic recession. We avoided a 10-12% unemployment rate, but the cost was 9% inflation. Consider the alternative, had both the Trump and Biden admins not cranked up the fiscal spend, people would have been furious at the failure to do anything1. It’s a Lose/Lose; whatever choice got made, half the population would have been furious. As angry as people are over high prices, they would have been even angrier at a do-nothing government letting an ugly recession take hold. 10. VALUATIONS: The Mag 7 remains pricey, even as Nvidia slides 13% off its highs. Its expensive, but it also generates $57 billion in quarterly revenues! Some sectors are extremely overpriced, others are more reasonable. The S&P 493 — S&P 500 minus the Magnificent 7 — is at 20.7 P/E. Pricey, but not ridiculous. Nuance is your friend. Safe travels! Previously: The Muted Impact of Tariffs on Inflation So Far (July 17, 2025) Make Thanksgiving Great Again! (November 23, 2023) Revisiting Greedflation (November 16, 2023) Who Is to Blame for Inflation, 1-15 (June 28, 2022) Miscalculated Housing Demand (July 29, 2021) How to Talk to a Fox News Viewer (November 22, 2018) How Everybody Miscalculated Housing Demand (July 29, 2021) This content, which contains security-related opinions and/or information, is provided for informational purposes only and should not be relied upon in any manner as professional advice, or an endorsement of any practices, products or services. There can be no guarantees or assurances that the views expressed here will be applicable for any particular facts or circumstances, and should not be relied upon in any manner. You should consult your own advisers as to legal, business, tax, and other related matters concerning any investment. The commentary in this “post” (including any related blog, podcasts, videos, and social media) reflects the personal opinions, viewpoints, and analyses of the Ritholtz Wealth Management employees providing such comments, and should not be regarded the views of Ritholtz Wealth Management LLC. or its respective affiliates or as a description of advisory services provided by Ritholtz Wealth Management or performance returns of any Ritholtz Wealth Management Investments client. References to any securities or digital assets, or performance data, are for illustrative purposes only and do not constitute an investment recommendation or offer to provide investment advisory services. Charts and graphs provided within are for informational purposes solely and should not be relied upon when making any investment decision. Past performance is not indicative of future results. The content speaks only as of the date indicated. Any projections, estimates, forecasts, targets, prospects, and/or opinions expressed in these materials are subject to change without notice and may differ or be contrary to opinions expressed by others. The Compound Media, Inc., an affiliate of Ritholtz Wealth Management, receives payment from various entities for advertisements in affiliated podcasts, blogs and emails. Inclusion of such advertisements does not constitute or imply endorsement, sponsorship or recommendation thereof, or any affiliation therewith, by the Content Creator or by Ritholtz Wealth Management or any of its employees. Investments in securities involve the risk of loss. For additional advertisement disclaimers see here: https://www.ritholtzwealth.com/advertising-disclaimers Please see disclosures here: https://ritholtzwealth.com/blog-disclosures/ Previous Post Next Post Read More Disclosures Privacy Policy Terms and Conditions Get subscriber-only insights and news delivered by Barry daily. How Greed and Easy Money Corrupted Wall Street and Shook the World Economy Learn More... © 2025 The Big Picture Get subscriber-only insights and news delivered by Barry every two weeks. Subscribe
--------------------------------------------------

Title: Google Is Getting the AI Spotlight, But Nvidia Says Its GPUs Are a ‘Generation Ahead.’ How Should You Play NVDA Stock Here?
URL: https://www.barchart.com/story/news/36331832/google-is-getting-the-ai-spotlight-but-nvidia-says-its-gpus-are-a-generation-ahead-how-should-you-play-nvda-stock-here
Time Published: 2025-11-26T18:53:24Z
Description: Nvidia stock inches up as Jensen Huang confirms its GPUs are a generation ahead of Google’s custom AI chips. Here’s what that means for NVDA shares.
--------------------------------------------------

Title: Jim Cramer drops blunt call on Nvidia stock
URL: https://www.thestreet.com/investing/stocks/jim-cramer-drops-blunt-call-on-nvidia-stock
Time Published: 2025-11-26T18:47:00Z
Description: AI bellwether Nvidia’s (NVDA) stock has shed nearly $200 billion in market cap (4.71% drop) in just the past week, making it a fresh target for nervous...
--------------------------------------------------

Title: Tech analyst Dan Ives flags 10 stocks to own, insists there's no AI bubble
URL: https://finance.yahoo.com/news/tech-analyst-dan-ives-flags-10-stocks-to-own-insists-theres-no-ai-bubble-180237596.html
Time Published: 2025-11-26T18:02:37Z
Description: The AI revolution is still in its early innings. Wedbush analyst Dan Ives argues that investors are misreading one of the most significant technology build...
--------------------------------------------------

Title: Down 45% From Its 52-Week High, Wall Street Still Loves This Semiconductor Stock
URL: https://www.barchart.com/story/news/36331064/down-45-from-its-52-week-high-wall-street-still-loves-this-semiconductor-stock
Time Published: 2025-11-26T18:00:02Z
Description: Despite an over 45% plunge from its highs, Astera Labs stock is still winning over top analysts who see its AI infrastructure edge as a powerful catalyst for...
--------------------------------------------------

Title: Microsoft Stock Faces An AI-Driven Physics Problem
URL: https://www.marketbeat.com/originals/microsoft-stock-faces-an-ai-driven-physics-problem/?utm_source=yahoofinance&amp;utm_medium=yahoofinance
Time Published: 2025-11-26T17:49:00Z
Description: Microsoft faces margin pressure from heavy AI spending, but long-term demand, new AI economics, and improving technicals support a bullish outlook
--------------------------------------------------

Title: ‘These Chips Will Profoundly Change the World’ and ‘Save Lives.’ Elon Musk Doubles Down on AI Chips as TSLA Stock Stagnates YTD.
URL: https://www.barchart.com/story/news/36329726/these-chips-will-profoundly-change-the-world-and-save-lives-elon-musk-doubles-down-on-ai-chips-as-tsla-stock-stagnates-ytd
Time Published: 2025-11-26T16:32:45Z
Description: Elon Musk posted on social media that the company has bold visions for its AI chips and is planning to build at higher volumes than all other AI chip...
--------------------------------------------------

Title: Stocks Climb on Hopes of Lower Interest Rates
URL: https://www.barchart.com/story/news/36329494/stocks-climb-on-hopes-of-lower-interest-rates
Time Published: 2025-11-26T16:14:27Z
Description: The S&P 500 Index ($SPX ) (SPY ) today is up by +0.49%, the Dow Jones Industrials Index ($DOWI ) (DIA ) is up by +0.51%, and the Nasdaq 100 Index ($IUXX...
--------------------------------------------------

Title: US stock market rises ahead of Thanksgiving as Dow, S&P 500, Nasdaq extend gains - Nvidia rebounds; Alphabet flirts with $4 trillion valuation
URL: https://economictimes.indiatimes.com/news/international/us/us-stock-market-rises-ahead-of-thanksgiving-as-dow-sp-500-nasdaq-extend-gains-nvidia-rebounds-alphabet-flirts-with-4-trillion-valuation/articleshow/125593608.cms
Time Published: 2025-11-26T16:11:23Z
Full Content:
US stock market rose on Wednesday as the Dow gained 0.3% to 47,293, the S&P 500 added nearly 0.5%, and the Nasdaq climbed over 0.6% ahead of Thanksgiving. Nvidia recovered about 2% after Tuesday’s 2.6% drop, while Alphabet eased slightly after a 1.6% jump that pushed it near a $4 trillion valuation. Traders priced in over an 80% chance of a December Fed rate cut as the Beige Book and earnings drove sentiment. US stock market rises ahead of Thanksgiving as Dow, S&P 500, Nasdaq extend gains — Nvidia rebounds as Alphabet nears $4 trillion value (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Explore More Stories Canadian Olympic swimming champion Penny Oleksiak suspended for two years over anti-doping whereabouts failures Who is Colleen Jones? Two-time World champion curler and veteran Canadian broadcaster dies at 65 What exactly is the Canada Pension Plan? New CPP payments rolling out nationwide on November 26; Check all the payment dates of 2025-26 - Why is Quebec not part of CPP? Madeleine Poulin, Radio-Canada’s first female correspondent in Ottawa and Paris, dies at 87 Yoplait recalls 'Yop yogurt' drinks in Canada over plastic contamination fears under Class 1 category - Check out which flavor and best-before dates beverage are subject to recall CMA Awards: Vince Gill honored with prestigious Willie Nelson Lifetime Achievement Award at 59th CMA Awards in Tennessee MGK rocks Canadian fans with explosive Grey Cup halftime show; Are Megan Fox, MGK really working on their relationship post daughter’s birth? Roughriders end 12-year drought, beat Alouettes for 5th Grey Cup title Who is Dr Sanjeev Sirpal? Trouble mounts for New Brunswick doctor accused of sexual assault in hospitals as he faces additional charges; what do we know so far Russian humanoid robot 'AIDOL' shockingly faceplants during much-hyped public debut - WATCH Who is Ben Watsa, the future chairman of Fairfax financial and heir to Prem Watsa’s $100-billion empire? Zdeno Chára, Alexander Mogilny, Joe Thornton among eight inducted into 2025 Hockey Hall of Fame class Are Facebook ads helping users buy illegal drugs by mail in Canada - Drugs travel through an ad to a postal delivery; $9 million worth of illegal drugs were turned over to police in 2024 by Canada Post India clinches 2030 Commonwealth Games host spot India rebukes China: ‘Arunachal is India’ Bengal SIR: ‘5.2cr forms issued, 13L not returned, 22L unmatched’ BJP's counterattack: 'Bengal is ready to shake Mamata first’ Babri Row In Bengal Cabinet clears Rs 7,280 cr rare earth magnet Scheme | Pune Metro | LIVE Omar Abdullah slams BJP over Vaishno Devi college row 'We will respond appropriately,' Afghanistan warns Pakistan 'Indian skies protected': Safran hails India’s defence, space advances Delhi introduces its first hot air balloon rides India clinches 2030 Commonwealth Games host spot India rebukes China: ‘Arunachal is India’ Bengal SIR: ‘5.2cr forms issued, 13L not returned, 22L unmatched’ BJP's counterattack: 'Bengal is ready to shake Mamata first’ Babri Row In Bengal Cabinet clears Rs 7,280 cr rare earth magnet Scheme | Pune Metro | LIVE Omar Abdullah slams BJP over Vaishno Devi college row 'We will respond appropriately,' Afghanistan warns Pakistan 'Indian skies protected': Safran hails India’s defence, space advances Delhi introduces its first hot air balloon rides Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Slideshow Top Prime Articles Top Definitions Most Searched IFSC Codes Top Story Listing Private Companies Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: 3 Stocks at Fresh 52-Week AND Record Highs to Buy Now
URL: https://www.barchart.com/story/news/36328605/3-stocks-at-fresh-52-week-and-record-highs-to-buy-now
Time Published: 2025-11-26T15:29:50Z
Description: Both the NYSE and Nasdaq had more stocks hitting new 52-week highs than lows yesterday. Among the many choices were three stocks also hitting all-time highs....
--------------------------------------------------

Title: How Is Advanced Micro Devices' Stock Performance Compared to Other Semiconductor Stocks?
URL: https://www.barchart.com/story/news/36328512/how-is-advanced-micro-devices-stock-performance-compared-to-other-semiconductor-stocks
Time Published: 2025-11-26T15:22:50Z
Description: Advanced Micro Devices has considerably outperformed the Semiconductor industry over the past year, and analysts are cautiously optimistic about the stock’s ...
--------------------------------------------------

Title: Stocks Supported by Fed Rate-Cut Optimism
URL: https://www.barchart.com/story/news/36328206/stocks-supported-by-fed-rate-cut-optimism
Time Published: 2025-11-26T15:09:12Z
Description: The S&P 500 Index ($SPX ) (SPY ) today is up by +0.60%, the Dow Jones Industrials Index ($DOWI ) (DIA ) is up by +0.62%, and the Nasdaq 100 Index ($IUXX...
--------------------------------------------------