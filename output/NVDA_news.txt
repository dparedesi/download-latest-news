List of news related to NVIDIA NVDA:

Title: Why China Startup DeepSeek Could Spell Trouble For Nvidia, AI Stocks
URL: https://www.investors.com/news/technology/deepseek-ai-stocks-nvidia-artificial-intelligence-capital-spending/
Time Published: 2025-01-27T01:54:10Z
Description: Wall Street mulled demand for Nvidia chips and huge capital spending.
--------------------------------------------------

Title: Top 5 Sectors To Invest In 2025
URL: https://www.forbes.com/sites/investor-hub/article/top-sectors-to-invest-2025/
Time Published: 2025-01-26T18:00:00Z
Full Content:
These five sectors offer substantial opportunities for investors looking to align with innovation ... [+] and market demand. The investment landscape in 2025 offers several promising opportunities in high-growth sectors. While market volatility continues to be a challenge, certain areas stand out for their strong growth potential, technological innovation, and ability to address critical global issues. This analysis, supported by comprehensive market research and economic indicators, highlights five sectors worth investors' attention. Successful investing in specific sectors requires analyzing several factors that indicate sustainable growth potential. These criteria are essential for making informed investment decisions based on solid fundamentals rather than market sentiment. Market trends involve both cyclical patterns and long-term changes in the economy. Key factors include trading volumes, price movements and sector rotation patterns. Economic indicators such as GDP growth, inflation, and interest rates are vital in understanding market movements. For instance, the U.S. economy grew by 2.1% in 2024, but inflation rates stabilized around 3.4%, affecting sectors differently. Shifts in global trade dynamics, such as increased trade agreements in Asia and the continued impact of supply chain disruptions, are shaping investment opportunities. Innovation remains a critical driver of growth. Metrics such as R&D spending, patent applications, and the pace of new product development highlight sectors leading in technological advancements. According to recent reports, global R&D spending reached $2.5 trillion in 2024, with industries like artificial intelligence and clean energy receiving a significant share of this investment. Companies that innovate consistently are better positioned for sustained growth and market leadership. Analyzing global demand patterns helps identify growth trajectories. Demographic trends, such as aging populations in developed markets and growing middle classes in emerging markets, shape consumption patterns. For example, India’s GDP growth is projected to exceed 6.3% in 2025, driven by increased domestic consumption and infrastructure spending. These factors create opportunities for sectors like healthcare, advanced manufacturing and clean energy. Environmental, Social and Governance (ESG) considerations are now central to evaluating sector performance. Sectors aligned with sustainability goals often benefit from supportive regulations and growing consumer demand for responsible business practices. In 2024, global ESG-focused assets under management surpassed $40 trillion, demonstrating the increasing investor focus on sustainability metrics such as carbon footprints, resource efficiency and governance standards. The sectors identified here were selected using a combination of quantitative and qualitative metrics. Financial indicators such as revenue growth, profit margins and return on invested capital (ROIC) were paired with assessments of market positioning, competitive dynamics and regulatory environments. As we head into 2025, identifying the right sectors to invest in is more important than ever. With shifting economic trends, emerging technologies and evolving consumer habits, certain industries are positioned to outperform others. In this section, we’ll explore five sectors that offer strong potential for growth and resilience in the year ahead, backed by market trends and data-driven insights. Whether you’re looking for stability or high-growth opportunities, these sectors deserve a closer look The Artificial Intelligence (AI) and Machine Learning (ML) sector is growing rapidly, with applications spreading across healthcare, finance and logistics industries. Major players are consolidating their market positions through acquisitions of innovative startups. AI adoption is accelerating, supported by technological advancements and increased investment. According to PwC, AI-driven technologies will contribute $15.7 trillion to the global economy by 2030. Regulatory frameworks are also evolving to ensure ethical AI usage, further boosting the sector's credibility. Clean energy has transitioned from an alternative option to a mainstream necessity. Advancements in battery technology and storage solutions have addressed previous challenges in adopting renewable energy. Governments worldwide are committing to net-zero emissions targets. The U.S. Inflation Reduction Act of 2022 continues to funnel billions into clean energy projects, and declining costs of renewable installations make this sector highly attractive. The healthcare technology sector encompasses digital health platforms, advanced diagnostics, and personalized medicine. The pandemic accelerated the adoption of digital health solutions while biotechnology continues to innovate. An aging population and rising healthcare costs are driving demand for innovative solutions. According to the WHO, global healthcare spending reached $10.3 trillion in 2024, highlighting the importance of this sector. As cyber threats grow in complexity, the cybersecurity sector is evolving rapidly. Comprehensive security platforms now incorporate AI-driven tools, zero-trust architectures and cloud-native solutions. Regulatory requirements, such as GDPR in Europe and increased focus on data privacy in the U.S., drive demand for robust cybersecurity solutions. Gartner predicts that worldwide spending on cybersecurity will exceed $200 billion by 2025. Technologies like industrial robotics, 3-D printing, and smart factories are revolutionizing traditional manufacturing processes. Supply chain disruptions in recent years have accelerated the adoption of automated solutions. Rising labor costs and the need for supply chain resilience have made automation a priority. According to McKinsey, automation could add $1.2 trillion to the global economy by 2030. Bottom Line These five sectors offer substantial opportunities for investors looking to align with innovation and market demand. While the potential for high returns exists, diversification and risk management are critical to navigating market volatility. Before investing, assess company fundamentals, valuation metrics, and broader economic conditions. Staying informed and monitoring trends within these sectors will be key to long-term success in 2025. Whether it’s mastering cutting-edge strategies, uncovering actionable investment opportunities from influential leaders, or breaking down complex topics, our in-depth journalism has you covered. Become a Forbes member and gain unlimited access to bold ideas shaking up industries, expert guides and practical investment advice that keeps you ahead of the market. Unlock Premium Access — Free For 25 Days. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Tesla's future under President Trump: Weigh AI potential against slowing EV demand
URL: https://finance.yahoo.com/news/teslas-future-under-president-trump-weigh-ai-potential-against-slowing-ev-demand-150331143.html
Time Published: 2025-01-26T15:03:31Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. Tesla’s (TSLA) eye-watering post-election rally may soon resume. The stock, which has rocketed a staggering 65% since Nov. 5, has taken a breather in recent weeks, as worry over slowing demand trends and a rollback of pro-EV policies raised fears of near-term financial performance. Tesla shares fell 4% during President Trump’s first week back in office, contributing to a one-month decline of 12%. Despite the recent dip, many analysts remain confident in the company heading into its critical fourth quarter earnings report. Rather than focusing on Tesla’s first annual sales decline in over a decade, they’re becoming increasingly optimistic about Tesla’s AI potential. Analysts at Piper Sandler and Wedbush were among the teams raising their 12-month price targets this week, with Piper Sandler naming Tesla its top "buy-and-hold idea" and Wedbush calling the new administration a "total game changer" for the company. Watch: GM's CEO Marry Barra on the company's EV plans “The $7,500 credit rollback is more of a negative for new entrants and Detroit’s big three automakers,” Ives told me on Yahoo Finance’s Morning Brief. “For Tesla, 90% of what's going to drive the stock is autonomous and AI … that's the ultimate story.” Ives predicts Tesla’s valuation will reach $2 trillion by the end of this year, driven by what he described as a “regulatory friendly” White House that will “fast-track the autonomous future.” Morgan Stanley’s Adam Jonas also sees AI as a key growth driver for Tesla. In a recent note to investors, the Tesla bull wrote that AI and robotics have become top of mind for clients following Nvidia (NVDA) CEO Jensen Huang’s CES presentation. The AI chip juggernaut announced a partnership with Toyota (TM) for autonomous vehicles and another deal with automotive company Continental and self-driving truck firm Aurora. However, Jonas has not yet priced embodied AI into his value modeling for Tesla. “We are receiving more incoming client requests to discuss humanoids than the entirety of our auto OEM, dealer and supplier coverage combined … While we continue to view Tesla as sort of an embodied AI ‘ETF’ and well positioned due to the underpinnings of our DREAMS framework, we note that we do not currently ascribe any value to Tesla for embodied AI in our $430 price target or $800 bull case,” Jonas wrote in a note to clients. Jonas also sees the Trump administration’s tariff policies as a potential catalyst for the company. He views Trump’s aggressive use of tariffs as a push to accelerate onshoring of AI-enabled technology, a move that could create future growth opportunities for Tesla. “We expect to see the Trump Administration’s EV and AV policies encouraging new domestic supply of critical technologies and supporting manufacturing and supply base,” Jonas wrote. “In our view, Tesla's role in helping to 'fill the void' of next-gen manufacturing and supply chain will be an increasingly consequential driver of growth and shareholder value.” And it’s the AI narrative that sets Tesla apart from the rest of the automakers. Traditional automakers, for comparison, are facing a different reality as President Trump’s threat of tariffs on Mexico and Canada weigh on stock performance. Shares of General Motors (GM) have climbed just 6% post-election, while Ford (F) and Stellantis (STLA) have declined 1% and 2%, respectively. According to RBC analysis, General Motors is most exposed to looming tariffs, given that Mexico production accounts for about 35% of US sales, followed by Stellantis and Ford. As the debate over whether Tesla can meet Wall Street’s lofty expectations under President Trump continues, the company is set to face its first major test this week. Tesla is scheduled to release fourth quarter results on Wednesday after the market close. Seana Smith is an anchor at Yahoo Finance. Follow Smith on X @SeanaNSmith. Tips on deals, mergers, activist situations, or anything else? Email seanasmith@yahooinc.com. Click here for the latest technology news that will impact the stock market Read the latest financial and business news from Yahoo Finance Sign in to access your portfolio
--------------------------------------------------

Title: Crypto narratives: How the absence of earnings fuels investor fervor
URL: https://cryptoslate.com/crypto-narratives-how-the-absence-of-earnings-fuels-investor-fervor/
Time Published: 2025-01-26T13:00:05Z
Full Content:
AI synergy with blockchain fuels new trading opportunities in the crypto landscape. Cover art/illustration via CryptoSlate. Image includes combined content which may include AI-generated content. The following is a guest post from Shane Neagle, Editor In Chief from The Tokenist. Not all narratives are created equal. In the age of digital financial platforms, investing in all kinds of assets has never been easier. This is happening at a time when the stakes are both clear and high. In order to outpace the erosion of money due to central banking, otherwise known as inflation, investing has to yield sustained high single-digit returns at a bare minimum. But in a rush to outpace inflation, in addition to offset capital gains tax, investing has become akin to gambling. This is especially apparent in the blockchain space. To become more resilient when staking an asset, what should investors keep in mind? Lowering the barrier to entry works both ways in the crypto world, but not so much in the stock world. On one hand, people have easier access to capital, but on the other hand, companies face increased scrutiny and regulatory burden by going public. This is evident by the stocks drop off since the mid-1990s, having fallen by 56% by 2020 from over 8,000 stocks. Because publicly traded companies are based on physical operations that exert expenditures and require quarterly financial reports, there is a cycle of inflows and drop offs.This leaves the number of stocks at approximately the same level, reliant on business cycles. In other words, stocks have an inherent barrier to entry, from the side of businesses, which also serves as a signal for value. The same is not the case for digital assets. Notwithstanding cryptocurrencies based on the proof-of-work algorithm like Bitcoin, the vast majority of crypto coins are based on proof-of-stake consensus. This means there is no longer a requirement for infrastructure in the form of mining hardware and electricity. In turn, there is no crypto equivalent when it comes to expenditures and earnings. Likewise, funded and generalized proof-of-stake platforms like Ethereum, BSC, Solana or Avalanche serve as a launching pad for easy token creation. These factors are driving the number of cryptocurrencies to ever increasing heights, greatly overshadowing the number of stocks at 16,218 (at press time). All of these coins compete for a finite amount of capital and human attention, which means the more tokens are birthed the greater the dilution effect. Not only is it easy to create cryptocurrencies ex nihilo, but it is also easier to access them via self-custodial wallets and decentralized exchanges. At first glance, this may seem beneficial, but does it benefit people’s portfolios? Among countless examples in the negative direction is the recent Hawk Tuah (HAWKTUAH) coin, having dropped by 99.14% in value since its inception in July. In the absence of quarterly earnings, and boosted by ease of entry, crypto traders have become reliant on “vibes”, or narratives: The exact same principle exists in the lottery. Because it is known that some people won lottery tickets, the potential for life-changing gains is established, regardless how remote it actually is. This is why the memecoin narrative has been so performant over the last year, as a market cap-weighted category. Of course, that “performance” accounts for the flood of memecoins that increased the market cap and opened up new narrative gambling opportunities. In actuality, most traders lose money based on such narratives. According to Pump.fun at Dune Analytics, 60% of memecoiners lost their narrative bets. A REMINDER THAT: • 60% of all memecoin traders lost money trading memecoins. • 4.7% made no money. • 24% made less than $100. • 11.2% made more than $100. • 3% made more than $1,000. • 0.5% made more than $10,000. • People who have made more than $10K can barely… pic.twitter.com/ADakThjOcX — Kermit 🐸 (@crypto__kermit) November 19, 2024 Although it is easy to see how memecoin trading is a simulacrum of gambling, these digital assets are traded equally with other coins. This includes Bitcoin with its fixed scarcity and vast computing infrastructure to anchor it into the physical world. Consequently, it is fair to say that some crypto narratives have a draining effect on the wider crypto market. After all, those capital inflows could have been better served elsewhere. But where exactly (excluding Bitcoin as the obvious candidate)? Just like the AI boom endowed Nvidia (NASDAQ: NVDA) with over $3 trillion market cap, the AI narrative has become dominant in the crypto space, even outpacing memecoins. Notwithstanding the buzzwordiness, what exactly does AI + blockchain bring to the crypto table? It turns out, these technologies are perfectly complementary. Blockchain represents trustless networks, which are prerequisites for autonomous AI agents to operate without centralized oversight. For instance, AI agents could be used to automate tasks such as managing portfolios or yields on DeFi platforms. In other words, they have the potential to turn digital assets into passive income streams. Virtuals Protocol (VIRTUAL) makes it possible to create and co-own AI agents, which will then interact with a wide range of blockchain networks. This potential already skyrocketed VIRTUAL token, having gained over 5,000% value in the last three months. Similarly, ai16z (AI16Z), inspired by entrepreneur Marc Andreessen who co-founded venture capital firm Andreessen Horowitz (a16z), is the first AI-governed Decentralized Autonomous Organization (DAO). Serving as a utility token, AI16Z pays for activities within this virtual fund to automatically trade tokens on DeFi platforms, post social media content or browse the web to glean insight. Over the last month, AI16Z gained 90% of value. Another notable project is NeuralAI (NEURAL), as a part of a wider ecosystem in which AI agents generate 3D objects from text descriptions. Although down 30% over the month, NEURAL token gained 911% value over the last year. Likewise, Morphware (XMW) gained 291% value in the same period. This project aims to decentralize computing power needed for AI infrastructure, with XMW monetizing GPU hardware across a peer-to-peer network. With 83% of companies stating AI is their top priority, many are already thinking about how blockchain fits in this new world order. If AI already has the capacity to generate coherent text based on input sentiment, it is not far off in analyzing market sentiment as well. In turn, machine learning (ML) and natural language processing (NLP), could end up systematizing complex derivatives trading such as futures contracts by leveraging technical indicators and social media content. In fact, such AI agents could be more adept at trading volatile memecoins as they churn out social media narratives 24/7. Namely, they could pick just the right time frame to enter and exit the market. Of course, this would end up in an agent-vs-agent trading landscape. However, just like in a traditional arms race, such an environment could amplify the value of AI agent-centric tokens, in addition to their supporting infrastructure. And just as lottery spectacles aren’t going away due to their potential, memecoins are likely to stay as well. But with emerging AI projects focused on trading, at least memecoins could work off their capital drain. Since 2015, Shane fervently backs decentralized finance, penning countless articles on digital securities and the fusion of traditional finance with DLT. He's intrigued by technology's influence on economics and life. CryptoSlate is a comprehensive and contextualized source for crypto news, insights, and data. Focusing on Bitcoin, macro, DeFi and AI. Get the latest crypto news and expert insights. Delivered to you daily. Disclaimer: Our writers' opinions are solely their own and do not reflect the opinion of CryptoSlate. None of the information you read on CryptoSlate should be taken as investment advice, nor does CryptoSlate endorse any project that may be mentioned or linked to in this article. Buying and trading cryptocurrencies should be considered a high-risk activity. Please do your own due diligence before taking any action related to content within this article. Finally, CryptoSlate takes no responsibility should you lose money trading cryptocurrencies. Bitcoin, a decentralized currency that defies the sway of central banks or administrators, transacts electronically, circumventing intermediaries via a peer-to-peer network. Virtuals Protocol is an AI x Metaverse Protocol that is building the future of virtual interactions.. ai16z is pioneering as the first venture capital firm operated by AI agents. NeuralAI enables users to convert text or images into 3D models, providing game-ready assets that can be immediately utilized across various design tools and game engines. Pump.Fun is a platform that enables users to create and trade memecoins easily, capitalizing on the novelty and community-driven appeal of these tokens. Daily digest of top crypto stories and market insights. Never miss out. Disclaimer: By using this website, you agree to our Terms and Conditions and Privacy Policy. CryptoSlate has no affiliation or relationship with any coin, business, project or event unless explicitly stated otherwise. CryptoSlate is only an informational website that provides news about coins, blockchain companies, blockchain products and blockchain events. None of the information you read on CryptoSlate should be taken as investment advice. Buying and trading cryptocurrencies should be considered a high-risk activity. Please do your own diligence before making any investment decisions. CryptoSlate is not accountable, directly or indirectly, for any damage or loss incurred, alleged or otherwise, in connection to the use or reliance of any content you read on the site. © 2025 CryptoSlate. All rights reserved. Disclaimers | Terms | Privacy Please add "[email protected]" to your email whitelist. Stay connected via
--------------------------------------------------

Title: 6 Top Stocks To Buy Now For February 2025
URL: https://www.forbes.com/sites/investor-hub/article/best-stocks-buy-now-february-2025/
Time Published: 2025-01-26T12:00:00Z
Full Content:
Will big tech continue to dominate in 2025, or will value stocks and underdogs shine bright? If your New Year's resolutions include increasing your net worth, you may need to review and revise your investment portfolio. You can use these expert-recommended top stocks for February 2025 to kickstart your research and get motivated for the wealth opportunities ahead. These top stock recommendations came from interviews with nine investing experts: The experts cited several factors that will shape the financial markets in 2025. Improving U.S. economics and potentially lower interest rates alongside high valuations and the threat of lingering inflation are stoking optimism and caution. And, despite President Trump's vow to boost U.S. oil and gas production, several experts also identified the clean energy transition and sustainability as key themes. Notably, the experts were divided on the influence of technology and AI in the year ahead. Shen, Kravetz, and Bennett believe strong AI demand will continue to lead the financial markets. Chashchin and Stanzione shared different views. Chashcin says market leadership could shift away from AI and big tech this year. "We've been seeing tech dominate for so long, but 2025 might be the year when other sectors, like financials, start to take center stage," he explains. Stanzione sees room for overlooked tech stocks, specifically Intel (INTC), to outperform. In short, uncertainty prevails. Reflecting a dynamic environment ahead, the list of top stocks below includes big tech plays, solid value stocks and a downtrodden underdog. The table below includes six best stocks for 2025 to buy now, according to experts. Let's review each of these stocks now. Apple makes smartphones, wearables, laptops and desktop computers. The company also operates the App Store, earning commissions on sales made by app publishers. Apple's stock price is down double digits in January after reports of soft device sales in China. However, Brown characterizes the downturn as a buying opportunity rather than a sign of permanent weakness. "The real play is Apple Intelligence," Brown explains. Apple Intelligence is Apple's suite of AI tools, available only on newer iPhones, tablets and computers. Those include the iPhone 16 and tablets and MacBooks with M1 or later chips. As Apple Intelligence features get more sophisticated, upgrades to Apple's AI-compatible devices should increase. Nvidia is a fabless semiconductor company, meaning it outsources chip manufacturing. The company's high-powered GPUs are the preferred AI computing chips. Nvidia sells GPUs and related products to original equipment and device makers, cloud computing providers, software vendors and internet providers. Shen and Bennett are bullish on AI. Bennett explains, "The growing integration of AI across industries is reshaping business models, boosting productivity and driving demand for automation." He expects the AI revolution to deliver sweeping gains in productivity and efficiency in technology, health care and manufacturing. Nvidia has been a primary beneficiary of that revolution. Between early 2023 and early 2025, NVDA stock grew more than 800%—driven primarily by AI demand. While the growth pace may moderate, Shen believes Nvidia's dominance and upside will continue. He cites the company's strategic collaborations and strong market positions in AI, machine learning, data centers and autonomous driving. Tesla makes and sells electric vehicles. The company also produces and sells solar energy generation and storage solutions for residential, commercial and industrial applications. Munoz sees growth potential for Tesla this year, despite the EV-maker's ongoing fight for market share in China. Positive factors for the company and shareholders include: Tesla should also benefit from "federal oversight of autonomous vehicle regulation," Munoz explains. On the other hand, the Trump administration may soon cancel the federal tax credit for EVs. This will be a short-term negative for Tesla. But Munoz believes it will hurt Tesla's competitors more and ultimately "solidify Tesla's leadership in EV sales." Berkshire Hathaway is a diversified conglomerate with subsidiaries in insurance, retail, packaged foods and transportation. The business portfolio includes Geico auto insurance, Duracell, Fruit of the Loom Companies, Pilot Travel Centers and International Dairy Queen. The company's chairman and CEO Warren Buffett is among the world's most successful and respected value investors. Materazzi likes the prospects for Berkshire Hathaway in early 2025 and beyond. He cites the conglomerate's portfolio of successful brands and businesses, which provides ETF-like diversification. "The company really remains undervalued despite concerns about its generational leadership transition," Materazzi explains. "Investors do not seem to give this stock the same attention as other high-profile names," he continues. Berkshire Hathaway's Vice Chairman Charlie Munger passed away in 2023. Buffett will celebrate his 95th birthday in 2025. Berkshire Hathaway is the 10th largest S&P 500 stock, measured by market capitalization. Intel designs, manufactures and sells CPUs, GPUs and related hardware for memory, storage, connectivity and networking. Intel holds a dominant market share in CPUs globally, though it faces competition from AMD in desktop computers and servers. Intel isn't an obvious choice. The chipmaker was barely profitable in 2023, lost 60% of its value in 2024 and was recently removed from the Dow Jones Industrial Average. Still, Stanzione believes Intel could have a big year in 2025—better than its current price point would suggest. Intel delivered an adjusted loss per share of $0.46 in the third quarter as the company worked to reduce costs and simplify its portfolio. CFO David Zinsner said the cost management initiatives would support improved profitability. This positivity was reflected in the fourth-quarter adjusted EPS guidance of $0.12. Intel will publish its fourth-quarter results on January 30. Reddit runs a website of interest-based communities with volunteer moderators. The company sells online advertising and subscription-based memberships. It also licenses its data for training generative AI applications. Reddit went public in 2024 at $47 per share. Less than a year later, RDDT trades for $184. Kacaba says the numbers "make Reddit a stock worth watching, but it's the strategy behind those gains that tells the bigger story." Reddit's allocation of IPO shares to loyal users has been pivotal to the company's success. "Reddit created alignment between its brand and its community, ensuring that those who helped build the platform could also share in its financial upside," Kacaba explains. Kacaba also highlights the business advantages of Reddit's deep community focus. The platform encourages intense user engagement and discussion. Both elements can be monetized, respectively, through advertising and AI-related licensing. Will big tech continue to dominate in 2025, or will value stocks and underdogs shine bright? Since no one knows the answer, you might consider preparing your portfolio to withstand either outcome. Diversification trims some growth potential but also limits your risk. In uncertain times, accepting that trade-off can be your best option. For more 2025 investing ideas, see best dividend stocks and best ETFs. Whether it’s mastering cutting-edge strategies, uncovering actionable investment opportunities from influential leaders, or breaking down complex topics, our in-depth journalism has you covered. Become a Forbes member and gain unlimited access to bold ideas shaking up industries, expert guides and practical investment advice that keeps you ahead of the market. Unlock Premium Access — Free For 25 Days. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Earnings Season Gets Real As The Magnificent 7 Begins Reporting
URL: https://www.forbes.com/sites/bill_stone/2025/01/26/earnings-season-gets-real-as-the-magnificent-7-begins-reporting/
Time Published: 2025-01-26T12:00:00Z
Full Content:
Microsoft (MSFT), Meta Platforms (META), and Tesla (TSLA) are scheduled to report on Wednesday after ... [+] the close, with Apple (AAPL) on Thursday night. The forward earnings guidance and any implications for artificial intelligence spending will be watched closely. (Photo by Drew Angerer/Getty Images) Within the S&P 500, 36 companies reported earnings last week, with a sizable number being financials. Eighty percent of S&P 500 firms reported better-than-expected earnings for the quarter. This week marks the second busiest of the fourth-quarter earnings season, with 104 S&P 500 companies scheduled to report. And most crucially, four of the Magnificent 7 report earnings this week. The Magnificent 7 consists of Microsoft (MSFT), Meta Platforms (META), Amazon.com (AMZN), Apple (AAPL), NVIDIA (NVDA), Alphabet (GOOGL), and Tesla (TSLA). S&P 500 Earnings Season Supportive earnings and stable bond yields sent stocks higher. The S&P 500 rose 1.8% for the week, and the Magnificent 7 gained 1.5%. Market Returns The financial sector again contributed most significantly to last week's earnings growth improvement. According to FactSet data, Travelers (TRV) and Discover Financial Services (DFS) were the most significant contributors to the increase in earnings for the financial sector, which rose to 49.4% from 47.5%. The financial sector is expected to show the most rapid year-over-year growth rate in the S&P 500, followed by technology and communications services. The energy sector is at the bottom, with a forecasted almost thirty-one percent decline in year-over-year earnings. 4Q 2024 Estimated Earnings By Sector Sales growth is closely tied to nominal gross domestic product growth, which combines after-inflation economic growth (real GDP) with inflation. At this point in the earnings season, sales growth at 4.6% has met expectations, with the tailwind from an estimated 5% year-over-year nominal GDP growth. Sales growth is expected to be the most robust in the technology sector, with the energy sector predicted to show the most significant decline in year-over-year revenue due to lower oil prices. 4Q 2024 Estimated Sales By Sector Due primarily to the robust earnings growth for banks and other financial companies, the blended earnings performance has outperformed expectations at the end of the quarter. Combining actual results with consensus estimates for companies yet to report, the blended earnings growth rate for the quarter is at +12.7% year-over-year, above the expectation of +11.9% at the end of the quarter. S&P 500 Earnings Estimate Summary Because these companies are a critical driver of earnings growth and a significant percentage of the S&P 500’s market capitalization, the Magnificent 7 are again the group to watch this earnings season and this week. According to FactSet, the Magnificent 7’s earnings are expected to grow by 21.7% year-over-year versus 9.7% for the rest of the S&P 500. Microsoft (MSFT), Meta Platforms (META), and Tesla (TSLA) are scheduled to report on Wednesday after the close, with Apple (AAPL) on Thursday night. The forward earnings guidance and any implications for artificial intelligence spending will be watched closely. This week, there are releases across the sectors, which will provide a better sense of the overall earnings environment. Beyond the Magnificent 7, other companies reporting are Starbucks (SBUX), Mastercard (MA), Visa (V), Chevron (CVX), and Exxon Mobil (XOM). Magnificent 7: Q4 Estimated Year-Over-Year Earnings Growth In addition to earnings, there are many economic releases and a Federal Reserve meeting. Notably, fourth-quarter U.S. GDP, a measure of economic growth, will be released on Thursday. The economy is likely to continue to show above-average growth and entered 2025 with good momentum. Consensus estimates for fourth-quarter GDP growth are 2.7%, with the Atlanta Fed projecting almost 3% growth. U.S. Fourth-Quarter U.S. GDP Growth Estimate The Fed meets on Wednesday before the GDP release but is armed with enough data to make the result of the gathering a foregone conclusion. The combination of resilient economic growth and above-target inflation should result in no change to short-term interest rates. While the conclusion is almost certain, Chair Jerome Powell’s comments will be watched closely for clues as to the timing of future rate cuts. Currently, expectations are for a total of two 0.25% reductions in 2025, with the first cut not likely until near midyear. Expected 2025 Fed Rate Cuts Financial companies provided an excellent start to earnings season, but this week will be crucial with more broad representation and the majority of the Magnificent 7 on the calendar. Because the Magnificent 7 companies are responsible for significant earnings growth, their forward guidance should significantly impact overall expectations. The Fed is almost sure to make no changes to short-term interest rates, but Chair Powell’s statement will impact expectations around the pace and timing of future cuts and, thus, markets. Disclosure: Glenview Trust holds many stocks mentioned in this article within its recommended investment strategies.
--------------------------------------------------

Title: 5 big analyst AI moves: Apple stock faces downgrades; New Street-high PT for Tesla
URL: https://www.investing.com/news/stock-market-news/5-big-analyst-ai-moves-apple-stock-faces-downgrades-new-streethigh-pt-for-tesla-3830081
Time Published: 2025-01-26T10:00:00Z
Description: 5 big analyst AI moves: Apple stock faces downgrades; New Street-high PT for Tesla
--------------------------------------------------

Title: Mark Zuckerberg's $65 Billion AI Bet Benefits Nvidia And Other Players, Says Top Analyst, But Warns Market Bull Run Will 'End In A Spectacular Bubble Burst'
URL: https://www.benzinga.com/startups/25/01/43217408/mark-zuckerbergs-65-billion-ai-bet-benefits-nvidia-and-other-players-says-top-analyst-but-warns-mark
Time Published: 2025-01-26T03:12:14Z
Full Content:
Deepwater Asset Management’s Gene Munster says Meta Platform Inc.’s META plans to invest up to $65 billion this year to expand its AI infrastructure will benefit Nvidia Corp. NVDA and other hardware players. What Happened: Meta CEO Mark Zuckerberg on Friday outlined the company’s capital spending plans for 2025 and its focus on artificial intelligence (AI). Don’t Miss: Zuckerberg expects Meta to allocate between $60 billion and $65 billion for capital spending, primarily aimed at expanding the company's AI teams and building a new data center. he described the new data center as being “so large that it would cover a significant part of Manhattan.” The tech mogul further noted that Meta aims to bring a gigawatt of computing power online by 2025 and is projected to finish the year with over 1.3 million graphics processing units. Weighing in on the announcement, Munster said Meta’s commitment “exceeds Street estimates of $51B.” "In 2025, I expect Meta AI will be the leading assistant serving more than 1 billion people, Llama 4 will become the leading state of the art model, and we'll build an AI engineer that will start contributing increasing amounts of code to our R&D efforts," Zuckerberg said. See Also: Nancy Pelosi Invested $5 Million In An AI Company Last Year — Here's How You Can Invest In Multiple Pre-IPO AI Startups With Just $1,000 According to Munster, the increased investment in AI infrastructure will particularly benefit Nvidia and other hardware players in the short term. In the long term, the higher capital expenditure (Capex) is expected to accelerate the AI flywheel, leading to more innovation, lower usage costs, an increase in customers, and consequently, more investment. He also issued a reminder, stating, “I believe the market is going higher and the run will end in a spectacular bubble burst.” The latest datapoint that the AI trade remains intact. Zuckerberg's comment that $META will spend $60–$65B in Capex for 2025 exceeds Street estimates of $51B.In the near term, increased investment in AI infrastructure benefits $NVDA and other hardware players.Long term,… Why It Matters: Zuckerberg’s announcement comes on the heels of Meta’s recent AI advancements. In December, Meta introduced the Llama 3.3 70B, a new AI model that outperforms competitors like Alphabet Inc.‘s GOOGL GOOG Google, OpenAI, and Amazon.com, Inc. AMZN. This model offers the performance of Meta's largest Llama model, Llama 3.1 405B, but at a reduced cost. Last year in April, Meta announced its plan to purchase 350,000 Nvidia H100 GPUs by 2024 to fuel its AI initiatives. Zuckerberg then said the 350,000 Nvidia H100 GPUs were initially intended to enhance Instagram Reels, not to position Meta as a leader in AI technology. Meanwhile, President Donald Trump criticized the European Union (EU) for focusing on U.S. tech giants like Meta, Apple, and Google. He accused EU regulators of unfairly targeting these companies, calling their actions “a form of taxation.” Price Action: Meta Platform’s stock closed at $647.49 on Friday, down 0.20% for the day, according to Benzinga Pro data. Read Next: Photo courtesy: Shutterstock © 2025 Benzinga.com. Benzinga does not provide investment advice. All rights reserved. Trade confidently with insights and alerts from analyst ratings, free reports and breaking news that affects the stocks you care about.
--------------------------------------------------

Title: The impact of competition and DeepSeek on Nvidia
URL: https://youtubetranscriptoptimizer.com/blog/05_the_short_case_for_nvda
Time Published: 2025-01-25T15:30:25Z
Full Content:
Jeffrey Emanuel January 25, 2025 60 min read As someone who spent ~10 years working as a generalist investment analyst at various long/short hedge funds (including stints at Millennium and Balyasny), while also being something of a math and computer nerd who has been studying deep learning since 2010 (back when Geoff Hinton was still talking about Restricted Boltzmann Machines and everything was still programmed using MATLAB, and researchers were still trying to show that they could get better results at classifying handwritten digits than by using Support Vector Machines), I'd like to think that I have a fairly unusual perspective on how AI technology is developing and how this relates to equity valuations in the stock market. For the past few years, I have been working more as a developer, and have several popular open-source projects for working with various forms of AI models/services (e.g., see LLM Aided OCR, Swiss Army Llama, Fast Vector Similarity, Source to Prompt, and Pastel Inference Layer for a few recent examples). Basically, I am using these frontier models all day, every day, in about as intense a way as possible. I have 3 Claude accounts so I don't run out of requests, and signed up for ChatGPT Pro within minutes of it being available. I also try to keep on top of the latest research advances, and carefully read all the major technical report papers that come out from the major AI labs. So I think I have a pretty good read on the space and how things are developing. At the same time, I've shorted a ton of stocks in my life and have won the best idea prize on the Value Investors Club twice (for TMS long and PDH short if you're keeping track at home). I say this not to brag, but rather to help establish my bona fides as someone who could opine on the subject without coming across as hopelessly naive to either technologists or professional investors. And while there are surely many people who know the math/science better, and people who are better at long/short investing in the stock market than me, I doubt there are very many who are in the middle of the Venn diagram to the extent I can claim to be. With all that said, whenever I meet with and chat with my friends and ex colleagues from the hedge fund world, the conversation quickly turns to Nvidia. It's not every day that a company goes from relative obscurity to being worth more than the combined stock markets of England, France, or Germany! And naturally, these friends want to know my thoughts on the subject. Because I am such a dyed-in-the-wool believer in the long term transformative impact of this technology— I truly believe it's going to radically change nearly every aspect of our economy and society in the next 5-10 years, with basically no historical precedent— it has been hard for me to make the argument that Nvidia's momentum is going to slow down or stop anytime soon. But even though I've thought the valuation was just too rich for my blood for the past year or so, a confluence of recent developments has caused me to flip a bit to my usual instinct, which is to be a bit more contrarian in outlook and to question the consensus when it seems to be more than priced in. The saying "what the wise man believes in the beginning, the fool believes in the end" became famous for a good reason. Before we get into the developments that give me pause, let's pause to briefly review the bull case for NVDA shares, which is basically now known by everyone and his brother. Deep learning and AI are the most transformative technologies since the internet, and poised to change basically everything in our society. Nvidia has somehow ended up with something close to a monopoly in terms of the share of aggregate industry capex that is spent on training and inference infrastructure. Some of the largest and most profitable companies in the world, like Microsoft, Apple, Amazon, Meta, Google, Oracle, etc., have all decided that they must do and spend whatever it takes to stay competitive in this space because they simply cannot afford to be left behind. The amount of capex dollars, gigawatts of electricity used, square footage of new-build data centers, and, of course, the number of GPUs, has absolutely exploded and seems to show no sign of slowing down. And Nvidia is able to earn insanely high 90%+ gross margins on the most high-end, datacenter oriented products. We've just scratched the surface here of the bull case. There are many additional aspects to it now, which have made even people who were already very bullish to become incrementally more bullish. Besides things like the rise of humanoid robots, which I suspect is going to take most people by surprise when they are rapidly able to perform a huge number of tasks that currently require an unskilled (or even skilled) human worker (e.g., doing laundry, cleaning, organizing, and cooking; doing construction work like renovating a bathroom or building a house in a team of workers; running a warehouse and driving forklifts, etc.), there are other factors which most people haven't even considered. One major thing that you hear the smart crowd talking about is the rise of "a new scaling law," which has created a new paradigm thinking about how compute needs will increase over time. The original scaling law, which is what has been driving progress in AI since AlexNet appeared in 2012 and the Transformer architecture was invented in 2017, is the pre-training scaling law: that the more billions (and now trillions) worth of tokens we can use as training data, and the larger the parameter count of the models we are training, and the more FLOPS of compute that we expend on training those models on those tokens, the better the performance of the resulting models on a large variety of highly useful downstream tasks. Not only that, but this improvement is somewhat knowable, to the point where the leading AI labs like OpenAI and Anthropic have a pretty good idea of just how good their latest models would be even before they started the actual training runs— in some cases, predicting the benchmarks of the final models to within a couple percentage points. This "original scaling law" has been vitally important, but always caused some doubts in the minds of people projecting the future with it. For one thing, we seem to have already exhausted the world's accumulated set of high quality training data. Of course, that's not literally true— there are still so many old books and periodicals that haven't yet been properly digitized, and even if they have, are not properly licensed for use as training data. The problem is that, even if you give credit for all that stuff— say the sum total of "professionally" produced English language written content from the year 1500 to, say, the year 2000, it's not such a tremendous amount in percentage terms when you're talking about a training corpus of nearly 15 trillion tokens, which is the scale of current frontier models. For a quick reality check of those numbers: Google Books has digitized around 40mm books so far; if a typical book has 50k to 100k words, or 65k to 130k tokens, then that's between 2.6T and 5.2T tokens just from books, though surely a large chunk of that is already included in the training corpora used by the big labs, whether it's strictly legal or not. And there are lots of academic papers, with the arXiv website alone having over 2mm papers. And the Library of Congress has over 3 billion digitized newspaper pages. Taken together, that could be as much as 7T tokens in total, but since much of this is in fact included in training corpora, the remaining "incremental" training data probably isn't all that significant in the grand scheme of things. Of course, there are other ways to gather more training data. You could automatically transcribe every single YouTube video for example, and use that text. And while that might be helpful on the margin, it's certainly of much lower quality than, say, a highly respected textbook on Organic Chemistry as a source of useful knowledge about the world. So we've always had a looming "data wall" when it comes to the original scaling law; although we know we can keep shoveling more and more capex into GPUs and building more and more data centers, it's a lot harder to mass produce useful new human knowledge which is correct and incremental to what is already out there. Now, one intriguing response to this has been the rise of "synthetic data," which is text that is itself the output of an LLM. And while this seems almost nonsensical that it would work to "get high on your own supply" as a way of improving model quality, it actually seems to work very well in practice, at least in the domain of math, logic, and computer programming. The reason, of course, is that these are areas where we can mechanically check and prove the correctness of things. So we can sample from the vast universe of possible math theorems or possible Python scripts, and then actually check if they are correct, and only include them in our corpus if they are. And in this way, we can very dramatically expand our collection of high quality training data, at least in these kinds of areas. And then there are all the other kinds of data we could be training AI on besides text. For example, what if we take the entire whole genome sequencing (around 200 GB to 300 GB uncompressed for a single human being) for 100 million people? That's a lot of data obviously, although the vast majority of it would be nearly identical between any two people. Of course, this could be misleading to compare to textual data from books and the internet for various reasons: But it's still another large source of diverse information that we could train huge models on in the future, which is why I included it. So while there is some hope in terms of being able to capture more and more additional training data, if you look at the rate at which training corpora have grown in recent years, it quickly becomes obvious that we are close to hitting a wall in terms of data availability for "generally useful" knowledge that can get us closer to the ultimate goal of getting artificial super-intelligence which is 10x smarter than John von Neumann and is an absolute world-class expert on every specialty known to man. Besides the limited amount of available data, there have always been a couple other things that have lurked in the back of the mind of proponents of the pre-training scaling law. A big one of these is, after you've finished training the model, what are you supposed to do with all that compute infrastructure? Train the next model? Sure, you can do that, but given the rapid improvement in GPU speed and capacity, and the importance of electricity and other opex in the economic calculations, does it even really make sense to use your 2 year old cluster to train your new model? Surely you'd rather use the brand new data center you just built that costs 10x the old data center and is 20x more powerful because of better technology. The problem is, at some point you do need to amortize the up-front cost of these investments and recoup it with a stream of (hopefully positive) operating profit, right? The market is so excited about AI that it has thankfully ignored this, allowing companies like OpenAI to post breathtaking from-inception, cumulative operating losses while garnering increasingly eye-popping valuations in follow-up investment rounds (although, to their credit, they have also been able to demonstrate very fast growing revenues). But eventually, for this situation to be sustainable over a full market cycle, these data center costs do need to eventually be recouped, hopefully with a profit, which over time is competitive with other investment opportunities on a risk-adjusted basis. OK, so that was the pre-training scaling law. What's this "new" scaling law? Well, that's something that people really just started focusing on in the past year: inference time compute scaling. Before, the vast majority of all the compute you'd expend in the process was the up-front training compute to create the model in the first place. Once you had the trained model, performing inference on that model— i.e., asking a question or having the LLM perform some kind of task for you— used a certain, limited amount of compute. Critically, the total amount of inference compute (measured in various ways, such as FLOPS, in GPU memory footprint, etc.) was much, much less than what was required for the pre-training phase. Of course, the amount of inference compute does flex up when you increase the context window size of the models and the amount of output that you generate from them in one go (although researchers have made breathtaking algorithmic improvements on this front relative to the initial quadratic scaling people originally expected in scaling this up). But essentially, until recently, inference compute was generally a lot less intensive than training compute, and scaled basically linearly with the number of requests you are handling— the more demand for text completions from ChatGPT, for instance, the more inference compute you used up. With the advent of the revolutionary Chain-of-Thought ("COT") models introduced in the past year, most noticeably in OpenAI's flagship O1 model (but very recently in DeepSeek's new R1 model, which we will talk about later in much more detail), all that changed. Instead of the amount of inference compute being directly proportional to the length of the output text generated by the model (scaling up for larger context windows, model size, etc.), these new COT models also generate intermediate "logic tokens"; think of this as a sort of scratchpad or "internal monologue" of the model while it's trying to solve your problem or complete its assigned task. This represents a true sea change in how inference compute works: now, the more tokens you use for this internal chain of thought process, the better the quality of the final output you can provide the user. In effect, it's like giving a human worker more time and resources to accomplish a task, so they can double and triple check their work, do the same basic task in multiple different ways and verify that they come out the same way; take the result they came up with and "plug it in" to the formula to check that it actually does solve the equation, etc. It turns out that this approach works almost amazingly well; it is essentially leveraging the long anticipated power of what is called "reinforcement learning" with the power of the Transformer architecture. It directly addresses the single biggest weakness of the otherwise phenomenally successful Transformer model, which is its propensity to "hallucinate". Basically, the way Transformers work in terms of predicting the next token at each step is that, if they start out on a bad "path" in their initial response, they become almost like a prevaricating child who tries to spin a yarn about why they are actually correct, even if they should have realized mid-stream using common sense that what they are saying couldn't possibly be correct. Because the models are always seeking to be internally consistent and to have each successive generated token flow naturally from the preceding tokens and context, it's very hard for them to course-correct and backtrack. By breaking the inference process into what is effectively many intermediate stages, they can try lots of different things and see what's working and keep trying to course-correct and try other approaches until they can reach a fairly high threshold of confidence that they aren't talking nonsense. Perhaps the most extraordinary thing about this approach, beyond the fact that it works at all, is that the more logic/COT tokens you use, the better it works. Suddenly, you now have an additional dial you can turn so that, as you increase the amount of COT reasoning tokens (which uses a lot more inference compute, both in terms of FLOPS and memory), the higher the probability is that you will give a correct response— code that runs the first time without errors, or a solution to a logic problem without an obviously wrong deductive step. I can tell you from a lot of firsthand experience that, as good as Anthropic's Claude3.5 Sonnet model is at Python programming— and it is indeed VERY good— whenever you need to generate anything long and complicated, it invariably ends up making one or more stupid mistakes. Now, these mistakes are usually pretty easy to fix, and in fact you can normally fix them by simply feeding the errors generated by the Python interpreter, without any further explanation, as a follow-up inference prompt (or, more usefully, paste in the complete set of detected "problems" found in the code by your code editor, using what something called a Linter), it was still an annoying additional step. And when the code becomes very long or very complicated, it can sometimes take a lot longer to fix, and might even require some manual debugging by hand. The first time I tried the O1 model from OpenAI was like a revelation: I was amazed how often the code would be perfect the very first time. And that's because the COT process automatically finds and fixes problems before they ever make it to a final response token in the answer the model gives you. In fact, the O1 model used in OpenAI's ChatGPT Plus subscription for $20/month is basically the same model as the one used in the O1-Pro model featured in their new ChatGPT Pro subscription for 10x the price ($200/month, which raised plenty of eyebrows in the developer community); the main difference is that O1-Pro thinks for a lot longer before responding, generating vastly more COT logic tokens, and consuming a far larger amount of inference compute for every response. This is quite striking in that, even a very long and complex prompt for Claude3.5 Sonnet or GPT4o, with ~400kb+ of context given, generally takes less than 10 seconds to begin responding, and often less than 5 seconds. Whereas that same prompt to O1-Pro could easily take 5+ MINUTES before you get a response (although OpenAI does show you some of the "reasoning steps" that are generated during the process while you wait; critically, OpenAI has decided, presumably for trade secret related reasons,to hide from you the exact reasoning tokens it generates, showing you instead a highly abbreviated summary of these). As you can probably imagine, there are tons of contexts where accuracy is paramount— where you'd rather give up and tell the user you can't do it at all rather than give an answer that could be trivially proven wrong or which involves hallucinated facts or otherwise specious reasoning. Anything involving money/transactions, medical stuff, legal stuff, just to name a few. Basically, wherever the cost of inference is trivial relative to the hourly all-in compensation of the human knowledge worker who is interacting with the AI system, that's a case where it become a complete no-brainer to dial up the COT compute (the major drawback is that it increases the latency of responses by a lot, so there are still some contexts where you might prefer to iterate faster by getting lower latency responses that are less accurate or correct). Some of the most exciting news in the AI world came out just a few weeks ago and concerned OpenAI's new unreleased O3 model, which was able to solve a large variety of tasks that were previously deemed to be out of reach of current AI approaches in the near term. And the way it was able to do these hardest problems (which include exceptionally tough "foundational" math problems that would be very hard for even highly skilled professional mathematicians to solve), is that OpenAI threw insane amount of compute resources at the problems— in some cases, spending $3k+ worth of compute power to solve a single task (compare this to traditional inference costs for a single task, which would be unlikely to exceed a couple dollars using regular Transformer models without chain-of-thought). It doesn't take an AI genius to realize that this development creates a new scaling law that is totally independent of the original pre-training scaling law. Now, you still want to train the best model you can by cleverly leveraging as much compute as you can and as many trillion tokens of high quality training data as possible, but that's just the beginning of the story in this new world; now, you could easily use incredibly huge amounts of compute just to do inference from these models at a very high level of confidence or when trying to solve extremely tough problems that require "genius level" reasoning to avoid all the potential pitfalls that would lead a regular LLM astray. Even if you believe, as I do, that the future prospects for AI are almost unimaginably bright, the question still remains, "Why should one company extract the majority of the profit pool from this technology?" There are certainly many historical cases where a very important new technology changed the world, but the main winners were not the companies that seemed the most promising during the initial stages of the process. The Wright Brothers' airplane company in all its current incarnations across many different firms today isn't worth more than $10b despite them inventing and perfecting the technology well ahead of everyone else. And while Ford has a respectable market cap of $40b today, it's just 1.1% of Nvidia's current market cap. To understand this, it's important to really understand why Nvidia is currently capturing so much of the pie today. After all, they aren't the only company that even makes GPUs. AMD makes respectable GPUs that, on paper, have comparable numbers of transistors, which are made using similar process nodes, etc. Sure, they aren't as fast or as advanced as Nvidia's GPUs, but it's not like the Nvidia GPUs are 10x faster or anything like that. In fact, in terms of naive/raw dollars per FLOP, AMD GPUs are something like half the price of Nvidia GPUs. Looking at other semiconductor markets such as the DRAM market, despite the fact that it is also very highly consolidated with only 3 meaningful global players (Samsung, Micron, SK-Hynix), gross margins in the DRAM market range from negative at the bottom of the cycle to ~60% at the very top of the cycle, with an average in the 20% range. Compare that to Nvidia's overall gross margin in recent quarters of ~75%, which is dragged down by the lower-margin and more commoditized consumer 3D graphics category. So how is this possible? Well, the main reasons have to do with software— better drivers that "just work" on Linux and which are highly battle-tested and reliable (unlike AMD, which is notorious for the low quality and instability of their Linux drivers), and highly optimized open-source code in popular libraries such as PyTorch that has been tuned to work really well on Nvidia GPUs. It goes beyond that though— the very programming framework that coders use to write low-level code that is optimized for GPUs, CUDA, is totally proprietary to Nvidia, and it has become a de facto standard. If you want to hire a bunch of extremely talented programmers who know how to make things go really fast on GPUs, and pay them $650k/year or whatever the going rate is for people with that particular expertise, chances are that they are going to "think" and work in CUDA. Besides software superiority, the other major thing that Nvidia has going for it is what is known as interconnect— essentially, the bandwidth that connects together thousands of GPUs together efficiently so they can be jointly harnessed to train today's leading-edge foundational models. In short, the key to efficient training is to keep all the GPUs as fully utilized as possible all the time— not waiting around idling until they receive the next chunk of data they need to compute the next step of the training process. The bandwidth requirements are extremely high— much, much higher than the typical bandwidth that is needed in traditional data center use cases. You can't really use traditional networking gear or fiber optics for this kind of interconnect, since it would introduce too much latency and wouldn't give you the pure terabytes per second of bandwidth that is needed to keep all the GPUs constantly busy. Nvidia made an incredibly smart decision to purchase the Israeli company Mellanox back in 2019 for a mere $6.9b, and this acquisition is what provided them with their industry leading interconnect technology. Note that interconnect speed is a lot more relevant to the training process, where you have to harness together the output of thousands of GPUs at the same time, than the inference process (including COT inference), which can use just a handful of GPUs— all you need is enough VRAM to store the quantized (compressed) model weights of the already-trained model. So those are arguably the major components of Nvidia's "moat" and how it has been able to maintain such high margins for so long (there is also a "flywheel" aspect to things, where they aggressively invest their super-normal profits into tons of R&D, which in turn helps them improve their tech at a faster rate than the competition, so they are always in the lead in terms of raw performance). But as was pointed out earlier, what customers really tend to care about, all other things being equal, is performance per dollar (both in up-front capex cost of equipment and in energy usage, so performance per watt), and even though Nvidia's GPUs are certainly the fastest, they are not the best price/performance when measured naively in terms of FLOPS. But the thing is, all other things are NOT equal, and the fact that AMD's drivers suck, that popular AI software libraries don't run as well on AMD GPUs, that you can't find really good GPU experts who specialize in AMD GPUs outside of the gaming world (why would they bother when there is more demand in the market for CUDA experts?), that you can't wire thousands of them together as effectively because of lousy interconnect technology for AMD— all this means that AMD is basically not competitive in the high-end data center world, and doesn't seem to have very good prospects for getting there in the near term. Well, that all sounds very bullish for Nvidia, right? Now you can see why the stock is trading at such a huge valuation! But what are the other clouds on the horizon? Well, there are few that I think merit significant attention. Some have been lurking in the background for the last few years, but too small to make a dent considering how quickly the pie has been growing, but where they are getting ready to potentially inflect upwards. Others are very recent developments (as in, the last 2 weeks) that might dramatically change the near-term trajectory of incremental GPU demand. At a very high level, you can think of things like this: Nvidia operated in a pretty niche area for a very long time; they had very limited competition, and the competition wasn't particular profitable or growing fast enough to ever pose a real threat, since they didn't have the capital needed to really apply pressure to a market leader like Nvidia. The gaming market was large and growing, but didn't feature earth shattering margins or particularly fabulous year over year growth rates. A few big tech companies started ramping up hiring and spending on machine learning and AI efforts around 2016-2017, but it was never a truly significant line item for any of them on an aggregate basis— more of a "moonshot" R&D expenditure. But once the big AI race started in earnest with the release of ChatGPT in 2022— only a bit over 2 years ago, although it seems like a lifetime ago in terms of developments— that situation changed very dramatically. Suddenly, big companies were ready to spend many, many billions of dollars incredibly quickly. The number of researchers showing up at the big research conferences like Neurips and ICML went up very, very dramatically. All the smart students who might have previously studied financial derivatives were instead studying Transformers, and $1mm+ compensation packages for non-executive engineering roles (i.e., for independent contributors not managing a team) became the norm at the leading AI labs. It takes a while to change the direction of a massive cruise ship; and even if you move really quickly and spend billions, it takes a year or more to build greenfield data centers and order all the equipment (with ballooning lead times) and get it all set up and working. It takes a long time to hire and onboard even smart coders before they can really hit their stride and familiarize themselves with the existing codebases and infrastructure. But now, you can imagine that absolutely biblical amounts of capital, brainpower, and effort are being expended in this area. And Nvidia has the biggest target of any player on their back, because they are the ones who are making the lion's share of the profits TODAY, not in some hypothetical future where the AI runs our whole lives. So the very high level takeaway is basically that "markets find a way"; they find alternative, radically innovative new approaches to building hardware that leverage completely new ideas to sidestep barriers that help prop up Nvidia's moat. For example, so-called "wafer scale" AI training chips from Cerebras, which dedicate an entire 300mm silicon wafer to an absolutely gargantuan chip that contains orders of magnitude more transistors and cores on a single chip (see this recent blog post from them explaining how they were able to solve the "yield problem" that had been preventing this approach from being economically practical in the past). To put this into perspective, if you compare Cerebras' newest WSE-3 chip to Nvidia's flagship data-center GPU, the H100, the Cerebras chip has a total die area of 46,225 square millimeters compared to just 814 for the H100 (and the H100 is itself considered an enormous chip by industry standards); that's a multiple of ~57x! And instead of having 132 "streaming multiprocessor" cores enabled on the chip like the H100 has, the Cerebras chip has ~900,000 cores (granted, each of these cores is smaller and does a lot less, but it's still an almost unfathomably large number in comparison). In more concrete apples-to-apples terms, the Cerebras chip can do around ~32x the FLOPS in AI contexts as a single H100 chip. Since an H100 sells for close to $40k a pop, you can imagine that the WSE-3 chip isn't cheap. So why does this all matter? Well, instead of trying to battle Nvidia head-on by using a similar approach and trying to match the Mellanox interconnect technology, Cerebras has used a radically innovative approach to do an end-run around the interconnect problem: inter-processor bandwidth becomes much less of an issue when everything is running on the same super-sized chip. You don't even need to have the same level of interconnect because one mega chip replaces tons of H100s. And the Cerebras chips also work extremely well for AI inference tasks. In fact, you can try it today for free here and use Meta's very respectable Llama-3.3-70B model. It responds basically instantaneously, at ~1,500 tokens per second. To put that into perspective, anything above 30 tokens per second feels relatively snappy to users based on comparisons to ChatGPT and Claude, and even 10 tokens per second is fast enough that you can basically read the response while it's being generated. Cerebras is also not alone; there are other companies, like Groq (not to be confused with the Grok model family trained by Elon Musk's X AI). Groq has taken yet another innovative approach to solving the same fundamental problem. Instead of trying to compete with Nvidia's CUDA software stack directly, they've developed what they call a "tensor processing unit" (TPU) that is specifically designed for the exact mathematical operations that deep learning models need to perform. Their chips are designed around a concept called "deterministic compute," which means that, unlike traditional GPUs where the exact timing of operations can vary, their chips execute operations in a completely predictable way every single time. This might sound like a minor technical detail, but it actually makes a massive difference for both chip design and software development. Because the timing is completely deterministic, Groq can optimize their chips in ways that would be impossible with traditional GPU architectures. As a result, they've been demonstrating for the past 6+ months inference speeds of over 500 tokens per second with the Llama series of models and other open source models, far exceeding what's possible with traditional GPU setups. Like Cerebras, this is available today and you can try it for free here. Using a comparable Llama3 model with "speculative decoding," Groq is able to generate 1,320 tokens per second, on par with Cerebras and far in excess of what is possible using regular GPUs. Now, you might ask what the point is of achieving 1,000+ tokens per second when users seem pretty satisfied with ChatGPT, which is operating at less than 10% of that speed. And the thing is, it does matter. It makes it a lot faster to iterate and not lose focus as a human knowledge worker when you get instant feedback. And if you're using the model programmatically via the API, which is increasingly where much of the demand is coming from, then it can enable whole new classes of applications that require multi-stage inference (where the output of previous stages is used as input in successive stages of prompting/inference) or which require low-latency responses, such as content moderation, fraud detection, dynamic pricing, etc. But even more fundamentally, the faster you can serve requests, the faster you can cycle things, and the busier you can keep the hardware. Although Groq's hardware is extremely expensive, clocking in at $2mm to $3mm for a single server, it ends up costing far less per request fulfilled if you have enough demand to keep the hardware busy all the time. And like Nvidia with CUDA, a huge part of Groq's advantage comes from their own proprietary software stack. They are able to take the same open source models that other companies like Meta, DeepSeek, and Mistral develop and release for free, and decompose them in special ways that allow them to run dramatically faster on their specific hardware. Like Cerebras, they have taken different technical decisions to optimize certain particular aspects of the process, which allows them to do things in a fundamentally different way. In Groq's case, it's because they are entirely focused on inference level compute, not on training: all their special sauce hardware and software only give these huge speed and efficiency advantages when doing inference on an already trained model. But if the next big scaling law that people are excited about is for inference level compute— and if the biggest drawback of COT models is the high latency introduced by having to generate all those intermediate logic tokens before they can respond— then even a company that only does inference compute, but which does it dramatically faster and more efficiently than Nvidia can— can introduce a serious competitive threat in the coming years. At the very least, Cerebras and Groq can chip away at the lofty expectations for Nvidia's revenue growth over the next 2-3 years that are embedded in the current equity valuation. Besides these particularly innovative, if relatively unknown, startup competitors, there is some serious competition coming from some of Nvidia's biggest customers themselves who have been making custom silicon that specifically targets AI training and inference workloads. Perhaps the best known of these is Google, which has been developing its own proprietary TPUs since 2016. Interestingly, although it briefly sold TPUs to external customers, Google has been using all its TPUs internally for the past several years, and it is already on its 6th generation of TPU hardware. Amazon has also been developing its own custom chips called Trainium2 and Inferentia2. And while Amazon is building out data centers featuring billions of dollars of Nvidia GPUs, they are also at the same time investing many billions in other data centers that use these internal chips. They have one cluster that they are bringing online for Anthropic that features over 400k chips. Amazon gets a lot of flak for totally bungling their internal AI model development, squandering massive amounts of internal compute resources on models that ultimately are not competitive, but the custom silicon is another matter. Again, they don't necessarily need their chips to be better and faster than Nvidia's. What they need is for their chips to be good enough, but build them at a breakeven gross margin instead of the ~90%+ gross margin that Nvidia earns on its H100 business. OpenAI has also announced their plans to build custom chips, and they (together with Microsoft) are obviously the single largest user of Nvidia's data center hardware. As if that weren't enough, Microsoft have themselves announced their own custom chips! And Apple, the most valuable technology company in the world, has been blowing away expectations for years now with their highly innovative and disruptive custom silicon operation, which now completely trounces the CPUs from both Intel and AMD in terms of performance per watt, which is the most important factor in mobile (phone/tablet/laptop) applications. And they have been making their own internally designed GPUs and "Neural Processors" for years, even though they have yet to really demonstrate the utility of such chips outside of their own custom applications, like the advanced software based image processing used in the iPhone's camera. While Apple's focus seems somewhat orthogonal to these other players in terms of its mobile-first, consumer oriented, "edge compute" focus, if it ends up spending enough money on its new contract with OpenAI to provide AI services to iPhone users, you have to imagine that they have teams looking into making their own custom silicon for inference/training (although given their secrecy, you might never even know about it directly!). Now, it's no secret that there is a strong power law distribution of Nvidia's hyper-scaler customer base, with the top handful of customers representing the lion's share of high-margin revenue. How should one think about the future of this business when literally every single one of these VIP customers is building their own custom chips specifically for AI training and inference? When thinking about all this, you should keep one incredibly important thing in mind: Nvidia is largely an IP based company. They don't make their own chips. The true special sauce for making these incredible devices arguably comes more from TSMC, the actual fab, and ASML, which makes the special EUV lithography machines used by TSMC to make these leading-edge process node chips. And that's critically important, because TSMC will sell their most advanced chips to anyone who comes to them with enough up-front investment and is willing to guarantee a certain amount of volume. They don't care if it's for Bitcoin mining ASICs, GPUs, TPUs, mobile phone SoCs, etc. As much as senior chip designers at Nvidia earn per year, surely some of the best of them could be lured away by these other tech behemoths for enough cash and stock. And once they have a team and resources, they can design innovative chips (again, perhaps not even 50% as advanced as an H100, but with that Nvidia gross margin, there is plenty of room to work with) in 2 to 3 years, and thanks for TSMC, they can turn those into actual silicon using the exact same process node technology as Nvidia. As if these looming hardware threats weren't bad enough, there are a few developments in the software world in the last couple years that, while they started out slowly, are now picking up real steam and could pose a serious threat to the software dominance of Nvidia's CUDA. The first of these is the horrible Linux drivers for AMD GPUs. Remember we talked about how AMD has inexplicably allowed these drivers to suck for years despite leaving massive amounts of money on the table? Well, amusingly enough, the infamous hacker George Hotz (famous for jailbreaking the original iphone as a teenager, and currently the CEO of self-driving startup Comma.ai and AI computer company Tiny Corp, which also makes the open-source tinygrad AI software framework), recently announced that he was sick and tired of dealing with AMD's bad drivers, and desperately wanted to be able to to leverage the lower cost AMD GPUs in their TinyBox AI computers (which come in multiple flavors, some of which use Nvidia GPUs, and some of which use AMD GPUS). Well, he is making his own custom drivers and software stack for AMD GPUs without any help from AMD themselves; on Jan. 15th of 2025, he tweeted via his company's X account that "We are one piece away from a completely sovereign stack on AMD, the RDNA3 assembler. We have our own driver, runtime, libraries, and emulator. (all in ~12,000 lines!)" Given his track record and skills, it is likely that they will have this all working in the next couple months, and this would allow for a lot of exciting possibilities of using AMD GPUs for all sorts of applications where companies currently feel compelled to pay up for Nvidia GPUs. OK, well that's just a driver for AMD, and it's not even done yet. What else is there? Well, there are a few other areas on the software side that are a lot more impactful. For one, there is now a massive concerted effort across many large tech companies and the open source software community at large to make more generic AI software frameworks that have CUDA as just one of many "compilation targets". That is, you write your software using higher-level abstractions, and the system itself can automatically turn those high-level constructs into super well-tuned low-level code that works extremely well on CUDA. But because it's done at this higher level of abstraction, it can just as easily get compiled into low-level code that works extremely well on lots of other GPUs and TPUs from a variety of providers, such as the massive number of custom chips in the pipeline from every big tech company. The most famous examples of these frameworks are MLX (sponsored primarily by Apple), Triton (sponsored primarily by OpenAI), and JAX (developed by Google). MLX is particularly interesting because it provides a PyTorch-like API that can run efficiently on Apple Silicon, showing how these abstraction layers can enable AI workloads to run on completely different architectures. Triton, meanwhile, has become increasingly popular as it allows developers to write high-performance code that can be compiled to run on various hardware targets without having to understand the low-level details of each platform. These frameworks allow developers to write their code once using high powered abstractions and then target tons of platforms automatically— doesn't that sound like a better way to do things, which would give you a lot more flexibility in terms of how you actually run the code? In the 1980s, all the most popular, best selling software was written in hand-tuned assembly language. The PKZIP compression utility for example was hand crafted to maximize speed, to the point where a competently coded version written in the standard C programming language and compiled using the best available optimizing compilers at the time, would run at probably half the speed of the hand-tuned assembly code. The same is true for other popular software packages like WordStar, VisiCalc, and so on. Over time, compilers kept getting better and better, and every time the CPU architectures changed (say, from Intel releasing the 486, then the Pentium, and so on), that hand-rolled assembler would often have to be thrown out and rewritten, something that only the smartest coders were capable of (sort of like how CUDA experts are on a different level in the job market versus a "regular" software developer). Eventually, things converged so that the speed benefits of hand-rolled assembly were outweighed dramatically by the flexibility of being able to write code in a high-level language like C or C++, where you rely on the compiler to make things run really optimally on the given CPU. Nowadays, very little new code is written in assembly. I believe a similar transformation will end up happening for AI training and inference code, for similar reasons: computers are good at optimization, and flexibility and speed of development is increasingly the more important factor— especially if it also allows you to save dramatically on your hardware bill because you don't need to keep paying the "CUDA tax" that gives Nvidia 90%+ margins. Yet another area where you might see things change dramatically is that CUDA might very well end up being more of a high level abstraction itself— a "specification language" similar to Verilog (used as the industry standard to describe chip layouts) that skilled developers can use to describe high-level algorithms that involve massive parallelism (since they are already familiar with it, it's very well constructed, it's the lingua franca, etc.), but then instead of having that code compiled for use on Nvidia GPUs like you would normally do, it can instead be fed as source code into an LLM which can port it into whatever low-level code is understood by the new Cerebras chip, or the new Amazon Trainium2, or the new Google TPUv6, etc. This isn't as far off as you might think; it's probably already well within reach using OpenAI's latest O3 model, and surely will be possible generally within a year or two. Perhaps the most shocking development which was alluded to earlier happened in the last couple of weeks. And that is the news that has totally rocked the AI world, and which has been dominating the discourse among knowledgeable people on Twitter despite its complete absence from any of the mainstream media outlets: that a small Chinese startup called DeepSeek released two new models that have basically world-competitive performance levels on par with the best models from OpenAI and Anthropic (blowing past the Meta Llama3 models and other smaller open source model players such as Mistral). These models are called DeepSeek-V3 (basically their answer to GPT-4o and Claude3.5 Sonnet) and DeepSeek-R1 (basically their answer to OpenAI's O1 model). Why is this all so shocking? Well, first of all, DeepSeek is a tiny Chinese company that reportedly has under 200 employees. The story goes that they started out as a quant trading hedge fund similar to TwoSigma or RenTec, but after Xi Jinping cracked down on that space, they used their math and engineering chops to pivot into AI research. Who knows if any of that is really true or if they are merely some kind of front for the CCP or the Chinese military. But the fact remains that they have released two incredibly detailed technical reports, for DeepSeek-V3 and DeepSeekR1. These are heavy technical reports, and if you don't know a lot of linear algebra, you probably won't understand much. But what you should really try is to download the free DeepSeek app on the AppStore here and install it using a Google account to log in and give it a try (you can also install it on Android here), or simply try it out on your desktop computer in the browser here. Make sure to select the "DeepThink" option to enable chain-of-thought (the R1 model) and ask it to explain parts of the technical reports in simple terms. This will simultaneously show you a few important things: One, this model is absolutely legit. There is a lot of BS that goes on with AI benchmarks, which are routinely gamed so that models appear to perform great on the benchmarks but then suck in real world tests. Google is certainly the worst offender in this regard, constantly crowing about how amazing their LLMs are, when they are so awful in any real world test that they can't even reliably accomplish the simplest possible tasks, let alone challenging coding tasks. These DeepSeek models are not like that— the responses are coherent, compelling, and absolutely on the same level as those from OpenAI and Anthropic. Two, that DeepSeek has made profound advancements not just in model quality, but more importantly in model training and inference efficiency. By being extremely close to the hardware and by layering together a handful of distinct, very clever optimizations, DeepSeek was able to train these incredible models using GPUs in a dramatically more efficient way. By some measurements, over ~45x more efficiently than other leading-edge models. DeepSeek claims that the complete cost to train DeepSeek-V3 was just over $5mm. That is absolutely nothing by the standards of OpenAI, Anthropic, etc., which were well into the $100mm+ level for training costs for a single model as early as 2024. How in the world could this be possible? How could this little Chinese company completely upstage all the smartest minds at our leading AI labs, which have 100 times more resources, headcount, payroll, capital, GPUs, etc? Wasn't China supposed to be crippled by Biden's restriction on GPU exports? Well, the details are fairly technical, but we can at least describe them at a high level. It might have just turned out that the relative GPU processing poverty of DeepSeek was the critical ingredient to make them more creative and clever, necessity being the mother of invention and all. A major innovation is their sophisticated mixed-precision training framework that lets them use 8-bit floating point numbers (FP8) throughout the entire training process. Most Western AI labs train using "full precision" 32-bit numbers (this basically specifies the number of gradations possible in describing the output of an artificial neuron; 8 bits in FP8 lets you store a much wider range of numbers than you might expect— it's not just limited to 256 different equal-sized magnitudes like you'd get with regular integers, but instead uses clever math tricks to store both very small and very large numbers— though naturally with less precision than you'd get with 32 bits.) The main tradeoff is that while FP32 can store numbers with incredible precision across an enormous range, FP8 sacrifices some of that precision to save memory and boost performance, while still maintaining enough accuracy for many AI workloads. DeepSeek cracked this problem by developing a clever system that breaks numbers into small tiles for activations and blocks for weights, and strategically uses high-precision calculations at key points in the network. Unlike other labs that train in high precision and then compress later (losing some quality in the process), DeepSeek's native FP8 approach means they get the massive memory savings without compromising performance. When you're training across thousands of GPUs, this dramatic reduction in memory requirements per GPU translates into needing far fewer GPUs overall. Another major breakthrough is their multi-token prediction system. Most Transformer based LLM models do inference by predicting the next token— one token at a time. DeepSeek figured out how to predict multiple tokens while maintaining the quality you'd get from single-token prediction. Their approach achieves about 85-90% accuracy on these additional token predictions, which effectively doubles inference speed without sacrificing much quality. The clever part is they maintain the complete causal chain of predictions, so the model isn't just guessing— it's making structured, contextual predictions. One of their most innovative developments is what they call Multi-head Latent Attention (MLA). This is a breakthrough in how they handle what are called the Key-Value indices, which are basically how individual tokens are represented in the attention mechanism within the Transformer architecture. Although this is getting a bit too advanced in technical terms, suffice it to say that these KV indices are some of the major uses of VRAM during the training and inference process, and part of the reason why you need to use thousands of GPUs at the same time to train these models— each GPU has a maximum of 96 gb of VRAM, and these indices eat that memory up for breakfast. Their MLA system finds a way to store a compressed version of these indices that captures the essential information while using far less memory. The brilliant part is this compression is built directly into how the model learns— it's not some separate step they need to do, it's built directly into the end-to-end training pipeline. This means that the entire mechanism is "differentiable" and able to be trained directly using the standard optimizers. All this stuff works because these models are ultimately finding much lower-dimensional representations of the underlying data than the so-called "ambient dimensions". So it's wasteful to store the full KV indices, even though that is basically what everyone else does. Not only do you end up wasting tons of space by storing way more numbers than you need, which gives a massive boost to the training memory footprint and efficiency (again, slashing the number of GPUs you need to train a world class model), but it can actually end up improving model quality because it can act like a "regularizer," forcing the model to pay attention to the truly important stuff instead of using the wasted capacity to fit to noise in the training data. So not only do you save a ton of memory, but the model might even perform better. At the very least, you don't get a massive hit to performance in exchange for the huge memory savings, which is generally the kind of tradeoff you are faced with in AI training. They also made major advances in GPU communication efficiency through their DualPipe algorithm and custom communication kernels. This system intelligently overlaps computation and communication, carefully balancing GPU resources between these tasks. They only need about 20 of their GPUs' streaming multiprocessors (SMs) for communication, leaving the rest free for computation. The result is much higher GPU utilization than typical training setups achieve. Another very smart thing they did is to use what is known as a Mixture-of-Experts (MOE) Transformer architecture, but with key innovations around load balancing. As you might know, the size or capacity of an AI model is often measured in terms of the number of parameters the model contains. A parameter is just a number that stores some attribute of the model; either the "weight" or importance a particular artificial neuron has relative to another one, or the importance of a particular token depending on its context (in the "attention mechanism"), etc. Meta's latest Llama3 models come in a few sizes, for example: a 1 billion parameter version (the smallest), a 70B parameter model (the most commonly deployed one), and even a massive 405B parameter model. This largest model is of limited utility for most users because you would need to have tens of thousands of dollars worth of GPUs in your computer just to run at tolerable speeds for inference, at least if you deployed it in the naive full-precision version. Therefore most of the real-world usage and excitement surrounding these open source models is at the 8B parameter or highly quantized 70B parameter level, since that's what can fit in a consumer-grade Nvidia 4090 GPU, which you can buy now for under $1,000. So why does any of this matter? Well, in a sense, the parameter count and precision tells you something about how much raw information or data the model has stored internally. Note that I'm not talking about reasoning ability, or the model's "IQ" if you will: it turns out that models with even surprisingly modest parameter counts can show remarkable cognitive performance when it comes to solving complex logic problems, proving theorems in plane geometry, SAT math problems, etc. But those small models aren't going to be able to necessarily tell you every aspect of every plot twist in every single novel by Stendhal, whereas the really big models can potentially do that. The "cost" of that extreme level of knowledge is that the models become very unwieldy both to train and to do inference on, because you always need to store every single one of those 405B parameters (or whatever the parameter count is) in the GPU's VRAM at the same time in order to do any inference with the model. The beauty of the MOE model approach is that you can decompose the big model into a collection of smaller models that each know different, non-overlapping (at least fully) pieces of knowledge. DeepSeek's innovation here was developing what they call an "auxiliary-loss-free" load balancing strategy that maintains efficient expert utilization without the usual performance degradation that comes from load balancing. Then, depending on the nature of the inference request, you can intelligently route the inference to the "expert" models within that collection of smaller models that are most able to answer that question or solve that task. You can loosely think of it as being a committee of experts who have their own specialized knowledge domains: one might be a legal expert, the other a computer science expert, the other a business strategy expert. So if a question comes in about linear algebra, you don't give it to the legal expert. This is of course a very loose analogy and it doesn't actually work like this in practice. The real advantage of this approach is that it allows the model to contain a huge amount of knowledge without being very unwieldy, because even though the aggregate number of parameters is high across all the experts, only a small subset of these parameters is "active" at any given time, which means that you only need to store this small subset of weights in VRAM in order to do inference. In the case of DeepSeek-V3, they have an absolutely massive MOE model with 671B parameters, so it's much bigger than even the largest Llama3 model, but only 37B of these parameters are active at any given time— enough to fit in the VRAM of two consumer-grade Nvidia 4090 GPUs (under $2,000 total cost), rather than requiring one or more H100 GPUs which cost something like $40k each. It's rumored that both ChatGPT and Claude use an MoE architecture, with some leaks suggesting that GPT-4 had a total of 1.8 trillion parameters split across 8 models containing 220 billion parameters each. Despite that being a lot more doable than trying to fit all 1.8 trillion parameters in VRAM, it still requires multiple H100-grade GPUs just to run the model because of the massive amount of memory used. Beyond what has already been described, the technical papers mention several other key optimizations. These include their extremely memory-efficient training framework that avoids tensor parallelism, recomputes certain operations during backpropagation instead of storing them, and shares parameters between the main model and auxiliary prediction modules. The sum total of all these innovations, when layered together, has led to the ~45x efficiency improvement numbers that have been tossed around online, and I am perfectly willing to believe these are in the right ballpark. One very strong indicator that it's true is the cost of DeepSeek's API: despite this nearly best-in-class model performance, DeepSeek charges something like 95% less money for inference requests via its API than comparable models from OpenAI and Anthropic. In a sense, it's sort of like comparing Nvidia's GPUs to the new custom chips from competitors: even if they aren't quite as good, the value for money is so much better that it can still be a no-brainer depending on the application, as long as you can qualify the performance level and prove that it's good enough for your requirements and the API availability and latency is good enough (thus far, people have been amazed at how well DeepSeek's infrastructure has held up despite the truly incredible surge of demand owing to the performance of these new models). But unlike the case of Nvidia, where the cost differential is the result of them earning monopoly gross margins of 90%+ on their data-center products, the cost differential of the DeepSeek API relative to the OpenAI and Anthropic API could be simply that they are nearly 50x more compute efficient (it might even be significantly more than that on the inference side— the ~45x efficiency was on the training side). Indeed, it's not even clear that OpenAI and Anthropic are making great margins on their API services— they might be more interested in revenue growth and gathering more data from analyzing all the API requests they receive. Before moving on, I'd be remiss if I didn't mention that many people are speculating that DeepSeek is simply lying about the number of GPUs and GPU hours spent training these models because they actually possess far more H100s than they are supposed to have given the export restrictions on these cards, and they don't want to cause trouble for themselves or hurt their chances of acquiring more of these cards. While it's certainly possible, I think it's more likely that they are telling the truth, and that they have simply been able to achieve these incredible results by being extremely clever and creative in their approach to training and inference. They explain how they are doing things, and I suspect that it's only a matter of time before their results are widely replicated and confirmed by other researchers at various other labs. The newer R1 model and technical report might even be even more mind blowing, since they were able to beat Anthropic to Chain-of-thought and now are basically the only ones besides OpenAI who have made this technology work at scale. But note that the O1 preview model was only released by OpenAI in mid-September of 2024. That's only ~4 months ago! Something you absolutely must keep in mind is that, unlike OpenAI, which is incredibly secretive about how these models really work at a low level, and won't release the actual model weights to anyone besides partners like Microsoft and other who sign heavy-duty NDAs, these DeepSeek models are both completely open-source and permissively licensed. They have released extremely detailed technical reports explaining how they work, as well as the code that anyone can look at and try to copy. With R1, DeepSeek essentially cracked one of the holy grails of AI: getting models to reason step-by-step without relying on massive supervised datasets. Their DeepSeek-R1-Zero experiment showed something remarkable: using pure reinforcement learning with carefully crafted reward functions, they managed to get models to develop sophisticated reasoning capabilities completely autonomously. This wasn't just about solving problems— the model organically learned to generate long chains of thought, self-verify its work, and allocate more computation time to harder problems. The technical breakthrough here was their novel approach to reward modeling. Rather than using complex neural reward models that can lead to "reward hacking" (where the model finds bogus ways to boost their rewards that don't actually lead to better real-world model performance), they developed a clever rule-based system that combines accuracy rewards (verifying final answers) with format rewards (encouraging structured thinking). This simpler approach turned out to be more robust and scalable than the process-based reward models that others have tried. What's particularly fascinating is that during training, they observed what they called an "aha moment," a phase where the model spontaneously learned to revise its thinking process mid-stream when encountering uncertainty. This emergent behavior wasn't explicitly programmed; it arose naturally from the interaction between the model and the reinforcement learning environment. The model would literally stop itself, flag potential issues in its reasoning, and restart with a different approach, all without being explicitly trained to do this. The full R1 model built on these insights by introducing what they call "cold-start" data— a small set of high-quality examples— before applying their RL techniques. They also solved one of the major challenges in reasoning models: language consistency. Previous attempts at chain-of-thought reasoning often resulted in models mixing languages or producing incoherent outputs. DeepSeek solved this through a clever language consistency reward during RL training, trading off a small performance hit for much more readable and consistent outputs. The results are mind-boggling: on AIME 2024, one of the most challenging high school math competitions, R1 achieved 79.8% accuracy, matching OpenAI's O1 model. On MATH-500, it hit 97.3%, and it achieved the 96.3 percentile on Codeforces programming competitions. But perhaps most impressively, they managed to distill these capabilities down to much smaller models: their 14B parameter version outperforms many models several times its size, suggesting that reasoning ability isn't just about raw parameter count but about how you train the model to process information. The recent scuttlebutt on Twitter and Blind (a corporate rumor website) is that these models caught Meta completely off guard and that they perform better than the new Llama4 models which are still being trained. Apparently, the Llama project within Meta has attracted a lot of attention internally from high-ranking technical executives, and as a result they have something like 13 individuals working on the Llama stuff who each individually earn more per year in total compensation than the combined training cost for the DeepSeek-V3 models which outperform it. How do you explain that to Zuck with a straight face? How does Zuck keep smiling while shoveling multiple billions of dollars to Nvidia to buy 100k H100s when a better model was trained using just 2k H100s for a bit over $5mm? But you better believe that Meta and every other big AI lab is taking these DeepSeek models apart, studying every word in those technical reports and every line of the open source code they released, trying desperately to integrate these same tricks and optimizations into their own training and inference pipelines. So what's the impact of all that? Well, naively it sort of seems like the aggregate demand for training and inference compute should be divided by some big number. Maybe not by 45, but maybe by 25 or even 30? Because whatever you thought you needed before these model releases, it's now a lot less. Now, an optimist might say "You are talking about a mere constant of proportionality, a single multiple. When you're dealing with an exponential growth curve, that stuff gets washed out so quickly that it doesn't end up matter all that much." And there is some truth to that: if AI really is as transformational as I expect, if the real-world utility of this tech is measured in the trillions, if inference-time compute is the new scaling law of the land, if we are going to have armies of humanoid robots running around doing massive amounts of inference constantly, then maybe the growth curve is still so steep and extreme, and Nvidia has a big enough lead, that it will still work out. But Nvidia is pricing in a LOT of good news in the coming years for that valuation to make sense, and when you start layering all these things together into a total mosaic, it starts to make me at least feel extremely uneasy about spending ~20x the 2025 estimated sales for their shares. What happens if you even see a slight moderation in sales growth? What if it turns out to be 85% instead of over 100%? What if gross margins come in a bit from 75% to 70%— still ridiculously high for a semiconductor company? At a high level, NVIDIA faces an unprecedented convergence of competitive threats that make its premium valuation increasingly difficult to justify at 20x forward sales and 75% gross margins. The company's supposed moats in hardware, software, and efficiency are all showing concerning cracks. The whole world— thousands of the smartest people on the planet, backed by untold billions of dollars of capital resources— are trying to assail them from every angle. On the hardware front, innovative architectures from Cerebras and Groq demonstrate that NVIDIA's interconnect advantage— a cornerstone of its data center dominance— can be circumvented through radical redesigns. Cerebras' wafer-scale chips and Groq's deterministic compute approach deliver compelling performance without needing NVIDIA's complex interconnect solutions. More traditionally, every major NVIDIA customer (Google, Amazon, Microsoft, Meta, Apple) is developing custom silicon that could chip away at high-margin data center revenue. These aren't experimental projects anymore— Amazon alone is building out massive infrastructure with over 400,000 custom chips for Anthropic. The software moat appears equally vulnerable. New high-level frameworks like MLX, Triton, and JAX are abstracting away CUDA's importance, while efforts to improve AMD drivers could unlock much cheaper hardware alternatives. The trend toward higher-level abstractions mirrors how assembly language gave way to C/C++, suggesting CUDA's dominance may be more temporary than assumed. Most importantly, we're seeing the emergence of LLM-powered code translation that could automatically port CUDA code to run on any hardware target, potentially eliminating one of NVIDIA's strongest lock-in effects. Perhaps most devastating is DeepSeek's recent efficiency breakthrough, achieving comparable model performance at approximately 1/45th the compute cost. This suggests the entire industry has been massively over-provisioning compute resources. Combined with the emergence of more efficient inference architectures through chain-of-thought models, the aggregate demand for compute could be significantly lower than current projections assume. The economics here are compelling: when DeepSeek can match GPT-4 level performance while charging 95% less for API calls, it suggests either NVIDIA's customers are burning cash unnecessarily or margins must come down dramatically. The fact that TSMC will manufacture competitive chips for any well-funded customer puts a natural ceiling on NVIDIA's architectural advantages. But more fundamentally, history shows that markets eventually find a way around artificial bottlenecks that generate super-normal profits. When layered together, these threats suggest NVIDIA faces a much rockier path to maintaining its current growth trajectory and margins than its valuation implies. With five distinct vectors of attack— architectural innovation, customer vertical integration, software abstraction, efficiency breakthroughs, and manufacturing democratization— the probability that at least one succeeds in meaningfully impacting NVIDIA's margins or growth rate seems high. At current valuations, the market isn't pricing in any of these risks. I hope you enjoyed reading this article. If you work at a hedge fund and are interested in consulting with me on NVDA or other AI-related stocks or investing themes, I'm already signed up as an expert on GLG and Coleman Research. Former Professional Investor, Software Engineer, and Founder of YouTube Transcript Optimizer The story of the most remarkable set of predictions in human history, which are hardly known in the West. An in-depth look at the technical challenges and solutions in creating the FastAPI backend for YouTubeTranscriptOptimizer.com, a powerful tool for transforming YouTube content into polished written documents and interactive quizzes. A look at our sleek and efficient new blogging system built with Next.js and Tailwind CSS, designed for fast, responsive, and visually appealing content delivery. YouTube Transcript Optimizer allows you to easily transform a YouTube video into a polished, beautifully formatted written document. It also includes a quiz generator that creates interactive multiple-choice quizzes based on the video content that you can actually take and get graded on. There are many other useful features, such as visual editing with revision history and easy one-button hosting, that make it a great tool for content creators and educators. © 2025 YouTube Transcript Optimizer. All rights reserved. reserved.
--------------------------------------------------

Title: Scale AI’s 28-Year-Old Billionaire CEO Warns About This Scarily Good Chinese Startup
URL: https://observer.com/2025/01/chinas-ai-capabilities-catching-up-us-scale-ai-alexandr-wang/
Time Published: 2025-01-25T13:00:10Z
Full Content:
For around a decade, the U.S. has retained its A.I. dominance over China, according to Alexandr Wang, the 28-year-old founder and CEO of Scale AI, a $13 billion startup. But with the Christmas release of an “earth-shattering” A.I. model from the Chinese startup DeepSeek, the gap between the two countries is becoming dangerously narrower, Wang said in an interview with CNBC on Jan. 23. Thank you for signing up! By clicking submit, you agree to our <a href="http://observermedia.com/terms">terms of service</a> and acknowledge we may use your information to send you emails, product samples, and promotions on this website and other properties. You can opt out anytime. Earlier this week, DeepSeek released a second A.I. model that demonstrates reasoning capabilities rivaling those from top U.S. companies like OpenAI. In addition to shocking researchers with its performance, the Chinese startup’s rapid progress has raised questions about the effectiveness of A.I. chip export controls intended to curb China’s access to the advanced graphics processing units (GPUs) underpinning A.I. tools. “The A.I. race and the A.I. war between the U.S. and China is one of the most important issues of today,” said Wang, who recently took out a full-page advertisement in The Washington Post urging the Trump administration to protect America’s lead over the emerging technology. The advertisement pointed readers to a letter from Wang that claimed China’s A.I. models are quickly catching up and called upon the federal government to pour more money into compute and data and unleash an energy plan to support the A.I. boom, among other recommendations. Wang is one of the world’s youngest self-made billionaires. He was born and raised in Los Alamos, New Mexico, as the son of weapons physicists who worked at the famed Los Alamos National Laboratory. After a brief stint in Silicon Valley working for fintech company Addepar and Q&A website Quora, Wang began studying machine learning at the Massachusetts Institute of Technology before dropping out to establish Scale AI, which uses contract work to provide troves of accurately labeled data for A.I. training efforts. Scale AI was valued at $13.8 billion last year and counts the U.S. Department of Defense and OpenAI among its clients. Wang is longtime friends with OpenAI CEO Sam Altman, having roomed with him during the Covid-19 pandemic. He launched Scale AI in 2016 within Y Combinator, the startup accelerator formerly helmed by Altman. Scale AI also recently partnered with the Center for A.I. Safety to release “Humanity’s Last Exam,” which they described as the toughest benchmark test yet for A.I. systems. While no model has been able to achieve more than 10 percent on the test so far, Wang said DeepSeek’s new reasoning model, DeepSeek-R1, has topped the leaderboard. “Their model is actually the top-performing, or roughly on par with the best American models,” he told CNBC. Due to tough GPU export controls in China, DeepSeek claims it has managed to create powerful models with far less computing power compared to American systems. DeepSeek-V3, the model released in December, was trained on around 2,000 A.I. chips from Nvidia (NVDA), according to DeepSeek researchers. Meta (META)’s Llama 3.1 model, for reference, was trained on 16,000 GPUs. Wang isn’t convinced. “The Chinese labs have more H100s than people think,” he said, referring to a type of Nvidia GPU that isn’t legally available in China. “My understanding is that DeepSeek has about 50,000 H100s—which they can’t talk about, obviously, because it is against the export controls that the United States has put in place.” To continue pushing its frontier models, the U.S. must unlock more computational capacity and infrastructure, according to Wang, who believes the A.I. market is on track to be worth $1 trillion once A.I. achieves the benchmark of artificial general intelligence (A.G.I.). While A.I. leaders have offered up various timelines and meanings for A.G.I., Wang’s definition refers to systems that “could basically be a remote worker in the most capable way,” an achievement he believes will be accomplished in the next two to four years. We get it: you like to have control of your own internet experience. But advertising revenue helps support our journalism. To read our full stories, please turn off your ad blocker.We'd really appreciate it. Below are steps you can take in order to whitelist Observer.com on your browser: Click the AdBlock button on your browser and select Don't run on pages on this domain. Click the AdBlock Plus button on your browser and select Enabled on this site. Click the AdBlock Plus button on your browser and select Disable on Observer.com.
--------------------------------------------------

Title: Nvidia Shares Tumbles as Japanese Rivals Gain Ground in AI Chips
URL: https://finance.yahoo.com/news/nvidia-shares-tumbles-japanese-rivals-060105606.html
Time Published: 2025-01-25T06:01:05Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. Shares of Nvidia Corp. (NVDA, Financial) fell more than 3% on Friday as investors worried about growing competition from Japan's semiconductor companies; Japan's Advantest renewed its leadership in chip testing equipment. With advancements in generative AI pushing demand, the global market is now dominated by Advantest , where the U.S. rival Teradyne (TER, Financial) commands about 50%, according to sources. Warning! GuruFocus has detected 4 Warning Signs with NVDA. SoC testing tool revenue is expected to climb 32 percent to JPY 324 billion ($2.1 billion) by March 2025, says the company. For such advanced AI chip processes as chiplets and 3D packaging, highly accurate and efficient tools are essential to yield improvement. Advantest's fiscal year 2024 R&D investments came in at JPY 65.5 billion ($424.5 million), illustrating the importance that is placed on innovation by Advantest. Much greater profitability has been made possible through its R&D efficiency. Advantest CEO Douglas Lefever, seeing continued strong growth in AI chip demand, views the technology as positive for the industry. While Nvidia falls, that speaks to the changing market dynamic around evolving semiconductor technologies. This article first appeared on GuruFocus. Sign in to access your portfolio
--------------------------------------------------

Title: 4 of the best ASX ETFs to buy now
URL: https://www.fool.com.au/2025/01/25/4-of-the-best-asx-etfs-to-buy-now/
Time Published: 2025-01-24T22:05:00Z
Description: Let's see why these funds could be among the best out there for Aussie investors.
The post 4 of the best ASX ETFs to buy now appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: Nvidia, Intuitive Surgical among Friday's market cap stock movers
URL: https://www.investing.com/news/stock-market-news/nvidia-intuitive-surgical-among-fridays-market-cap-stock-movers-93CH-3830344
Time Published: 2025-01-24T19:01:56Z
Description: Nvidia, Intuitive Surgical among Friday's market cap stock movers
--------------------------------------------------

Title: Trump’s $500B Stargate A.I. Project: What Will It Build and Does It Actually Have the Money?
URL: https://observer.com/2025/01/trumps-500b-stargate-ai-project/
Time Published: 2025-01-24T18:21:09Z
Full Content:
On Tuesday (Jan. 21), President Donald Trump announced an A.I. initiative called Stargate. During a press conference at the White House attended by OpenAI CEO Sam Altman, SoftBank (SFTBF) CEO Masayoshi Son and Oracle founder Larry Ellison, the returning President told reporters the initiative will invest $500 billion—equivalent to almost 2 percent of the annual U.S. economic output—over the next four years to bolster A.I. infrastructure in the U.S. and help the nation gain an edge over China. Trump estimates the Stargate will create 100,000 jobs. Thank you for signing up! By clicking submit, you agree to our <a href="http://observermedia.com/terms">terms of service</a> and acknowledge we may use your information to send you emails, product samples, and promotions on this website and other properties. You can opt out anytime. OpenAI will lead the project alongside Oracle, SoftBank and the UAE government’s tech investment Arm, MGX. Other key players in the tech alliance include Nvidia (NVDA), Microsoft (MSFT) and Arm. Seemingly a nod to Roland Emmerich’s 1994 sci-fi film “Stargate,” which explores the concept of intergalactic portals—Stargate will develop “physical and virtual infrastructure” to power advanced A.I. systems, according to the President. The project is launching with an immediate $100 billion funding commitment to address critical challenges in A.I. infrastructure, such as the need for cutting-edge data centers across the U.S. and the electricity to power them. The partnership capitalizes on OpenAI’s established ties with Nvidia, which began in 2016, and a newer collaboration with Oracle, leveraging the company’s expertise in data centers and enterprise analytics. SoftBank’s financial resources further bolster the project, but details remain scarce. Nvidia and Oracle didn’t respond to requests for comment by Observer. Stargate’s first data center, which will span one million square feet, is already under construction in Abilene, Texas, Ellison said at this week’s press conference, adding that “more will be built” across other U.S. locations. Tech leaders not involved in the project are skeptical about its viability, especially given its massive size. Elon Musk, who has been vocal in his criticism of OpenAI, challenged the viability of the project’s funding. “They don’t actually have the money,” Musk wrote on X. “SoftBank has well under $10B secured. I have that on good authority.” Altman responded to Musk’s doubt by offering to show him the ongoing construction of the data centers. “Want to come visit the first site already under way?,” Altman wrote in a retort on X. “I realize what is great for the country isn’t always what’s optimal for your companies, but in your new role I hope you’ll mostly put America first.” Musk also questioned Altman’s newfound support for Trump, citing his anti-Trump tweets during the 2016 Presidential election. He shared a screenshots of Altman’s 2021 post where the OpenAI CEO expressed gratitude to LinkedIn co-founder Reid Hoffman for funding legal efforts to preventing Trump’s re-election in 2020. In a post yesterday, Altman said watching the President more carefully recently has really changed my perspective on him. I’m not going to agree with him on everything, but I think he will be incredible for the country in many ways!” Big tech companies are racing to build their own data centers. Amazon’s AWS recently pledged $11 billion to expand data centers in Georgia; Meta announced a $10 billion A.I. hub in Louisiana; and Musk’s xAI recently built a data center in Memphis. At the 2025 World Economic Forum in Davos, Switzerland this week, Microsoft CEO Satya Nadella addressed Musk’s remarks on Stargate, reaffirming the company’s $80 billion commitment to expanding A.I. infrastructure globally. “All I know is, I’m good for my $80 billion,” Nadella said. “And all this money is not about hyping A.I., but is about building useful things for the real world!” He commented under Musk’s post on X. Despite the tensions, experts see the project as a necessary step to secure the U.S.’s leadership in the global A.I. race. “Stargate Project is the foundation we need to power the next generation of A.I. agents,” Claudionor Coelho, chief A.I. officer of the cloud security company Zscaler, told Observer. Matt Zeiler, founder and CEO of the deep learning and A.I. lifecycle platform Clarifai, said for Stargate to be truly successful, it must make a mark across multiple domains. He pointed to the rapid progress of foundational A.I. models in fields ranging from protein folding to patient communication. “In areas where A.I. architectures like transformers are already proving effective or where data can be easily tokenized, Stargate has the potential to be a game changer,” he told Observer. We get it: you like to have control of your own internet experience. But advertising revenue helps support our journalism. To read our full stories, please turn off your ad blocker.We'd really appreciate it. Below are steps you can take in order to whitelist Observer.com on your browser: Click the AdBlock button on your browser and select Don't run on pages on this domain. Click the AdBlock Plus button on your browser and select Enabled on this site. Click the AdBlock Plus button on your browser and select Disable on Observer.com.
--------------------------------------------------

Title: Trump's Stargate AI project backed by OpenAI, Oracle, and SoftBank is taking shape: Here's what we know
URL: https://finance.yahoo.com/news/trumps-stargate-ai-project-backed-by-openai-oracle-and-softbank-is-taking-shape-heres-what-we-know-165053861.html
Time Published: 2025-01-24T16:50:53Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. One of the Trump White House's first big announcements aligned itself with the artificial intelligence boom. In a splashy announcement on his second day in office, President Trump told Americans that a new $500 billion project — backed by SoftBank (SFTBY), Oracle (ORCL), and OpenAI — would generate “ over 100,000 American jobs almost immediately.” The project created a company called Stargate, which said in Tuesday's announcement it would “begin deploying $100 billion immediately" starting with data centers in Texas and spend an additional $400 billion over the next four years. Trump called the project, "a resounding declaration of confidence in America's potential under a new president." Microsoft (MSFT), AI chip giant Nvidia (NVDA), and British chip designer Arm (ARM) were named as “key initial technology partners” by Stargate. The stocks of all companies involved soared, with Oracle and Nvidia rising 7% and 4%, respectively, the day after the announcement. But these early announcements have been followed by social media feuds featuring Elon Musk, questions about funding, and rival announcements about AI investments slated for this year. Trump's close ally, Elon Musk, took aim at Stargate’s backers on his social media platform X shortly after the announcement. Responding to a post from OpenAI about the project, he wrote, "They don’t actually have the money," adding, "SoftBank has well under $10B secured." Sam Altman, OpenAI's CEO, wrote back, “Wrong, as you surely know.” OpenAI has raised nearly $17 billion in the past two years and opened up a $4 billion credit line with various banks, though the company said it expected to lose $5 billion in 2024 while generating only $3.7 billion in revenue. OpenAI would need to raise an additional $19 billion to fund Stargate, according to The Information. A person familiar with Stargate refuted Musk's claims, telling Yahoo Finance that the project is indeed prepared to immediately deploy the initial $100 billion investment, noting that SoftBank had $24 billion in cash as of its most recent quarterly earnings. The Financial Times, citing unnamed sources, reported this week that Stargate has not yet secured its required funding, will receive no government financing, and will only serve OpenAI once completed. So far, only about 4% of Americans report using AI daily in their workplaces, though almost all Americans use AI-enabled products in their daily lives, often without realizing it. Then there’s the question of whether the project will hold true to Trump’s jobs promise, as data centers typically employ fewer than 100 people. Apple’s (AAPL) $1.3 billion data center project in Iowa, for example, committed to employing just 50 people in permanent positions, according to a Forbes investigation. All the while, critics say nondisclosure agreements between data centers and local officials mean residents are often left in the dark about the colossal buildings in their backyards driving up their utility bills. And some lawmakers argue that tax breaks for data centers aren’t worth the cost. Bloomberg reported this week that the first data center associated with Stargate in Abilene, Texas, will employ just 57 people at an average wage of $57,600 per year. On the heels of the Stargate announcement, Mark Zuckerberg said in a Facebook post Friday that Meta (META) will spend between $60 billion and $65 billion in 2025, far higher than the estimated spending of $38 billion to $40 billion in 2024 it disclosed to investors back in October. Wall Street analysts had expected Meta's capital expenditures to tally closer to $51 billion this year, according to analysts tracked by Bloomberg. Much of Meta's spending would go into building a 2-gigawatt data center “that is so large it would cover a significant part of Manhattan,” Zuckerberg said. Two gigawatts is enough energy to power 700 electric vehicles for a full year. To be sure, proponents argue that the data centers have ripple effects on local economies. “Each direct job in the U.S. data center industry helps to create 7.4 ancillary jobs on average throughout the U.S. economy,” commercial real estate firm CBRE wrote in a report last year. As OpenAI's Altman wrote on X: "This is great for the country." Laura Bratton is a reporter for Yahoo Finance. Follow her on Bluesky @laurabratton.bsky.social. Email her at laura.bratton@yahooinc.com. Click here for the latest technology news that will impact the stock market Read the latest financial and business news from Yahoo Finance Sign in to access your portfolio
--------------------------------------------------

Title: Is NVIDIA Corporation (NVDA) the Best Dow Stock to Buy Right Now?
URL: https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_12fae84e-8366-495d-b382-a486e100e046
Time Published: 2025-01-24T12:25:24Z
Description: None
--------------------------------------------------

Title: The Perils of Abusing Leverage
URL: https://dailyreckoning.com/the-perils-of-abusing-leverage/
Time Published: 2025-01-23T22:52:58Z
Full Content:
By Adam Sharp Posted January 23, 2025 In 2004, I imploded my first trading account. I was 24 years old and thought I was hot stuff. You see, I had just gotten my stockbroker and financial advisor licenses. I was young, naive, and ready to take on the world. My journey began trading hot stocks like Research in Motion (RIMM). On my second trade, I made $1,400. Wow, this is easy! That win was the worst thing that could have happened. I ramped it up and started using absurd amounts of leverage. Every trade was at least 30% of the account, and occasionally more than 100%. Swing trading, daytrading, momentum, I tried it all. And eventually my trading account balance ticked down to zero. That was my first and only account wipeout. It was a painful – but important – lesson. Fortunately I was young and didn’t have much to lose. I haven’t used margin since. I learned from the experience, and began holding the vast majority of my portfolio in long-term positions. Eventually I learned to use options properly, and those are now the only way I employ leverage. When buying options, potential losses are limited to your initial investment. And the possible upside is just as lucrative as margin. For most of us, however, options should make up a limited portion of our portfolios. Yes, the profit potential is high, but so can be the losses. Keep things balanced. I recently found a painful margin story on Reddit’s infamous WallStreetBets forum. Back in 2018, a user going by the handle “DubiouslyCurious” correctly predicted the rise of NVIDIA (NVDA). He saw that the company was moving beyond video game hardware and into the nascent AI space. So DubiouslyCurious went all-in on NVIDIA with his net worth of approximately £100,000 (British pounds). He used margin to leverage his position up 3x. For a while, everything was going according to plan. NVIDIA shares rose, and his position was in profit. DubiouslyCurious wrote that, “For the first time in my life, I saw a path to financial security.” And then 2020 happened. The COVID pandemic was a classic black swan event, and the crash wiped out his position. The chart below, posted by DubiouslyCurious himself, tells the story. The red line is his leveraged position, and the blue line shows how his investment would have performed without 3x leverage. Source: Yahoo Finance The story was hard to read. I felt that pain. He had correctly assessed NVIDIA’s prospects, but due to his aggressive use of leverage (combined with Dr. Fauci’s aggressive use of gain-of-function research on coronaviruses), he lost it all. “I sold when I had to take out the pittance remaining just to buy food. Ironically, of course, as soon as I sell it jumped back up. But by that point the damage had been done.” If he had simply bought NVIDIA shares without leverage, his position today would be worth over £2 million. Leverage is not something to be played with or taken lightly. With 3x margin, it only takes a 33% correction to wipe out a position. On top of that, you’re paying interest to borrow the funds. Many investors have a story like mine or DubiouslyCurious’. It’s practically an initiation ritual for traders. Most of us take a major loss at some point in our experience. The important thing is that we learn from our mistakes, and incorporate real change into our investing strategies going forward. The earlier an investor learns this lesson, the better. So in the near future I’m going to let my kids trade with $100 each. If the plan works, they’ll lose most of it. And by learning this lesson so early in life, they’ll be ahead of the game once they begin their real investing journey. Then I’ll teach them about the virtues of a long-term, balanced approach.
--------------------------------------------------

Title: Is Quantum Computing Investable As The Next AI ?
URL: https://www.forbes.com/sites/vineerbhansali/2025/01/23/is-quantum-computing-investable-as-the-next-ai-/
Time Published: 2025-01-23T20:48:33Z
Full Content:
quantum computer The boom and bust and re-boom in quantum computing stocks over the last few weeks (look for example at the price gain and collapse and re-gain of RGTI, QBTS, QUBT, IONQ and others) has drawn comments from analysts that, on the one hand, equate quantum computing with a “bubble,” and on the other hand compare it with the early days of AI. Questions include: How far can quantum computing go? Can it challenge classical computing in the near future? Are quantum computing stocks worth investing in? Is one of these companies, or maybe one that is not even a company yet, but just in the heads of physicists somewhere in one of the tech universities around the world, the next big thing? Is this a lot of hocus pocus or a computational paradigm that might blow current capabilities out of the water and make NVDA look like, well, Intel of prior years? Investing in quantum computing is additionally scary today because the price of quantum computing stocks is being driven by both speculative instinct and the opinions of some of the rock stars of the AI revolution, who, if I may say so, probably do not know much more than the rest of us when it comes to forecasting the potential power and timing of when the strategy may become practical. One might be forgiven for thinking that comments regarding the near-term viability of quantum computing by Jensen Huang and Mark Zuckerberg from a few weeks ago are somewhat self-serving, looking to protect their turf from a potential new game-changing technology, though in fairness they are also quite involved in pushing the frontiers of quantum computing technology. For sure, if Nvidia has ushered in exponential growth in computational power behind AI, then the potential of quantum computing, which can provide a “double exponential” speedup in computation (this is known as Neven’s law), will make classical computing based on even the fastest NVDA GPUs look linear. And there are good reasons why NVDA cannot just come out and say quantum is the next big thing, because doing so would likely decimate the market for their current chips and the profitability that has propelled it to be the most valuable company in the world (as of the date of this writing). In the world of computing, linear speedup, even relatively speaking, is considered an insult, because quantum computing theoretically has the potential of not just exponential, but a “double exponential” speedup. So, the simple answer to whether practical quantum computing is on the horizon in the near future is that no one really knows. It is similar to the task of predicting the probability that a meteor will hit the earth over the next decade. We simply don’t know. But that should not prevent us from being somewhat prepared. To me, the real question is not whether practical quantum computing will happen in the next few years, but what are the consequences if it does, and how, as an investor can I rationally position myself for the rare but impactful consequences. The physical laws behind quantum computing are on solid ground, unlike many other fads like negative interest rates that were implemented in Germany, or perpetual motion machines of eras past. First, let us put some confusion to rest. There is already a “practical” quantum computer in operation today. As a matter of fact, it has been in operation for as long as our physical world has existed. Yes, current physics says that nature is a massive quantum computer. I did my Ph.D. in Theoretical Physics, in the area of Quantum Field Theory, which strangely enough, works amazingly well in explaining the real world. The fact that nature seems to obey the rules of quantum mechanics all the way down to elementary particles and beyond is now beyond doubt. I also had the distinct pleasure of being at Caltech around the time that Nobel laureate Richard Feynman started to seriously think about quantum computers and around when he uttered the famous quote that “Nature isn’t classical, dammit, and if you want to make a simulation of nature, you’d better make it quantum mechanical”. In other words, even the fastest NVDA chips only get you a classical approximation to the real thing when it comes to simulation. It is akin to trying to simulate the nature of sub-atomic particles by applying Newton’s laws to smaller and faster bouncing balls. At some point the classical simulation will just fail because bouncing balls will never be able to replicate the peculiar quantum mechanical laws obeyed by sub-atomic particles. There are indeed problems today, though somewhat academic, that are practically impossible for classical computers to solve. As far as practicality goes, let’s not forget that the period between when proclamations were made that humans could not fly to when airplanes showed it could be done was quite short. What it took was a change of mindset and the development of a parallel framework - humans fly in planes by understanding the basic principles of aerodynamics, not by flapping their wings like birds. The quantum approach to computing will also develop parallel ways of solving the problems of computation, and they cannot be extrapolated from classical ways of computing. Its a new paradigm, and new paradigms can quickly shatter the constraints of old paradigms. After close to three decades of research, quantum computers have been built that can theoretically solve problems that cannot be solved on a classical computer (see the discussion of “quantum supremacy,” and a more sober look in light of recent claims by the Google team here by one of my other professors from Caltech, the theoretical physicist John Preskill), and the question now is not whether quantum computers that can exceed current classical computing is possible, but how hard really the current problems like error correction are. Impossible is bad, hard is good! The history of human innovation is that when the possibility of a game-changing technology exists, the speed at which hard problems become easier to solve is much faster than linear thinking allows us to imagine. When the rate at which the innovation — i.e. quantum computing’s rate of growth — is doubly exponential, not just exponential, the possibility of missing the inflection point is very real. And much of the acceleration in such situations also comes from incumbents who are competing amongst themselves for the next big thing. No wonder that after tanking quantum computing stocks, NVDA announced a” Quantum Day” at its GTC conference on March 20 (here). To me, it seems that the commercial race is already on, though it might be beneficial to do the development in stealth mode for most so they don’t get crushed out of their path to progress. This brings me to investability. Stock market investing is always risky, and speculative stock-market investing, as in quantum computing stocks (or AI, turn-of-the-century dot-coms or Bitcoin not too long ago), is doubly risky. But the upside compensation for taking the risk can also be large, which might justify a thoughtful, carefully considered, small allocation, to moonshot investments. The precise modeling of potential gains and losses for such speculative investments is basically impossible. At the simplest level, one can simply think of investing in such stocks as a biased coin flip experiment, where the odds of the coin flip coming up heads — i.e. a “win” is some number you make up — and the odds of the stock going belly up (i.e. price falling to zero), is one minus that number. An unbiased coin would have a probability of coming up heads or coming up tails to be equal or 50%. What makes things even more tricky is that these odds are dynamic and ever-changing, and can be subject to the opinions of experts who also know very little about the future but still have influence on pricing. Which means that proper sizing, or allocation of capital, is one of the only ways to ensure that one can stay the course, “HODL,” and not be shaken out. The mathematics of biased coin-flip betting is well known. As a matter of fact, it forms the foundation of all option pricing. For our purposes here let us imagine that the coin we are tossing has a very low probability of coming up heads. If this probability is labelled p, then the expected value, or average return over many such bets, assuming equal payoffs in the win and the loss states, is simply equal to p. So if the probability of success is 10% on one trial, then over many independent trials, the expected payoff can be estimated by multiplying 10% by the payoff if there is success. The lower the probability of success, the lower the expected value. Just like lottery tickets which have a very low probability of success, the expected value of speculative stock investing is very low. But that’s not the end of the story. As one would expect, both lottery tickets and speculative stocks have massive “skewness” and “kurtosis” when the probability of success is low, despite having very low expected value. Skewness is a measure of the asymmetry of payoffs, and kurtosis is a measure of how “fat” the tail is. For a coin toss with probability of success p, the skewness and kurtosis both go to infinity when the probability of success is very low. This should not be a shocking result — we all know how low probability investments, e.g. bitcoin, can deliver massive returns (just recall bitcoin’s experience over the last few years). While it is easy to pooh pooh those dumb souls who bet on fat right tails, or lottery ticket-like investments, the fact of the matter is that such investments can make life-changing impact for investors, if they are made at a time when the price is not already discounting the massive positive returns. For a stock bought cheaply enough, this upside potential can counter the low expected return. Also note that both the skewness and kurtosis for a coin toss is minimum when the risks are balanced; i.e. the probability of success and failure are equal to 50% each. The danger of investing in such speculative investments arises from taking on too much exposure. This is because a low probability of success effectively guarantees that on average, returns will be low or negative. So, the bet one is making really is that by investing at the right price, the potential of outsized gains on the winners makes up for the almost certain losses on the losers. Again, as most readers undoubtedly know or have read, the optimal exposure that maximizes the long term growth rate of simple binary investments is given by the almost too simple (and magical) Kelly criterion (see here) which says that the optimal fraction to invest is simply p-(1-p)/b, where b are the odds upon success. For example, if one bets a dollar and upon a win is returned three dollars, and upon a loss loses the one dollar he bet, then b=2. So, we can see that if the probability of success is very low, then the payoff has to be very high for a profitable bet to be possible. If the payoff is not high enough then the optimal allocation must be very small. Let us calibrate this to investing in quantum computing. If we believe that quantum computing has the potential to revolutionize computation, then it is not hard to imagine a 100 to 1 payoff though it could take years for this to happen. So, if the probability of success is 5% , then the “optimal” fraction to bet at this enormous potential payout is about 4% of the portfolio. In this example, the investor should bet nothing if the potential payoff is less than about 19 times (obtained by dividing the probability of failure which is 95% by the probability of success which is 5%). In its current state, no one knows which type of quantum computing technology and which company will succeed. So, it also behooves the investor to divvy up the total allocation amongst many different technologies. Clearly, I take the view that quantum computing is very risky but investable, regardless of what the talking heads say, as long as there is potential for massive upside, the entry price is right, and as long as the exposure is kept small enough to not create a situation where the investor is forced out at the wrong time. The gut wrenching 50% decline in one day on an off-the-cuff comment by Jensen Huang last week illustrates that those who had positions too large or leveraged probably got forced out at the wrong price, creating the buying opportunity for those who were able to step in at the right size. At the end of the day, sizing exposures is probably as important, if not more important than getting the direction of investments right for any investment. In the case of highly speculative moonshots like quantum computing stocks, sizing things right — i.e. big enough but not too big —and owning the upside, skewness and the kurtosis, is probably the approach that balances out the risks with the potential of outsized returns. Just as there are catastrophic “left tails” which can destroy value, there are also “right tails”, which, if not managed for, can create massive missed opportunities. Like they say — you have to be “in it to win it.” Just be careful how much you risk. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: NVDA, TSMC, and Broadcom: Top Semiconductor Plays as SMH Hits New Highs
URL: https://stockcharts.com/articles/chartwatchers/2025/01/nvda-tsmc-and-broadcom-top-sem-903.html
Time Published: 2025-01-23T19:43:25Z
Description: In the last quarter of 2024, semiconductors have been walking a tightrope between tariff fears and supply chain uncertainties. Geopolitical tensions between the US and China cast a long shadow over the industry, holding our proxy, VanEck Vectors Semiconductor…
--------------------------------------------------

Title: Stock market today: Dow, Nasdaq, S&P 500 falter as Trump takes the spotlight
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-nasdaq-sp-500-falter-as-trump-takes-the-spotlight-143103099.html
Time Published: 2025-01-23T14:31:03Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. US stocks faltered on Thursday, pulling back from a bid for fresh records as AI optimism waned and markets waited for more detail on President Donald Trump's tariff plans. The S&P 500 (^GSPC) was little changed, coming off a three-day win streak that saw the benchmark index close Wednesday on the cusp of setting a new all-time high. The Dow Jones Industrial Average (^DJI) traded 0.2% higher, with a record high not far off. Meanwhile, The Nasdaq Composite (^IXIC) fell 0.4% as tech stocks struggled to regain the momentum that propelled the previous day's gains. Nvidia (NVDA) shares dropped, as fellow megacap techs Apple (AAPL) and Google-parent Alphabet (GOOG, GOOGL) also lost ground. Investors are still digesting Trump's early-days policy charge, which brought an AI push that invigorated tech names but left unclear when the outlined tariffs on big trading partners — a risk for inflation and stocks — might hit. The focus is now on Trump's speech at the World Economic Forum in Davos later Thursday for more insight into his trade policy. Yahoo Finance is on the ground at Davos. See the latest from the world's business leaders here. Shares in Amazon (AMZN) and Tesla (TSLA) edged lower after business shifts by the tech leaders in Canada, a prime tariff target for Trump. The e-commerce giant will close its warehouses in Quebec, with a loss of about 1,700 jobs, while Tesla plans hefty price hikes on all its EV models sold in Canada. Eyes are on earnings to potentially buoy markets, after Netflix (NFLX) set that tone on Wednesday. GE Aerospace (GE) shares popped after the jet engine maker said it will raise share buybacks to $7 billion. In air carriers, American Airlines (AAL) stock slid after a downbeat 2025 profit forecast, and Alaska Airlines (ALK) shares rose thanks to a smaller-than-expected first quarter loss forecast. On the data front, US jobless claims increased by 6,000 to 223,000, official figures released on Thursday showed. Economists had expected a reading of 220,000 for the week. Electronic Arts (EA) stock tumbled 15% on Thursday after the game publisher said quarterly bookings fell to roughly $2.22 billion, missing analyst expectations. The decline was due to poor performance of two of its video games: its soccer title, EA Sports FC 2025, and Dragon Age: The Veilguard. Both of those missed expectations. The company cut its bookings outlook for the 2025 fiscal year to a range of $7 billion to $7.15 billion. EA is expected to release its complete results on February 4. US stocks were mixed on Thursday, pulling back from near record levels as the markets awaited a speech from President trump at the World Economic Forum in Davos. The S&P 500 (^GSPC) slipped roughly 0.2%, coming off a record intraday high on Wednesday, while the Dow Jones Industrial Average (^DJI) was little changed. Meanwhile, The Nasdaq Composite (^IXIC) fell 0.5% as tech stocks struggled. Nvidia (NVDA) shares sliped along with Apple (AAPL) and Google parent Alphabet (GOOG, GOOGL). Investors are still awaiting any new developments on the tariff front. President Trump is expected to speak at the World Economic Forum in Davos later Thursday. Economic data: Initial jobless claims (week ending Jan. 18); Kansas City Fed. Manufacturing Activity (January) Earnings: American Airlines (AAL), Alaska Airlines (ALK), CSX Corporation (CSX), Freeport-McMoRan (FCX), GE Aerospace (GE), Intuitive Surgical (ISRG), Texas Instruments (TXN), Union Pacific Corporation (UNP) Here are some of the biggest stories you may have missed overnight and early this morning: Trump's new crypto token is the industry in its purest form Why Trump is laser-focused on tariffs for Canada Intel races to find next CEO before it sinks into irrelevance Amazon to cut 1,700 jobs, close warehouses in Quebec GE Aerospace posts earnings beat, plans $7B buyback American Air stock drops as 2025 profit forecast falls short Tesla to significantly hike prices for all models in Canada Musk clashes with OpenAI's Altman over 'Stargate' Sign in to access your portfolio
--------------------------------------------------

Title: Stock market today: Dow rises, Nasdaq slips as Trump takes the spotlight at Davos
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-rises-nasdaq-slips-as-trump-takes-the-spotlight-at-davos-143103433.html
Time Published: 2025-01-23T14:31:03Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. US stocks were mixed Thursday as investors stayed attuned to remarks from President Donald Trump, who addressed the World Economic Forum in Davos. The S&P 500 (^GSPC) rose more than 0.1%, coming off a three-day win streak that saw the benchmark index close Wednesday on the cusp of setting a new all-time high. The Dow Jones Industrial Average (^DJI) traded 0.6% higher, with a record not far off. Meanwhile, the Nasdaq Composite (^IXIC) fell around 0.2% as tech stocks struggled to regain the momentum that propelled the previous day's gains. Nvidia (NVDA) shares dropped after its supplier SK Hynix (000660.KS) flagged uncertainty about chip demand this year, which also weighed on other related stocks. The focus turned to Trump's remote speech at the World Economic Forum in Davos for more insight into his trade policy. The president said he will "demand that interest rates drop immediately." He also said he will ask OPEC to bring down the price of oil. On the tariff front, the president offered a warning to companies. "If you don't make your product in America, which is your prerogative, then very simply you will have to pay a tariff — differing amounts, but a tariff." Investors are still digesting Trump's early-days policy charge, which brought an AI push that invigorated tech names but left unclear when the outlined tariffs on big trading partners — a risk for inflation and stocks — might hit. Yahoo Finance is on the ground at Davos. See the latest from the world's business leaders here. Meanwhile, eyes are on earnings to potentially buoy markets, after Netflix (NFLX) set the stage for next week's Big Tech flurry. On the data front, US jobless claims increased by 6,000 to 223,000, official figures released on Thursday showed. Economists had expected a reading of 220,000 for the week. President Trump said he will ask the Organization of the Petroleum Exporting Countries (OPEC), led by Saudi Arabia to bring down the price of oil. "I'm also going to ask Saudi Arabia and OPEC to bring down the cost of oil. You've got to bring it down," said Trump during his remote speech at the World Economic Forum in Davos, Switzerland. "If the price came down, the Russia-Ukraine war would end immediately. Right now the price is high enough that that war will continue, you've got to bring down the oil price. You've got to end that war," he said. Russia, a major producer of oil and gas, is an OPEC ally. The cartel currently has production cuts in place in order to keep a floor on prices. Both West Texas Intermediate (CL=F) and Brent (BZ=F) fell roughly 1% following Trump's comments. President Trump said he will demand that interest rates drop in the US. "I’ll demand that interest rates drop immediately, and likewise, they should be dropping all over the world. Interest rates should follow us,” Trump said remotely at the World Economic Forum in Davos, Switzerland. Dow Jones Industrial Average (^DJI) moved to a session high, up 0.5% while the S&P 500 (^GSPC) gained 0.2%. The Nasdaq Composite (^IXIC) fell slightly. President Trump spoke remotely at the World Economic Forum in Davos, Switzerland, appearing via video to a packed room of CEOs and world officials. The President first listed a series of actions he took upon entering the White House, including withdrawing from the Paris climate accord and declaring a national energy emergency. On the topic of tariffs, the President said: "My message to every business in the world is very simple: Come make your product in America, and we will give you among the lowest taxes of any nation on Earth." "But if you don't make your product in America, which is your prerogative, then very simply you will have to pay a tariff — differing amounts, but a tariff. Attendees at the World Economic Forum in Davos are waiting for President Trump's special address via video. Investors watching the remote speech will pay attention to any talk about the President's tariff policy. President Trump will also take questions after his remarks. Bitcoin (BTC-USD) rose after Republican Sen. Cynthia Lummis said in a post on X that she will chair the Senate Banking Subcommittee on digital assets. The token moved up to about $105,000 at around 10:50 a.m. ET. Earlier this week, Lummis said she spoke with Eric Trump about the possibility of creating a strategic bitcoin reserve at the inaugural lunch on Monday. Late last year, she introduced a proposal that would give the Federal Reserve the right to own the cryptocurren. Yahoo Finance's Laura Bratton reports: Chip stocks fell across the board early on Thursday after a Nvidia (NVDA) supplier made comments on its earnings call that indicated uncertainty about semiconductor demand this year. South Korea-based SK Hynix (000660.KS) makes memory chips for Nvidia’s GPUs (graphics processing units), which are used in data centers to underpin artificial intelligence software. After SK Hynix reported fourth quarter earnings that surpassed analysts’ expectations, its head of finance, Woo-Hyun Kim, painted a careful picture of the year ahead. “2025's memory demand outlook is clouded by inventory adjustments from PC and smartphone OEMs [original equipment manufacturers] as well as strengthened protective trade policies and geopolitical risks,” Kim said on a post-earnings conference call. Following the comments, Nvidia shares sank as much as 2%. British chip designer Arm's (ARM) stock dropped nearly 6%, and SK Hynix rival Micron (MU) shares fell roughly 4%. Read more here. Yahoo Finance's Brian Sozzi reports from Davos: Up and down the promenade at the World Economic Forum this week, leaders agree on one thing about artificial intelligence: It's darn powerful, and it's starting to materially impact how business is done and will be done. Think AI on the cusp of rendering human call centers and HR departments obsolete. What leaders can't agree on is whether AI is already out of control — whether it's beyond the point of being successfully regulated or having the proper guardrails. "I'm not sure it's [AI] out of control. I think there are use cases that sometimes people want to showcase [for] ... their own benefits, to see, hey, this is the potential. But the reality is that enterprises are very smart," HPE CEO Antonio Neri told Yahoo Finance at the annual gathering of top business leaders. Read more here. American Airlines (AAL) stock fell nearly 8% after the carrier forecast a surprise loss for its first quarter. The outlook is a departure from the major air travel carriers' bullish performance in recent quarters. American expects an adjusted loss of as much as $0.40 a share for the current quarter. Analysts were expecting a profit of at least $0.01, according to estimates compiled by Bloomberg. The airline posted record fourth quarter revenue of $13.7 billion and record full-year revenue of $54.2 billion. The company's profit for the current quarter is expected to be impacted by a rise in non-fuel expenses as it continues to win back business after changing its corporate sales strategy. American had implemented changes that pushed corporate customers to book directly with the airline, instead of third-party travel managers. Electronic Arts (EA) stock tumbled 15% on Thursday after the game publisher said quarterly bookings fell to roughly $2.22 billion, missing analyst expectations. The decline was due to the poor performance of two of its video games: soccer title "EA Sports FC 2025" and "Dragon Age: The Veilguard." Both of those missed bookings expectations. The company cut its bookings outlook for the 2025 fiscal year to a range of $7 billion to $7.15 billion. EA is expected to release its complete results on Feb. 4. US stocks were mixed on Thursday, pulling back from near record levels as the markets awaited a speech from President Trump at the World Economic Forum. The S&P 500 (^GSPC) slipped roughly 0.2%, coming off a record intraday high on Wednesday, while the Dow Jones Industrial Average (^DJI) was little changed. Meanwhile, the Nasdaq Composite (^IXIC) fell 0.5% as tech stocks struggled. Nvidia (NVDA) shares slipped, along with those of Apple (AAPL) and Google parent Alphabet (GOOG, GOOGL). Investors are still awaiting new developments on the tariff front. Trump is expected to remotely give an address to the WEF in Davos later in the day. Economic data: Initial jobless claims (week ended Jan. 18); Kansas City Fed. Manufacturing Activity (January) Earnings: American Airlines (AAL), Alaska Airlines (ALK), CSX Corporation (CSX), Freeport-McMoRan (FCX), GE Aerospace (GE), Intuitive Surgical (ISRG), Texas Instruments (TXN), Union Pacific Corporation (UNP) Here are some of the biggest stories you may have missed overnight and early this morning: Trump's new crypto token is the industry in its purest form Why Trump is laser-focused on tariffs for Canada Intel races to find next CEO before it sinks into irrelevance Amazon to cut 1,700 jobs, close warehouses in Quebec GE Aerospace posts earnings beat, plans $7B buyback American Air stock drops as 2025 profit forecast falls short Tesla to significantly hike prices for all models in Canada Musk clashes with OpenAI's Altman over 'Stargate' Sign in to access your portfolio
--------------------------------------------------

Title: AI crypto coins take over: Is Trump’s $500B investment behind it all?
URL: https://ambcrypto.com/ai-crypto-coins-take-over-is-trumps-500b-investment-behind-it-all/
Time Published: 2025-01-23T12:00:46Z
Full Content:
The future of crypto is AI—and it’s happening now. Artificial Intelligence (AI) has real-world applications across industries like business, technology, healthcare, and finance. The crypto sector is no exception, with AI-based tokens now commanding a market cap of $48 billion. Trump’s ‘no crypto’ stance may have rattled the broader market, but his $500 billion AI investment sent AI crypto coins soaring, with low-caps jumping 15% in 24 hours. Nvidia (NVDA) has now dethroned Apple (AAPL) as the world’s largest company, with a market cap soaring to $311 trillion – up 4.43% since Trump’s $500 billion AI investment. This is a clear sign that the AI revolution is taking off. AI-based crypto tokens aren’t far behind. In just two years, the market cap of AI-focused crypto coins surged by a massive 2,592%, reaching a staggering $70 billion by November last year. Leading the charge? NEAR Protocol [NEAR], boasting a solid market value of $5.89 billion. Source: CoinMarketCap But that’s just the beginning. Take Virtuals Protocol (VIRTUAL), for instance, jumping into the ‘AI agent’ space, which Microsoft’s CEO just called the future of software development. Clearly, these coins are tapping into the tech to create real-world applications that are catching the eye of both individuals and institutional investors. However, the AI crypto market isn’t immune to volatility. In just two months, its market cap took a 31% dip, as the broader crypto market saw massive profit-taking. Can Trump’s $500 billion AI investment turn things around? Right now, Trump’s actions are shaking up the crypto world. Take the TRUMP memecoin, for example, which has seen billions in market cap vanish after Trump’s ‘no crypto’ stance in his first 42 executive orders. Bitcoin is also feeling the squeeze, with its market share down 7% to $2.02 trillion. And AI crypto coins? They’re not immune, with many slipping into the red. Source: CoinMarketCap But there’s hope on the horizon. Trump’s massive $500 billion investment could spark a major rebound for AI tokens. We’ve seen these coins surge after Nvidia’s quarterly reports in the past, and with AI sentiment at an all-time high in the U.S., the future looks promising. Read NEAR Protocol’s [NEAR] Price Prediction 2025–2026 The road ahead won’t be easy, though. AI crypto coins need to break free from Bitcoin’s volatility and carve out their own space. But with a 2592% growth in less than three years, they’re on the right track. As tech giants gear up for the AI revolution, more capital could flood into the AI crypto coins. 2025 could be their year – definitely one to watch. Disclaimer: AMBCrypto's content is meant to be informational in nature and should not be interpreted as investment advice. Trading, buying or selling cryptocurrencies should be considered a high-risk investment and every reader is advised to do their own research before making any decisions. © 2025 AMBCrypto
--------------------------------------------------

Title: Why Planning Now For 2025 Taxes Can Save You Money
URL: https://www.investors.com/etfs-and-funds/personal-finance/tax-brackets-2025/
Time Published: 2025-01-23T12:00:16Z
Description: It's too late to plan for taxes you pay on April 15. But now's the ideal time to get a head start on ways to navigate tax brackets for 2025.
--------------------------------------------------

Title: Stock market today: Nasdaq futures slide, S&P 500 backs off from record as techs lose steam
URL: https://finance.yahoo.com/news/live/stock-market-today-nasdaq-futures-slide-sp-500-backs-off-from-record-as-techs-lose-steam-114920885.html
Time Published: 2025-01-23T11:49:20Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. US stocks faltered before the bell on Thursday, pulling back from a bid for fresh records as AI optimism waned and markets waited for more detail on President Donald Trump's tariff plans. S&P 500 futures (ES=F) slipped roughly 0.2%, coming off a three-day win streak that saw the benchmark index close Wednesday on the cusp of setting a new all-time high. Dow Jones Industrial Average futures (YM=F) traded broadly flat with a record still within reach. Meanwhile, Nasdaq 100 futures (NQ=F) fell 0.5% as tech stocks struggled to regain the momentum that propelled the previous day's gains. Nvidia (NVDA) shares tipped lower, as fellow megacap techs Apple (AAPL) and Google parent Alphabet (GOOG, GOOGL) also lost ground. Investors are still digesting Trump's early-days policy charge, which brought an AI push that invigorated tech names but left unclear when the outlined tariffs on big trading partners — a risk for inflation and stocks — might hit. The focus is now on Trump's speech at the World Economic Forum in Davos later Thursday for more insight into his "shock and awe" trade policy. Yahoo Finance is on the ground at Davos. See the latest from the world's business leaders here. Shares in Amazon (AMZN) and Tesla (TSLA) edged lower after business shifts by the tech leaders in Canada, a prime tariff target for Trump. The e-commerce giant will close its warehouses in Quebec, with a loss of about 1,700 jobs, while Tesla plans hefty price hikes on all its EV models sold in Canada. Earnings are providing a bright spot for markets, after Netflix (NFLX) blew past Wall Street expectations. Quarterly reports from American Airlines (AAL), GE Aerospace (GE), and Texas Instruments (TXN) are on the docket on Thursday. On the economic front, a reading on initial jobless claims in the past week will feed into expectations for the Federal Reserve's path on interest rates. Sign in to access your portfolio
--------------------------------------------------

Title: Microsoft Eases Control on OpenAI in $500B Venture
URL: https://www.c-sharpcorner.com/news/microsoft-eases-control-on-openai-in-500b-venture
Time Published: 2025-01-23T00:00:00Z
Full Content:
In a significant development within the tech industry, Microsoft (MSFT) announced on Tuesday that it has revised key terms of its longstanding partnership with OpenAI. This change comes in the wake of OpenAI's announcement of a joint venture with Oracle (ORCL) and Japan's SoftBank Group aimed at establishing a groundbreaking $500 billion network of AI data centers in the United States. The partnership dubbed the "Stargate" initiative, is designed to bolster the U.S.'s position in the global AI race, particularly against competitors like China. During a press conference at the White House, President Donald Trump unveiled the Stargate project, emphasizing its importance for national competitiveness. The initiative aims to leverage advanced chips from Nvidia (NVDA) to enhance AI capabilities across various sectors. As part of this venture, OpenAI will have an equity stake and operational control, with SoftBank CEO Masayoshi Son appointed as chairman of the board. Historically, Microsoft has held exclusive rights to provide cloud computing infrastructure for OpenAI since 2019. However, the new agreement allows OpenAI to explore additional capacity options, primarily for research and model training. This shift opens the door for OpenAI to collaborate with Oracle and potentially other cloud service providers. Despite these changes, Microsoft reassured stakeholders that its core partnership with OpenAI remains intact. The tech giant retains exclusive rights to offer OpenAI's API—an essential revenue source for the company—ensuring that Oracle will not host this critical service. In a blog post, Microsoft stated that "the key elements of our partnership remain in place for the duration of our contract through 2030," including access to OpenAI's intellectual property and ongoing revenue-sharing arrangements. Microsoft also highlighted a recent commitment from OpenAI to utilize its Azure cloud services extensively, further solidifying their collaborative efforts. This commitment underscores Microsoft's role as a significant investor and partner in advancing OpenAI's technology. As the landscape of AI development evolves, this partnership adjustment signals a strategic pivot for both Microsoft and OpenAI. The Stargate project represents one of the largest investments in AI infrastructure to date, with initial funding of $100 billion set to expand over four years. This ambitious plan aims not only to enhance AI capabilities but also to create jobs and foster innovation within the U.S. The collaboration between Microsoft, Oracle, and other technology partners like Nvidia and Arm will be crucial in building out this infrastructure. As noted by Oracle Chairman Larry Ellison during the announcement, construction is already underway in Texas, with plans for multiple data centers designed to support expansive AI operations. This evolving landscape presents an exciting opportunity for innovation in artificial intelligence. The collaboration between major players like Microsoft and OpenAI illustrates a commitment to advancing technology while navigating competitive pressures. As these companies work together to build robust AI infrastructure, it will be interesting to see how they balance exclusivity agreements with collaborative efforts that benefit the broader tech ecosystem. As we witness these transformative partnerships unfold, it is essential for stakeholders—be they investors, developers, or consumers—to stay informed about how these changes will impact technological advancements and market dynamics in the coming years. The future of AI is not just about competition; it's about collaboration that drives progress and innovation forward. ©2025 C# Corner. All contents are copyright of their authors.
--------------------------------------------------