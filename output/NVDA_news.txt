List of news related to NVIDIA NVDA:

Title: Wall Street is Still Pounding the Table Over Nvidia, Alphabet and Wynn
URL: https://247wallst.com/investing/2025/12/01/wall-street-is-still-pounding-the-table-over-nvidia-alphabet-and-wynn/
Time Published: 2025-12-01T15:20:32Z
Full Content:
Investing Analysts at Guggenheim says Alphabet could run even higher. The firm raised its price target on GOOG to $375 from $330 with a buy rating. Analysts at Goldman Sachs just added Wynn to its conviction buy list. Sending You to Google News in 3 © Thapana_Studio / Shutterstock.com December is historically strong for markets. In fact, “December could bring seasonal tailwinds back to the stock market and return it to all-time highs. Historically, since 1950, it’s the third-best month of the year for the Dow and S&P 500; it’s also the third-best month for the Nasdaq, since 1971, according to the Stock Trader’s Almanac,” as noted by CNBC. In addition, rate cuts are still on the table, especially after weaker-than-expected private sector jobs growth, retail sales, and consumer confidence. Plus, we strongly believe the worst of the volatility for 2025 is now behind us, as investors start to look ahead to 2026 and start to price in continued earnings growth and artificial intelligence excitement. Even better, analysts still like what they’re seeing in the markets, upgrading: Analysts at Morgan Stanley just reiterated an overweight rating on Nvidia (NASDAQ: NVDA), with a $250 price target. The firm says NVDA will maintain a dominant market share and that threats are becoming overstated. “We continue to see NVIDIA maintaining dominant market share, as threats are becoming overstated, though we aren’t sure exactly what will turn sentiment around,” they said, as quoted by CNBC. “Customers’ biggest anxiety for the next 12 months is their ability to procure enough NVIDIA product generally, and Vera Rubin specifically.” Analysts at Guggenheim say Alphabet (NASDAQ: GOOG) could run even higher. The firm raised its price target on GOOG to $375 from $330 with a buy rating. The firm is confident in Alphabet because of strong cloud backlog growth, which is being supported by enterprise AI demand. It’s also confidence with Google Gemini’s rise as a leading AI platform with rapidly growing adoption metrics, as noted by CNBC. Analysts at Goldman Sachs just added Wynn to its conviction buy list. The firm is confident with Wynn’s (NASDAQ: WYNN) “Wynn Al Marjan in the UAE in 1Q27, plus WYNN’s best-in-class Las Vegas assets, leverage to a higher-income consumers, a strong 2026 Las Vegas event calendar, and an improving backdrop in Macau should drive transformative upside at WYNN,” as quoted by CNBC. Latest Podcast Episode Amazon Strikes Back, And A Surprise $10,000 Portfolio Buy 38 min See us invest in our favorite AI stock ideas for free Live Dec 1, 2025 Nov 9, 2025 Analysts are doubling down on market leaders, with fresh upgrades for Nvidia, AMD, Walmart, Amazon, and Broadcom. Firms like Cantor… UBS is bullish on Applied Materials (NASDAQ: AMAT). Just this morning, the firm upgraded AMAT to a buy rating with… Analysts are pounding the table over Tesla (NASDAQ: TSLA). In fact, Wedbush just reiterated an outperform rating on the tech… The broad markets are mixed to kick off trading today. S&P 500 down -0.10% Dow Jones Industrial Average up 0.13%… The pullback is getting worse for markets. The S&P 500 is now below the 50-day moving average, which has been… With AI bubble talk, markets are swimming in red. However, that’s not stopping analysts from pounding the table over stocks… Even with the trade war and a high likelihood that the government won’t open until after Thanksgiving, markets are still… Analysts are still upgrading Nvidia heading into earnings. Just today, analysts at William Blair reiterated an outperform rating on Nvidia… Live Updates Get The Best NVIDIA Live Earnings Coverage Like This Every Quarter Get earnings reminders, our top analysis on…
--------------------------------------------------

Title: US stock market crashes today: Dow, S&P 500, Nasdaq all in red — Is Wall Street’s December rally already breaking as crypto tumbles?
URL: https://economictimes.indiatimes.com/news/international/us/us-stock-market-crashes-today-dow-sp-500-nasdaq-all-in-red-is-wall-streets-december-rally-already-breaking-as-crypto-tumbles/articleshow/125697378.cms
Time Published: 2025-12-01T15:05:04Z
Full Content:
US stock market crashes today as the Dow drops 127 points, the S&P 500 slips 29 points, and the Nasdaq sinks 161 points. Tech and crypto stocks drag markets lower. Bitcoin’s fall hits Bitfarms and Cleanspark. Nvidia, Snap, and Intel also decline. Q32 Bio and AHMA surge on heavy volume. December starts with weak sentiment and rising volatility. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Explore More Stories Amazon warns over 300 million users of major Black Friday impersonation scams targeting personal data What is Canada’s new colour-coded alert system and how it will ensure public safety during extreme weather Canada launches new alert system for extreme weather; Colour-coded system marks shift to impact-based forecasting Canadian Olympic swimming champion Penny Oleksiak suspended for two years over anti-doping whereabouts failures Who is Colleen Jones? Two-time World champion curler and veteran Canadian broadcaster dies at 65 What exactly is the Canada Pension Plan? New CPP payments rolling out nationwide on November 26; Check all the payment dates of 2025-26 - Why is Quebec not part of CPP? Madeleine Poulin, Radio-Canada’s first female correspondent in Ottawa and Paris, dies at 87 Yoplait recalls 'Yop yogurt' drinks in Canada over plastic contamination fears under Class 1 category - Check out which flavor and best-before dates beverage are subject to recall CMA Awards: Vince Gill honored with prestigious Willie Nelson Lifetime Achievement Award at 59th CMA Awards in Tennessee MGK rocks Canadian fans with explosive Grey Cup halftime show; Are Megan Fox, MGK really working on their relationship post daughter’s birth? Roughriders end 12-year drought, beat Alouettes for 5th Grey Cup title Who is Dr Sanjeev Sirpal? Trouble mounts for New Brunswick doctor accused of sexual assault in hospitals as he faces additional charges; what do we know so far Russian humanoid robot 'AIDOL' shockingly faceplants during much-hyped public debut - WATCH NHRC issues notice to Railways over Bhopal complaint on halal-only non-veg train meals EC extends voter roll revision; Oppn calls the exercise ‘impossible, irresponsible...’ NC’s Md. Ramzan, part of INDIA bloc says J&K polls were 'most fair' Rajya Sabha clash: Kharge vs Rijiju over Dhankar's exit PM welcomes new Rajya Sabha Chairman CP Radhakrishnan MPs react to Modi’s ‘No Drama’ call; Congress pushes for SIR debate ‘Not drama, need delivery’: PM urges oppn to focus on strong issues Winter session: ‘Sin Goods’ tax among 13 bills; oppn ready for SIR fight Ramaphosa rejects Trump’s threat to bar S Africa from G20 'Should throw her hell out of...': Trump hits out at Ilhan Omar NHRC issues notice to Railways over Bhopal complaint on halal-only non-veg train meals EC extends voter roll revision; Oppn calls the exercise ‘impossible, irresponsible...’ NC’s Md. Ramzan, part of INDIA bloc says J&K polls were 'most fair' Rajya Sabha clash: Kharge vs Rijiju over Dhankar's exit PM welcomes new Rajya Sabha Chairman CP Radhakrishnan MPs react to Modi’s ‘No Drama’ call; Congress pushes for SIR debate ‘Not drama, need delivery’: PM urges oppn to focus on strong issues Winter session: ‘Sin Goods’ tax among 13 bills; oppn ready for SIR fight Ramaphosa rejects Trump’s threat to bar S Africa from G20 'Should throw her hell out of...': Trump hits out at Ilhan Omar Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Slideshow Top Prime Articles Top Definitions Most Searched IFSC Codes Top Story Listing Private Companies Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Michael Burry says Tesla is 'ridiculously overvalued,' slams Musk pay package
URL: https://finance.yahoo.com/news/michael-burry-says-tesla-is-ridiculously-overvalued-slams-musk-pay-package-150113993.html
Time Published: 2025-12-01T15:01:13Z
Description: Short seller Michael Burry just took a swipe at another richly valued stock: Tesla.
--------------------------------------------------

Title: AI Leader Nvidia Invests $2 Billion In Chip Design Firm Synopsys
URL: https://www.investors.com/news/technology/nvidia-stock-synopsys-stock-engineering-marketing-partnership/
Time Published: 2025-12-01T13:58:45Z
Description: Artificial intelligence chip maker Nvidia announced a $2 billion investment in electronic design automation firm Synopsys.
--------------------------------------------------

Title: Why Bitcoin price (BTC USD) crashed today: December crypto market sell-off reasons explained
URL: https://economictimes.indiatimes.com/news/international/us/bitcoin-price-crash-today-btc-usd-crash-why-crypto-market-falling-sell-off-reason-explained/articleshow/125695814.cms
Time Published: 2025-12-01T13:50:00Z
Full Content:
Bitcoin price crash today: Cryptocurrencies experienced significant declines on the first trading day of December, with Bitcoin dropping 5% and other major coins like Ethereum and Solana also falling. This downturn is attributed to Federal Reserve interest rate uncertainty and volatility in AI stocks, impacting investor sentiment and leading to broad selling pressure across the crypto market. Listen to this article in summarized format Unlock AI Briefing and Premium Content Bitcoin price USD crash 2025 (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Chinese spy ships have entered Indian Ocean? Navy chief Tripathi clarifies ‘Sea will drive India’s to $5 trillion economy’: Navy Chief 'Love lost over caste': Mah woman uses bf’s blood as vermilion, accuses cops of provocation Govt tables sin tax bill in Lok Sabha, tobacco products to be dearer now NHRC issues notice to Railways over Bhopal complaint on halal-only non-veg train meals EC extends voter roll revision; Oppn calls the exercise ‘impossible, irresponsible...’ NC’s Md. Ramzan, part of INDIA bloc says J&K polls were 'most fair' Rajya Sabha clash: Kharge vs Rijiju over Dhankar's exit PM welcomes new Rajya Sabha Chairman CP Radhakrishnan MPs react to Modi’s ‘No Drama’ call; Congress pushes for SIR debate Chinese spy ships have entered Indian Ocean? Navy chief Tripathi clarifies ‘Sea will drive India’s to $5 trillion economy’: Navy Chief 'Love lost over caste': Mah woman uses bf’s blood as vermilion, accuses cops of provocation Govt tables sin tax bill in Lok Sabha, tobacco products to be dearer now NHRC issues notice to Railways over Bhopal complaint on halal-only non-veg train meals EC extends voter roll revision; Oppn calls the exercise ‘impossible, irresponsible...’ NC’s Md. Ramzan, part of INDIA bloc says J&K polls were 'most fair' Rajya Sabha clash: Kharge vs Rijiju over Dhankar's exit PM welcomes new Rajya Sabha Chairman CP Radhakrishnan MPs react to Modi’s ‘No Drama’ call; Congress pushes for SIR debate Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Prime Articles Top Slideshow Top Commodities Private Companies Top Story Listing Top Definitions Most Searched IFSC Codes Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: The Energy Breakthrough That Could Power the AI Era
URL: https://financialpost.com/globe-newswire/the-energy-breakthrough-that-could-power-the-ai-era-2
Time Published: 2025-12-01T13:31:08Z
Description: This article has been disseminated on behalf of MAX Power Mining Corp. and may include a paid advertisement. AUSTIN, Texas, Dec. 01, 2025 (GLOBE NEWSWIRE) — MiningNewsWire Editorial Coverage: Global electricity demand is entering a historic inflection point. …
--------------------------------------------------

Title: The Energy Breakthrough That Could Power the AI Era
URL: https://www.globenewswire.com/news-release/2025/12/01/3197002/0/en/The-Energy-Breakthrough-That-Could-Power-the-AI-Era.html
Time Published: 2025-12-01T13:30:00Z
Full Content:
December 01, 2025 08:30 ET | Source: MiningNewsWire MiningNewsWire This article has been disseminated on behalf of MAX Power Mining Corp. and may include a paid advertisement. AUSTIN, Texas, Dec. 01, 2025 (GLOBE NEWSWIRE) -- MiningNewsWire Editorial Coverage: Global electricity demand is entering a historic inflection point. The International Energy Agency (IEA) now forecasts that worldwide data center electricity consumption will nearly double by 2030, with AI-driven centers multiplying their energy use more than four-fold over the same period, a pace that strains already overloaded grids across the United States, China, Europe, Southeast Asia and elsewhere. The constraint is no longer bandwidth or chip capability — it is electricity itself. Amid this tightening energy landscape, a compelling new frontier is emerging natural hydrogen, a geologic form of hydrogen being generated continuously within the Earth’s subsurface. Unlike manufactured hydrogen, geologic hydrogen can be produced without electrolysis and emits only water when used for energy production. It may represent the first scalable, low-carbon baseload power source for AI-era demand. That is why MAX Power Mining Corp. (OTC: MAXXF) (CSE: MAXX) (profile) has become the first publicly traded company in North America to advance a massive land package (1.3 million acres) permitted specifically for natural hydrogen exploration and development, including a commercial-scale natural hydrogen well, positioning itself at the forefront of a new energy class. MAX Power is working to establish itself as a leader among well-known companies that are innovating and leading in the AI space, including Microsoft Corporation (NASDAQ: MSFT), Apple Inc. (NASDAQ: AAPL), NVIDIA Corporation (NASDAQ: NVDA) and Alphabet Inc. (NASDAQ: GOOG).Disclosure: This does not represent material news, partnerships, or investment advice. Click here to view the custom infographic of the MAX Power Mining Corp. editorial. The Global Energy Crunch Meets a New ResourceArtificial intelligence is expanding at a pace that far exceeds the design capacity of existing electricity systems. In addition to IEA’s forecast, Bloomberg projections suggest that U.S. data centers alone could consume as much as 9% of all American electricity by 2035, a dramatic increase that underscores the shifting energy footprint of digital infrastructure. The IEA also reports that per-capita U.S. data-center usage could reach 1,200 kilowatt-hours per year, greatly surpassing consumption levels in most other countries and highlighting the intense energy requirements of emerging technologies. Hydrogen is often highlighted as a key decarbonization tool, yet nearly 99% of global hydrogen production comes from fossil fuels, resulting in significant emissions and high costs. Green hydrogen, while cleaner, requires large amounts of renewable electricity and is expensive to scale. This widening mismatch between electricity supply and clean-energy demand is prompting rapid scientific and commercial interest in geologic hydrogen. Unlike manufactured hydrogen, natural hydrogen arises from water–rock interactions, tectonic processes, and mineral reactions occurring deep underground. The U.S. Geological Survey and other research groups have noted that naturally occurring hydrogen may be far more abundant than previously understood, describing natural hydrogen as a possible near-limitless clean fuel generated by Earth itself. Investors are also taking notice. CNBC reports that Bill Gates and Jeff Bezos have backed Koloma, one of the leading natural hydrogen exploration companies, calling the emerging field a gold hydrogen rush. Natural hydrogen offers logistical advantages as well. If sourced near industrial corridors or AI compute clusters, it can serve as a continuous, on-site baseload power source. This would reduce the need for extensive transmission infrastructure and enable operators to place computing power directly above their energy source. For the first time, a clean and potentially low-cost resource may be emerging that aligns with the unprecedented demands of modern computing and electrification. MAX Power Delivers a Historic Milestone As scientific interest grows, MAX Power has made a landmark contribution by delivering the most extensive subsurface indications of natural hydrogen ever recorded in Canada, and potentially the first large-scale commercial discovery globally. This achievement marks a significant step forward for natural hydrogen development globally and positions North America as the breakout leader for hydrogen-focused geological study. MAX Power controls approximately 1.3 million acres of permitted land in Saskatchewan. The region has been studied for its subsurface gas potential, including helium and hydrogen precursors that form in geological systems known to generate natural hydrogen. Through these holdings, the company has successfully drilled North America’s first commercial-scale well dedicated to evaluating natural hydrogen at depth. Saskatchewan’s geological environment, with its mix of sedimentary basins, fault systems, and reactive rock formations, provides a strong basis for exploration. Government and academic institutions have noted that these subsurface conditions may be conducive to hydrogen generation. MAX Power’s approach focuses on testing these zones directly rather than relying solely on geological modeling. This form of empirical measurement represents an important advance, as it provides real data that can be used to validate long-standing scientific theories. The company’s early findings highlight why the region may have significant potential and why global attention is turning toward places with large-scale land packages capable of hosting hydrogen systems. The Lawson Well: From Theory to Measured Evidence MAX Power’s Lawson well provides the first technical data set confirming the presence of natural hydrogen in the Canadian subsurface. The company reported natural hydrogen across multiple horizons, supported by gas sampling, chemical analysis and downhole logging tools capable of identifying trace hydrogen concentrations. These indicators are significant because they move natural hydrogen from theoretical potential to measurable evidence within the region. Prior to this work, much of the scientific understanding relied on surface seeps and global analogs. The Lawson results demonstrate that hydrogen may be present in multiple geological layers, broadening the scope for future exploration. The data sets collected from Lawson will be among the most comprehensive ever assembled for natural hydrogen anywhere in the world. This includes gas-geochemical measurements, core and cuttings analysis, and structural data that may help map hydrogen migration pathways. These findings will influence the design of future wells, including decisions about target depth, casing, and resource evaluation strategies. Independent analysis is currently underway by third-party scientific institutions. These assessments will help determine whether the hydrogen observed corresponds to deeper, potentially productive reservoirs. Confirmation of such systems would strengthen the case for developing a broader natural hydrogen industry in Saskatchewan. If follow-up wells exhibit similar results, Canada could become the first region in North America to establish a commercial natural hydrogen basin. The Genesis Trend: A Potential Basin-Scale System MAX Power’s exploration footprint includes the Genesis Trend, a minimum 300-mile geological corridor that extends from Saskatchewan into Montana and the Dakotas. The Genesis Trend contains structural features and rock types associated with hydrogen generation in other parts of the world, including deep crustal faults, reactive ultramafic rocks and pathways for fluid movement. MAX Power’s permitted acreage across the province spans 1.3 million acres, with an additional 5.7 million acres under review, creating the opportunity to evaluate whether the region hosts a large-scale hydrogen system. A second fully funded well, located approximately 200 miles from the Lawson site, is expected to soon test a different structural region. If drilling in this area reveals similar hydrogen signatures, it would greatly strengthen the interpretation that Saskatchewan may contain a basin-wide hydrogen system rather than isolated pockets. This kind of large-scale potential aligns with global research published in “Science Advances,” which describes how tectonically active geological settings can generate sustained hydrogen through ongoing rock-water reactions. Continuous hydrogen generation is considered one of the most compelling features of natural hydrogen systems. Unlike fossil fuels, which take millions of years to accumulate and are finite, geologic hydrogen may be produced continuously by Earth’s internal processes. If commercial flow rates are established, this could create a renewable-like energy resource with implications for grid baseload supply and industrial energy use. Leading Global Scientists Join the Effort MAX Power has attracted collaboration from respected scientific institutions as interest in natural hydrogen grows. According to company reports, experts from the Petroleum Technology Research Centre, the University of Regina, the Saskatchewan Geological Survey, and the Colorado School of Mines recently visited the Lawson site and are participating in data analysis. The involvement of these organizations lends credibility to MAX Power’s findings and strengthens the scientific understanding of natural hydrogen systems in Canada. These groups bring expertise in subsurface analysis, basin modeling, mineralogy, and gas characterization, all disciplines essential to determining whether hydrogen accumulations could become commercially viable. The Petroleum Technology Research Centre is well known for its work in subsurface resource evaluation, including carbon capture and geological storage. The University of Regina and Saskatchewan Geological Survey contribute regional geological knowledge and scientific rigor into the study of hydrogen-bearing formations. Specialists from the Colorado School of Mines add further insight from global research programs examining natural hydrogen potential. This multi-institution approach ensures that the data collected is interpreted using internationally recognized scientific standards. It also helps support Canada’s emergence as a research hub for geologic hydrogen, especially as global interest expands in response to the world’s increasing energy challenges. Leadership Aligns with Pivotal Moment MAX Power recently appointed Ran Narayanasamy as chief executive officer, bringing nearly two decades of experience from SaskPower and leadership at the Petroleum Technology Research Centre. His background in energy systems, subsurface research, and transition planning provides the company with expertise aligned to the technical and regulatory demands of natural hydrogen development. Narayanasamy’s long-standing relationships across energy, academic and government sectors may help accelerate the company’s progress as it advances Canada’s first dedicated natural hydrogen well. His leadership arrives at a critical moment, as the company transitions from early scientific confirmation toward testing commercial viability. As the first-mover public company in this space, MAX Power holds approximately 1.3 million permitted acres in Saskatchewan and has identified multiple high-priority targets. The company has drilled Canada’s first deep well specifically designed to evaluate natural hydrogen as a commercial resource. If these efforts confirm consistent flow rates, natural hydrogen could become a significant new source of clean baseload power positioned directly near industrial centers and agricultural corridors. The implications of such a discovery would be substantial. Natural hydrogen could complement or even replace some manufactured hydrogen pathways, offering cleaner production and potentially lower costs. The combination of scientific progress, expanding technical datasets, and strengthened executive leadership positions MAX Power at the forefront of an evolving clean-energy sector. With growing interest from policymakers, researchers and global investors alike, the company’s development timeline coincides with what may be the early stage of the next major energy transition. AI Leaders Advance Next-Generation Intelligent Technologies Across the global technology landscape, leading innovators continue to push the boundaries of what artificial intelligence can deliver. New breakthroughs in enterprise automation, chip architecture, generative-AI development and multimodal reasoning are reshaping how organizations operate and how individuals interact with digital tools. Microsoft Corporation (NASDAQ: MSFT) was named a leader in the first AI-centric IDC MarketScape: Worldwide AI-Enabled Large Enterprise ERP Applications 2025 Vendor Assessment. Microsoft believes this recognition highlights the strength of Dynamics 365, particularly its Copilot and agent capabilities, redefining procure-to-pay and record-to-report processes with intelligent automation, predictive insights and streamlined integration. Apple Inc. (NASDAQ: AAPL) has launched M5, delivering the next big leap in AI performance and advances to nearly every aspect of the chip. Built using third-generation 3-nanometer technology, M5 introduces a next-generation 10-core GPU architecture with a Neural Accelerator in each core, enabling GPU-based AI workloads to run dramatically faster, with more than four times the peak GPU compute performance compared to M4.1. NVIDIA Corporation (NASDAQ: NVDA) has released NVIDIA Blueprints, which include everything an enterprise developer needs to build and deploy customized generative AI applications that make a transformative impact on business objectives. NVIDIA Blueprints are reference AI workflows tailored for specific use cases. They include sample applications built with NVIDIA NIM and partner microservices, reference code, customization documentation and a Helm chart for deployment. Alphabet Inc. (NASDAQ: GOOG) has announced Gemini 3, the best model in the world for multimodal understanding and the company’s most powerful agentic and vibe coding model. Gemini 3 delivers richer visualizations and deeper interactivity, all built on a foundation of state-of-the-art reasoning. The company is releasing Gemini 3 Pro in preview and making it available across a suite of Google products so users can use it in their daily lives to learn, build and plan anything. As these advancements move from research labs into everyday products and workflows, the possibilities for transformation expand dramatically. The latest wave of AI-driven systems promises faster insights, more intuitive user experiences, and powerful new foundations for innovation. Together, these developments signal a future in which intelligent technologies play an increasingly central role in how businesses grow, create, and solve complex challenges. For further information about MAX Power Mining Corp., visit the MAX Power Mining profile. About MiningNewsWire MiningNewsWire (“MNW”) is a specialized communications platform with a focus on developments and opportunities in the Global Mining and Resources sectors. It is one of 70+ brands within the Dynamic Brand Portfolio @ IBN that delivers: (1) access to a vast network of wire solutions via InvestorWire to efficiently and effectively reach a myriad of target markets, demographics and diverse industries; (2) article and editorial syndication to 5,000+ outlets; (3) enhanced press release enhancement to ensure maximum impact; (4) social media distribution via IBN to millions of social media followers; and (5) a full array of tailored corporate communications solutions. With broad reach and a seasoned team of contributing journalists and writers, MNW is uniquely positioned to best serve private and public companies that want to reach a wide audience of investors, influencers, consumers, journalists and the general public. By cutting through the overload of information in today’s market, MNW brings its clients unparalleled recognition and brand awareness.MNW is where breaking news, insightful content and actionable information converge. To receive SMS alerts from MiningNewsWire, text “BigHole” to 888-902-4192 (U.S. Mobile Phones Only)For more information, please visit https://www.MiningNewsWire.com Please see full terms of use and disclaimers on the MiningNewsWire website applicable to all content provided by MNW, wherever published or republished: https://www.MiningNewsWire.com/Disclaimer MiningNewsWireLos Angeles, CAwww.MiningNewsWire.com310.299.1717 OfficeEditor@MiningNewsWire.com MiningNewsWire is powered by IBN
--------------------------------------------------

Title: NVIDIA and Synopsys Announce Strategic Partnership to Revolutionize Engineering and Design
URL: https://nvidianews.nvidia.com/news/nvidia-and-synopsys-announce-strategic-partnership-to-revolutionize-engineering-and-design
Time Published: 2025-12-01T13:00:00Z
Full Content:
Key Highlights NVIDIA and Synopsys, Inc. (NASDAQ: SNPS) today announced an expanded, strategic partnership to revolutionize design and engineering across industries. R&D teams, from the semiconductor industry to aerospace, automotive, industrial and beyond, face significant engineering challenges including increasing workflow complexity, escalating development costs and time-to-market pressure. This expanded partnership will integrate the strengths of NVIDIA’s AI and accelerated computing with Synopsys’ market-leading engineering solutions to deliver capabilities enabling R&D teams to design, simulate and verify intelligent products with greater precision, speed and at lower cost. In addition, NVIDIA invested $2 billion in Synopsys common stock at a purchase price of $414.79 per share. “CUDA GPU-accelerated computing is revolutionizing design — enabling simulation at unprecedented speed and scale, from atoms to transistors, from chips to complete systems, creating fully functional digital twins inside the computer,” said Jensen Huang, founder and CEO of NVIDIA. “Our partnership with Synopsys harnesses the power of NVIDIA accelerated computing and AI to reimagine engineering and design — empowering engineers to invent the extraordinary products that will shape our future.” “The complexity and cost of developing next-generation intelligent systems demands engineering solutions with a deeper integration of electronics and physics, accelerated by AI capabilities and compute. No two companies are better positioned to deliver AI-powered, holistic system design solutions than Synopsys and NVIDIA,” said Sassine Ghazi, president and CEO of Synopsys. “Together we will re-engineer engineering and empower innovators everywhere to more efficiently realize their innovations.” Joint Development to Enable Future of Engineering on Accelerated Computing The multiyear partnership builds on strong, existing technology collaborations between the companies and includes the following initiatives: This partnership is not exclusive. NVIDIA and Synopsys continue to partner with the broader semiconductor and electronic design automation (EDA) ecosystem to create shared growth opportunities for the future of engineering and design. Press Conference: The CEOs of NVIDIA and Synopsys will conduct a webcast press conference today at 7 a.m. PST (10 a.m. EST) to discuss the announcement. The webcast will be available for the public to listen in here: https://events.q4inc.com/attendee/183492820 About NVIDIA NVIDIA (NASDAQ: NVDA) is the world leader in AI and accelerated computing. NVIDIA Forward-Looking Statements Certain statements in this press release including, but not limited to, statements as to: CUDA GPU-accelerated computing revolutionizing design — enabling simulation at unprecedented scale, from atoms to transistors, from chips to complete systems, creating fully functional digital twins inside the computer; NVIDIA’s partnership with Synopsys harnessing the power of NVIDIA accelerated computing and AI to reimagine engineering and design — empowering engineers to explore more ideas, simulate faster, and invent the extraordinary products that will shape the future; expectations with respect to growth, performance and benefits of NVIDIA’s products, services, and technologies, and related trends and drivers; expectations with respect to supply and demand for NVIDIA’s products, services, and technologies; expectations with respect to NVIDIA’s third party arrangements, including with its collaborators and partners; expectations with respect to technology developments and related trends and drivers; expectations with respect to market growth and trends; expectations with respect to AI and related industries; and other statements that are not historical facts are forward-looking statements within the meaning of Section 27A of the Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934, as amended, which are subject to the “safe harbor” created by those sections based on management’s beliefs and assumptions and on information currently available to management and are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic and political conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners’ products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; and changes in applicable laws and regulations, as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company’s website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances. © 2025 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, CUDA-X, NVIDIA Cosmos, NVIDIA NeMo, Nemotron, NVIDIA NIM and NVIDIA Omniverse are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice. About Synopsys Synopsys, Inc. (Nasdaq: SNPS) is the leader in engineering solutions from silicon to systems, enabling customers to rapidly innovate AI-powered products. We deliver industry-leading silicon design, IP, simulation and analysis solutions, and design services. We partner closely with our customers across a wide range of industries to maximize their R&D capability and productivity, powering innovation today that ignites the ingenuity of tomorrow. Learn more at www.synopsys.com. © 2025 Synopsys, Inc. All rights reserved. Synopsys, Ansys, the Synopsys and Ansys logos, and other Synopsys trademarks are available at https://www.synopsys.com/company/legal/trademarks-brands.html. Other company or product names may be trademarks of their respective owners. Synopsys Forward-Looking Statements This press release includes certain forward-looking statements regarding the demand and market outlook, products, business, strategies and opportunities of Synopsys and NVIDIA, including the benefits, impact and performance of NVIDIA’s AI and accelerated computing platform and Synopsys’ engineering solutions; Synopsys and NVIDIA’s collaboration efforts to redesign engineering and design across industries; expectations regarding the anticipated benefits of the multi-year partnership and specific initiatives, including for engineering teams and customers; expectations regarding the ability to harness AI efficiencies; and expectations with respect to technology developments and trends. These statements involve risks, uncertainties and other factors that could cause actual results, time frames or achievements to differ materially from those expressed or implied in such forward-looking statements. Such risks, uncertainties and factors include but are not limited to macroeconomic environment and global economic conditions; NVIDIA’s reliance on third parties to manufacture, assemble, package and test its products; the impact of technological development and competition; development of new products and technologies or enhancements to Synopsys’ and NVIDIA’s existing product and technologies; market acceptance of Synopsys’, NVIDIA’s or their partners’ products; changes in consumer preferences or demands; changes in industry standards and interfaces; and changes in applicable laws and regulations, including the impact of China export control restrictions; and the risks more fully described in filings Synopsys and NVIDIA make with the SEC from time to time, including in the “Risk Factors” section of their respective Annual Reports on Form 10-K, Quarterly Reports on Form 10-Q and other documents filed by either of them from time to time with the SEC. The information provided herein is as of the date hereof. Synopsys and NVIDIA assume no obligation and do not intend to update or revise any forward-looking statement, whether as a result of new information, future events or otherwise, unless required by law. Neither Synopsys nor NVIDIA gives any assurance that either Synopsys or NVIDIA will achieve its expectations. Many of the features and products described herein remain in various stages and will be offered on a when-and-if-available basis. The statements above are not intended to be, and should not be interpreted as a commitment, promise, or legal obligation, and the development, release, and timing of any features or functionalities described are subject to change and remains at the sole discretion of Synopsys. Synopsys will have no liability for failure to deliver or delay in the delivery of any of the products, features or functions set forth herein.
--------------------------------------------------

Title: US stock futures drift lower as Dow, S&P 500, Nasdaq slip today: Here are the top gainers and losers shaping Wall Street, with big earnings on deck
URL: https://economictimes.indiatimes.com/news/international/us/us-stock-futures-drift-lower-as-dow-sp-500-nasdaq-slip-today-here-are-the-top-gainers-and-losers-shaping-wall-street-with-big-earnings-on-deck/articleshow/125692196.cms
Time Published: 2025-12-01T10:49:30Z
Full Content:
US stock futures fell early Monday, with the Dow down 191 points, the S&P 500 off 37 points, and the Nasdaq lower by 171.50 points as yields hovered near 4.04%. NIO dropped 4%, Nvidia slipped 1.05%, and crypto miners tumbled. INVO Fertility jumped 16.7% and Twin Vee gained 7.34%. Markets await key tech earnings and the Fed’s rate decision. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Explore More Stories Amazon warns over 300 million users of major Black Friday impersonation scams targeting personal data What is Canada’s new colour-coded alert system and how it will ensure public safety during extreme weather Canada launches new alert system for extreme weather; Colour-coded system marks shift to impact-based forecasting Canadian Olympic swimming champion Penny Oleksiak suspended for two years over anti-doping whereabouts failures Who is Colleen Jones? Two-time World champion curler and veteran Canadian broadcaster dies at 65 What exactly is the Canada Pension Plan? New CPP payments rolling out nationwide on November 26; Check all the payment dates of 2025-26 - Why is Quebec not part of CPP? Madeleine Poulin, Radio-Canada’s first female correspondent in Ottawa and Paris, dies at 87 Yoplait recalls 'Yop yogurt' drinks in Canada over plastic contamination fears under Class 1 category - Check out which flavor and best-before dates beverage are subject to recall CMA Awards: Vince Gill honored with prestigious Willie Nelson Lifetime Achievement Award at 59th CMA Awards in Tennessee MGK rocks Canadian fans with explosive Grey Cup halftime show; Are Megan Fox, MGK really working on their relationship post daughter’s birth? Roughriders end 12-year drought, beat Alouettes for 5th Grey Cup title Who is Dr Sanjeev Sirpal? Trouble mounts for New Brunswick doctor accused of sexual assault in hospitals as he faces additional charges; what do we know so far Russian humanoid robot 'AIDOL' shockingly faceplants during much-hyped public debut - WATCH NC’s Md. Ramzan, part of INDIA bloc says J&K polls were 'most fair' Rajya Sabha clash: Kharge vs Rijiju over Dhankar's exit PM welcomes new Rajya Sabha Chairman CP Radhakrishnan MPs react to Modi’s ‘No Drama’ call; Congress pushes for SIR debate ‘Not drama, need delivery’: PM urges oppn to focus on strong issues Winter session: ‘Sin Goods’ tax among 13 bills; oppn ready for SIR fight Ramaphosa rejects Trump’s threat to bar S Africa from G20 'Should throw her hell out of...': Trump hits out at Ilhan Omar Delhi Police bust Pakistan-backed module Parliament Winter Session | Kiren Rijiju says opposition wants SIR debate NC’s Md. Ramzan, part of INDIA bloc says J&K polls were 'most fair' Rajya Sabha clash: Kharge vs Rijiju over Dhankar's exit PM welcomes new Rajya Sabha Chairman CP Radhakrishnan MPs react to Modi’s ‘No Drama’ call; Congress pushes for SIR debate ‘Not drama, need delivery’: PM urges oppn to focus on strong issues Winter session: ‘Sin Goods’ tax among 13 bills; oppn ready for SIR fight Ramaphosa rejects Trump’s threat to bar S Africa from G20 'Should throw her hell out of...': Trump hits out at Ilhan Omar Delhi Police bust Pakistan-backed module Parliament Winter Session | Kiren Rijiju says opposition wants SIR debate Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Story Listing Top Prime Articles Top Definitions Top Slideshow Private Companies Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq futures sink, bitcoin plummets in downbeat start to December
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-futures-sink-bitcoin-plummets-in-downbeat-start-to-december-011408181.html
Time Published: 2025-12-01T01:14:08Z
Description: Wall Street's strong late-November rebound looks set to hit a speed bump on the first trading day of December.
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq futures retreat from rally in downbeat start to December
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-futures-retreat-from-rally-in-downbeat-start-to-december-011408498.html
Time Published: 2025-12-01T01:14:08Z
Description: Wall Street's strong late-November rebound looks set to hit a speed bump on the first trading day of December.
--------------------------------------------------

Title: These 5 charts hint at where stocks might go next after a wild November for the market
URL: https://www.marketwatch.com/story/these-5-charts-hint-at-where-stocks-might-go-next-after-a-wild-november-for-the-market-47fd5583
Time Published: 2025-11-30T17:00:00Z
Description: November historically has been a strong month for the U.S. stock market. This year’s gains didn’t come easy.
--------------------------------------------------

Title: Experts Weigh In: Can Apple’s Stock Inch Past $300 per Share Before Year’s End?
URL: https://finance.yahoo.com/news/experts-weigh-apple-stock-inch-153905971.html
Time Published: 2025-11-30T15:39:05Z
Description: Apple shares have been surging lately. But can they reach $300 before 2025 ends? Analysts reveal some key factors that could drive Apple’s next big move.
--------------------------------------------------

Title: Stocks drift back toward record highs as the final month of 2025 gets underway: What to watch this week
URL: https://finance.yahoo.com/news/stocks-drift-back-toward-record-highs-as-the-final-month-of-2025-gets-underway-what-to-watch-this-week-122743521.html
Time Published: 2025-11-30T12:27:43Z
Description: As the market moves into December, investors will be watching for Fed updates and a steadier performance environment than they got in November.
--------------------------------------------------

Title: Google TPUv7: “perf per TCO advantage of TPUs is so strong that you already get the gains from adopting TPUs even before turning one on”
URL: https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the
Time Published: 2025-11-30T05:47:02Z
Full Content:
The two best models in the world, Anthropic’s Claude 4.5 Opus and Google’s Gemini 3 have the majority of their training and inference infrastructure on Google’s TPUs and Amazon’s Trainium. Now Google is selling TPUs physically to multiple firms. Is this the end of Nvidia’s dominance? The dawn of the AI era is here, and it is crucial to understand that the cost structure of AI-driven software deviates considerably from traditional software. Chip microarchitecture and system architecture play a vital role in the development and scalability of these innovative new forms of software. The hardware infrastructure on which AI software runs has a notably larger impact on Capex and Opex, and subsequently the gross margins, in contrast to earlier generations of software, where developer costs were relatively larger. Consequently, it is even more crucial to devote considerable attention to optimizing your AI infrastructure to be able to deploy AI software. Firms that have an advantage in infrastructure will also have an advantage in the ability to deploy and scale applications with AI. Google had peddled the idea of building AI-specific infrastructure as far back as 2006, but the problem came to a boiling point in 2013. They realized they needed to double the number of datacenters they had if they wanted to deploy AI at any scale. As such, they started laying the groundwork for their TPU chips which were put into production in 2016. It’s interesting to compare this to Amazon, who in the same year, realized they needed to build custom silicon too. In 2013, they started the Nitro Program, which was focused on developing silicon to optimize general-purpose CPU computing and storage. Two very different companies optimized their efforts for infrastructure for different eras of computing and software paradigms. We’ve long believed that the TPU is among the world’s best systems for AI training and inference, neck and neck with king of the jungle Nvidia. 2.5 years ago we wrote about TPU supremacy, and this thesis has proven to be very correct. TPU’s results speak for themselves: Gemini 3 is one of the best models in the world and was trained entirely on TPUs. In this report, we will talk about the huge changes in Google’s strategy to properly commercialize the TPU for external customers, becoming the newest and most threatening merchant silicon challenger to Nvidia. We plan to: (Re-)Educate our clients and new readers about the rapidly growing commercial success of external TPU customers, starting with Anthropic and extending to Meta, SSI, xAI and even potentially OpenAI... Show that: The more (TPU) you buy, the more (NVIDIA GPU capex) you save! OpenAI hasn’t even deployed TPU yet and already increased perf per TCO by getting ~30% off their compute fleet due to competitive threats Explain the circular economy deals for AI Infrastructure. Revisit our original TPU deep dive with a refresher on the TPU hardware stack from silicon down to the software layer. Cover the positive developments on the open software ecosystem front as well as the critical missing ingredient for Google to make the TPU ecosystem a viable challenger to the CUDA moat: open source their XLA:TPU compiler, runtime, and multi-pod “MegaScaler” code. In the paywall, we will discuss the implications for Nvidia’s moat and compare Vera Rubin to the next gen TPUv8AX/8X (aka Sunfish/Zebrafish) Also cover the long term threat to Nvidia. First, let’s talk about the impact this news has had on the ecosystem. TPU performance has clearly caught the attention of its rivals. Sam Altman has acknowledged “rough vibes” ahead for OpenAI as Gemini has stolen the thunder from OpenAI. Nvidia even put out a reassuring PR telling everyone to keep calm and carry on — we are well ahead of the competition. We understand why. These past few months have been win after win for the Google Deepmind, GCP, and TPU complex. The huge upwards revisions to TPU production volumes, Anthropic’s >1GW TPU buildout, SOTA models Gemini 3 and Opus 4.5 trained on TPU, and now an expanding list of clients being targeted (Meta, SSI, xAI, OAI) lining up for TPUs. This has driven a huge re-rating of the Google and TPU supply chain at the expense of the Nvidia GPU-focused supply chain. While the “sudden” emergence of Google and the TPU supply chain has caught many by surprise, SemiAnalysis institutional product subscribers have been anticipating this for the last year. Another reason Nvidia has been on the defensive is a growing chorus of skeptics who argue the company is propping up a “circular economy” by funding cash-burning AI startups, essentially moving money from one pocket to another with extra steps. We think this view is misplaced, but it has clearly struck a nerve inside Nvidia. The finance team issued a detailed response, reproduced below. We think a more realistic explanation is that Nvidia aims to protect its dominant position at the foundation labs by offering equity investment rather than cutting prices, which would lower Gross margins and cause widespread investor panic. Below, we outline the OpenAI and Anthropic arrangements to show how frontier labs can lower GPU TCO by buying, or threatening to buy TPUs. OpenAI hasn’t even deployed TPUs yet and they’ve already saved ~30% on their entire lab wide NVIDIA fleet. This demonstrates how the perf per TCO advantage of TPUs is so strong that you already get the gains from adopting TPUs even before turning one on. Our Accelerator Industry Model, Datacenter Industry Model and Core Research subscribers saw the industry implications well before this was announced and became market consensus. In early August, we shared with our Accelerator Model clients that we saw massive upward revisions for Broadcom / Google TPU orders in the supply chain for 2026. We also revealed that the reason for these order increases was the fact that Google would begin selling systems externally to multiple customers. In early September, we revealed that one of the big external customers will be Anthropic, with demand of at least 1 million TPUs. This was officially confirmed by Anthropic and Google in October. We also called out Meta as a big TPU customer on the 7th of November, weeks before others. In addition we have discussed other customers as well. As a result, our institutional clients have had plenty of heads up on one of the largest performance dispersions in the AI Trade to date. SemiAnalysis was the first to break all these insights because no other research firm can connect the dots from the fabs to supply chain through the datacenters to the labs. To get access to these insights as and stay ahead of the curve: sales@semianalysis.com Onto the deal. The TPU stack has long rivaled Nvidia’s AI hardware, yet it has mostly supported Google’s internal workloads. In typical Google fashion, it never fully commercialized the TPU even after making it available to GCP customers in 2018. That is starting to change. Over the past few months, Google has mobilized efforts across the whole stack to bring TPUs to external customers through GCP or by selling complete TPU systems as a merchant vendor. The search giant is leveraging its strong in-house silicon design capabilities to become a truly differentiated cloud provider. Furthermore, it aligns with marquis customer Anthropic’s continued push to diversify away from its dependence on NVDA. The Anthropic deal marks a major milestone in this push. We understand that GCP CEO Thomas Kurian played a central role in the negotiations. Google committed early by investing aggressively in Anthropic’s funding rounds, even agreeing to no voting rights and a 15% cap on their ownership to expand the use of TPUs beyond internal Google. This strategy was eased by the presence of former DeepMind TPU talent within the foundation lab, resulting in Anthropic training Sonnet and Opus 4.5 on multiple types of hardware including TPUs. Google has already built a substantial facility for Anthropic, as shown below as part of our building-by-building tracker of AI labs. Beyond renting capacity in Google datacenters through GCP, Anthropic will deploy TPUs in its own facilities, positioning Google to compete directly with Nvidia as a true merchant hardware vendor. In terms of the split of the 1M TPUs: The first phase of the deal covers 400k TPUv7 Ironwoods, worth ~$10 billion in finished racks that Broadcom will sell directly to Anthropic. Anthropic is the fourth customer referenced in Broadcom’s most recent earnings call. Fluidstack, a gold-rated ClusterMax Neocloud provider, will handle on-site setup, cabling, burn-in, acceptance testing, and remote hands work as Anthropic offloads managing physical servers. DC infrastructure will be supplied by TeraWulf (WULF) and Cipher Mining (CIFR). The remaining 600k TPUv7 units will be rented through GCP in a deal we estimate at $42 billion of RPO, accounting for most of the $49 billion increase in GCP’s backlog reported in the third quarter. We believe additional deals with Meta, OAI, SSI, and xAI could provide additional RPO + direct hardware sales for GCP in coming quarters. Despite heavy internal and external demand, Google has not been able to deploy TPUs at the pace it wants. Even though it has more control over its hardware supply than other hyperscalers that still need curry favor with Jensen, Google’s main bottleneck is power. While other hyperscalers have expanded their own sites and secured significant colocation capacity, Google has moved more slowly. We believe the core issue is contractual and administrative. Each new datacenter vendor requires a Master Services Agreement, and these are multibillion-dollar, multiyear commitments that naturally involve some bureaucracy. Yet Google’s process is especially slow, often taking up to three years from initial discussions to a signed MSA. Google’s workaround carries major implications for Neocloud providers and cryptominers looking to pivot to AI DC infrastructure. Instead of leasing directly, Google offers a credit backstop, an off-balance-sheet “IOU” to step in if Fluidstack cannot pay its datacenter rent. Neoclouds like Fluidstack are nimble and flexible, making it easier for them to deal with new datacenter vendors like reformed cryptominers. This mechanic has been key to our bullish views on the cryptomining industry – notably we were calling out numerous including IREN and Applied Digital at the beginning of the year when stock prices were materially lower. The opportunity for miners rests on a simple dynamic: the datacenter industry faces acute power constraints power, and cryptominers already control capacity through their PPAs and existing electrical infrastructure. We expect many more agreements to follow in the coming weeks and quarters. Prior to the Google/Fluidstack/TeraWulf deal, we had not seen any deal with a mere off-balance-sheet “IOU” in the Neocloud market. After the deal, we believe it has become the new de-facto standard financing template. This solves a key headache for Neoclouds looking to secure datacenter capacity and grow their business: A GPU cluster has a useful and economic life of 4-5years A large datacenter lease is typically 15+ years, with a typical payback period of ~8 years. This duration mismatch has made it very complicated for both Neoclouds and Datacenter vendors to secure financing for projects. But with the rise of the “hyperscaler backstop”, we believe the financing issues are solved. We expect a new wave of growth for the NeoCloud industry. Check out our Accelerator and Datacenter models to understand the key beneficiaries. These are the how’s and why’s behind the Anthropic deal, now let’s get into the hardware. Furthermore, Neoclouds who count Jensen as an investor such as CoreWeave, Nebius, Crusoe, Together, Lambda, Firmus, and Nscale all have a notable incentive to not adopt any competing technology in their datacenter: TPUs, AMD GPUs, or even Arista switches are off limits! This leaves a gaping hole in the market for TPU hosting that is currently filled by a combination of crypto miners + Fluidstack. In the coming months, we expect to see more Neoclouds make the tough decision between pursuing a growing TPU hosting opportunity and securing allocations of the latest and greatest Nvidia Rubin systems. The answer is simple. It is a strong chip inside an excellent system, and that combination offers compelling performance and TCO for Anthropic. 2.5 years ago, we wrote about Google’s compute infrastructure advantage. Even with silicon that lagged Nvidia’s on paper, Google’s system-level engineering allowed the TPU stack to match Nvidia in both performance and cost efficiency. We argued then that “systems matter more than microarchitecture,” and the past two years have reinforced that view. Anthropic’s massive TPU orders are a direct validation of the platform’s technical strength. The GPU ecosystem has shifted forward as well. Nvidia’s GB200 represents a major leap forward, pushing Nvidia toward becoming a true systems company that designs full servers rather than only the chip package inside. While we are on the topic of the GB200’s huge innovation in rack-scale interconnect, one underappreciated point is that Google has been scaling up TPUs within and across racks since TPU v2 back in 2017! Further down in the report, we feature a deep dive on Google’s ICI scale-up networking, the only real rival to Nvidia’s NVLink. Google’s recent Gemini 3 model is now viewed as the state of the art frontier LLM. Like all earlier versions of Gemini, it was trained entirely on TPUs. That result offers concrete proof of both TPU capability and Google’s broader infrastructure advantage. Today’s attention often centers on hardware for inference and post-training, yet pre-training a frontier model remains the hardest and most resource-intensive challenge in AI hardware. The TPU platform has passed that test decisively. This stands in sharp contrast to rivals: OpenAI’s leading researchers have not completed a successful full-scale pre-training run that was broadly deployed for a new frontier model since GPT-4o in May 2024, highlighting the significant technical hurdle that Google’s TPU fleet has managed to overcome. One of the key highlights of the new model include noticeable gains in tool calling and agentic capability, especially on longer-horizon tasks for economically valuable tasks. Vending Bench is an evaluation that aims to measure how well models would run a business over a long period of time by placing them as the owner of a simulated vending machine business and Gemini 3 destroyed the competition. This launch brought not just improved capabilities but new products. Antigravity, a product born from the acqui-hire of former Windsurf CEO Varun Mohan and team, is Google’s answer to OpenAI’s Codex, officially entering Gemini into the vibe coding token guzzling wars. For Google to quietly muscle in and establish a performance lead in one of the most challenging hardware problems is a truly impressive feat for a company whose core business isn’t, or should we say, wasn’t in hardware business. The corollary to “Systems matter more than Microarchitecture”, is that while Google has been pushing the boundary on system and networking design, TPU silicon itself wasn’t too ground-breaking. Since then, TPU silicon has made massive strides with the latest generations. From the outset, Google’s design philosophy has been more conservative on silicon relative to Nvidia. Historically, TPUs have shipped with significantly fewer peak theoretical FLOPs, and lower memory specs than corresponding Nvidia GPUs. There are 3 reasons for this. First, Google places a high internal emphasis on ‘RAS’ (Reliability, Availability, and Serviceability) for their infrastructure. Google prefers to sacrifice absolute performance in exchange for greater hardware uptime. Running things to the limit means higher instances of hardware mortality which has a real TCO impact in terms of system downtime and hot spares. After all, the hardware you cannot use has infinite TCO relative to performance. The second reason is that up until 2023, Google’s primary AI workload was recommendation system models to power their core Search and Ad properties. RecSys workloads carry much lower arithmetic intensity compared to LLM workloads which means fewer FLOPs are required relative to every bit of data that is transferred. The third comes down to the utility of “peak theoretical FLOPs” numbers that are marketed and how they can be manipulated. Merchant GPU providers like Nvidia and AMD want to market the best performance specifications possible for their chips. This incentivizes them to stretch marketed FLOPs to the highest number possible. In practice, these numbers are unable to be sustained. On the other hand, the TPU has primarily been internal facing, with much less pressure to inflate these specifications externally. This has important implications that we’ll discuss further. The generous way to look at it would be Nvidia is better at DVFS therefore happy to report peak specs only. After we ushered in the LLM era, there has been a clear shift in Google’s TPU design philosophy. We can see that with the 2 most recent TPU generations that were designed-post LLM: TPUv6 Trillium (Ghostlite) and TPUv7 Ironwood (Ghostfish) reflect that change. We can see in the chart below that for TPUv4 and v5, compute throughput was much lower than the Nvidia flagship at the time. TPUv6 came very close to the H100/H200 on FLOPs, but it came 2 years later than the H100. With TPU v7, the gap narrows further with servers available only a few quarters later, while delivering almost the same level of peak theoretical FLOPs. What drove these performance gains? Partially it’s that Google started announcing TPUs as they ramp into production rather then after the next generation was being deployed. Furthermore, TPU v6 Trillium is manufactured on the same N5 node as TPU v5p with similar silicon area but was able to deliver a whopping 2x increase in peak theoretical FLOPs with significantly less power! For Trillium, Google quadrupled the size of each systolic array to 256 x 256 tiles from 128 x 128, and this increase in array size is what has delivered the increase in compute. Trillium was also the last of the “E” (lite) SKUs which meant it was equipped with only 2 sites of HBM3. While Trillium closed the gap to Hopper on compute, it fell far short of the H100/H200 on memory capacity and bandwidth, with only 2 stacks of HBM3 vs 5 and 6 stacks of HBM3 and HBM3E respectively. This made it painful to use for novices, but the performance TCO achieved for Trillium is unbeatable if you get shard your model properly and utilize all those cheap FLOPS. TPU v7 Ironwood is the next iteration where Google nearly completely closes the gap to the corresponding Nvidia flagship GPU on FLOPs, memory, and bandwidth albeit with general availability 1 year later than Blackwell. Compared to the GB200, FLOPs and memory bandwidth only have a slight shortfall, with capacity being the same with 8-Hi HBM3E, which is of course a significant shortfall to GB300 which has 288GB of 12-Hi HBM3E. Theoretical absolute performance is one thing but what matters is real world performance per Total Cost of Ownership (TCO). While Google procures TPUs through Broadcom and pays a hefty margin, it is significantly less than the margin Nvidia earns on not only the GPUs they sell but entire the whole system including CPUs, Switches, NICs, system memory, cabling and connectors. From Google’s perspective this results in the all-in TCO per Ironwood chip for the full 3D Torus configuration being ~44% lower than the TCO of a GB200 server. This more than makes up for the ~10% shortfall on peak FLOPs and peak memory bandwidth. This is from the perspective of Google and the price they procure TPU servers at. What about Google’s external customers when Google adds their margin on top? We assume that in the case where Google earns a margin on leasing TPU 7 to external customers that the TCO per hour can still be up to ~30% lower than the cost of the GB200 and ~41% lower than the cost of the GB300. This is what we believe is reflective of Anthropic’s pricing via GCP. Comparing theoretical FLOPs tells only part of the story. What matters is effective FLOPs, since peak numbers are almost never reached in real-world workloads. In practice, Nvidia GPUs typically achieve only about a small portion of their theoretical peak once communication overhead, memory stalls, power limits, and other system effects are factored in. A good rule of thumb for training is 30%, but utilization also varies heavily by workload. A large share of the gap comes down to software and compiler efficiency. Nvidia’s advantage here stems from the CUDA moat and the wide set of open source libraries that come out of the box, helping workloads run efficiently with high realized FLOPs and memory bandwidth. The TPU software stack is not as easy to use, though this is beginning to change. Inside Google, TPUs benefit from excellent internal tooling that is not exposed to external customers, which makes out of the box performance weaker. However, this only applies to small and/or lazy users, and Anthropic is neither of those. Anthropic has strong engineering resources and ex-Google compiler experts who know both the TPU stack and understand their own model architecture well. They can invest in custom kernels to drive high TPU efficiency. As a result, they can reach substantially higher MFU and much better $/PFLOP performance. We believe that despite lower marketed peak FLOPs, TPUs can reach higher realized Model FLOP Utilization (MFU) than Blackwell, which translates into higher effective FLOPs for Ironwood. A major reason is that marketed GPU FLOPs from Nvidia and AMD are significantly inflated. Even in tests designed to maximize throughput through GEMMs shaped far from real workloads, Hopper only reached about ~80% of peak, Blackwell landed in the 70s, and AMD’s MI300 series in the 50s-60s. The limiting factor is power delivery. These chips cannot sustain the clock speeds used in the peak math. Nvidia and AMD implement Dynamic Voltage and Frequency Scaling which means that the chip’s clock frequency is dynamically adjusted based on power consumption and thermals rather than a stable clock frequency that can actually be sustained. Nvidia and AMD then select the highest clock frequency that could possibly be delivered, even if very intermittently, to be used in the calculation of peak theoretical FLOPs (operations per cycle per ALU x number of ALUs x cycles per second i.e. clock frequency). There are other tricks that are employed, like running GEMMs on tensors filled with zeroes, as 0x0=0, the transistors don’t need to switch state from 0 to 1, therefore reducing the power draw of each operation. Of course, in the real world, zero-filled tensors are not multiplied together. When we put together much lower TCO and higher effective FLOPs utilization, from the perspective of Google the $ per effective FLOP becomes much cheaper, with ~15% MFU being the breakeven with GB300 at 30% MFU. This means if Google (or Anthropic) manages to hit half the FLOPs utilization of GB300, they still come out even. Of course, with Google’s elite compiler engineer team and deep understanding of their own models, the MFU they can realize on TPUs could be 40% potentially. That would be a whopping ~62% reduction in cost per effective training FLOP! However, when looking at the 600K rented TPUs, when we incorporate the higher TCO that Anthropic pays (ie inclusive of Google’s margin stacking) into this analysis, we estimate the cost to Anthropic to be $1.60 per TPU-hour from GCP, narrowing the TCO advantage. We believe that Anthropic can realize 40% MFU on TPUs due to both their focus on performance optimization as well as the TPU’s marketed FLOPs inherently being more realistic. This provides Anthropic with a staggering ~52% lower TCO per effective PFLOP compared to GB300 NVL72. The equilibrium where TCO per effective FLOP compared to the GB300 baseline is the same is at a much lower 19% extracted MFU for Anthropic. This means that Anthropic can suffer a sizeable performance shortfall relative to the baseline GB300 and the perf/TCO for training FLOPs still ends up being the same as the baseline Nvidia system. FLOPs are not the end all and be all for performance, memory bandwidth is super important for inference, especially on the bandwidth intensive decode step. It should be no surprise that $ per memory bandwidth for the TPU also ends up being much cheaper than GB300. There is significant evidence that at small message sizes such as 16MB to 64MB (loading an expert of a single layer), TPU’s even achieve higher memory bandwidth utilization then GPUs. All of this translates into more efficient compute to train and serve a model. Anthropic’s release of Opus 4.5 continued the usual focus on coding, setting a new SWE-Bench record. The main surprise was a ~67% price cut on the API. This price cut paired with the lower verbosity and higher token efficiency of the model compared to Sonnet (76% fewer tokens to match Sonnet’s best score, and 45% fewer to exceed it by 4 points) means Opus 4.5 is the best model for coding use cases and could effectively raise Anthropic’s realized token pricing as Sonnet is over 90% of token mix today. When it comes to pricing for external customers, Google needs to thread the needle to balance their own profitability whilst offering customers a competitive proposition. Our estimate for Anthropic pricing is on the lower end of ranges we’ve heard for external pricing. For a flagship customer such as Anthropic, who will provide valuable input into both the software and hardware roadmap whilst ordering a huge amount of volumes, we’d expect sweetheart pricing. While Nvidia’s eye-watering 4x markup (~75% gross margin) offers a lot of room for pricing flexibility, a good amount of oxygen is sucked away by Broadcom. Broadcom, as the TPU’s co-designer, earns a high margin on the silicon which is the largest component of system BOM. Still, this leaves a lot of room for Google to earn very good acceptable margins. We can see this from comparing the GCP Anthropic deal to other large GPU-based cloud deals. Note that this is looking at the 600k TPUs that is being rented with the remaining 400k TPU v7 chips being bought upfront by Anthropic. Under these assumptions, the TPU v7 economics show superior EBIT margins than the other large GPU-based cloud deals we have observed, with only OCI-OpenAI coming close. Even with Broadcom’s margin stack on the chip-level BOM, Google can still eke out far superior margins and returns than much more commoditized GPU deals. This is where the TPU stack allows GCP to be a truly differentiated CSP. Meanwhile someone like Microsoft Azure, whose ASIC program is struggling, is confined to earning more mediocre returns in the mere business of leasing merchant hardware. We’ve so far discussed how TPUs compare to Nvidia GPUs, focusing on per chip specs and the shortcomings. Now, let’s get back to the system discussion which is where TPU’s capabilities really start to diverge. One of the most distinctive features of the TPU is its extremely large scale up world size through the ICI protocol. The world size of a TPU pod reaches 9216 Ironwood TPUs, with large pod sizes being a feature of TPUs as early as TPUv2 back in 2017 scaling up to a full 256 1024-chip cluster size. Let’s start at the rack level, the basic building block of each TPU superpod. The TPU rack has a similar design over the last couple of generations. Each rack consists of 16 TPU Trays, 16 or 8 Host CPU Trays depending on the cooling configuration, a ToR Switch, power supply units, and BBUs. Each TPU tray consists of 1 TPU board with 4 TPU chip packages mounted. Each Ironwood TPU will have 4 OSFP cages for ICI connections and 1 CDFP PCIe cage for the connection to the Host CPU. Google has been implementing liquid cooled TPU racks since TPU v3 in 2018, but there are still some TPU generations in between that were designed to be air-cooled. The main difference between the liquid cooled and the air-cooled rack is that the air-cooled rack has 2 TPU trays to 1 host CPU tray ratio while the liquid cooled rack has a 1 to 1 ratio instead. An innovative design of TPU’s liquid cooling is that the flow rate of the coolant is actively controlled by the valves. This enables much more efficient cooling as the flow can be adjusted depending on the amount of workload that each chip has at any given time. Google’s TPU has also long adopted vertical power delivery, in which the VRM modules of the TPUs are on the other side of the PCB board. These VRM modules also require a cold plate for cooling. Overall, the TPU rack design is much simpler than that of the Nvidia Oberon NVL72 design, which has a much higher density and utilizes a backplane to connect GPUs to scale up switches. The scale up connections between the TPU trays are all over external copper cables or optics, which will be explained in the ICI section below. The connection between the TPU tray and the CPU tray is also over PCIe DAC cable. The building block of Google’s ICI scale-up network for TPUv7 is a 4x4x4 3D torus consisting of 64 TPUs. Each 4x4x4 cube of 64 TPUs maps to one physical rack of 64 TPUs. This is an ideal dimension as all 64 TPUs can be connected electrically to each other and still fit in a physical rack. The TPUs are connected to each other in a 3D torus configuration, with each TPU connecting to 6 neighbors total – 2 logically adjacent neighboring TPUs for each of the X, Y and Z axes. Each TPU is always connected to 2 other TPUs via PCB traces within the compute tray but depending on where the TPU is located within the 4x4x4 cube, it will connect to 4 other neighbors either via Direct Attach Copper (DAC) cables or via an Optical Transceiver. Connections within the interior of the 4x4x4 cube happen over copper, while connections outside of the 4x4x4 cube (including wrap-around connections back to the other side of the cube as well as connections to neighboring 4x4x4 cubes) will use optical transceivers and OCSs. In the below diagram, we see that as this is a 3D Torus network: TPU 2,3,4 (on the Z+ face) has a wraparound connection back to the opposite Z-axis face to TPU 2,3,1 (on the Z- face) using an 800G optical transceiver and routing through an OCS. As mentioned above, in addition to the 2 neighboring TPUs that are always connected via PCB traces, TPUs will connect to 4 other neighbors using DACs, transceivers or a mix of both depending on where in the 4x4x4 cube they are. TPUs in the interior of the 4x4x4 cube will connect to the 4 other neighbors exclusively using DACs, TPUs on the face of the cube will connect via 3 DACs and 1 optical transceiver, TPUs on the edge of the cube will connect via 2 optical transceivers and 2 DACs, while TPUs on the corners will connect via 1 DAC and 3 optical transceivers. You can remember how many transceivers a given TPU will use by looking at how many of the TPU’s sides are facing the “outside” of the cube. The diagram above, as well as the table below, summarizes the number of respective location types for TPUs and can be used to derive the attach ratio of 1.5 Optical Transceivers per TPU v7. These transceivers connect to Optical Circuit Switches (OCSs) which enable connections between 4x4x4 cubes – more on that in the next section. Google adopts a software-defined networking approach to manage network routes through Optical Circuit Switches (OCSs). An NxN OCS is basically a massive train station with N tracks in and N tracks out. Any train coming in can be transferred to any train coming out, but this has to be reconfigured at the station. Trains cannot be “looped back” or sent back on another N track in, they must be routed only to one of the N tracks out. The benefit of this approach is that the network can assemble smaller logical TPU slices – for different workloads from the theoretical maximum of 9,216 chips in the ICI network layer. By rerouting ICI paths around faults in the network through slicing a larger cluster, cluster availability improves. Unlike Electronic Packet Switching (EPS) switches such as an Arista Tomahawk 5 where there is a fixed total bandwidth that is further split into several ports of smaller bandwidth sizes, OCSs allow any bandwidth of optical fiber to be connected to its ports. OCSs is also low latency compared to EPSs because optical signals entering an OCS simply bounce from the input port to the output port. For EPSs, optical signals must be converted to electrical signals when entering the switch – one key reason why an OCS is typically more power efficient than an EPS. An EPS also allows routing of packets from any port to any port, while an OCS only allows you to route an “in” port to any other “out” port only. OCS ports only route individual fiber strands. This becomes a challenge for standard duplex transceivers because bandwidth is transmitted over multiple fiber strands, which reduces the effective radix and bandwidth of the OCS. To solve this problem, an FR optical transceiver is used to consolidate all wavelengths onto a single fiber strand to be connected to 1 OCS port. The Apollo Project innovatively achieved this in two steps. First, the 8 wavelengths – 1 wavelength for each 100G lane – are multiplexed through Coarse Wave Division Multiplexing (CWDM8) to transmit 800G over a single fiber pair, instead of 8 fiber pairs. Second, an optical circulator is integrated on the wave division multiplexing (WDM) transceiver to enable full duplex data flow, reducing requirements from 1 fiber pair to only 1 fiber strand. The circulator forms a bi-directional link by combining the Tx and Rx fiber strands at the transceiver onto a single fiber strand that is sent to the OCS switch. Google’s ICI scale-up network is unique in that it allows the connection of multiple 64 TPU 4x4x4 Cubes together in a 3D torus configuration to create massive world sizes. The TPUv7 has a stated maximum world size of 9,216 TPUs but today, Google supports the configuration of TPUs into multiple different slice sizes of between 4 TPUs all the way up to 2,048 TPUs. While Google can innovatively achieve an impressive scale-up cluster of 9,216 TPUs, the benefit of running training workloads on incrementally larger block sizes of up to approximately 8,000 TPUs at any point in time decreases. This is because larger block sizes are more prone to failures and disruption, therefore decreasing slice availability, which is defined by the fraction of time in which the ICI cluster is able to form a contiguous 3D torus slice. For slices that can fit entirely within a 4x4x4 Cube, we can simply carve these slices out of that cube using the copper interconnects within the rack as well as the optical transceivers on the face/edge/corner of the cube to wrap around and complete the 3D Torus if needed.To see how wraparound and inter-cube connections are made, let’s start by looking at how we would create a 64 TPU slice in a 4x4x4 topography. We can use the unit 4x4x4 cube of 64 TPUs corresponding to one physical 64 TPU rack to build up this topography. All 8 TPUs in the interior of the 4x4x4 cube can fully connect to all 6 neighbors using copper. If a TPU does not have an interior neighbor along a given axis, it will wrap around and connect to a TPU on the opposite side of the cube. For example, TPU 4,1,4 has no interior neighbor in the Z+ direction, so it will use one 800G optical transceiver to connect to an OCS assigned to the Z-axis, with the OCS configured to direct this connection to the Z- side of the cube, connecting to TPU 4,1,1. In the Y- direction, TPU 1,1,1 will use an optical transceiver to connect to a Y-axis OCS to link to the Y+ side of TPU 1,4,1 and so on. Each face of the 4x4x4 cube will connect via 16 different OCSs – one OCS for each TPU on each face. For example, in the diagram below, on the X+ face, TPU 4,3,2 connects to the input side of OCS X,3,2. OCS X,3,2’s input side will also connect to the same TPU Index (4,3,2) on the X+ face of all 144 4x4x4 Cubes in the 9,216 TPU cluster. OCS X,3,2’s output side will then connect to the same TPU Index for every single cube in the cluster except this time on the X- face – so it will connect to TPU 1,3,2 on all 144 cubes of the cluster. The diagram below illustrates how all 16 TPUs on the X+ face of Cube A connect via 16 OCSs to 16 TPUs on the X- of Cube B. These connections allow any “+” face of any cube to connect to the “-“ face of any other cube, enabling complete fungibility of cubes when forming slices. There are two constraints to briefly point out. First, TPUs of one Index on a given face can never connect directly to a different index – so TPU 4,3,2 could never be configured to connect to TPU 1,2,3. Second, as the OCS essentially acts as a patch panel – TPUs connected on the input side cannot “loop back” to connect to any other TPU that is also connected on the input side of the OCS - as an example, TPU 4,3,2 can never connect to TPU 4,3,3. So – any TPU on the “+” face can never connect to the “+” face of any other cube, and any TPU on the “-” face can never connect to the “-” face of any other cube. Let’s go larger and see how a 4x4x8 Topography could be set up. In this configuration, we extend the slice by connecting two 64 TPU 4x4x4 cubes along the Z-axis. In this case, the OCS will reconfigure the optical port that TPU 4,1,4 is connected to so that it now connects to TPU 4,1,5 instead of wrapping around back to TPU 4,1,1 as was the case for a standalone 4x4x4 topography. Extending this, we will have 16 optical connections extending from the Z- and Z+ faces of each of the two 4x4x4 TPU cubes, for a total of 64 Fiber strands connected into 16 Z-Axis OCSs. It is important to remind readers that Cube A and Cube B depicted below are not necessarily physically located next to each other. Instead, they are connected via OCSs and they could each be in completely different locations in the datacenter. We will now move to a much larger topology – the 16x16x16 topology, which brings us up to 4,096 TPUs. In this topology, we use a total of 48 OCSs to connect 64 Cubes of 64 TPUs each. In the diagram below, each multi-colored cube represents one 64 TPU 4x4x4 cube. Taking the bottom right 4x4x4 cube as an example – this cube is connected to adjacent cubes along the Y-axis via OCSs. The maximum world size of 9,216 TPUs is built up using 144 4x4x4 cubes requiring 96 optical connections each amounting to a total requirement of 13,824 ports. Dividing this total port requirement by 288 (144 input and 144 output ports on each OCS) means we need 48 144x144 OCSs to support this maximum world size. But what is so great about Google’s unique ICI scale-up network – other than all the fancy cube diagrams one can spend countless hours drawing? World Size: The most obvious benefit is the very large 9,216 TPU maximum world size that the TPUv7 Ironwood supports. Even though the maximum slice size of 9,216 may rarely be used due to the drawback of diminished goodput, slices of thousands of TPUs can and are commonly used. This is far larger than the 64 or 72 GPU world size that is common in the merchant accelerator market and for other custom silicon providers. Reconfigurable and Fungibility: The use of OCSs mean that the network topology inherently supports the reconfiguration of network connections to support a high number of different topologies – in theory thousands of topologies. Google’s documentation site lists out 10 different combinations (image earlier in this section), but these are only the most common 3D slice shapes – there are many more available. Even slices of the same size can be reconfigured differently. In the simple example of a Twisted 2D Torus diagrammed below, we see how looping across to an index of a different X coordinate instead of an index of the same X coordinate can reduce the worst-case number of hops and the worst-case bisection bandwidth. This can help improve all to all collective throughput. A TPUv7 cluster will twist at the 4x4x4 cube level. Reconfigurability also opens the door to a broad diversity of parallelisms. In a 64 or 72 GPU world size, different parallelism combinations are generally limited to the factors of 64. When it comes to the ICI scale-up network, the possibilities for implementing topologies to precisely match the combination of data parallelism, tensor parallelism and pipeline parallelism desired are plentiful. The fact that OCSs allow one to connect any “+” face of any cube to the “-“ face of any other cube means that there is complete fungibility of cubes. Slices can be formed out of any set of cubes. So if there are any faults or change in user demands or usage, this will not obstruct the formation of new topology slices. Lower Cost: Google’s ICI network has a lower cost than most switched scale-up networks. Though the FR optics used can be slightly expensive due to the use of circulators, the mesh network reduces the overall number of switches and ports that are needed and eliminates cost arising from connections between switches. Low Latency and Better Locality: The use of direct links between TPUs means that it is possible to achieve much lower latency for TPUs that are physically located close to one another or are reconfigured to connect directly to each other. TPUs that are close to each other also have better data locality. The Datacenter Network (DCN) is a network separate to ICI that serves the role of both a typical backend and front-end network. It connects across an even larger domain – 147k TPUs in the case of TPUv7 clusters. As discussed in our earlier post on Mission Apollo, where Google proposed replacing the Electronic Packet Switch (EPS)-containing spine layer of the traditional “Clos” architecture with Paloma Optical Circuit Switches (OCS), Google’s DCN consists of an optically switched Datacenter Network Interconnect (DCNI) layer that combines several aggregation blocks, each of which connects several 9,216 TPU ICI clusters. In 2022, Google’s Apollo project proposed a DCN architecture that described using 136x136 OCS switches for TPUv4 pods with a pod size of 4,096 TPUs. OCS switches at the DCNI layer were organized into 4 Apollo zones, each containing a maximum of 8 racks of 8 OCS switches for a total of 256 OCS switches. When it comes to Ironwood, to support up to 147k TPUv7s on the same network, we hypothesize that the number of ports on the OCS will nearly double as opposed to increasing the maximum number of OCS switches.The diagram below illustrates what an Ironwood DCN network using 32 racks holding 256 300x300 OCS switches could look like. Assuming that there is no oversubscription between the spine layers of each aggregation block, a maximum of 16 ICI pods can be connected in the DCN with 4 aggregation blocks connecting 4 ICI pods each – a total of 147,456 TPUs. The DCNI layer connects the 4 aggregation blocks – depicted as the top layer in the diagram below. As with ICI, FR Optics are used to connect to the OCSs in order to maximize bandwidth per port on each OCS. While existing Ironwood clusters may only have 1 or 2 aggregation blocks, Google DCN’s unique architecture allows for new aggregation blocks of TPUs to be added to the network without significant rewiring. By using OCSs for the DCNI layer, the size of the DCN fabric can be incrementally expanded and the network can be re-striped to support new aggregation blocks. Furthermore, the bandwidth of aggregation blocks can be upgraded without having to change the make-up of the DCN layer. This allows the link speeds of existing aggregation blocks to be refreshed without changing the fundamental architecture of the network itself. The process of fabric expansion cannot go on indefinitely – at significant scale, it becomes unmanageable to rewire the network. Traditionally, TPU software and hardware teams have been internal-facing. This comes with advantages such as the absence of pressure by marketing teams to inflate stated theoretical FLOPs. Another advantage of being only internal facing is that the TPU teams heavily prioritized internal feature requests & optimizing internal workloads. The disadvantage is that they did not care much about external customers or workloads. The number of external developers in the TPU ecosystem is way lower than in the CUDA ecosystem. This is one of the main weaknesses of the TPU as it is with all non-Nvidia accelerators. Google has since revised their software strategy for externally-facing customers and has already made major changes to their TPU team’s KPIs and how they approach contributing to the AI/ML ecosystem. There are 2 major changes that we will discuss: Massive engineering effort on PyTorch TPU “native” support Massive engineering effort on vLLM/SGLang TPU support The externalization strategy is clear to see by looking at the number of contributions from various TPU software repos by Google. We can see a noticeable increase in vLLM contributions starting from March. Then from May, the “tpu-inference” repo was created which is the official vLLM TPU unified back-end, and since then there has been a flurry of activity. Traditionally, Google only had first class support on the Jax/XLA:TPU stack (and TensorFlow/TF-Mesh RIP), but treated PyTorch on TPU as second class citizen. It relied on lazy tensor graph capture through PyTorch/XLA instead of having a first-class eager execution mode. Furthermore, it did not support PyTorch native distributed APIs (torch.distributed.*) or support PyTorch native parallelism APIs (DTensor, FSDP2, DDP, etc), but relied on weird out of tree XLA SPMD APIs (torch_xla.experimental.spmd_fsdp, torch_xla.distributed.spmd, etc.). This has led to a subpar non-native experience for external users that are used to the native PyTorch CUDA backend on GPUs and trying to switch to TPUs. In October, Google’s “Captain Awesome” Robert Hundt quietly announced in the XLA repo that they will be moving away from a non-native lazy tensor backend towards a “native” TPU PyTorch backend that will support eager execution by default & integration with torch.compile & DTensor & torch.distributed APIs, etc. They will be doing this through the use of PrivateUse1 TorchDispatch key. This will mainly be done for Meta who has renewed interest in buying TPUs & does not want to move to JAX. It will also make it for people that enjoy PyTorch and do not like JAX to use TPUs too. Previously from 2020 to 2023, heavily used by a couple of teams at Meta FAIR used PyTorch XLA on TPUs but it was not widely adopted thus Meta leadership ended up cancelling the contracts in 2023. PyTorch XLA on TPUs is not a fun experience. The Meta FAIR GCP TPUs back then were even run using SLURM and not anything typical you would find on TPU stack like GKE/Xmanager/borg/etc. This new PyTorch <> TPU will create a smoother transition for ML scientists that are used to PyTorch on GPUs to switch to PyTorch on TPUs and take advantage of the higher performance per TCO on TPUs. Pallas is the kernel authoring language for writing custom kernels for TPU (similar to cuTile or Triton or CuTe-DSL). Meta & Google have also started work on supporting Pallas kernels as a codegen target for the Torch Dynamo/Inductor compile stack. This will allow for native TPU integration with PyTorch’s native torch.compile API & allow for end users to register custom pallas ops into PyTorch. In addition to the core in tree PyTorch native APIs, there is also work behind the scenes on integrating TPU pallas kernel language as a codegen target for Helion. You can think of Helion as a higher-level language for writing decently performing kernels in a high level language. Users can think about Helion as a low level Aten operators rather than as high level Triton/Pallas due to its similarity matching much closer to the Native PyTorch Aten ops. Another area where the CUDA ecosystem is supreme is for open ecosystem inference. Historically, vLLM & SGLang support CUDA as first class (as ROCm as 2nd class citizen). Now Google wants in to the vLLM & SGlang open inference ecosystem and have announced beta TPU v5p/v6e support for vLLM & SGLang through a very “unique” integration. vLLM& SGLang currently does this by lowering the PyTorch modelling code into JAX and taking advantage of the existing mature JAX TPU compilation flow. In the future once PyTorch XLA RFC #9684 (aka native TPU PyTorch backend) gets implemented, vLLM & SGLang plan on evaluating whether to switch to using that instead of translating modelling from PyTorch to JAX through TorchAX. Google & vLLM claim that this lowering to jax path does not require any changes to the PyTorch modelling code but given how few models vLLM TPU supports so far, we doubt this is true. Furthermore, Google has open-sourced & integrated some of their TPU kernels into vLLM such as a TPU optimized paged attention kernels, compute-comms overlapped GEMM kernels & a couple other quantized matmul kernels. They do not yet have MLA-friendly TPU kernels. It would be interesting to see once Inductor Pallas TPU codegen integration is more mature, whether it is possible to integrate kernel fusion & pattern matching into the existing vLLM PassManager. SGLang is also looking into implementing an torch.compile PassManager to make managing kernel fusions for many models more maintainable. For Ragged Paged Attention v3, TPU handles it quite differently from vLLM GPU. vLLM manages KV cache with a technique similar to virtual memory and paging. However, this technique requires fetching dynamic addresses and performing scatter operations, something TPUs don’t support well. As a result, TPU kernels leverage fine-grained operation pipelining. Specifically, TPU’s page attention kernel prefetches query and KV blocks for the next sequence, so the memory loading is overlapped with computation. In the existing vLLM MoE kernel, we sort tokens by expert ID, dispatch tokens to the devices with the corresponding experts, perform group matrix multiplication, and combine tokens from experts back to original devices. However, the kernel performs poorly for two reasons: TPUs are slow at performing sorting operations, and the kernel is unable to overlap communication with computation. To work around this issue, Google developers designed all-fused MoE. All-fused MoE dispatches tokens for one expert per device at a time while overlapping MoE dispatch & MoE combine communications & avoiding sorting tokens by expert ID. With all-fused MoE, the Google engineer reported 3 - 4x speedup over existing kernels. Furthermore, another hardware unit in TPUs is the SparseCore (SC) used to accelerate embedding lookups and updates. SC comes with a scalar subcore SparseCore Sequencer (SCS) and multiple vector subcores SparseCore Tiles (SCT). SCT supports local and remote direct memory access at a more fine-grained 4-byte or 32-byte granularity, compared to TPU TensorCore’s 512-byte loads. This enables SC to perform gather/scatter operations and ICI communications while overlapping with TensorCore operations. At JAX DevLabs, we learned that programmability of SparseCore is a work in progress. We can expect Mosaic, the TPU custom kernel compiler, to compile in an MPMD fashion, where SCS and SCT executes different kernels, and different SparseCores can run different programs. We suspect once the programmability catches up, TPU MoE kernels would be able to perform dispatch and combine operations in a similar way as GPUs, instead of dispatching by expert IDs. In terms of disaggregated prefill decode, which we described in depth in our AMD 2.0 post, Google has experimental support on vLLM for single host disagg PD, not they do not support multi-host wideEP disagg prefill or MTP yet. These inference optimizations are critical to lower the TCO per million tokens and increase the perf per dollar and perf per watt. Furthermore, they have not yet integrated TPU vLLM inference support into popular RL frameworks like VERL, etc. Google is slowly moving in the correct direction in terms of how they approach the open AI/ML ecosystem especially for their “native” TPU backend. This week, there was an new inference benchmark on TPUv6e that dropped claiming that TPUv6e has 5x worst performance per dollar than NVIDIA GPUs. We disagree mainly due to 2 reasons. First of all, this is benchmark is on vLLM on TPUs which was only released an couple month ago thus does not yet have optimized performance. Google internal Gemini workloads & Anthropic workloads work on an internal custom inference stack that has better perf per TCO than NVIDIA GPUs. Secondly, Aritifical Analysis’s cost per million tokens is using the list price of $2.7/hr/chip for TPUv6e. No major customers of TPUs is paying anywhere close to that much for TPUv6e given the BOM is a tiny fraction of the H100. As everyone knows, most clouds have an high ball list price such that their account sales executives can do “car salesman” tactics and give massive discounts so that the customer thinks they are getting a good deal. The SemiAnalysis AI TCO Model tracks the acutal market rental price of TPUs across all the various contract lengths (1 month, 1 year, 3 years, etc). One part where Google is still approaching their software strategy incorrectly is with their XLA graph compiler & networking libraries & TPU runtime is still not open sourced nor well documented. This has led to frustrated users across the spectrum from advanced users to the average user of not being able to debug what is going wrong with their code. Furthermore their MegaScale codebase for multi-pod training is not open source either. We strongly believe that in order to accelerate the adoption, Google should open source it and the increased user adoption will outweigh all the software IP they will make public & free. Just like how PyTorch or Linux being open sourced rapidly increased adoption, open sourcing XLA:TPU & TPU runtime & networking libs will rapidly accelerated this too. Now that Google has gotten their act together on TPU and is selling them externally for people to put in their own datacenters, what are the implications on Nvidia’s business? Does Nvidia finally have a legitimate competitor that will put its market share and margins at threat? Behind the paywall, we will share our thoughts on what this means for Nvidia as well as reveal more about the TPU roadmap.
--------------------------------------------------

Title: Wildly Violent (and Profitable)
URL: https://dailyreckoning.com/wildly-violent-and-profitable/
Time Published: 2025-11-29T23:00:03Z
Full Content:
By Adam Sharp PostedNovember 29, 2025 The story of the world’s first IPO is a wild one. The Dutch East India Company (VOC) was founded in Amsterdam in 1602. Over the next hundred years, the VOC would dominate trade with Asia, wage multiple wars, commit genocide, and build monopolies. Along the way they sold gobs of spices, silks, and opium. But before all of that could happen, the VOC’s founders needed to raise money. So in 1602 the Dutch East India Company conducted the first initial public offering, giving all Dutchmen the chance to invest in this new venture. VOC Co-Founder Dirck Van Os | Source: Wikipedia The VOC IPO was unique because it was open to anyone, not just wealthy merchants and royalty. In total, 1,143 investors participated. Co-founder Dirck Van Os’ maid, Neeltgen Cornelis, was the second-to-last investor on the registry, having subscribed 100 guilders, which was likely her life’s savings. If Mrs. Cornelis the maid held onto her shares for the long run, she did quite well. In some years, VOC investors received a dividend equivalent to 75% of the capital they initially committed. Now, that dividend occasionally came in the form of spices (cloves and mace in particular). But hey, profit is profit… Dutch historian Lodewijk Petram has calculated the return of VOC shares (with dividends reinvested), and from its founding in 1602 through 1698, each 100 guilders invested turned into about 65,000. Source: The World’s First Stock Exchange VOC shares were groundbreaking because they were tradable on what would become the world’s first stock exchange. In addition, the Dutch East India Company introduced the concept of limited liability for shareholders. In other words, investors could only lose the amount they invested. They wouldn’t be responsible for the company’s debts or losses if the venture failed. The VOC set a new standard for how companies could raise capital and distribute profits. But the business model was downright diabolical… The VOC quickly became a publicly-traded empire. By 1637 it was worth the equivalent of around $6.9 trillion in today’s money. That’s double the value of the largest modern companies such as Apple (AAPL), Microsoft (MSFT), and Nvidia (NVDA). The Dutch East India Company maintained its own navy with around 40 warships and 200 trading vessels at any given time. It fought wars against Spain, Portugal, Britain, and various Asian powers. These great powers clashed over lucrative trade routes and assets in Asia. They fought over territories such as the Spice Islands and the Philippines, and sold opium to China. In one notorious incident, the VOC invaded the Banda Islands, the world’s only source of nutmeg and mace at the time. Back then these spices were more valuable than gold. Using mercenaries which included Japanese samurai, VOC Governor-General Jan Pieterszoon Coen brutally massacred the Bandanese, publicly executing its leaders. Of around 15,000 natives, only a few hundred are believed to have survived. The VOC brought in slaves and other workers to take over Banda’s nutmeg and mace operations. These newly-acquired plantations were wildly profitable for shareholders. The company now had a total monopoly on two of the most valuable spices, which they sold into European markets at 1,000% markups. The Dutch East India Company was basically a publicly-traded evil empire. But for many years it did succeed in its mission of rewarding shareholders. Over time, however, the company lost its monopoly on spices and trade with Asia. Nutmeg and mace were smuggled out of Banda and cultivated elsewhere. Eventually the price of these once-rare goods collapsed. Artificial monopolies can only last so long. The VOC became increasingly corrupt and bureaucratic. It took on too much debt, and in time it essentially became a ponzi scheme. Ever-increasing debt loads were required to pay dividends. That’s a classic sign of the end approaching. The British East India Company eventually became a fierce competitor and seized many of the VOC’s prime territories. In 1799 the Dutch government dissolved the VOC and absorbed its assets and debts. The company had lasted almost 200 years. Despite the evil deeds committed by the Dutch East India Company, the firm played a critical role in the history of finance. It set a new standard for how companies could raise money and distribute profits to shareholders.
--------------------------------------------------

Title: Jim Cramer Explains Why He Says “Own, Don’t Trade the Stock of NVIDIA”
URL: https://finance.yahoo.com/news/jim-cramer-explains-why-says-175304790.html
Time Published: 2025-11-29T17:53:04Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the stocks Jim Cramer recently talked about. Cramer explained why he “won’t give up” on the stock, as he commented...
--------------------------------------------------

Title: Dan Ives Says These Are the Top 3 Stocks to Buy Right Now
URL: https://www.barchart.com/story/news/36371180/dan-ives-says-these-are-the-top-3-stocks-to-buy-right-now
Time Published: 2025-11-29T17:00:02Z
Description: The Wedbush analyst reckons these stocks to be table pounders at the moment.
--------------------------------------------------

Title: Meta Platforms May Ditch NVIDIA Chips—Here’s Why Investors Care
URL: https://www.marketbeat.com/originals/meta-to-buy-google-chips-why-it-could-shift-demand-from-nvidia/?utm_source=yahoofinance&amp;utm_medium=yahoofinance
Time Published: 2025-11-29T15:12:00Z
Description: Meta Platforms may be looking to alter where it spends its AI dollars. See what benefits the firm could reap from Google and Broadcom's chips.
--------------------------------------------------

Title: Nvidia (NVDA) Responds to Competition Fears as Meta Explores Google’s TPUs
URL: https://finance.yahoo.com/news/nvidia-nvda-responds-competition-fears-110654850.html
Time Published: 2025-11-29T11:06:54Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the AI Stocks Making Headlines on Wall Street. On November 25, Bank of America maintained a positive outlook on...
--------------------------------------------------

Title: ChatGPT was unveiled 3 years ago, kicking off the AI revolution. For investors, it did even more.
URL: https://finance.yahoo.com/news/chatgpt-was-unveiled-3-years-ago-kicking-off-the-ai-revolution-for-investors-it-did-even-more-110014912.html
Time Published: 2025-11-29T11:00:14Z
Description: OpenAI released ChatGPT on Nov. 30, 2022, not only kicking off the AI revolution but turning around one of the worst market environments of this century.
--------------------------------------------------

Title: Jim Cramer on NVIDIA: “Anything That Allows the Company to Sell in China Would Be Huge”
URL: https://finance.yahoo.com/news/jim-cramer-nvidia-anything-allows-064250979.html
Time Published: 2025-11-29T06:42:50Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the stocks that received Jim Cramer’s latest comments. Cramer noted the recent chatter around the President...
--------------------------------------------------

Title: Is NVIDIA (NVDA) One of the Best Semiconductor Stocks to Buy Heading into 2026?
URL: https://finance.yahoo.com/news/nvidia-nvda-one-best-semiconductor-061117049.html
Time Published: 2025-11-29T06:11:17Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the best Semiconductor Stocks to Buy Heading into 2026. On November 20, Raymond James analyst Simon Leopold...
--------------------------------------------------

Title: Alphabet: The AI Leader Best Positioned to Dominate 2026
URL: https://www.marketbeat.com/originals/alphabet-the-ai-leader-best-positioned-to-dominate-2026/?utm_source=yahoofinance&amp;utm_medium=yahoofinance
Time Published: 2025-11-28T20:07:00Z
Description: Alphabet has flipped its H1 sentiment to overwhelmingly bullish, boosted by accelerating growth, the release of Gemini 3, and Berkshire’s recent stake.
--------------------------------------------------

Title: Intel's Stock Pops as Rumors Swirl About a Big New Customer
URL: https://www.investopedia.com/intel-s-stock-pops-as-rumors-swirl-about-a-big-new-customer-apple-intc-11858735
Time Published: 2025-11-28T20:06:12Z
Full Content:
Could Intel be closer to scoring a deal with iPhone maker Apple? The chipmaker's shares surged Friday as an analyst added fuel to rumors that the iPhone maker could become a new customer for the chipmaker. Shares of Intel (INTC) popped over 10% during Friday's shortened trading session to lead gains on the S&P 500 and Nasdaq. (Read Investopedia's daily markets coverage here.) The likelihood of Apple (AAPL) becoming a new customer for Intel "has recently improved significantly," TF International Securities analyst Ming-Chi Kuo, citing industry surveys, posted on X Friday, suggesting that Intel could start shipping Apple processors as soon as 2027. A deal with Apple could boost confidence in a turnaround for Intel, which still faces persistent worries about its ability to secure long-term commitments to its manufacturing business. Apple and Intel did not respond to Investopedia's requests for comment in time for publication. Shares of Intel have roughly doubled in value this year after a flurry of recent deals, including a partnership with AI chip leader Nvidia (NVDA). Still, they remain well off their historical highs as the company is still working to convince investors regarding a sustainable turnaround in its business. One of those challenges continues to be securing new customers for its manufacturing operation. The deal with Nvidia did not include commitments to Intel's foundry, raising speculation about whether the relationship could expand later or if the lack of foundry commitments could point to troubles convincing customers. A deal with former Intel customer Apple could go a long way in helping assuage those concerns. Shares of Apple rose a bit less than 0.5% on Friday.
--------------------------------------------------

Title: Stock market today: S&P 500, Dow rise to end a rocky month, Nasdaq snaps 7-month win streak
URL: https://finance.yahoo.com/news/live/stock-market-today-sp-500-dow-rise-to-end-a-rocky-month-nasdaq-snaps-7-month-win-streak-180355656.html
Time Published: 2025-11-28T18:03:55Z
Description: US stocks rose for the fifth day in a row on Friday, but the Nasdaq snapped its monthly winning streak.
--------------------------------------------------

Title: Michael Burry just started a massive group chat on Substack, and it's as chaotic as you'd expect
URL: https://www.businessinsider.com/michael-burry-substack-group-chat-big-short-subscribers-ai-stocks-2025-11
Time Published: 2025-11-28T17:39:20Z
Full Content:
Every time Theron publishes a story, you’ll get an alert straight to your inbox! Enter your email By clicking “Sign up”, you agree to receive emails from Business Insider. In addition, you accept Insider’s Terms of Service and Privacy Policy. Michael Burry's group chat is going off. The investor of "The Big Short" fame invited all of his paid Substack subscribers to a group chat on Friday, and it quickly led to hundreds of replies ranging from memes to questions for Burry. This month, Burry pivoted from running a hedge fund to publishing a newsletter named "Cassandra Unchained." It has amassed more than 97,000 subscribers since it launched on Sunday. "This is a conversation space exclusively for paid subscribers—kind of like a group chat or live hangout," reads Burry's introductory post. "I plan to post updates that come my way, and you can jump into the discussion." The first reply on the chat reads: "I think Dr. Burry just broke Substack." Another early response jokes about Burry's disclosure this week that he owns bearish put options on Nvidia and Palantir stock: "I think Dr Burry is going to make more money from Substack than his NVDA and pltr puts 🤣" A third poked fun at a potential spike in traffic to Substack. "Someone pray for substacks backend engineers." "It's gonna be legendburry!!" one subscriber wrote, while another noted: "This chat is gonna be nuts." "Bro don't allow anyone to start threads. This a spam fest," one concerned poster added. Other subscribers rushed to post memes, videos, and even photos of Black Friday crowds. The questions to Burry ranged from who the next chair of the Federal Reserve might be, to how an 80-year-old should invest to prepare for a crash, to how the dollar stacks up against other currencies. Burry resurfaced on X in late October after more than two years of silence, and has wasted no time issuing numerous warnings of an AI bubble and taking aim at key players such as Nvidia and Palantir. The investor, who has 1.6 million X followers, is best known for predicting and profiting from the collapse of the US housing bubble that triggered a global financial crisis, and for issuing dire pronouncements of crashes and recessions. He became famous in financial circles after his bet against the subprime mortgage market was featured in author Michael Lewis' book "The Big Short," and actor Christian Bale played him in the movie adaptation. Jump to
--------------------------------------------------

Title: AI Stocks You Should Buy to Boost and Reenergize Your Portfolio
URL: https://finance.yahoo.com/news/ai-stocks-buy-boost-reenergize-163800128.html
Time Published: 2025-11-28T16:38:00Z
Description: Here, we have picked three AI stocks, NVDA, MU and ADI, which are well-poised to benefit from AI's growing use and ability to solve complex problems.
--------------------------------------------------

Title: Earnings live: S&P 500 on track for solid Q3 season, with reports from Macy's, C3.ai, Salesforce on deck
URL: https://finance.yahoo.com/news/live/earnings-live-sp-500-on-track-for-solid-q3-season-with-reports-from-macys-c3ai-salesforce-on-deck-151055563.html
Time Published: 2025-11-28T15:10:55Z
Description: The third quarter earnings season has been mostly positive, with most of the reports in the rearview mirror.
--------------------------------------------------

Title: Dow, S&P 500, Nasdaq futures muted as rocky month draws to an end, CME restores trading
URL: https://finance.yahoo.com/news/live/dow-sp-500-nasdaq-futures-muted-as-rocky-month-draws-to-an-end-cme-restores-trading-140109707.html
Time Published: 2025-11-28T14:01:09Z
Description: CME is gradually resuming operations after a futures outage that halted trading in US stock indexes.
--------------------------------------------------

Title: Dow, S&P 500, Nasdaq open muted as rocky month draws to an end, CME restores trading
URL: https://finance.yahoo.com/news/live/dow-sp-500-nasdaq-open-muted-as-rocky-month-draws-to-an-end-cme-restores-trading-140109595.html
Time Published: 2025-11-28T14:01:09Z
Description: CME is gradually resuming operations after a futures outage that halted trading in US stock indexes.
--------------------------------------------------

Title: Stock market today: Nasdaq, S&P 500, Dow rise toward a 5th straight day of gains to cap a rocky month
URL: https://finance.yahoo.com/news/live/stock-market-today-nasdaq-sp-500-dow-rise-toward-a-5th-straight-day-of-gains-to-cap-a-rocky-month-140109029.html
Time Published: 2025-11-28T14:01:09Z
Description: CME is gradually resuming operations after a futures outage that halted trading in US stock indexes.
--------------------------------------------------

Title: Wall Street Likes Server Stocks After Nvidia’s Q3. Is DELL or HPE Stock a Better Buy Here?
URL: https://www.barchart.com/story/news/36356183/wall-street-likes-server-stocks-after-nvidias-q3-is-dell-or-hpe-stock-a-better-buy-here
Time Published: 2025-11-28T12:30:02Z
Description: NVDA results and commentary from CEO Jensen Huang support a continued bull market for AI stocks. Dell and HPE are both expected to benefit.
--------------------------------------------------

Title: US stock futures rose today as Dow, S&P 500 and Nasdaq all in green – here's top pre-market gainers
URL: https://economictimes.indiatimes.com/news/international/us/us-stock-futures-rose-today-as-dow-sp-500-and-nasdaq-all-in-green-heres-top-pre-market-gainers/articleshow/125635580.cms
Time Published: 2025-11-28T10:34:00Z
Full Content:
U.S. stock futures rose on Friday. Dow Futures hit 47,542, up 0.11%. S&P 500 Futures touched 6,835, up 0.10%. Nasdaq Futures moved to 25,347.75, up 0.18%. Pre-market gainers were strong. SMX jumped 78.16%. CDT rose 15.46%. Micron gained 3.24%. Crypto-linked stocks also advanced. The market showed steady buying ahead of new economic data. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Explore More Stories What is Canada’s new colour-coded alert system and how it will ensure public safety during extreme weather Canada launches new alert system for extreme weather; Colour-coded system marks shift to impact-based forecasting Canadian Olympic swimming champion Penny Oleksiak suspended for two years over anti-doping whereabouts failures Who is Colleen Jones? Two-time World champion curler and veteran Canadian broadcaster dies at 65 What exactly is the Canada Pension Plan? New CPP payments rolling out nationwide on November 26; Check all the payment dates of 2025-26 - Why is Quebec not part of CPP? Madeleine Poulin, Radio-Canada’s first female correspondent in Ottawa and Paris, dies at 87 Yoplait recalls 'Yop yogurt' drinks in Canada over plastic contamination fears under Class 1 category - Check out which flavor and best-before dates beverage are subject to recall CMA Awards: Vince Gill honored with prestigious Willie Nelson Lifetime Achievement Award at 59th CMA Awards in Tennessee MGK rocks Canadian fans with explosive Grey Cup halftime show; Are Megan Fox, MGK really working on their relationship post daughter’s birth? Roughriders end 12-year drought, beat Alouettes for 5th Grey Cup title Who is Dr Sanjeev Sirpal? Trouble mounts for New Brunswick doctor accused of sexual assault in hospitals as he faces additional charges; what do we know so far Russian humanoid robot 'AIDOL' shockingly faceplants during much-hyped public debut - WATCH Who is Ben Watsa, the future chairman of Fairfax financial and heir to Prem Watsa’s $100-billion empire? 'Kumbh Shahi Snan in Haridwar on…': CM Dhami announces key dates Deaths, ICU cases and FIRs trigger outrage during voter-list revision drive Air Marshal Dixit explains Integrated Theatre Command Assam Polygamy Ban sparks controversy Imran Khan’s Sister speaks out - full interview PM Modi at Sri Krishna Matha, Udupi, Karnataka ‘Suspect worked with CIA in Afghanistan’: Kash Patel reveals... Sambit Patra Slams Rahul Gandhi over 'Toolkit' controversy Ex‑CJI Gavai comments on Justice Varma cash‑recovery scandal ‘Faith, not merit?’: BJP's demand for Hindu quota at Vaishno Devi med college 'Kumbh Shahi Snan in Haridwar on…': CM Dhami announces key dates Deaths, ICU cases and FIRs trigger outrage during voter-list revision drive Air Marshal Dixit explains Integrated Theatre Command Assam Polygamy Ban sparks controversy Imran Khan’s Sister speaks out - full interview PM Modi at Sri Krishna Matha, Udupi, Karnataka ‘Suspect worked with CIA in Afghanistan’: Kash Patel reveals... Sambit Patra Slams Rahul Gandhi over 'Toolkit' controversy Ex‑CJI Gavai comments on Justice Varma cash‑recovery scandal ‘Faith, not merit?’: BJP's demand for Hindu quota at Vaishno Devi med college Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Slideshow Top Prime Articles Private Companies Top Story Listing Top Definitions Top Market Pages Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Nvidia (NVDA) Memo Calms Investors as Bernstein Reaffirms Outperform
URL: https://finance.yahoo.com/news/nvidia-nvda-memo-calms-investors-062120029.html
Time Published: 2025-11-28T06:21:20Z
Description: NVIDIA Corporation (NASDAQ:NVDA) is one of the AI stocks analysts are betting on. On November 26, Bernstein reaffirmed its Outperform rating on Nvidia and...
--------------------------------------------------

Title: This AI Dividend Stock Is a Buy Even as the S&P 500’s Yield Falls to Dot-Com Lows
URL: https://www.barchart.com/story/news/36349747/this-ai-dividend-stock-is-a-buy-even-as-the-s-p-500s-yield-falls-to-dot-com-lows
Time Published: 2025-11-28T00:30:02Z
Description: The S&P 500 Index’s dividend yield is now at levels last seen during the dot-com boom. For tech investors, though, Microsoft looks like a good buy given its ...
--------------------------------------------------

Title: Dow, S&P 500, Nasdaq futures muted as rocky month draws to an end, before CME glitch halts trading
URL: https://finance.yahoo.com/news/live/dow-sp-500-nasdaq-futures-muted-as-rocky-month-draws-to-an-end-before-cme-glitch-halts-trading-000737050.html
Time Published: 2025-11-28T00:07:37Z
Description: CME is gradually resuming operations after a futures outage that halted trading in US stock indexes.
--------------------------------------------------

Title: Analyst revisits Nvidia stock after Google-Meta news
URL: https://www.thestreet.com/investing/analyst-revisits-nvidia-stock-after-google-meta-news
Time Published: 2025-11-27T18:17:00Z
Description: Nvidia (NVDA) shares were volatile this week after news that Meta (META) is in talks to buy billions of dollars of Google (GOOG) chips starting in 2027. The ...
--------------------------------------------------

Title: ACM Research, Inc. (ACMR): A Bull Case Theory
URL: https://finance.yahoo.com/news/acm-research-inc-acmr-bull-181311084.html
Time Published: 2025-11-27T18:13:11Z
Description: We came across a bullish thesis on ACM Research, Inc. on FluentInQuality’s Substack. In this article, we will summarize the bulls’ thesis on ACMR. ACM...
--------------------------------------------------

Title: The easy set and forget ASX share portfolio I'd build today
URL: https://www.fool.com.au/2025/11/28/the-easy-set-and-forget-asx-share-portfolio-id-build-today/
Time Published: 2025-11-27T16:13:00Z
Description: It is easier than you think to build a winning portfolio.
The post The easy set and forget ASX share portfolio I'd build today appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: Using Probability Density to Extract a Huge Payout from Microchip’s Potential Breakout
URL: https://www.barchart.com/story/news/36344520/using-probability-density-to-extract-a-huge-payout-from-microchips-potential-breakout
Time Published: 2025-11-27T14:15:01Z
Description: You’re not going to find informational arbitrage opportunities with middle school math. Instead, MCHP stock requires a quantitative approach.
--------------------------------------------------

Title: TPUs vs. GPUs and why Google is positioned to win AI race in the long term
URL: https://www.uncoveralpha.com/p/the-chip-made-for-the-ai-inference
Time Published: 2025-11-27T13:28:34Z
Full Content:
Hey everyone, As I find the topic of Google TPUs extremely important, I am publishing a comprehensive deep dive, not just a technical overview, but also strategic and financial coverage of the Google TPU. Topics covered: The history of the TPU and why it all even started? The difference between a TPU and a GPU? Performance numbers TPU vs GPU? Where are the problems for the wider adoption of TPUs Google’s TPU is the biggest competitive advantage of its cloud business for the next 10 years How many TPUs does Google produce today, and how big can that get? Gemini 3 and the aftermath of Gemini 3 on the whole chip industry Let’s dive into it. The history of the TPU and why it all even started? The story of the Google Tensor Processing Unit (TPU) begins not with a breakthrough in chip manufacturing, but with a realization about math and logistics. Around 2013, Google’s leadership—specifically Jeff Dean, Jonathan Ross (the CEO of Groq), and the Google Brain team—ran a projection that alarmed them. They calculated that if every Android user utilized Google’s new voice search feature for just three minutes a day, the company would need to double its global data center capacity just to handle the compute load. At the time, Google was relying on standard CPUs and GPUs for these tasks. While powerful, these general-purpose chips were inefficient for the specific heavy lifting required by Deep Learning: massive matrix multiplications. Scaling up with existing hardware would have been a financial and logistical nightmare. This sparked a new project. Google decided to do something rare for a software company: build its own custom silicon. The goal was to create an ASIC (Application-Specific Integrated Circuit) designed for one job only: running TensorFlow neural networks. Key Historical Milestones: 2013-2014: The project moved really fast as Google both hired a very capable team and, to be honest, had some luck in their first steps. The team went from design concept to deploying silicon in data centers in just 15 months—a very short cycle for hardware engineering. 2015: Before the world knew they existed, TPUs were already powering Google’s most popular products. They were silently accelerating Google Maps navigation, Google Photos, and Google Translate. 2016: Google officially unveiled the TPU at Google I/O 2016. This urgency to solve the “data center doubling” problem is why the TPU exists. It wasn’t built to sell to gamers or render video; it was built to save Google from its own AI success. With that in mind, Google has been thinking about the »costly« AI inference problems for over a decade now. This is also one of the main reasons why the TPU is so good today compared to other ASIC projects. The difference between a TPU and a GPU? To understand the difference, it helps to look at what each chip was originally built to do. A GPU is a “general-purpose” parallel processor, while a TPU is a “domain-specific” architecture. The GPUs were designed for graphics. They excel at parallel processing (doing many things at once), which is great for AI. However, because they are designed to handle everything from video game textures to scientific simulations, they carry “architectural baggage.” They spend significant energy and chip area on complex tasks like caching, branch prediction, and managing independent threads. A TPU, on the other hand, strips away all that baggage. It has no hardware for rasterization or texture mapping. Instead, it uses a unique architecture called a Systolic Array. The “Systolic Array” is the key differentiator. In a standard CPU or GPU, the chip moves data back and forth between the memory and the computing units for every calculation. This constant shuffling creates a bottleneck (the Von Neumann bottleneck). In a TPU’s systolic array, data flows through the chip like blood through a heart (hence “systolic”). It loads data (weights) once. It passes inputs through a massive grid of multipliers. The data is passed directly to the next unit in the array without writing back to memory. What this means, in essence, is that a TPU, because of its systolic array, drastically reduces the number of memory reads and writes required from HBM. As a result, the TPU can spend its cycles computing rather than waiting for data. Google’s new TPU design, also called Ironwood also addressed some of the key areas where a TPU was lacking: They enhanced the SparseCore for efficiently handling large embeddings (good for recommendation systems and LLMs) It increased HBM capacity and bandwidth (up to 192 GB per chip). For a better understanding, Nvidia’s Blackwell B200 has 192GB per chip, while Blackwell Ultra, also known as the B300, has 288 GB per chip. Improved the Inter-Chip Interconnect (ICI) for linking thousands of chips into massive clusters, also called TPU Pods (needed for AI training as well as some time test compute inference workloads). When it comes to ICI, it is important to note that it is very performant with a Peak Bandwidth of 1.2 TB/s vs Blackwell NVLink 5 at 1.8 TB/s. But Google’s ICI, together with its specialized compiler and software stack, still delivers superior performance on some specific AI tasks. The key thing to understand is that because the TPU doesn’t need to decode complex instructions or constantly access memory, it can deliver significantly higher Operations Per Joule. For scale-out, Google uses Optical Circuit Switch (OCS) and its 3D torus network, which compete with Nvidia’s InfiniBand and Spectrum-X Ethernet. The main difference is that OCS is extremely cost-effective and power-efficient as it eliminates electrical switches and O-E-O conversions, but because of this, it is not as flexible as the other two. So again, the Google stack is extremely specialized for the task at hand and doesn’t offer the flexibility that GPUs do. Performance numbers TPU vs GPU? As we defined the differences, let’s look at real numbers showing how the TPU performs compared to the GPU. Since Google isn’t revealing these numbers, it is really hard to get details on performance. I studied many articles and alternative data sources, including interviews with industry insiders, and here are some of the key takeaways. The first important thing is that there is very limited information on Google’s newest TPUv7 (Ironwood), as Google introduced it in April 2025 and is just now starting to become available to external clients (internally, it is said that Google has already been using Ironwood since April, possibly even for Gemini 3.0.). And why is this important if we, for example, compare TPUv7 with an older but still widely used version of TPUv5p based on Semianalysis data: TPUv7 produces 4,614 TFLOPS(BF16) vs 459 TFLOPS for TPUv5p TPUv7 has 192GB of memory capacity vs TPUv5p 96GB TPUv7 memory Bandwidth is 7,370 GB/s vs 2,765 for v5p We can see that the performance leaps between v5 and v7 are very significant. To put that in context, most of the comments that we will look at are more focused on TPUv6 or TPUv5 than v7. Based on analyzing a ton of interviews with Former Google employees, customers, and competitors (people from AMD, NVDA & others), the summary of the results is as follows. Most agree that TPUs are more cost-effective compared to Nvidia GPUs, and most agree that the performance per watt for TPUs is better. This view is not applicable across all use cases tho. A Former Google Cloud employee: »If it is the right application, then they can deliver much better performance per dollar compared to GPUs. They also require much lesser energy and produces less heat compared to GPUs. They’re also more energy efficient and have a smaller environmental footprint, which is what makes them a desired outcome. The use cases are slightly limited to a GPU, they’re not as generic, but for a specific application, they can offer as much as 1.4X better performance per dollar, which is pretty significant saving for a customer that might be trying to use GPU versus TPUs.« source: AlphaSense Similarly, a very insightful comment from a Former Unit Head at Google around TPUs materially lowering AI-search cost per query vs GPUs: »TPU v6 is 60-65% more efficient than GPUs, prior generations 40-45%« This interview was in November 2024, so the expert is probably comparing the v6 TPU with the Nvidia Hopper. Today, we already have Blackwell vs V7. Many experts also mention the speed benefit that TPUs offer, with a Former Google Head saying that TPUs are 5x faster than GPUs for training dynamic models (like search-like workloads). There was also a very eye-opening interview with a client who used both Nvidia GPUs and Google TPUs as he describes the economics in great detail: »If I were to use eight H100s versus using one v5e pod, I would spend a lot less money on one v5e pod. In terms of price point money, performance per dollar, you will get more bang for TPU. If I already have a code, because of Google’s help or because of our own work, if I know it already is going to work on a TPU, then at that point it is beneficial for me to just stick with the TPU usage. In the long run, if I am thinking I need to write a new code base, I need to do a lot more work, then it depends on how long I’m going to train. I would say there is still some, for example, of the workload we have already done on TPUs that in the future because as Google will add newer generation of TPU, they make older ones much cheaper.For example, when they came out with v4, I remember the price of v2 came down so low that it was practically free to use compared to any NVIDIA GPUs. Google has got a good promise so they keep supporting older TPUs and they’re making it a lot cheaper. If you don’t really need your model trained right away, if you’re willing to say, “I can wait one week,” even though the training is only three days, then you can reduce your cost 1/5.« source: AlphaSense Another valuable interview was with a current AMD employee, acknowledging the benefits of ASICs: »I would expect that an AI accelerator could do about probably typically what we see in the industry. I’m using my experience at FPGAs. I could see a 30% reduction in size and maybe a 50% reduction in power vs a GPU.« We also got some numbers from a Former Google employee who worked in the chip segment: »When I look at the published numbers, they (TPUs) are anywhere from 25%-30% better to close to 2x better, depending on the use cases compared to Nvidia. Essentially, there’s a difference between a very custom design built to do one task perfectly versus a more general purpose design.« What is also known is that the real edge of TPUs lies not in the hardware but in the software and in the way Google has optimized its ecosystem for the TPU. A lot of people mention the problem that every Nvidia »competitor« like the TPU faces, which is the fast development of Nvidia and the constant »catching up« to Nvidia problem. This month a former Google Cloud employee addressed that concern head-on as he believes the rate at which TPUs are improving is faster than the rate at Nvidia: »The amount of performance per dollar that a TPU can generate from a new generation versus the old generation is a much significant jump than Nvidia« In addition, the recent data from Google’s presentation at the Hot Chips 2025 event backs that up, as Google stated that the TPUv7 is 100% better in performance per watt than their TPUv6e (Trillium). Even for hard Nvidia advocates, TPUs are not to be shrugged off easily, as even Jensen thinks very highly of Google’s TPUs. In a podcast with Brad Gerstner, he mentioned that when it comes to ASICs, Google with TPUs is a »special case«. A few months ago, we also got an article from the WSJ saying that after the news publication The Information published a report that stated that OpenAI had begun renting Google TPUs for ChatGPT, Jensen called Altman, asking him if it was true, and signaled that he was open to getting the talks back on track (investment talks). Also worth noting was that Nvidia’s official X account posted a screenshot of an article in which OpenAI denied plans to use Google’s in-house chips. To say the least, Nvidia is watching TPUs very closely. Ok, but after looking at some of these numbers, one might think, why aren’t more clients using TPUs? Where are the problems for the wider adoption of TPUs The main problem for TPUs adoption is the ecosystem. Nvidia’s CUDA is engraved in the minds of most AI engineers, as they have been learning CUDA in universities. Google has developed its ecosystem internally but not externally, as it has used TPUs only for its internal workloads until now. TPUs use a combination of JAX and TensorFlow, while the industry skews to CUDA and PyTorch (although TPUs also support PyTorch now). While Google is working hard to make its ecosystem more supportive and convertible with other stacks, it is also a matter of libraries and ecosystem formation that takes years to develop. It is also important to note that, until recently, the GenAI industry’s focus has largely been on training workloads. In training workloads, CUDA is very important, but when it comes to inference, even reasoning inference, CUDA is not that important, so the chances of expanding the TPU footprint in inference are much higher than those in training (although TPUs do really well in training as well – Gemini 3 the prime example). The fact that most clients are multi-cloud also poses a challenge for TPU adoption, as AI workloads are closely tied to data and its location (cloud data transfer is costly). Nvidia is accessible via all three hyperscalers, while TPUs are available only at GCP so far. A client who uses TPUs and Nvidia GPUs explains it well: »Right now, the one biggest advantage of NVIDIA, and this has been true for past three companies I worked on is because AWS, Google Cloud and Microsoft Azure, these are the three major cloud companies. Every company, every corporate, every customer we have will have data in one of these three. All these three clouds have NVIDIA GPUs. Sometimes the data is so big and in a different cloud that it is a lot cheaper to run our workload in whatever cloud the customer has data in. I don’t know if you know about the egress cost that is moving data out of one cloud is one of the bigger cost. In that case, if you have NVIDIA workload, if you have a CUDA workload, we can just go to Microsoft Azure, get a VM that has NVIDIA GPU, same GPU in fact, no code change is required and just run it there. With TPUs, once you are all relied on TPU and Google says, “You know what? Now you have to pay 10X more,” then we would be screwed, because then we’ll have to go back and rewrite everything. That’s why. That’s the only reason people are afraid of committing too much on TPUs. The same reason is for Amazon’s Trainium and Inferentia.« source: AlphaSense These problems are well known at Google, so it is no surprise that internally, the debate over keeping TPUs inside Google or starting to sell them externally is a constant topic. When keeping them internally, it enhances the GCP moat, but at the same time, many former Google employees believe that at some point, Google will start offering TPUs externally as well, maybe through some neoclouds, not necessarily with the biggest two competitors, Microsoft and Amazon. Opening up the ecosystem, providing support, etc., and making it more widely usable are the first steps toward making that possible. A former Google employee also mentioned that Google last year formed a more sales-oriented team to push and sell TPUs, so it’s not like they have been pushing hard to sell TPUs for years; it is a fairly new dynamic in the organization. Google’s TPU is the biggest competitive advantage of its cloud business for the next 10 years The most valuable thing for me about TPUs is their impact on GCP. As we witness the transformation of cloud businesses from the pre-AI era to the AI era, the biggest takeaway is that the industry has gone from an oligopoly of AWS, Azure, and GCP to a more commoditized landscape, with Oracle, Coreweave, and many other neoclouds competing for AI workloads. The problem with AI workloads is the competition and Nvidia’s 75% gross margin, which also results in low margins for AI workloads. The cloud industry is moving from a 50-70% gross margin industry to a 20-35% gross margin industry. For cloud investors, this should be concerning, as the future profile of some of these companies is more like that of a utility than an attractive, high-margin business. But there is a solution to avoiding that future and returning to a normal margin: the ASIC. The cloud providers who can control the hardware and are not beholden to Nvidia and its 75% gross margin will be able to return to the world of 50% gross margins. And there is no surprise that all three AWS, Azure, and GCP are developing their own ASICs. The most mature by far is Google’s TPU, followed by Amazon’s Trainum, and lastly Microsoft’s MAIA (although Microsoft owns the full IP of OpenAI’s custom ASICs, which could help them in the future). While even with ASICs you are not 100% independent, as you still have to work with someone like Broadcom or Marvell, whose margins are lower than Nvidia’s but still not negligible, Google is again in a very good position. Over the years of developing TPUs, Google has managed to control much of the chip design process in-house. According to a current AMD employee, Broadcom no longer knows everything about the chip. At this point, Google is the front-end designer (the actual RTL of the design) while Broadcom is only the backend physical design partner. Google, on top of that, also, of course, owns the entire software optimization stack for the chip, which makes it as performant as it is. According to the AMD employee, based on this work split, he thinks Broadcom is lucky if it gets a 50-point gross margin on its part. Without having to pay Nvidia for the accelerator, a cloud provider can either price its compute similarly to others and maintain a better margin profile or lower costs and gain market share. Of course, all of this depends on having a very capable ASIC that can compete with Nvidia. Unfortunately, it looks like Google is the only one that has achieved that, as the number one-performing model is Gemini 3 trained on TPUs. According to some former Google employees, internally, Google is also using TPUs for inference across its entire AI stack, including Gemini and models like Veo. Google buys Nvidia GPUs for GCP, as clients want them because they are familiar with them and the ecosystem, but internally, Google is full-on with TPUs. As the complexity of each generation of ASICs increases, similar to the complexity and pace of Nvidia, I predict that not all ASIC programs will make it. I believe outside of TPUs, the only real hyperscaler shot right now is AWS Trainium, but even that faces much bigger uncertainties than the TPU. With that in mind, Google and its cloud business can come out of this AI era as a major beneficiary and market-share gainer. Recently, we even got comments from the SemiAnalysis team praising the TPU: »Google’s silicon supremacy among hyperscalers is unmatched, with their TPU 7th Gen arguably on par with Nvidia Blackwell. TPU powers the Gemini family of models which are improving in capability and sit close to the pareto frontier of $ per intelligence in some tasks« source: SemiAnalysis How many TPUs does Google produce today, and how big can that get? Here are the numbers that I researched:
--------------------------------------------------

Title: As Founder Ray Dalio Warns the Market Is in a Bubble, Bridgewater Associates Just Bought CoreWeave Stock
URL: https://www.barchart.com/story/news/36343272/as-founder-ray-dalio-warns-the-market-is-in-a-bubble-bridgewater-associates-just-bought-coreweave-stock
Time Published: 2025-11-27T13:00:02Z
Description: Down by over 60% from all-time highs, CoreWeave stock offers significant upside potential for shareholders, despite concerns about an AI bubble.
--------------------------------------------------

Title: Sands Capital Select Growth Strategy’s Top Absolute Individual Contributor: NVIDIA Corporation (NVDA)
URL: https://finance.yahoo.com/news/sands-capital-select-growth-strategy-125514597.html
Time Published: 2025-11-27T12:55:14Z
Description: Sands Capital, an investment management company, released its “Sands Capital Select Growth Strategy” Q3 2025 investor letter. A copy of the letter can be...
--------------------------------------------------

Title: Will Michael Burry’s Big Nvidia Bet Pay Off? NVDA Stock Tumbles As Polymarket Rakes It In
URL: https://consent.yahoo.com/v2/collectConsent?sessionId=1_cc-session_e3f583e2-6ef1-4dc7-a603-897b8f620cb0
Time Published: 2025-11-27T11:48:47Z
Description: None
--------------------------------------------------