List of news related to Meta stock price META:

Title: Best VR Porn Games to Play in 2025
URL: http://chicagoreader.com/adult/vr-porn-games/
Time Published: 2025-02-12T05:11:42Z
Full Content:
VR porn games have taken adult entertainment to the next level, thanks to huge leaps in headset technology and production quality. Gone are the days of static screens, now, you can step inside the action with hyper-realistic 3D visuals, smooth animations, and responsive gameplay that puts you in control. Whether curious or a longtime fan, VR offers a completely new way to experience adult content, making it more immersive and interactive than ever. If you haven’t tried VR porn yet, now’s the perfect time to dive in and see what all the hype is about! To save you the trial and error, we’ve put together the ultimate list of the 10 Best VR porn games on the market. We’ve done deep dives into the top ten, breaking down what makes them stand out, and rounded up the rest as honorable mentions. Expect cutting-edge technology, jaw-dropping realism, immersive storylines, and erotic encounters that push the limits of virtual pleasure. Ooh la la! Is it all fun and games? While VR porn games offer immersive experiences, it’s essential to remember that they also come with potential downsides, particularly in how they can affect players’ socialization, relationships, and worldview. One major concern is social isolation. Because VR can feel incredibly realistic, some users may find themselves spending more time in virtual relationships rather than engaging with real-world connections. Over time, this could lead to difficulty in forming and maintaining personal relationships, as interacting with AI-driven characters is often easier and less demanding than dealing with real people. Another issue is the reinforcement of unrealistic expectations. Many VR adult games allow for complete customization and control, something that simply isn’t possible in real-world relationships. This can create distorted perceptions of intimacy, where players expect perfect responses, unrealistic bodies, or total dominance over interactions, which can lead to dissatisfaction with real-life partners. The Impact on Attitudes Toward Women and Femme-Presenting People Another potential downside of VR porn games is how they reinforce misogynistic attitudes or unrealistic expectations toward women and femme-presenting people. Many of these games prioritize male-centered fantasies, often portraying women as hypersexual, submissive, or endlessly accommodating without emotional needs or boundaries. Over time, frequent exposure to these dynamics can shape unrealistic or harmful views on real-life relationships. It’s possible that players may come to expect one-sided interactions that don’t require mutual respect or effort. Yikes! Additionally, some games promote power imbalances, reinforcing a false sense of intimacy that is inherently based on dominance and control, rather than consent and connection. While fantasy has its place, repeated exposure without critical thinking can blur the line between healthy kinks and problematic beliefs. For those already struggling with social interactions, this can potentially deepen resentment or entitlement, particularly toward women who don’t conform to these scripted, fantasy-based behaviors. Engaging with VR porn games doesn’t automatically lead to misogynistic beliefs, but it’s important to critically examine the content, challenge harmful narratives, and balance virtual experiences with real-world perspectives on intimacy, agency, and respect. But it’s not all bad VR porn games also offer unique social and personal benefits, beyond just entertainment. For some, they provide a safe space to explore fantasies, build confidence, and understand personal desires without pressure. Multiplayer games foster social connections, allowing users to interact with others in a low-stress environment. VR can also enhance long-distance relationships, creating shared intimate experiences despite physical distance. When balanced with real-world interactions, VR porn games can be a fun, engaging, and even confidence-boosting outlet. 1. VRPorn – Best for overall variety VRPorn is one of the best destinations for high-quality adult virtual reality content, and it’s not just about the videos, it’s the tech that makes it stand out. If you’re into immersive experiences, VRPorn takes things to another level with cutting-edge visuals, optimized playback, and seamless compatibility with all the major VR headsets. What makes it worthwhile? First, the sheer variety. Whether you’re into big-budget studio productions or independent creators making waves with top notch interactive adult content, there’s a huge range of options. The site also stays ahead of the curve with crisp 6K and even 8K resolution videos, making everything feel incredibly lifelike. Plus, their smart UI makes it easy to navigate, download, and stream without the hassle. Another big win is the tech integration. VRPorn works with multiple devices, from Oculus to PlayStation VR, ensuring that no matter what setup you have, you get a smooth, high-quality experience. Many scenes also feature depth and head-tracking, so it’s not just a 360° video—it actually feels like you’re there. If you’re curious about VR adult content, VRPorn is a solid choice, balancing high-tech innovation with a massive, well-curated library. Key Features: Sign-up grants you access to thousands of VR adult scenes, not just games Compatible Tech: Pricing: To maximize your experience: VRPorn is perfect if you’re new to the VR porn world and looking to figure out what you’re into and what kind of games you want to play. 2. Holodexxx – Best for visual realism Holodexxx is challenging the limits of adult content by blending cutting-edge technology with hyper-realistic virtual experiences. Unlike standard VR porn sites, Holodexxx is all about high-fidelity, interactive adult entertainment, bringing digital performers to life in a way that feels next-level. Holodexxx uses advanced 3D scanning and motion capture to create ultra-detailed, lifelike models of real adult performers. Instead of just watching a scene, you can interact with these digital avatars in real-time, making the experience feel much more immersive than traditional VR. The attention to detail in textures, lighting, and movement adds an extra layer of realism that sets it apart from standard 360° videos. Another standout feature is the level of interactivity. Instead of being a passive viewer, you have control over certain aspects of the experience, which makes everything feel more personal and engaging. The platform also keeps evolving, integrating AI and improving realism with each update. For anyone interested in the future of adult entertainment, Holodexxx is worth keeping an eye on. It’s not just about consuming content—it’s about stepping into a fully immersive, interactive world. Key Features: Compatible Tech: Pricing: To maximize your experience: Join the Holodexxx Patreon and Discord for deals, updates, and special members-only benefits 3. DezyRed – Best for immersive and mentally stimulating interactions DezyRed is carving out a space in the world of interactive adult entertainment by combining high-end technology with immersive experiences. Unlike traditional cam sites or VR platforms, DezyRed focuses on real-time interactivity, allowing users to engage with digital models in a way that feels dynamic and personal. This is thanks to AI-powered avatars that aren’t just static characters but responsive, learning entities that adapt to interactions. This means conversations and experiences feel more fluid and natural, rather than pre-scripted or repetitive. The site also utilizes motion capture and realistic rendering to create high-detail visuals that go beyond standard adult CGI. Another big plus is accessibility. DezyRed works across multiple platforms, from desktop to VR, making it easy to dive in no matter what your setup is. If you’re into next-gen adult technology where AI, VR, and real-time interaction blend seamlessly DezyRed is definitely worth exploring. Key Features: Compatible Tech: Pricing: To maximize your experience: You can watch for free, but you’ll need to purchase credits to interact, so use your initial free credits wisely! 4. XStoryPlayer – Best for the thrill of the chase XStoryPlayer is a unique platform because it’s a blend of adult gaming and interactive storytelling, offering an experience that goes well beyond passive viewing. If you’re into narrative-driven adult content with actual gameplay mechanics, this is a site worth exploring! The beauty of XStoryPlayer is that it uses a physics-based engine to create realistic character movements, responsive environments, and interactive dialogue. Instead of just watching a scene unfold, you get to shape the experience, making choices that influence how the story progresses. The game mechanics include everything from conversation algorithms to physics-driven intimacy, making interactions feel much more engaging than traditional adult content. Another highlight is the modding potential: the platform is built to be customizable, allowing users to tweak or even create their own scenarios. If you’re looking for something that combines adult content with immersive, player-driven storytelling, XStoryPlayer offers a compelling, tech-forward experience. Key Features: Compatible Tech: Pricing: To maximize your experience: You can purchase straight from XMoon Productions 5. DominatrixSimulator – Best for virtual BDSM enthusiasts Dominatrix Simulator is a standout in the adult gaming space, offering a deeply immersive, BDSM-themed experience that goes beyond standard VR content. Instead of just watching, you participate, stepping into a submissive role where your choice, and obedience, shape the interactions. The tech behind it is what makes it really shine. Designed for VR, the game uses voice recognition, motion tracking, and realistic AI-driven Dommes who react dynamically to your responses. The visuals are polished, with detailed environments and fluid character animations that make the experience feel more lifelike. What sets it apart is its emphasis on psychological engagement. It’s not just about visuals, it’s about power exchange, control, and submission in a way that feels organic and immersive. Whether you’re new to BDSM or experienced, Dominatrix Simulator offers a specific and unique, tech-driven way to explore dominance and submission in a safe, virtual space. Key Features: Compatible Tech: Pricing: To maximize your experience: Since the whole vibe of this game is about FemDom and being a submissive man “under her boot”, if this is a major kink for you, consider the pro-rated savings of the Yearly or Lifetime memberships. 6. SinVR – Best for fantasy fulfillment If you’re into adult content with a bit of fantasy and interactivity, SinVR is worth checking out.Unlike standard VR porn, it offers a more game-like experience where you can customize characters, explore different scenarios, and interact with models in a way that feels dynamic. The tech behind SinVR is what makes it stand out. The characters are well-rendered, with detailed animations and realistic physics that add to the full immersion. The platform also supports many, but not all, headsets, making it accessible to most users. Key Features: Compatible Tech: Pricing: To maximize your experience: Push your imagination to the limit with the pre-set characters and scenes, including 200+ sex positions and scene options. 7. Citor3 – Best for games to get you off under a time crunch Citor3 is an adult gaming platform that blends interactive storytelling with immersive 3D gameplay. Unlike some VR porn games that rely on pre-rendered scenes, Citor3 lets you erotically engage with AI-driven characters, explore detailed environments, and experience fluid, physics-based interactions. The gameplay focuses on immersion, featuring smooth animations and responsive mechanics that let you control the pace and direction of encounters. Whether you’re following a structured narrative or experimenting with freeform interactions, the tech behind Citor3 ensures a visually polished and engaging experience. For those looking for a more hands-on, interactive approach to adult gaming, Citor3’s 30-minute gameplay is a great way to see how its mechanics and world-building come together. If you’re into adult content with a focus on interactivity and player-driven experiences, Citor3 is definitely worth a look. Key Features: Compatible Tech: Pricing: To maximize your experience: You can play some Citor3 games on VRPorn.com (and they have great prices) 8. VRLove – Best for a choose-your-own-adventure experience VRLove goes beyond simple 360° videos, offering a more immersive and customizable approach. Instead of just watching, you engage with AI-driven characters in a fully 3D environment, creating a more dynamic and personal experience. The technology behind VRLove is a major highlight: realistic character models, lifelike animations, and thoughtful details in the surroundings make everything feel more natural. In terms of immersion, VRLove is next level, with teledildonic support, which allows users to sync compatible devices for real-time, interactive pleasure. This means the experience isn’t just visual, it becomes physical; the integration of haptic feedback enhances the realism, making interactions with AI-driven characters feel more responsive and engaging. Combined with VRLove’s high-quality visuals, smooth animations, and customization options, this feature adds a whole new layer of interactivity and sexiness. Key Features: Compatible Tech: Pricing: To maximize your experience: Take advantage of the Story Mode/Free Mode options to experience a variety of scenes. 9. 3DXChat – Best for players looking for real human sexual interaction via VR 3DXChat is more than just a VR adult game—it’s a full-fledged social platform where users can connect, chat, and explore intimate experiences together. Unlike solo VR experiences, 3DXChat thrives on real-time multiplayer interaction, letting you meet and engage with others in a shared virtual world. The game features detailed avatar customization, so you can create a look that suits your style. You can then interact through text or voice chat, dance at virtual clubs, hang out in scenic environments, or engage in more … intimate encounters. VR enhances the social element, making it easy to interact, and giving you the feeling of being present in the room with other users. Whether you’re into casual flirting, role playing, or building deeper connections, 3DXChat offers a unique blend of social networking and adult entertainment in a visually engaging space. Key Features: Compatible Tech: Pricing: To maximize your experience: Players who are used to interacting with stock characters, not real humans, may need to adjust their approach and behavior. 10. Let’s Play with Nanai – Best for a virtual girlfriend experience Let’s Play With Nanai is a hentai VR game that focuses on interactivity and immersion, offering a more hands-on experience than standard animated content. Designed specifically for VR, it lets players engage with Nanai, a “virtual girlfriend”, in a fully 3D space, with smooth animations and responsive mechanics that make Nanai come alive. Users will be drawn to the high level of interactivity and realism compared to traditional hentai games. Instead of just watching or clicking through a scripted scene, players get full control over interactions, making the experience feel more immersive, interpersonal and dynamic. Nanai is also popular for its VR optimization and runs smoothly on major VR headsets, with high-quality character models, lifelike animations, and realistic physics that enhance movement and touch responsiveness. Key Features: Compatible Tech: Pricing: To maximize your experience: Let’s Play with Nanai is easy to find, but you can purchase straight from IMagineVR and join their Patreon for extra goodies and up to date info on new releases, etc. Can’t find quite what you’re looking for in our top 10? Try these: Choosing the right adult VR game depends on what kind of experience you’re looking for. Here are some key factors to consider: VR Compatibility Not all VR games work with every headset. Check if the game supports your specific device (e.g., Oculus, HTC Vive, Valve Index, PSVR). Some games also run in desktop mode if you don’t have VR gear. Level of Interactivity Do you want a passive viewing experience (like 360° videos) or a fully interactive game with AI-driven characters, physics-based interactions, and player choices? Graphics & Animation Quality Some games focus on highly detailed, realistic models, while others go for a stylized or anime-inspired look. Consider what visual style appeals to you. Customization Options Many games allow you to modify characters, outfits, and scenarios. If personalizing the experience is important, look for a game with strong customization features. Multiplayer vs. Solo Experience Some games, like 3DXChat, emphasize social interaction, while others are single-player-focused. Decide if you want a solo experience or a multiplayer world with real-time interactions. Teledildonics Support If you’re interested in haptic feedback devices, check if the game is compatible with interactive toys like Lovense or Kiiroo for a more immersive experience. Storyline & Gameplay Depth Some games offer deep narratives and role-playing elements, while others are more focused on quick, no-strings-attached experiences. Choose based on whether you prefer a story-driven game or a casual encounter. Common questions about this new and exciting erotic technology. Are VR porn games safe to download and play? As with any software, it’s best to download from official websites or trusted platforms to avoid malware. Always check reviews, system requirements, and privacy settings before installing. What do I need to play adult VR games? To play adult VR games, you’ll need a VR-compatible headset (like Oculus Quest, HTC Vive, or Valve Index), a VR-ready PC (for high-end games), and enough storage space for downloads. Some games also offer non-VR modes for standard screens. What are the best VR headsets? The best headset depends on your budget and preferences. High-end options like the Valve Index and HTC Vive Pro offer top-tier visuals and tracking, while more affordable headsets like the Oculus Quest 2 provide wireless freedom and solid performance. Many PC-based games also support Windows Mixed Reality headsets. Can I play VR games without a VR headset? Yes! Some games offer a desktop mode where you can play with standard controls and a monitor. However, you won’t get the same level of immersion as you would in VR. Do VR games require an internet connection? It depends on the game. Single-player experiences usually work offline, while multiplayer games (like 3DXChat) need an active internet connection for real-time interaction with other players. Are VR porn games multiplayer or single-player? Both options exist! Games like 3DXChat offer multiplayer experiences where you can interact with other players, while games like Dominatrix Simulator focus on single-player immersion with AI-driven characters. Can I customize characters and experiences? Many adult VR games offer deep customization, allowing you to adjust character appearances, outfits, and even personalities. Some also let you modify environments and interaction styles. Do VR porn games support teledildonics? Many do, yes! Some games sync with interactive sex toys (check out Lovense or Kiiroo) to provide haptic feedback, making in-game actions feel more realistic. Are there VR porn games with more intellectual or story-driven gameplay? Absolutely! Games like XStoryPlayer and Dominatrix Simulator feature narrative-driven experiences, where your choices affect the outcome. These are great if you prefer an interactive story over casual encounters. How realistic are the animations and physics in adult VR games? It varies by game. Some use advanced physics engines for realistic movements and interactions, while others prioritize a stylized or fantasy aesthetic. Games that support teledildonics also add an extra layer of realism with synchronized haptic feedback. Love them or hate them, VR porn games have revolutionized the way people engage with adult content, offering unparalleled interactivity, and personalization. With advancements in VR headset technology these experiences have evolved far beyond traditional adult media, and who knows where they will take us next! When enjoyed in moderation, VR porn games can be a fun, confidence-boosting outlet, allowing users to safely explore desires, improve social skills in virtual spaces, or even strengthen connections in long-distance relationships. The addition of teledildonics further enhances realism, bridging the gap between fantasy and physical sensation. Of course, as with all media, critical engagement is key. Some VR porn games reinforce harmful stereotypes or unrealistic portrayals of women and relationships, so always remain critical of your consumption. It’s essential to recognize the difference between fantasy and reality, boundaries and consent, and to balance virtual experiences with real-life connections for healthy perspectives on intimacy. Ultimately, VR porn games are a thrilling and ever-evolving frontier in adult entertainment. Whether you’re a casual player or a dedicated enthusiast, they offer a unique way to engage with sexuality, pushing the boundaries of pleasure and personal exploration. As technology continues to evolve, so too will the experiences available, making this an exciting space to watch, and play in, for years to come. Violet Fawkes is a pleasure educator and sex-positive advocate committed to guiding individuals on their journeys of sexual self-discovery. She offers insightful product reviews, including analyses of sex toys and pleasure products. Her writing delves into topics like body acceptance, ethical non-monogamy, and the nuances of intimate relationships, fostering a comprehensive understanding of sexual wellness. Violet’s mission is encapsulated in her guiding principles: Explore. Empower. Enrich. She encourages curiosity, creativity, and courage in personal exploration, aiming to dismantle societal taboos surrounding sexuality and promote a more open, informed, and accepting dialogue. Instagram @violet_fawkes Threads @violet_fawkes Bluesky @violet-fawkes.bsky.social
--------------------------------------------------

Title: Super Micro 'confident' it will meet SEC deadline and reach $40 billion next fiscal year
URL: https://www.cnbc.com/2025/02/11/super-micro-confident-it-will-meet-sec-deadline.html
Time Published: 2025-02-11T22:59:13Z
Description: Super Micro expressed confidence it will meet a key SEC deadline this month and hit $40 billion in revenue in fiscal 2026.
--------------------------------------------------

Title: Elon Musk’s Now $42 Billion Poorer This Month—As Scrutiny On Trump Ties Grows
URL: https://www.forbes.com/sites/dereksaul/2025/02/11/elon-musks-now-42-billion-poorer-this-month-as-scrutiny-on-trump-ties-grows/
Time Published: 2025-02-11T20:34:07Z
Full Content:
Tesla stock’s steep losses in early 2025 have cut tens of billions of dollars away from the net worth of the world’s richest man, the electric vehicle maker’s CEO Elon Musk, whose outspoken role in President Donald Trump’s administration draws questions about what it may mean for Tesla. Musk talks at an October rally for Trump in New York. Shares of Tesla declined 6.3% to $328.50 in Tuesday trading, closing at its lowest share price since Nov. 15. Catalyzing Tuesday’s drop were advancements in autonomous driving from its Chinese EV rival BYD and a skeptical note from Oppenheimer analysts led by Colin Rusch warning Musk’s “political activity risks consumer backlash.” The Oppenheimer note, which referenced “concerning” January sales in China and Europe, comes a day after Stifel analysts led by Stephen Gengaro similarly warned “the negative downturn in consumers' perception of Elon Musk could result in a ‘headwind to sales’ for Tesla,” which inspired a 3% loss in Monday trading for Tesla shares. Tesla stock is now down 18.8%, or more than $76 per share, in February, declining almost 32% from its all-time closing high of $479.86 set Dec. 17. Nobody has been affected more on a gross basis by the Tesla slump than its largest shareholder, Musk, who owns nearly 13% of the company and a further 9% equity award pending legal appeal (Forbes discounts Musk’s 9% bonus by 50% in its valuation to reflect its legal uncertainty). Musk was worth $378.8 billion Tuesday, according to Forbes’ latest calculations – a $42.8 billion decrease from his $421.6 billion net worth at the end of January – though he remains about $130 billion wealthier than the next richest person on the planet, Meta CEO Mark Zuckerberg. $12.5 billion. That’s how much Musk’s fortune declined during Tuesday’s Tesla slump as the 53-year-old registered his lowest end-of-day net worth since Dec. 10. The Oppenheimer analysts wrote they believe “Musk's political activity has fans in certain circles, but that his public life risks alienating consumers and employees as the Trump administration tests the limits of its power.” Musk’s more than $150 billion estimated Tesla stake is the biggest contributor to his net worth, while his stakes in his private multi-billion-dollar companies SpaceX, X and xAI make up a majority of the rest of his fortune. Tesla stock is still up about 30% since Election Day, though the initial rally has lost steam as the company’s fourth-quarter earnings fell short of Wall Street consensus forecasts and its car sales in Europe and China declined. After donating nearly $290 million toward Trump and other GOP causes ahead of the 2024 election, Musk has played a critical role in the opening weeks of Trump’s second term, most notably heading the DOGE agency aimed at significantly pulling back the federal government’s footprint. Musk submitted a $97.4 billion bid Monday to buy the nonprofit arm of OpenAI, a competitor to his generative artificial intelligence startup, led by rival Sam Altman. Musk’s offer is “a distraction from TSLA's challenges,” wrote Rusch. Musk and Altman cofounded OpenAI in 2015 before a messy breakup over the company’s mission. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Here are 3 of my most profitable investments in ASX shares ever (and which one I'd buy more of right now)
URL: https://www.fool.com.au/2025/02/12/here-are-3-of-my-most-profitable-investments-in-asx-shares-ever-and-which-one-id-buy-more-of-right-now/
Time Published: 2025-02-11T20:30:00Z
Description: I reckon only one of these shares is worthy of a buy today. 
The post Here are 3 of my most profitable investments in ASX shares ever (and which one I'd buy more of right now) appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: This corner of the stock market could be an under-the-radar winner of the AI spending boom
URL: https://markets.businessinsider.com/news/stocks/retail-stocks-ai-spending-mega-cap-capex-artificial-intelligence-walmart-2025-2
Time Published: 2025-02-11T20:15:10Z
Full Content:
Wall Street is assessing the impact of Big Tech's massive capex plans for this year, but there is another corner of the stock market that's set to benefit from big AI spending, Morgan Stanley said. According to Morgan Stanley, the spending race among AI "hyperscalers" is an overlooked tailwind for retail sector stocks like Walmart and Target. "Retail is on the cusp of a technology leap with AI, data, and automation. While retailers may not pursue AI infrastructure investments similar to tech companies, the tech capex boom suggests retailers are on the verge of and should benefit from a technology inflection," equity analyst Simeon Gutman wrote. This year's combined $300 billion of announced AI investing between Microsoft, Amazon, Google, and Meta, might not seem like a development set to directly impact retailers, but Morgan Stanley predicts is is likely to lead to a boost in capex among retailers who can afford it. That's as the spending boom will offer big-box stores investing opportunities to improve in-store experiences, advertising, and automation. Those best equipped to invest will see market shares gain, a trend likely to appear amid coming earnings reports. "The big should keep getting bigger and at a faster pace," the analysts wrote. Capital expenditures among retailers are projected to reach $55 billion, marking an average of around 7% year-on-year growth in 2025. Walmart, Costco, Target, Kroger, and Home Depot account for about 69% of total spend in hard, soft, and food retail. Walmart will lead the cohort in capex spening. Estimated to spend $22 billion this year, Walmart's expenditures are four times larger than what Costco is forecast to spend. Bank of America holds a $110 price target for Walmart stock, expecting improved profitability as well as digital advertising and marketplace growth. One challenge could pose a risk Gutman's thesis — cheaper AI allows smaller retailers to join in on AI growth. The possibility has emerged amid the recent introduction of DeepSeek, a Chinese AI that is supposedly cheaper and as capable as Silicon Valley tech. Its implications have sparked doubt about Big Tech's AI spending, briefly causing a trillion-dollar market wipeout last month. If DeepSeek does discount AI expenditures, under-the-radar retail names aren't the only ones set to win out. This scenario would make software stocks also worth buying. "That said, we do not think this will be the case in the near-term and, for now, retailers best equipped to spend should be able to widen their advantages," Gutman wrote. Indices Commodities Currencies Stocks
--------------------------------------------------

Title: 3 new reasons to dump Magnificent 7 stocks
URL: https://finance.yahoo.com/news/3-new-reasons-to-dump-magnificent-7-stocks-200016898.html
Time Published: 2025-02-11T20:00:16Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. With the often-hot Magnificent Seven trade on the skids less than two months into the year, it may be time to exit stage left before the selling picks up. "Over the last several years we have maintained the view, that it was prudent for long-only US equity managers to be at least market-weight the Mag 7. Today, our views have evolved to the point where we are changing our mind and believe lowering exposure is prudent," Trivariate Research founder and CEO Adam Parker said in a new note Tuesday. The Magnificent Seven trade of Meta (META), Amazon (AMZN), Google (GOOG), Apple (AAPL), Nvidia (NVDA), Microsoft (MSFT), and Tesla (TSLA) has underwhelmed of late. Only one of the big-cap tech components — Meta — has posted double-digit gains out of the box, more in line with the sector's usual strong performance. Amazon is the only other Mag Seven component to be up on the year to the tune of 5.2%, slightly ahead of the 3.5% increase for the S&P 500 (^GSPC). Alphabet, Apple, Nvidia, Microsoft, and Tesla are all down year to date, with an average drop of 3% based on Yahoo Finance's calculations. Tesla is the worst performer, off by 17% this year. Reasons for the sell-off range from weakening sales (Tesla) to rising fears tech companies are spending too much to build AI infrastructure (the rest of the Mag Seven). Veteran markets expert Parker thinks now is a good time for investors to reduce exposure for three reasons. For one, the Street is unlikely to stop scrutinizing how much is being spent on capex for AI in 2025 and 2026. Meta, Microsoft, Amazon, and Alphabet are slated to spend a cumulative $325 billion in capital expenditures and investments this year, Yahoo Finance's Laura Bratton reports. This would mark a 46% increase year over year for the four tech stalwarts. Amazon alone sees $104 billion in capital expenditures this year, well above prior analyst forecasts of $80 billion to $85 billion. The stocks have tended to react negatively to these bold spending commitments, points out Parker. "There is no question either way that the high capital spending will continue to come under increasing scrutiny until investors can better understand the return on today’s massive investments," says Parker. Valuation on Mag Seven stocks — despite their sell-off — also remains a concern for Parker. Parker's research shows the relative price to forward earnings multiple of the Mag Seven versus the rest of the S&P 500 is at a 42% premium. That's toward the upper range of its 25-year average. Warns Parker, "The high beta and increasingly high capital intensity combined with the elevated valuation of the Magnificent 7 is, in our judgment, an increasing cause for concern." Podcast: Buy, sell, or hold Mag 7 darling Nvidia? And lastly, the stocks just look too over-owned by investors. "We are apprehensive of the elevated beta-adjusted exposure of the Mag 7 vs. the top 500 US equities excluding the Mag-7. Today, the aggregate exposure of the Mag 7 is 31.3%, or almost a third of the market cap opportunity is Mag-7. However, on a beta-adjusted basis the current exposure of the Mag-7 is 44.7%. This means that a portfolio manager who owns in market-weight all the Magnificent 7 stocks has nearly half their fund’s beta-adjusted exposure in these stocks! This remains near highs of the last 25 years," Parker said. One bonus reason Yahoo Finance found when studying Parker's research: The stocks have an outsized exposure to all-out Wall Street bullishness. Only 4.8% of the 504 analyst recommendations on the Mag Seven are a Sell, Parker's team found. With the investment thesis on the Mag Seven changing, the uber-bullishness could prove to be misaligned with reality. — Brian Sozzi is Yahoo Finance's Executive Editor. Follow Sozzi on X @BrianSozzi, Instagram, and LinkedIn. Tips on stories? Email brian.sozzi@yahoofinance.com. Click here for the latest technology news that will impact the stock market Read the latest financial and business news from Yahoo Finance Sign in to access your portfolio
--------------------------------------------------

Title: The Raging Debate: When Will Quantum Arrive?
URL: https://www.forbes.com/sites/karlfreund/2025/02/11/the-raging-debate-when-will-quantum-arrive/
Time Published: 2025-02-11T18:51:09Z
Description: They are important niches to the scientists who can now solve previously unsolvable problems, but they may not constitute another multi-billion-dollar market.
--------------------------------------------------

Title: How Tariffs Make CFOs’ Jobs More Complex
URL: https://www.forbes.com/sites/cfo/2025/02/11/how-tariffs-make-cfos-jobs-more-complex/
Time Published: 2025-02-11T18:15:30Z
Full Content:
Plus: Consumers Feel Economic Change Slowly, Planned Aluminum And Steel Tariffs Rile Allies, Vought Uses OMB To Halt CFPB, Tech Companies Invest In AI Infrastructure There’s been a huge amount of change in the last month, and from an economic standpoint, it seems to be hitting consumers slowly, but surely. Job growth is stuck, prices don’t seem to be going anywhere, and many feel they are financially worse off than a year ago. January’s jobs data from the Labor Department, released last week, significantly missed growth projections and signified the weakest start to a year in terms of new jobs since 2016. Economists were expecting 170,000 new jobs, but the report showed the addition of just 143,000. And the expectation of job losses is growing. In the New York Fed’s Survey of Consumer Expectations, released this week, 14.2% of people feel a probability of losing their jobs in the next 12 months, up 2.3% from December. About a third of consumers said their household financial situation is worse off than a year ago, the Survey of Consumer Expectations shows. And while this is down slightly month-over-month, only about 19% say they are doing better than a year ago—a 3% decrease from December. Over a fifth aren’t optimistic about the economy and say they will be doing worse a year from now. January’s Consumer Confidence Index, reported two weeks ago, showed a 5.4-point decline from December. The data shows some optimism. Labor Department data showed record-high average hourly wages of $35.87, as well as an unemployment rate of 4%, down from 4.1% in December. And 36.6% feel their household finances will be better a year from now, according to the Survey of Consumer Expectations. However, there’s lots of room for improvement. Two-thirds of Americans feel that President Donald Trump administration’s policy focus so far isn’t doing enough to lower prices, according to a CBS News/YouGov poll last week. The next round of consumer price index numbers come out this week, which will show just how much prices are changing—though the consumer expectations survey shows that Americans anticipate price increases for food, gas, rent and medical care. CFOs are increasingly having issues with making projections since so many companies rely on imports, and so many different tariff threats are being quickly proposed, rescinded and taking effect. I talked to Bob Stark, global head of enablement of cash management software platform Kyriba, about how CFOs can plan in today’s circumstances. An excerpt from our conversation is later in this newsletter. Workers in Camden, N.J. use a crane to load a 60,000 pound coil of steel onto a railroad car. President Donald Trump signed another round of tariffs this week: 25% on all steel and aluminum imports. “This is a big deal, the beginning of making America rich again,” Trump said as he signed the tariff orders in the Oval Office, CBS News reported. This is a familiar policy to Trump, who assessed tariffs on steel and aluminum in his first term to bolster domestic industry. Those tariffs—25% on steel imports and 10% on aluminum imports—had several exemptions, and ended up bringing the biggest benefit to steel companies, according to a 2018 analysis from the Peterson Institute for International Economics. Allies from the EU and Australia spoke out about the planned tariffs, which are reported to go into effect next month, saying that they will harm U.S. consumers and long-established partnerships. The U.S. imports about a quarter of all of the steel it uses, and about half of the aluminum, Reuters reports. Most of the steel the U.S. imports comes from Canada, Brazil and Mexico, while the lion’s share of aluminum imports come from Canada. Several analysts have said these new tariffs will have limited benefit to the U.S.—likely just for the steel companies—and will not likely create jobs. They are also likely to raise prices for most things made from metal. Even the United Steelworkers International Union feels the tariff proposal goes too far, and wrote the U.S. needs to take a more measured approach, distinguishing trusted trade partners. The move caused significant shifts in the stock market, boosting prices of domestic metals companies, including Alcoa and U.S. Steel. It also boosted gold prices to a record high of $2,938 per troy ounce, as investors search for stability. This isn’t the only move on tariffs in the last week. Trump announced during a Friday meeting with Japanese Prime Minister Shigeru Ishiba that he would announce reciprocal tariffs this week—tariffs equal to what other countries impose on U.S. goods. There’s no word yet on how those might work. Russell Vought at a confirmation hearing in January in front of the Senate Homeland Security and Governmental Affairs Committee. Last Thursday, the Senate confirmed Russell Vought as the head of the Office of Management and Budget with a vote along party lines. Vought, founder of the prominent right-wing Center for Renewing America and key adviser to the Heritage Foundation’s controversial Project 2025, was considered one of Trump’s more controversial nominees because of his viewpoint of expansive executive branch control on spending. One of his first big moves is essentially halting all work of the Consumer Finance Protection Bureau, taking its website offline and telling workers not come in and stop work activities. The CFPB was created in 2011 following the Great Recession and financial crisis. It’s an agency that protects consumers from unfair banking practices, but it also has deep-pocketed enemies. Elon Musk, who has been working with his Department of Government Efficiency task force to mercilessly cut perceived wasteful spending, has railed against CFPB because “there are too many duplicative regulatory agencies.” He said this in November, shortly after the agency announced more oversight of big tech companies and others offering digital funds transfers and payment wallet apps. Meta founder and CEO and billionaire Marc Andreessen also have spoken out against the agency, which has threatened action against tech companies’ use of financial data. Rendering of Meta's Sucre data center in Louisiana. AI evangelists say the technology can do a lot for companies, but to continue to grow—both revenues for companies and capabilities for users—some infrastructure investments are needed. Big companies are announcing bigger AI data center projects, with bigger budgets and footprints. Forbes’ Christopher Helman runs down many of these plans, which include not only huge facilities like Meta’s $10 billion planned Sucré data center in Louisiana—covering 4 million square feet of floor space on 2,250 acres—but enough infrastructure to power it all—$3.2 billion worth for Meta, coming from twin high-efficiency natural gas turbines. In all, Meta plans to spend $60 to $65 billion on these data centers in the next year. Meta’s not alone in these investments. Sundar Pichai, CEO of Google parent Alphabet, announced on last week’s earnings call that the next year will see about $75 billion in capital expenditures for technical infrastructure. CFO Anat Ashkenazi said that in the last quarter Google hit capacity issues with its cloud and services business, which constrained its growth. Microsoft is planning to invest $80 billion in data centers. And through a partnership announced from the White House between SoftBank, Oracle and OpenAI called “Stargate,” SoftBank will invest $500 billion in U.S. data centers during the next four years. Kyriba Global Head of Enablement Bob Stark and the Port of Los Angeles. Economic predictions were challenging for CFOs before Trump’s second presidency began, but announcements of potential tariffs on different countries and industries, and along a variety of timelines, add a lot more uncertainty to the future. I talked to Bob Stark, global head of enablement at cash management software platform Kyriba about what CFOs are going through and what they can do to prepare for the short- and long-term future. This conversation has been edited for length, clarity and continuity. A longer version is available here. Tariffs were also a feature of President Trump’s first term. How similar is what’s happening right now to then? Stark: You look at what was announced around steel and aluminum, and that feels very similar to what we saw in the previous term. That said, I think the differences—and our clients had the same reaction: the blanket tariffs, the punitive tariffs, if you tariff us, then we’ll tariff you back and it’s sort of back and forth—feel a little bit more significant than what we had experienced previously. It certainly feels like an acceleration. CFOs and finance teams are definitely looking at that as a lot of different potential scenarios that you have to prepare for. It’s not as simple as: This industry is affected [with a] 25% hit, so if you’re a Canadian steel exporter, you have this problem—which is fairly predictable. You can figure what 25% looks like. Same with importers on the U.S. side. [They’re] just trying to understand what does our cost look like? They can map out demand, but it’s the back and forth and the uncertainty around that. It’s difficult to predict exactly when impacts will be felt, which is a big challenge for CFOs. Many of them are looking to provide updates to guidance quarterly. In this quarter, do we have a cashflow impact that we need to control? Do we have a liquidity challenge where we need to make some plans? Do we need to provide different or maybe more expansive hedging of FX [foreign currency exchange] because we recognize that it’s these short and sharp movements that can have a significant impact in the balance sheet and income statement reporting at the end of the quarter. There’s a lot. It’s the uncertainty of what is the actual impact. There’s so many more scenarios you have to play out in different ways that feel different than what we saw several years back when we first saw this idea of tariffs being proposed and implemented. This time around, it feels to me like some of the tariffs that are being threatened and announced are more unexpected. Is that your impression? Absolutely. It feels to a corporate CFO that there’s a daily news cycle that you have to invest more time in following. You have to then build that into your analysis and reporting. If you’re doing something as simple as: What is the impact of cashflow if this blanket tariff goes in. What’s the difference? It’s being delayed by 30 days. What does that mean? There’s a lot of uncertainty and unpredictability around that. That’s the biggest challenge for CFOs. It’s like playing that hand game with your kids. You put one down, then there’s another, and then there’s another, and then there’s another. And then suddenly you think, ‘Wow, where did this go?’ That’s the part that’s really difficult to predict and build into an analysis. When the CEO is calling you because they just saw this on CNBC, and they’re asking the question: What’s the impact on our cash flow? Do we need to change our guidance? You have to juggle not just one or two different possibilities, but [there’s] probably three or four different scenarios you need to model against the same data, and then you have to be in a position to answer with precision on demand. And, ‘I’ll get back to you tomorrow’ is not an acceptable response. It’s ‘I’ll tell you right now, if these tariffs go through, this is what the impact is. If they’re delayed out 30 days, if they’re delayed out six months…’ Those are the scenarios that you have to be able to report to the board with confidence so that everyone's prepared. How do you think a company can make its way through the next four years? There’s three things that CFOs can do, aside from watching all the news and being able to utilize AI to summarize the things that they can’t watch live. Once they’re armed with that insight and understanding of sentiment, they’re in a position to do three things. Number one: FX. Those sharp, short-term movements can be the difference between hitting and missing guidance. Every CFO has to be able to quantify the impact of currency on the balance sheet, income statement and cash flow. If they’re not in a position to do that, then it erodes confidence a little bit. No one wants choppiness. They want certainty, and they want at least three months—ideally more—in advance. If you can eradicate the impact of FX, or mute it significantly, you’re in a better position than many of your peers. Number two: forecasting and planning. Every time there’s impactful news, CFOs need to answer: What would this mean to our cash and liquidity in the future? Finance leaders, they must have precise answers. They have to have them at the ready. They have to be available for multiple scenarios so that hypothetically speaking, they can answer a question like this: Our cash flow decreases by 30% if this tariff actually happens in the next 20 days. You need to be able to answer the question with that level of precision. That CEO and the board, they’re not waiting an hour. They’re probably not waiting five minutes either. They expect data and insights right now. The third part: Getting control over the cash lifecycle. CFOs need to be able to pull multiple levers to maintain control of their cash, especially in response to customers wanting to bring forward orders to get ahead of tariffs. That’s a perfect example of being able to have control, making sure that they can provide the structure, the systems financing to allow changes in the supply chain to occur. Also [to be able to provide control] in response to potential reshoring or recomposition of supply chains. Some of that is a little bit more longer term. That might be later in 2025 or into 2026. We recognize we just can’t compose our supply chain this way. We need to reshore this part of it. We need to eliminate this country out of our supply chain. It just doesn’t work in terms of the cost structure. Whatever that scenario looks like for them, the CFO needs to be able to put in the structure. It’s very tactical. Everything from bank accounts to cash management structures, the ability to sweep the and pool cash and mobilize it wherever it needs to be, to not only invest, but also repatriate cash so you can meet those cashflow objectives of the organization. There’s a lot of complexity there. Nevermind working with supply chains, working with customers to be able to pull the lever of: We’re going to pay you earlier, but it’s going to come at a cost, and as a result, we need to be able to do that. You want to get paid earlier, Mr. and Mrs. Supplier, we’ll do that, but here’s the program to do it. Same thing with accelerating receipt of cash. If you need to accelerate collections, you need a structure with that. It’s not a program that just happens like that. It’s a year in the making to put that structure in with your banks and with your finance and providers. Billionaire Bill Ackman’s Pershing Square Capital Management disclosed a major stake in ride hailing company Uber on Friday, boosting the company’s stock. $2.3 billion: Value of Pershing’s stake in the company, which is 30.3 million shares 17%+: Increase in Uber’s share price over the last five trading days ‘One of the best managed and highest quality businesses in the world’: What Ackman wrote about Uber on X Billionaire Fernando De Leon has an unusual backstory and advice to build a business that will grow. Here are seven of his tips. Work is important, but stressful. Here are five ways you can rest and recharge without compromising your drive and ambition. President Donald Trump made an announcement about currency this week. What was it? A. Pennies will no longer be minted because they are wasteful B. Harriet Tubman will not be appearing on future $20 bills C. The U.S. is returning to the gold standard D. $1 coins will replace bills in three years See if you got it right here. Editor’s note: Bob Stark’s title has been corrected to global head of enablement. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Do Our Courses Need An Update To Reflect The Workings Of Information Businesses?
URL: https://www.forbes.com/sites/shivaramrajgopal/2025/02/11/do-our-courses-need-an-update-to-reflect-the-workings-of-information-businesses/
Time Published: 2025-02-11T16:02:04Z
Full Content:
1936: British actor and director Charles Chaplin (1889 - 1977), wearing overalls and holding a ... [+] wrench, sits on an enormous set of gears in a still from Chaplin's film, 'Modern Times'. (Photo by Hulton Archive/Getty Images) Syllabi at management schools and senior management at perhaps many businesses don’t fully appreciate how information businesses work. I was chatting with a successful Silicon Valley venture capitalist last week about how we may have under-invested in understanding the impact of technology on our economy. The VC suggested, “macroeconomic modeling generally treats technology as an external parameter like Total Factor Productivity, rather than as an endogenously determined, explicitly engineered system. As a result, conventional models struggle to address the most critical structural issues facing the U.S. and global economy. They neither integrate short-run dynamics with long-run growth considerations nor capture how digital technologies — AI, 5G, robotics, 3D printing, zero-cost digital services — are developed, diffused, and shaped by policy and market forces. This limitation leaves unexplored how these technologies affect productivity, output, time use, education, skill requirements, and well-being.” The VC sounds more professorial than me but I am not a macro-economist and hence say much about the above statement. However, I suspect the same sentiment applies at the micro level to technology companies or information businesses, more broadly. I suspect that we have not updated our syllabi and course offerings at management schools as much as we should have, after the U.S. lost its crown as a manufacturing powerhouse. I compared the list of course offerings in say 1998 when I started teaching with what is offered today by top business schools. I found a few changes but nothing radical seems to have been restructured. So, what was taught in 1998 and to a great extent today? I conjecture we mostly teach management insights, as practiced in the manufacturing era. There was a fair degree of excitement about management theories as applied to manufacturing processes back in the day. I am thinking of Fredrick Taylor’s scientific management theories, the Hawthorne experiments, the Japanese ideas of Total Quality Management, lean manufacturing, and GE’s six sigma mindset. None of these ideas, perhaps formulated explicitly for the manufacturing age, fully apply or explain how information companies operate. Consider a summary of these ideas and how these apply or not to the modern information giants: Frederick Taylor is widely recognized for “time and motion” studies conducted in the 1880s where he observed the specific steps a worker takes to lay bricks and the time it takes to go through these steps. The idea is to optimize the number of steps and time taken to accomplish the task. More broadly, Taylor is known for two big ideas: (i) scientific management, based on evidence and observation, as opposed to gut feel and rules of thumb; and (ii) the conflict between management and labor is somewhat unnecessary as both parties would benefit with better productivity which would manifest as higher wages and higher profits. Taylor emphasized standardized tools and procedures. He wanted managers to set specified goals and assign tasks to workers to motivate workers to perform better. He argued that workers should be paid a bonus of 30% to 100% of wages for learning to do the job as per the principles of scientific management. Taylor also advocated individual work and productivity and was somewhat skeptical of group work. Taylor suggested choosing the worker with the right aptitude as such a worker would be far more productive than the average worker. He also pushed for rest pauses and shorter working hours, especially if the task is hard. Taylorism is perhaps the precursor to a lot of what is taught in operations research, human relations and cost accounting courses in B schools. I suspect Amazon relied on time and motion studies to understand how to optimize packing boxes in their distribution centers. The idea of choosing the right worker with aptitude for the job is an established tenet in HR (Human Relations) groups all over the world. The Hawthorne studies are credited with recognizing the influence of human relations or social factors in motivating workers. The Hawthorne researchers found the workers’ response to a managerial intervention is a function of the attitudes the workers bring to their job, the informal work group they belong to, their personal history and their social situation at work. Worker culture in big tech companies is widely discussed. Netflix’s culture deck, which potentially owes its inspiration to the early Hawthorne studies, is legendary in Bay area circles. The deck emphasizes the freedom to excel, deciding which worker is a keeper and paying them a premium over their market wage, the idea that the best workers are 10X better than average, and that teams ought to be highly aligned in their goals but only loosely coupled to stay nimble. The Total Quality Management way of thinking aims to make quality the concern of every member of the firm. Customers are the focus of all activities of the firm and improvements in quality are directed at improving customer satisfaction. Such a focus is expected to lead to better financial performance in two ways: (i) in the manufacturing process, we are expected to encounter fewer defects and rework leading to lower costs and more dependable processes and hence better earnings; and (ii) in the product market, more focus on quality is expected to lead to higher market share, less elastic demand, higher prices and hence higher earnings. The idea of lean manufacturing, a related concept, is to deliver a quality product to the customer at a reasonable cost. Incidentally, Trevor Harris, my co-author and emeritus professor at Columbia Business School, tells me that Edward Deming, another CBS professor, pioneered the thinking behind TQM and even the Six-Sigma movement. I don’t know whether Tesla follows TQM and lean manufacturing but I did come across a help wanted ad from Tesla looking for a quality engineer whose job description sounded similar to what a TQM engineer would do. In a 1997 annual letter Jack Welch sent to GE stockholders, he explains: "Six Sigma project work consists of five basic activities: Defining, Measuring, Analyzing, Improving and then Controlling processes. These projects usually focus on improving our customers’ productivity and reducing their capital outlays, while increasing the quality, speed and efficiency of our operations….The Six Sigma quality initiative, very briefly, means going from approximately 35,000 defects per million operations, which is average for most companies, including GE, to fewer than 4 defects per million in every element in every process that this company engages in every day.” Welch goes on to explain how Six Sigma works with the “A” leaders in each function of the company: “In finance, for example, “A’s” will be people whose talents include, but transcend, traditional controllership. The bigger role is one of full-fledged participant in driving the business to win in the marketplace — a role far bigger than the dreary and wasteful budget “drills” and bean counting that once defined and limited the job. In engineering, “A’s” are those who embrace the methodology of Design for Six Sigma. “A” engineers can’t stand the thought of “riding it out” in the lab, but rather relish the rapid pace of technological change and continually re-educate themselves to stay on top of it. In manufacturing, “A” players will be people who are immersed in Six Sigma technology, who consider inventory an embarrassment, especially with a whiff of deflation in the air — people who understand how to drive asset turns and reduce inventory while at the same time increasing our readiness to serve the customer. In sales, “A” players will use the enormous customer value that Six Sigma generates to differentiate GE from the competition, to find new accounts, and to refresh and expand the old ones — as contrasted with “C” players whose days are spent visiting “friends” on the “milk-run” circuit of customer calls.” Amazon reportedly relied on Six Sigma type ideas to reduce the error rate on their package delivery to minuscule levels considering that they processed 5.9 billion packages in 2023. As stated, modern information businesses, such as Amazon, Google, Meta, Netflix, and Uber, rely on many of these ideas inspired by manufacturing. But these tech businesses also do not seem to fit the traditional mold of management theories well. In a seminal paper written around 2001, Hal Varian discusses ways in which the “new economy” information businesses are different from traditional manufacturing firms: Not quite. Let me start with my home field. Trevor Harris points out that statutory reporting, that is done once a quarter in a rigid regulation bound format, is a relic of a bygone era. Good businesses have a dashboard to track the key KPIs that matter for value creation. Why don’t we ask firms to disclose that dashboard on a more frequent cadence to investors? Of course, there are concerns about proprietary costs and so on, but I have not seen many creative conversations about how to leverage technology and/or better reflect the economics of technology and information businesses in our textbooks and syllabi. For another smaller illustration of this problem, our textbooks treat intangible assets in a somewhat rule-oriented framework. The FASB says X and not X. So, we explain that in the text but rarely take the conversation to the next level to consider how technology creates value. My co-authors and I have a few modest contributions in this area, generally lamenting how little information about value creation potential of technology spending is shared with the investing public and the absence of data to calculate unit economics or lifetime value of a customer but much more needs to be done. I have not seen a good treatment of how cost accounting should think about supply and demand side economies of scale, standards, systems effects and computer mediated transactions. A critic may argue that the basics of cost accounting have not changed. But one must wonder whether the field is stuck around the time manufacturing left the U.S. for Asia. Have we updated thinking in cost accounting for the bundled information products, why some of Amazon’s divisions are never supposed to make money (free shipping, Kindle) so that they can cross-sell other products in the retail store, how does Amazon price the time of developers working on features and software products or how does a firm value its data or the data acquired in an M&A transaction? What kind of cost data does an Amazon, or an Uber collect? Do they conduct cost-volume-profit analyses to determine breakeven sales quantity for a product, given the extent of bundling and portfolio type thinking necessary to accomplish this? How does Amazon calculate the lifetime value of a customer? How is common overhead across segments and how did that practice potentially fund Amazon’s internal capital markets such as books leading to video to music to third party selling to AWS? How does one think about budgeting and sales and cost variances for a modern tech business? A related problem in finance has been the persistent puzzle on how to value intangibles. Absence of even modest accounting data to perform such valuations is a big hindrance. Do frameworks such as real options plausibly explain the valuations of a Tesla or an Amazon or an Nvidia? How does one stress test or falsify these black box valuations? Have the low-interest rate environments, coincidental with the rise of Big Tech, defaulted us to faith-based valuations? How has corporate finance changed with the rise of information businesses? Is capital less of a constraint than skilled labor? How does one come up with capital budgeting for information products or bundled systems? How has cash and liquidity management changed? Has big data and AI improved scenario planning and hence enabled better capital allocation? Has granular data made it easier to dissect why some acquisitions succeeded while others failed? Now, I stray into areas that are not my own and hence rely on observation, as opposed to deep analysis. Marketing and Operations Research are arguably the two disciplines in a B school that have adapted and embraced the practices of Big Tech firms better than other units. There are tons of classes on digital marketing, social influencers, ad platforms, AI offered by these two groups. Human Relations: Jensen Huang’ statement illustrates the profound change that HR groups are/will experience in the near future: “In a lot of ways, the IT department of every company is going to be the HR department of AI agents in the future.” That is, the IT departments of the future will train and “onboard” AI agents as though these were employees to ensure that such agents work to enhance human workers’ productivity. Recruiting is already mediated by AI algorithms, for better or for worse. Work from home, enabled by technology, is not going away. This is already creating profound challenges on how to integrate younger workers into a company. Many of these workers were hired on a zoom call and will potentially not get promoted by senior leaders who don’t know them as individuals or human beings. Ironically, I would suggest that we need to invest more in traditional HR. Do our newer leaders have the training and outlook to look a worker in the eye and say to them in a respectful way that they are being laid off? If they did, we would not come across so many cases of mass layoffs via email or zoom. Our slowness, in the B school, to adapt and teach management practices of Big Tech or enabled by technology in general, would be less of a concern if these considerations only mattered to a handful of tech giants. But that is certainly not the case. I routinely come across senior managers of companies that are both enthusiastically and reluctantly digitizing without fully understanding the profound change that Varian’s forces will have on their firms. This is not to say that people have not worked on documenting management practices at Amazon or the other technology firms. I have not seen a book or a manual or a coherent framework that one can hand to future managers of non-digital firms to prepare for a digital/tech leavy/information dense tomorrow. Constructive comments welcome, as always.
--------------------------------------------------

Title: Why Tesla's stock has been cratering
URL: https://finance.yahoo.com/news/why-teslas-stock-has-been-cratering-152645143.html
Time Published: 2025-02-11T15:26:45Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. Elon Musk may be President Trump's first buddy, but Tesla (TSLA) investors have swung back to focusing more on the company's fundamentals than the billionaire's close proximity to the Resolute desk. Their decision? To send Tesla's stock tumbling less than two months into 2025. Shares of the electric vehicle maker led by Musk are down 28% to $349.18 since hitting a record high on Dec. 18, 2024, weeks after the Election Day win for Trump. The stock is the worst-performing member of the closely watched "Magnificent Seven," which includes Meta (META), Amazon (AMZN), Microsoft (MSFT), Nvidia (NVDA), Google (GOOG), and Apple (AAPL). At the time, enthusiasm was running high that Musk's fierce support of Trump would unlock big-time profits as driverless cars would get quicker approval — among other bullish lines of thinking. But since reaching that peak, Tesla's stock has fallen below its key 50-day moving average, according to Yahoo Finance data. The next test of investor sentiment on Tesla is coming at $334, or the 100-day moving average. Should the stock break below that important support level, it could have a clear shot to the 200-day moving average of $286. Tesla's 52-week low is $142.05 hit on April 22, 2024. "Given the considerable run up in the shares as performance and expected future performance have by equal measure deteriorated, we sense a high risk of mean reversion (including by potential catalysts that may be difficult to foresee at present) and for this reason continue to recommend caution with regard to an investment in Tesla shares," JPMorgan analyst Ryan Brinkman wrote in a new client note. Brinkman is one of the most bearish sell-side analysts on Tesla, with an Underperform rating and $135 price target. The reasons Tesla's stock is under pressure are numerous. For one, numbers on Tesla sales from important overseas markets have come in soft to kick off the year. The readings have triggered some concerns in Tesla circles that Musk's close proximity to Trump is damaging its brand. Tesla sold 63,238 vehicles in China in January, according to new data released this week from China Passenger Car Association (CPCA). The figure marked a steep 33% drop from December. Australia's Electric Vehicle Council (EVC) reported Tesla's overall sales fell 33% year over year in January. Meanwhile, fresh tariffs from President Trump stand to raise costs for Tesla and other EV makers. On Monday, Trump signed two executive orders imposing new 25% tariffs on steel and aluminum. Both steel and aluminum are key raw materials used by Tesla. Trump's new trade war with China doesn't help either — a 2023 study by Nikkei found that 40% of the suppliers for materials used in Tesla's batteries are Chinese companies. "Changes in government and economic policies, incentives or tariffs may also impact our production, sales, cost structure and the competitive landscape," Tesla pointed out in its latest 10-K filing. And lastly, Tesla's fourth quarter left a lot to be desired. The company's EPS missed analyst estimates by a penny. Automotive sales fell 8% year over year due to price cuts across the Tesla vehicle lineup. "They missed their 2024 delivery guidance, which we knew about a month ago. Their margins were pretty weak ... And, if we're reading this correctly, the full self-driving rollout in Europe and China is not going to happen in the first quarter," Canaccord Genuity managing director George Gianarikas said on Yahoo Finance's Morning Brief, calling the results "kind of squishy." Yahoo Finance data shows analysts have aggressively marked down earnings per share estimates on Tesla for 2025 and 2026 in the wake of the lackluster earnings report. "Tesla faces a variety of risks given its broad reach. On automotive, we believe the largest risk is demand softness/stagnation related to EV fatigue and/or competition," Deutsche Bank analyst Edison Yu pointed out in a note. Brian Sozzi is Yahoo Finance's Executive Editor. Follow Sozzi on X @BrianSozzi, Instagram, and LinkedIn. Tips on stories? Email brian.sozzi@yahoofinance.com. Click here for the latest technology news that will impact the stock market Read the latest financial and business news from Yahoo Finance Sign in to access your portfolio
--------------------------------------------------

Title: Meta Rally Makes History as It Goes on Offensive With AI
URL: https://finance.yahoo.com/news/meta-rally-makes-history-goes-103919299.html
Time Published: 2025-02-11T14:41:39Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. (Bloomberg) -- Even in a market where artificial intelligence winners are rewarded, Meta Platforms Inc. shares stand out. Most Read from Bloomberg Why American Mobility Ground to a Halt Saudi Arabia’s Neom Signs $5 Billion Deal for AI Data Center SpaceX Bid to Turn Texas Starbase Into City Is Set for Vote in May Cutting Arena Subsidies Can Help Cover Tax Cuts, Think Tank Says The Facebook parent’s record 16-day rally has been driven by all manner of news — even the kind that has pressured its megacap rivals — pushing its valuation toward $2 trillion for the first time. “I have long viewed Meta as the biggest beneficiary of AI outside of maybe Nvidia, and I think more people are coming around to that idea,” said Conrad van Tienhoven, a portfolio manager at Riverpark Capital. While the likes of Microsoft Corp. and Alphabet Inc. face questions on when they will see a return on their billions spent on AI, “Meta spent the money on AI solutions that have had an immediate impact on how it targets and measures ads, the outcome of which has been faster growth and a higher average revenue per user.” The stock fell 0.8% on Tuesday, but the 16-day rally represented the longest winning streak of any current Nasdaq 100 Index component going back to 1990, according to data compiled by Bloomberg. It added more than 17% over the surge, bringing its market capitalization above $1.8 trillion. Despite the gain, it remains one of the cheapest big tech plays. The string of catalysts has been remarkable. In January, Chief Executive Officer Mark Zuckerberg said the company plans to invest as much as $65 billion on AI projects in 2025, much more than had been expected. While capex plans have come under increased scrutiny, Meta’s target reinforced the perception it is investing from a place of strength. While tech was rocked by the emergence of DeepSeek, a Chinese AI startup that claimed strong performance despite costing less and requiring fewer chips, Meta shares climbed. Investors took DeepSeek’s success as a validation of open-sourced models, as employed by Meta’s Llama. The firm’s results underlined how AI is flowing through to its financials, improving how ads are targeted to its billions of users. Zuckerberg said 2025 would be a “really big year” for AI. Then, Meta on Monday began notifying staff of job cuts as it focuses on AI talent. Zuckerberg has also made some high-profile changes that have served to better align Meta with the Trump administration. The stock’s streak began before the inauguration in January. The recent strength stands in contrast to Alphabet, the other major player in online advertising, which disappointed with its results, putting shares in negative territory for the year, compared with Meta’s gain of about 22%. “Meta is really ahead of its competitors in proving that the capex it is pouring into AI is working, which is why investors continue to gravitate toward it,” said Jim Polk, head of equity investments at Homestead Advisers. “It has shown that AI is having an impact on user engagement, on margins, whereas Alphabet still needs to prove its capex is working and that its market share in search won’t be eroded.” Revenue growth at Meta is seen rising almost 15% in 2025, before modestly decelerating over the subsequent years, according to data compiled by Bloomberg. However, net earnings growth is seen accelerating from 6.2% this year to 15% in 2026. The extraordinary length of Meta’s rally has brought its 14-day relative strength index to about 77, notably over the level of 70 that signals to some technical analysts that a stock is overbought. However, even with the streak, Meta’s valuation doesn’t scan as especially high. The company currently trades at nearly 27 times forward earnings, the highest level for shares since 2020, but in line with the Nasdaq 100. Among the Magnificent Seven group of tech stocks, only Alphabet Inc. trades at a lower multiple. “It’s in algo heaven because it’s one of the few large cap stocks that had a perfect quarter,” said Rhys Williams of Wayve Capital Management. “Models really like reasonably priced stocks where fundamentals are solid.” Meta’s current success represents a dramatic turnaround from just a couple years ago, when shares cratered amid its strategic pivot to focusing on the metaverse, along with a privacy-policy change from Apple Inc. that diminished Meta’s ability to target ads, and negative user trends, especially amid competition from TikTok. Those concerns have all been reduced. Meta’s AI investments have offset the impact of Apple’s policy and contributed to improved user engagement, and while TikTok remains a significant competitor, its future is uncertain ahead of a US ban that could go into effect in early April. “Right now I suspect the market is pricing in TikTok not being banned in the U.S., but if the ban does go through, it would be a huge positive for Meta,” Polk said. “I don’t know how likely it is, but that is something that could keep the rally going.” Tech Chart of the Day Tesla Inc. shares are extending losses for a fifth consecutive session, edging lower in Tuesday trading. Elon Musk’s net worth fell below $400 billion for the first time in two months, dragged down by the slide in Tesla’s share price. In recent weeks the stock has been weighed down by bad sales reports from around the world, while Chinese rival BYD has upped the ante through its smart-driving strategy. Top Tech Stories A group of investors led by Musk has offered to buy the nonprofit that controls OpenAI for $97.4 billion, escalating a clash between the Tesla chief executive and the artificial intelligence company he co-founded. Stripe Inc. is in discussions to arrange sales of stock by employees at a $85 billion valuation, people familiar with the matter said, a deal that could help the Silicon Valley payments startup regain some of the capitalization it lost during a post-Covid slump. Saudi Arabia’s Neom, the megacity being developed on the Red Sea coast, secured a $5 billion investment from a local firm to establish an AI data center. Elon Musk’s net worth fell below $400 billion for the first time in two months, dragged down by a double-digit slide in Tesla Inc.’s share price. Meta Platforms Inc. began notifying staff of job cuts on Monday, kick-starting a process that will terminate thousands of people as the company cracks down on “low-performers” and scours for new talent to dominate the AI race. Earnings Due Tuesday Premarket Shopify GlobalFoundries IPG Photonics Sequans Cineplex Postmarket Confluent Freshworks BlackLine Inc Mirion Technologies Teradata Diodes SimilarWeb IAC Inc Angi Inc --With assistance from Carmen Reinicke, Tom Contiliano and Subrat Patnaik. (Updates to market open.) Most Read from Bloomberg Businessweek The Game Changer: How Ely Callaway Remade Golf Elon Musk’s DOGE Is a Force Americans Can’t Afford to Ignore How Oura’s Smart Ring Bridged the Gap From Tech Bros to Normies Why Fast Food Could Be MAHA’s Next Target Trump’s Tariffs Make Currency Trading Cool Again After Years of Decline ©2025 Bloomberg L.P. Sign in to access your portfolio
--------------------------------------------------

Title: Super Micro's roller coaster continues into earnings with 59% stock pop in past week
URL: https://www.cnbc.com/2025/02/11/super-micro-roller-coaster-has-stock-soaring-ahead-of-q2-2025-earnings.html
Time Published: 2025-02-11T12:00:01Z
Description: Super Micro has a lot on the line heading into its earnings report on Tuesday after a tumultuous 2024.
--------------------------------------------------

Title: SEBI bars nil revenue firm LS Industries, promoter in alleged pump & dump scheme, impounds ₹1.14 crore
URL: https://www.thehindubusinessline.com/markets/sebi-bars-nil-revenue-firm-ls-industries-promoter-in-alleged-pump-dump-scheme-impounds-114-crore/article69205871.ece
Time Published: 2025-02-11T08:54:06Z
Full Content:
-1,121.52 -324.55 + 7.00 -175.00 -1,024.00 -1,121.52 -324.55 -324.55 + 7.00 + 7.00 -175.00 Get businessline apps on Connect with us TO ENJOY ADDITIONAL BENEFITS Connect With Us Get BusinessLine apps on By BL Mumbai Bureau Comments READ LATER The Securities and Exchange Board of India (Sebi) has barred LS Industries - a company with zero revenue but thousands of crores in market capitalisation - its NRI shareholder and four promoter-linked entities from the securities market, and directed impounding of ₹1.14 crore illegal gains made by inflating the company’s stock price. The trouble started when an ex-director, Suet Meng Chay, transferred a 12 per cent stake—nominally valued at ₹10.28 crore—to an NRI based in Dubai, Jahangir Panikkaveettil Perumbarambathu, for a token amount of just $1. He was supposedly a stranger, who Sebi suspects to be linked to the promoter family through Meta. This token-dollar transfer set off an alleged pump-and-dump strategy orchestrated through connected entities to artificially inflate the stock price. This enabled the NRI as well as the promoter, and connected entities to sell shares at a higher value, the regulator observed. A series of early-morning buy orders at the upper circuit limits were seen driving the share price from a low of around ₹22.50 to an astronomical high of ₹267.50 within two months—even though the company’s financials showed negligible revenue and virtually no business activity. “At the dollar-rupee conversion rate of ₹83.75, this made the purchaser richer by $328.60 million (₹2752 crore) at the company’s peak market capitalization of ₹22,700 crore. The said transaction not only appears to be too good to be true, but also raises the possibility of FEMA violations,” said SEBI’s whole-time member Ashwani Bhatia in the interim order. Bhatia said that the case painted a picture of “absurdities and anomalies,” and required interim action before another pump and dump takes place and more innocent investors lose money and burn their fingers. He noted that the number of public shareholders has nearly doubled in six months to 6,106 as on December 31, 2024. “There is a real risk of the noticees off-loading their shares at the cost of investors, who get drawn to the shares of the company seeing high valuations and volumes and would end up getting duped and taken for a ride…Further, action needs to be taken promptly to prevent more money leaving the shores of India through sale of shares by the NRI shareholder,” the order said. The regulator will be conducting a detailed investigation in the matter by May 15. In the meantime, Sebi has directed the NRI to provide a full inventory of his assets, investments, and bank accounts, as well as freeze any debits in his accounts. Comments BACK TO TOP Comments have to be in English, and in full sentences. They cannot be abusive or personal. Please abide by our community guidelines for posting your comments. We have migrated to a new commenting platform. If you are already a registered user of TheHindu Businessline and logged in, you may continue to engage with our articles. If you do not have an account please register and login to post comments. Users can access their older comments by logging into their accounts on Vuukle. Terms & conditions | Institutional Subscriber
--------------------------------------------------

Title: Can AI Fix Succession Planning?
URL: https://www.forbes.com/sites/tomaspremuzic/2025/02/11/can-ai-fix-succession-planning/
Time Published: 2025-02-11T08:30:22Z
Full Content:
Succession planning sign and figurines with arrows. Few decisions in business and leadership matter more than choosing the right successor. When done well, it ensures continuity, preserves institutional knowledge, and often propels an organization to new heights. When done poorly, it can lead to instability, lost value, and even outright failure. Consider the success stories: Tim Cook taking over from Steve Jobs at Apple, seamlessly continuing and even accelerating the company’s trajectory, boosting the company’s market cap from around $340B under Steve Jobs to over $3T now (if you invested $10,000 of Apple stock at the start of Cook's tenure you would have around $160,000 right now). The figures are similarly impressive for Satya Nadella’s impact at Microsoft, who arguably had a bigger strategic and innovation effect by not just rejuvenated the company, but also shifting its focus first from Windows to cloud computing, and more recently and AI (anticipating the current AI boom). Even Pope Francis’s ascension after Pope Benedict XVI’s resignation demonstrated the power of a well-managed leadership transition, bringing fresh energy and a sense of modernity and empathy (not to mention an instant PR upgrade) to the Vatican, a 2,000-year institution. Contrast that with the pitfalls of poor succession planning. Since legendary Manchester United manager Sir Alex Ferguson retired, the club has been cycling through multiple coaches (seven and still counting) without much financial or sporting success, and the decline appears to be ongoing. In the corporate world, J.C. Penney’s appointment of Ron Johnson as CEO led to a dramatic and costly strategic misfire. And in the hit show Succession, the fictional Roy family’s lack of a clear plan for leadership leads to chaos, power struggles, and a tanking stock price, which makes for great television but a horrific case study in succession. Back in the real-world, succession stories captivate the public’s attention. Bernard Arnault, the chairman and CEO of LVMH and one of the richest people in the world, is known to have been interviewing each of his five children for the big job, fostering speculation about his eventual successor. In recent years, Warren Buffett announced his successor, but at 94, Buffett still maintains a hands-on role in managing Berkshire Hathaway's portfolio. As for Jamie Dimon, there is seemingly no end to the uncertainty about his successor. These cases all highlight the same truth: leadership transitions can make or break an organization. The Science and Art of Succession Planning At its core, succession planning is part science and part art. The science is well established: industrial-organizational psychology offers robust and reliable frameworks for assessing leadership potential, ensuring a data-driven approach to selection, and derisking the process (including the significant probability that incompetent men become leaders). Predictive analytics, performance reviews, and evidence-based competency models help organizations make objective choices. Yet, the process is also partly an art — navigating internal politics, winning over key stakeholders, and crafting a shared vision for the future. As in any other area of life, AI has the potential to bring rigor and objectivity to the science while enhancing the human aspects of the art. Here’s how: 1. Evaluating Past Performance Beyond Politics The barrier to data-driven succession planning is contamination in past performance data, which corrupts a proper examination of candidates’ track record and potential, introducing invisible biases in the process, and contaminating candidates’ “score” with politics, subjectivity, and noise (the nefarious 9-box grid, the unreliable supervisor or managerial ratings, and the political high-potential nominations). AI can mitigate this by analyzing vast amounts of historical performance data, including predictive signals usually missed by the “naked” human eye, and finding hidden patterns that connect each candidate with different success profiles. By measuring real contributions—financial results, operational efficiencies, strategic insights, and even mining granular signals, such as language, meta-data, the depth and breadth of social networks, and markers of leadership style and dynamics — AI can cut through biases, ensuring the most qualified leaders rise to the top based on merit, not alliances. To be sure, asking AI to predict who would normally be nominated in a given organization will probably not result in an upgrade from past and current candidates, but rather more of the same (which is rarely the right choice when it comes to effective successions). 2. Expanding the Talent Pool for More Inclusive Assessments Historically, succession planning has been limited to a narrow group of insiders, often overlooking diverse and high-potential candidates among unconventional or outgroup profiles. AI-driven talent identification can analyze broader datasets to uncover hidden gems, ensuring organizations consider leaders from different backgrounds, functions, and geographies. In essence, innovations in assessments have leveraged AI to improve the candidate experience, reduce test-taking time, and increase the speed and efficiencies of putting a much larger sample of potential candidates through the assessment. Since quantity leads to quality, making assessment inclusive will generally improve the quality of your final choice, as well as increasing the range of candidates and profiles you can shortlist. 3. Using Passive Data to Predict Leadership Success AI can leverage passive data—communication patterns, collaboration metrics, and behavioral insights — to assess leadership traits. The work of David Stillwell, Sandra Matz and Michal Kosinski demonstrates how AI can infer personality traits and leadership potential from digital footprints, as well as internal company data not historically seen as critical to leadership talent. In essence, you want leaders to be smart, driven, and ethical, and each of these foundational ingredients of leadership potential can be broken down into narrower traits (e.g., expertise, curiosity, learning ability, motivation, conscientiousness, EQ, people-skills, integrity) that are expressed, display and reflected in people’s online behaviors (captured by internal company data or in the external digital universe). By incorporating these insights, organizations can move beyond traditional assessments and gain a more holistic view of a candidate’s fit. In fact, even generative AI and large language models provide an easy way to access the public reputation of many senior leaders. For instance, try prompting ChatGPT to give you a leadership and personality profile for Elon Musk, Donald Trump, or Angela Merkel, and you will see there is not much left for a traditional executive assessment to add (same goes for many less famous leaders, including those mentioned at the beginning of this article). 4. Debiasing Leadership Selection Unconscious and conscious biases remain a significant issue in leadership transitions, including when succession planning is driven by a desire to enhance “culture-fit” (which is a subliminal and politically legitimized way to perpetuate the existing organizational biases that confer privilege to the historical in-group, the status-quo, and the ruling elite). AI can flag potential biases in hiring and promotion decisions, ensuring selections align with data rather than stereotypes. For example, when AI is trained to predict who would get promoted and it recommends a surplus of Middle-aged white male engineers, it is accurate exposing existing biases in the organization. Humans are (usually) very good at learning, but very bad at unlearning. Conversely, AI is good at both learning and unlearning. Thus, we can train AI to not just identify the kind of leader who has succeeded in the past, but the kind of leader we would need for the future, if we simply program or prompt it to seek for relevant signals of potential while ignoring irrelevant signals. In contrast, humans will never be able to ignore a candidate’s gender, age, race, social class, or attractiveness, all of which conflates and distorts their choices. By training algorithms on diverse, high-performing leadership examples, companies can create models that prioritize competence and potential over legacy and favoritism, increasing meritocracy (which even those who have jumped on the anti-DEI bandwagon should appreciate). Ethical Considerations Of course, AI is not a magic bullet. Ethical concerns around data privacy, algorithmic bias, and transparency must be addressed. AI should be a tool to enhance human judgment, not replace it. Leaders must ensure AI is used responsibly—applying it as a guide rather than a dictator. The one advantage here, is that AI does not need to be perfect in order to bring about progress. In fact, all it must do is to produce an improvement over the status-quo, which is a very low bar. Indeed, even in its current form AI is less biased than humans – not because AI is perfect, but because humans are biased by design. With AI, as with any other technology, the goal is not instant perfection, but incremental improvements over the current state-of-affairs, and finding better ways of being wrong. To be sure, there are no reasons whatsoever to have confidence in humans eliminating their own biases, but human ingenuity and expertise has been able to create a tool, a technology, that may well de-bias human decision making – but we must open to not just having ethical and competent humans involved in the design and auditing of these algorithms, but also remove them from the decision-making look (as they are often more likely to introduce than eliminate biases). The Future of Leadership Selection AI won’t eliminate the need for human intuition and relationship-building in succession planning, but it can elevate decision-making. By focusing on objective performance evaluation, expanding candidate pipelines, leveraging behavioral insights, and reducing bias, AI has the potential to make leadership transitions smoother, fairer, and more effective. The future of leadership isn’t about human vs. machine. It’s more likely human with machine — using AI as a partner in one of the most crucial tasks any organization faces: securing its next great leader. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Don’t Abandon Your Diversification
URL: https://www.whitecoatinvestor.com/dont-abandon-your-diversification/
Time Published: 2025-02-11T07:30:37Z
Description: Just because large value stocks have performed well lately doesn't mean you should stop diversifying your portfolio. Here's why not.
The post Don’t Abandon Your Diversification appeared first on The White Coat Investor - Investing & Personal Finance for Docto…
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq futures slip as Trump's tariffs and inflation prey on minds
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-futures-slip-as-trumps-tariffs-and-inflation-prey-on-minds-003956519.html
Time Published: 2025-02-11T00:39:56Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. US stock futures edged lower on Tuesday as investors braced for more tariff policy shifts from President Donald Trump and focus turned to inflation with testimony from Federal Chair Jerome Powell on deck. Dow Jones Industrial Average futures (YM=F) moved 0.2% lower, while S&P 500 futures (ES=F) dropped 0.3%. Contracts on the tech-heavy Nasdaq 100 (NQ=F) pulled back 0.4%, after a winning day on Wall Street. Markets have taken a cautious tone in the wait for Trump to reveal the plan for universal like-for-like tariffs promised for announcement midweek. The president on Monday imposed 25% tariffs on all steel and aluminum imports from March 4, putting further pressure on top trading partners Canada and Mexico. Investors are trying to gauge how far Trump's tariff threats will translate into action, as they worry about the potential impact of trade war on corporate earnings, the global economy, and on inflation in particular. Amid the uncertainty, gold (GC=F) set a fresh record — the latest in a series — as investors sought out less-risky assets, before retreating on Tuesday. Given that, markets are looking to Powell's two-day testimony in Congress, starting Tuesday in the Senate, for any hint to the Fed's thinking how the tariffs could affect price pressures. With inflation seen as persistent, the countdown is already on for January's Consumer Price Index reading on Wednesday and its wholesale counterpart on Thursday. Markets are seeking insight into whether conditions are right for interest-rate cuts, though a looming tariff impact could cloud any sign of recent easing. In tech, an Elon Musk-led bid to buy OpenAI captured Wall Street's attention as AI spending fears continue to weigh on the Magnificent Seven. The AI nonprofit's CEO Sam Altman shot down the unsolicited offer of $97.4 billion, a significant undershoot of its valuation. Meanwhile, Meta (META) began to lay off workers as part of CEO Mark Zuckerberg's pledge to cut thousands of jobs in a pivot to finding AI talent. On Tuesday, Coca-Cola earnings are expected to show a quarter of growth in the company's battle with PepsiCo. Wall Street anticipates Q4 revenue of $10.67 billion and earnings per share of $0.52. Super Micro Computer (SMCI), Lyft (LYFT), and DoorDash (DASH) are also set to report results later. Gold (GC=F) continues to reap the benefits of uncertainty in stock markets. President Donald Trump's 25% tariffs on steel and aluminum have pushed the safe-haven asset back to an all-time high for the second consecutive week. Bullion touched an all-time peak above $2,921 an ounce, maintaining momentum from a 1.7% boost in the day's session prior. Bloomberg reports: Sign in to access your portfolio
--------------------------------------------------

Title: Skyharbour Announces Closing of Option and Purchase Agreements with Hatchet Uranium for Several of its Uranium Projects Located in the Athabasca Basin
URL: https://financialpost.com/globe-newswire/skyharbour-announces-closing-of-option-and-purchase-agreements-with-hatchet-uranium-for-several-of-its-uranium-projects-located-in-the-athabasca-basin
Time Published: 2025-02-11T00:00:51Z
Description: Vancouver, BC, Feb. 10, 2025 (GLOBE NEWSWIRE) — Skyharbour Resources Ltd. (TSX-V: SYH) (OTCQX: SYHBF) (Frankfurt: SC1P) (“Skyharbour” or the “Company”), is pleased to announce that, further to its news release dated November 4th, 2024, closing has occurred on…
--------------------------------------------------

Title: Skyharbour Announces Closing of Option and Purchase Agreements with Hatchet Uranium for Several of its Uranium Projects Located in the Athabasca Basin
URL: https://www.globenewswire.com/news-release/2025/02/11/3023858/36591/en/Skyharbour-Announces-Closing-of-Option-and-Purchase-Agreements-with-Hatchet-Uranium-for-Several-of-its-Uranium-Projects-Located-in-the-Athabasca-Basin.html
Time Published: 2025-02-11T00:00:00Z
Full Content:
February 10, 2025 19:00 ET | Source: Skyharbour Resources Ltd Skyharbour Resources Ltd Vancouver, BC, Feb. 10, 2025 (GLOBE NEWSWIRE) -- Skyharbour Resources Ltd. (TSX-V: SYH) (OTCQX: SYHBF) (Frankfurt: SC1P) (“Skyharbour” or the “Company”), is pleased to announce that, further to its news release dated November 4th, 2024, closing has occurred on the option agreement (the “Agreement”) with Hatchet, whereby Hatchet Uranium Corp. (“Hatchet”) may acquire an 80% interest in the Company’s 17,606 ha Highway Uranium Property (the “Optioned Property”) and a 100% interest, subject to a claw-back provision for Skyharbour, in the Company’s Genie, Usam and CBX/Shoe Uranium Projects (the “Purchased Property”). The properties total 66,358 ha and are all located in the Athabasca Basin of Northern Saskatchewan, Canada. The Agreement on the Optioned Property provides Hatchet an opportunity to earn an 80% interest in the claims over a three-year period by fulfilling combined cash, share issuance and exploration expenditure commitments of CAD $3,345,000. For the Purchased Property, Skyharbour will also receive units in the capital of Hatchet consisting of a share and a warrant (“Hatchet Units”) equal to 9.9% of the issued and outstanding shares of Hatchet. Highway, Genie, Usam, CBX and Shoe Project Map:https://skyharbourltd.com/_resources/images/Sky_Highway.jpg Terms of the Optioned Property: The Optioned Property, Highway, consists of nine (9) mineral claims comprising approximately 17,606 hectares. Hatchet may acquire an 80% interest in the Optioned Property by (i) issuing common shares in the capital of Hatchet (“Shares”) having an aggregate value of CAD $1,050,000; (ii) making aggregate cash payments of CAD $245,000; and (iii) incurring an aggregate of CAD $2,050,000 in exploration expenditures on the Optioned Property over a three-year period, as follows: (1) Deemed pricing of Shares is based on the twenty (20) day volume weighted average price on the stock exchange in which Hatchet shall list its Shares for trading, being either the TSX Venture Exchange or the Canadian Securities Exchange (“Deemed Price”) or the last sale price, if not listed on a stock exchange at the time of issuance. In the event that the issuance of any Shares pursuant to the above would result in the Company holding 10% or more of the outstanding Shares of Hatchet, Hatchet will issue that number of Shares which would result in the Company receiving 9.9% of the issued and outstanding Shares post-issuance and will pay cash in lieu of the Shares for the difference. The Company shall retain a 2% net smelter returns royalty from minerals mined and removed from the Optioned Property, of which Hatchet may purchase one-half, being 1%, at any time for $1,000,000. Terms of the Purchased Property: The Purchased Property consists of twenty-five (25) mineral claims comprising approximately 66,358 hectares across the Genie, Usam and CBX/Shoe projects. Hatchet has acquired a 100% interest in the Purchased Property by, on the date of closing (the “Closing Date”), paying the Company $25,000 and issuing to the Company such number of Units in the capital of Hatchet equal to 9.9% of the issued and outstanding shares immediately following the issuance. Each Hatchet Unit shall be comprised of one Share and one share purchase warrant, entitling Skyharbour to purchase one additional Share for a period of three years at a price that is a 25% premium to the deemed value of the Shares in both years 1 and 2, and then increases to a 50% premium to the issuance value of the Shares in year 3. The Company shall retain a claw-back provision whereby, within 90 days after the 3rd anniversary of the Closing Date, the Company may elect by written notice to Hatchet of its intention to purchase back a twenty-five percent (25%) interest in the Purchased Property by, within 90 days of delivery of such notice, incurring exploration expenditures or paying cash in lieu of to fund future exploration, equivalent to fifty percent (50%) of the total amount that Hatchet had spent during the term that is three years from the Closing Date in exploration expenditures on the Purchased Property. If Hatchet has not incurred any exploration expenditures during the three years following the closing date, then Skyharbour shall automatically receive the 25% interest in the Property. The Company shall also retain a 2% net smelter returns royalty from minerals mined and removed from the Purchased Property, of which Hatchet may purchase one-half, being 1%, at any time for $2,000,000. One of the conditions precedent for Hatchet prior to closing on both agreements was to close a financing for minimum gross proceeds of $1,500,000 which is now complete. Furthermore, Hatchet will proceed to list on the TSX Venture Exchange or the Canadian Securities Exchange or will have sold its interest to or combined with a similarly listed issuer. If this is not complete within 18 months, Hatchet’s right to acquire the Purchased Property will terminate. If after 12 months Hatchet has not listed then it shall pay Skyharbour a monthly fee of $10,000 until such conditions are satisfied or an aggregate of $60,000 has been paid, whichever occurs first. Highway Property Summary: The Highway Uranium Project consists of nine claims covering 17,606 hectares, approximately 41 km south of the Rabbit Lake Mine and 11 km southwest of Uranium Energy Corp.’s (UEC, formerly UEX) West Bear U and Co-Ni Deposits. Highway 905 runs through the property, providing excellent access for exploration and the project is in close proximity to regional infrastructure. There has been limited modern exploration carried out on the project but there is the potential for high-grade basement-hosted and unconformity-related uranium mineralization. Highway Property Map:https://skyharbourltd.com/_resources/images/Sky_Highway.jpg‎ The project is underlain by Wollaston Supergroup metasedimentary gneisses (pelitic to psammopelitic and psammitic to meta-arkosic) folded around and overlying an Archean felsic gneiss dome which outcrops in the southwestern portion of the property and cores a northeast trending antiformal fold nose. The Highway Project is located approximately 7 km east of the present-day margin of the Athabasca Basin but is believed to have been covered by Athabasca sandstone in the past. Genie Property Summary: The Genie property consists of five claims totalling 16,930 ha, and is located approximately 48 km northeast of Cameco’s Eagle Point Uranium Mine (Rabbit Lake Operation) and 40 km north of Wollaston Lake Post. The project is underlain by Wollaston Superground metasedimentary gneisses and Archean granitoids, with highly prospective pelitic to psammopelitic gneisses (including graphitic varieties) and several north-trending faults related to the Tabbernor fault system being mapped on the property. The project lies outside the current extent of the Athabasca Basin, but is believed to have been overlain by now-eroded Athabasca sandstones in the past and has the potential for high-grade basement-hosted and unconformity-related uranium mineralization. The property is underlain by a series of linear magnetic highs (interpreted as granitoids) and magnetic lows (interpreted as metasedimentary gneisses), cross-cut by a highly magnetic northwest-trending Mackenzie Diabase dyke. Genie Property Map:https://skyharbourltd.com/_resources/images/Sky_Genie.jpg Previous work on the Genie project includes limited diamond drilling (three historical drill holes, of which one was abandoned in overburden) and a variety of airborne and ground geophysical surveys, prospecting, geological mapping, lake sediment and overburden sampling, and soil sampling. Most of this exploration work took place between 1966 to 1980, prior to the advent of modern geophysical methods and geological models, but in 2014 part of the Genie property was covered by a helicopter-borne DIGHEM magnetic, electromagnetic, and radiometric survey. The survey showed a strong central EM conductor following a magnetically inferred contact on the two northeastern most claims, which is locally disrupted by several moderately conductive N-S trending structural breaks, inferred to be faults. This strong conductor is highly prospective for uranium mineralization, and drilling done in 1969 and 1971 has confirmed the presence of graphitic and sulfide-containing pelitic gneisses on the property. Lake sediment samples also collected at Genie during the 2014 exploration program, contained up to 63.3 ppm U, further showcasing the prospectivity of the property. Usam Property Summary: The Usam Project consists of twelve claims totalling 40,041 ha and is located approximately 16 km northeast of Cameco’s Eagle Point Mine (Rabbit Lake Operation). The project has numerous EM conductors that are associated with significant magnetic lows of the Wollaston Domain. While the project is outside the current confines of the Athabasca Basin, the area was overlain by Athabasca sandstones historically. Basement rocks on the property include Wollaston Supergroup metasediments and Archean granitoid gneisses, with highly prospective pelitic to psammopelitic gneisses (including graphitic varieties) making up the largest proportion of the basement rocks. Several north-trending faults related to the Tabbernor fault system cross-cut the property. Usam Property Map:https://skyharbourltd.com/_resources/images/Sky_Usam.jpg Previous work on the project includes diamond drilling (12 holes), lake sediment sampling, soil sampling, geological mapping, ground and airborne geophysics, marine seismic, prospecting, and other geochemical sampling, the majority of which was done in the 1980’s and 1970’s. Modern exploration of the property has been limited to geophysics and ground prospecting. As such there is a significant untested potential on the project. Trenching on Cleveland Island uncovered up to 0.31% U3O8 in mineralized pegmatite, and diamond drilling on Gilles Island intersected anomalous uranium, indicating that the basement rocks underling the Usam property are fertile sources of uranium in addition to containing pegmatite- and granite-hosted U-Th-REE mineralization. There are also several sedimentary-hosted base metals (i.e. Cu and Zn) showings on the project and in the surrounding area, which show similarities to the sedimentary-hosted Cu mineralization previously discovered by Rio Tinto and its partners at the Janice Lake Project further southwest in the Wollaston Domain. CBX/Shoe Property Summary: The CBX property has been recently expanded through staking to include five additional claims adjoining the previously staked CBX and Shoe properties, which have been combined to include a total of seven claims covering 8,777 hectares. The 609 ha Shoe property has remained unchanged, with both CBX and Shoe now consisting of eight non-contiguous claims totalling 9,386 hectares. CBX/Shoe Property Map:https://skyharbourltd.com/_resources/images/Sky_Shoe.jpg‎ The new claims lie approximately 6.5 km to 25 km northeast of the Eagle Point uranium mine and cover the northern shore of Wollaston Lake including parts of Cunning Bay. Outcrop exposure on the property is poor, but historical mapping and drilling shows that the newly expanded CBX project is underlain by a mixture of Wollaston Supergroup metasedimentary gneisses, Hudsonian intrusives, and Archean felsic gneisses of the Western Wollaston Domain. Similar lithologies host uranium mineralization at the Rabbit Lake operation, including the Eagle Point deposit, and other uranium deposits in the Athabasca Basin and surrounding regions. The CBX and Shoe properties have had historical exploration, including airborne and ground geophysical surveys, lake sediment, soil, and spruce geochemical surveys, till sampling, prospecting, geological mapping, and a marine seismic survey, but the majority of this work took place in the 1960’s to 1980’s, with limited modern exploration work being carried out on a small portion of the CBX and Shoe properties. Grant of Incentive Stock Options: Skyharbour also announces that the Company has granted 3,500,000 incentive stock options (the "Options") to officers, directors and consultants of the Company. The Options are exercisable at $0.40 per share for a period of five years from the date of grant. The Options have been granted under and are governed by the terms of the Company's Incentive Stock Option Plan. Qualified Person: The technical information in this news release has been prepared in accordance with the Canadian regulatory requirements set out in National Instrument 43-101 and reviewed and approved by Serdar Donmez, P.Geo., VP of Exploration for Skyharbour as well as a Qualified Person. About Skyharbour Resources Ltd.: Skyharbour holds an extensive portfolio of uranium exploration projects in Canada's Athabasca Basin and is well positioned to benefit from improving uranium market fundamentals with interest in thirty-six projects covering over 614,000 hectares (over 1.5 million acres) of land. Skyharbour has acquired from Denison Mines, a large strategic shareholder of the Company, a 100% interest in the Moore Uranium Project, which is located 15 kilometres east of Denison's Wheeler River project and 39 kilometres south of Cameco's McArthur River uranium mine. Moore is an advanced-stage uranium exploration property with high-grade uranium mineralization in several zones at the Maverick Corridor. Adjacent to the Moore Project is the Russell Lake Uranium Project, in which Skyharbour is operator with joint-venture partner RTEC. The project hosts widespread uranium mineralization in drill intercepts over a large property area with exploration upside potential. The Company is actively advancing these projects through exploration and drilling programs. Skyharbour also has joint ventures with industry leader Orano Canada Inc., Azincourt Energy, and Thunderbird Resources at the Preston, East Preston, and Hook Lake Projects, respectively. The Company also has several active earn-in option partners, including CSE-listed Basin Uranium Corp. at the Mann Lake Uranium Project; TSX-V listed North Shore Uranium at the Falcon Project; UraEx Resources at the South Dufferin and Bolt Projects; Hatchet Uranium at the Highway Project; CSE-listed Mustang Energy at the 914W Project; and TSX-V listed Terra Clean Energy at the South Falcon East Project. In aggregate, Skyharbour has now signed earn-in option agreements with partners that total to over $36 million in partner-funded exploration expenditures, over $20 million worth of shares being issued, and $14 million in cash payments coming into Skyharbour, assuming that these partner companies complete their entire earn-ins at the respective projects. Skyharbour's goal is to maximize shareholder value through new mineral discoveries, committed long-term partnerships, and the advancement of exploration projects in geopolitically favourable jurisdictions. Skyharbour’s Uranium Project Map in the Athabasca Basin:https://www.skyharbourltd.com/_resources/images/SKY_SaskProject_Locator_2024-11-21_v1.jpg To find out more about Skyharbour Resources Ltd. (TSX-V: SYH) visit the Company’s website at www.skyharbourltd.com. SKYHARBOUR RESOURCES LTD. “Jordan Trimble”__________________________________Jordan TrimblePresident and CEO For further information contact myself or:Nicholas ColturaInvestor Relations Manager ‎Skyharbour Resources Ltd. ‎Telephone: 604-558-5847 ‎Toll Free: 800-567-8181 ‎Facsimile: 604-687-3119 ‎Email: info@skyharbourltd.com NEITHER THE TSX VENTURE EXCHANGE NOR ITS REGULATION SERVICES PROVIDER ACCEPTS RESPONSIBILITY FOR THE ADEQUACY OR ACCURACY OF THE CONTENT OF THIS NEWS RELEASE. Forward-Looking Information This news release contains “forward‐looking information or statements” within the meaning of applicable securities laws, which may include, without limitation, completing ongoing and planned work on its projects including drilling and the expected timing of such work programs, other statements relating to the technical, financial and business prospects of the Company, its projects and other matters. All statements in this news release, other than statements of historical facts, that address events or developments that the Company expects to occur, are forward-looking statements. Although the Company believes the expectations expressed in such forward-looking statements are based on reasonable assumptions, such statements are not guarantees of future performance and actual results may differ materially from those in the forward-looking statements. Such statements and information are based on numerous assumptions regarding present and future business strategies and the environment in which the Company will operate in the future, including the price of uranium, the ability to achieve its goals, that general business and economic conditions will not change in a material adverse manner, that financing will be available if and when needed and on reasonable terms. Such forward-looking information reflects the Company’s views with respect to future events and is subject to risks, uncertainties and assumptions, including the risks and uncertainties relating to the interpretation of exploration results, risks related to the inherent uncertainty of exploration and cost estimates and the potential for unexpected costs and expenses, and those filed under the Company’s profile on SEDAR+ at www.sedarplus.ca. Factors that could cause actual results to differ materially from those in forward looking statements include, but are not limited to, continued availability of capital and financing and general economic, market or business conditions, adverse weather or climate conditions, failure to obtain or maintain all necessary government permits, approvals and authorizations, failure to obtain or maintain community acceptance (including First Nations), decrease in the price of uranium and other metals, increase in costs, litigation, and failure of counterparties to perform their contractual obligations. The Company does not undertake to update forward‐looking statements or forward‐looking information, except as required by law.
--------------------------------------------------

Title: ValOre Announces Closing of Agreements with Skyharbour for Uranium Projects Located in Saskatchewan
URL: https://financialpost.com/globe-newswire/valore-announces-closing-of-agreements-with-skyharbour-for-uranium-projects-located-in-saskatchewan
Time Published: 2025-02-10T22:19:35Z
Description: VANCOUVER, British Columbia, Feb. 10, 2025 (GLOBE NEWSWIRE) — ValOre Metals Corp. (“ValOre”; TSX‐V: VO; OTCQB: KVLQF; Frankfurt: KEQ0) today provided an update on developments concerning Hatchet Uranium Corp. (“Hatchet”), in which ValOre currently holds an ap…
--------------------------------------------------

Title: ValOre Announces Closing of Agreements with Skyharbour for Uranium Projects Located in Saskatchewan
URL: https://www.globenewswire.com/news-release/2025/02/10/3023833/0/en/ValOre-Announces-Closing-of-Agreements-with-Skyharbour-for-Uranium-Projects-Located-in-Saskatchewan.html
Time Published: 2025-02-10T22:15:00Z
Full Content:
February 10, 2025 17:15 ET | Source: ValOre Metals Corporation ValOre Metals Corporation VANCOUVER, British Columbia, Feb. 10, 2025 (GLOBE NEWSWIRE) -- ValOre Metals Corp. (“ValOre”; TSX‐V: VO; OTCQB: KVLQF; Frankfurt: KEQ0) today provided an update on developments concerning Hatchet Uranium Corp. (“Hatchet”), in which ValOre currently holds an approximate 51.5% partially diluted ownership interest. ValOre, further to its news releases dated November 4th, 2024, and February 5th, 2025, announces that closing has now occurred on the option agreement (the “Agreement”) with Skyharbour Resources Ltd. (“Skyharbour”), whereby Hatchet may acquire an 80% interest in Skyharbour’s 17,606 ha Highway Uranium Property (“Highway”) and a 100% interest, subject to a claw-back provision for Skyharbour, in Skyharbour’s Genie, Usam and CBX/Shoe Uranium Projects (the “Purchased Properties”) totalling 66,358 ha, all located to the northeast of the Athabasca Basin, northern Saskatchewan, Canada. The Agreement on Highway provides Hatchet an opportunity to earn an 80% interest in the related claims over a three-year period by fulfilling combined cash, share issuance and exploration expenditure commitments of CAD $3.345 million. Terms of Highway Property Agreement: Highway, now consists of nine (9) mineral claims comprising approximately 17,606 hectares, due to the recent addition of five (5) mineral claims comprising 8,267 ha. Hatchet may acquire an 80% interest in Highway by (i) issuing common shares in the capital of Hatchet (“Shares”) having an aggregate value of CAD $1,050,000; (ii) making aggregate cash payments of CAD $245,000; and (iii) incurring an aggregate of CAD $2,050,000 in exploration expenditures on Highway over a three-year period, as follows: (1) Deemed pricing of Shares is based on the twenty (20) day volume weighted average price on the stock exchange in which Hatchet shall list its Shares for trading, being either the TSX Venture Exchange or the Canadian Securities Exchange (“Deemed Price”) or the last sale price, if not listed on a stock exchange at the time of issuance. In the event that the issuance of any Shares pursuant to the above would result in Skyharbour holding 10% or more of the outstanding Shares of Hatchet, Hatchet will issue that number of Shares which would result in Skyharbour receiving 9.9% of the issued and outstanding Shares post-issuance and will pay cash in lieu of the Shares for the difference. Skyharbour shall retain a 2% net smelter returns royalty from minerals mined and removed from Highway, of which Hatchet may purchase one-half, being 1%, at any time for $1,000,000. Terms of the Purchased Properties: The Purchased Properties consists of twenty-five (25) mineral claims comprising approximately 66,358 hectares across the Genie, Usam and CBX/Show projects. Hatchet acquired a 100% interest in the Purchased Properties by, on the date of closing (the “Closing Date”), paying Skyharbour $25,000 and issuing to Skyharbour such number of units in the capital of Hatchet (“Hatchet Units”) equal to 9.9% of the issued and outstanding Shares immediately following issuance. Each Hatchet Unit shall be comprised of one Share and one share purchase warrant, entitling Skyharbour to purchase one additional Share for a period of three years at a price that is a 25% premium to the deemed value of the Shares in both years 1 and 2, and then increases to a 50% premium to the issuance value of the Shares in year 3. Skyharbour shall retain a claw-back provision whereby, within 90 days after the 3rd anniversary of the Closing Date, Skyharbour may elect by written notice to Hatchet of its intention to purchase back a twenty-five percent (25%) interest in the Purchased Properties by, within 90 days of delivery of such notice, incurring exploration expenditures or paying cash in lieu of to fund future exploration, equivalent to fifty percent (50%) of the total amount that Hatchet had spent during the term that is three years from the Closing Date in exploration expenditures on the Purchased Properties. If Hatchet has not incurred any exploration expenditures during the three years following the closing date, then Skyharbour shall automatically receive the 25% interest in the Purchased Properties. Skyharbour shall also retain a 2% net smelter returns royalty from minerals mined and removed from the Purchased Properties, of which Hatchet may purchase one-half, being 1%, at any time for $2,000,000. One of the conditions precedent for Hatchet prior to closing on both agreements was to close a financing for minimum gross proceeds of $1,500,000 which is now complete. Furthermore, Hatchet will proceed to list on the TSX Venture Exchange or the Canadian Securities Exchange or will have sold its interest to or combined with a similarly listed issuer. If this is not complete within 18 months, Hatchet’s right to acquire the Purchased Property will terminate. If after 12 months Hatchet has not listed then it shall pay Skyharbour a monthly fee of $10,000 until such conditions are satisfied or an aggregate of $60,000 has been paid, whichever occurs first. Highway Property Summary: The Highway Uranium Project consists of nice (9) claims covering 17,606 hectares, approximately 41 km south of the Rabbit Lake Mine and 11 km southwest of Uranium Energy Corp.’s (UEC, formerly UEX) West Bear U and Co-Ni Deposits. The Highway Project is located approximately 7 km east of the present-day margin of the Athabasca Basin but is believed to have been covered by Athabasca sandstone in the past. Highway 905 runs through the property, providing excellent access for exploration and in close proximity to regional infrastructure. There has been limited modern exploration performed on the project but there is the potential for high-grade basement-hosted-uranium mineralization. The project is underlain by Wollaston Supergroup metasedimentary gneisses (pelitic to psammopelitic and psammitic to meta-arkosic) folded around and overlying an Archean felsic gneiss dome which outcrops in the southwestern portion of the property and cores a northeast trending antiformal fold nose. Figure 1: Highway Property Location MapGenie Property Summary: The Genie property consists of five claims totalling 16,930 ha, and is located approximately 48 km northeast of Cameco’s Eagle Point Uranium Mine (Rabbit Lake Operation) and 40 km north of Wollaston Lake Post. The project is underlain by Wollaston Superground metasedimentary gneisses and Archean granitoids, with prospective pelitic to psammopelitic gneisses (including graphitic varieties) and several north-trending faults related to the Tabbernor fault system being mapped on the property. The project lies outside the current extent of the Athabasca Basin, but is believed to have been overlain by now-eroded Athabasca sandstones in the past and has the potential for high-grade basement-hosted uranium mineralization. The property is underlain by a series of linear magnetic highs (interpreted as granitoids) and magnetic lows (interpreted as metasedimentary gneisses), cross-cut by a highly magnetic northwest-trending Mackenzie Diabase dyke. Previous work on the Genie project includes limited diamond drilling (three historical drill holes, of which one was abandoned in overburden) and a variety of airborne and ground geophysical surveys, prospecting, geological mapping, lake sediment and overburden sampling, and soil sampling. Most of this exploration work took place between 1966 to 1980, prior to the advent of modern geophysical methods and geological models, but in 2014 part of the Genie property was covered by a helicopter-borne DIGHEM magnetic, electromagnetic, and radiometric survey. The survey showed a strong central EM conductor following a magnetically inferred contact on the two northeastern most claims, which is locally disrupted by several moderately conductive N-S trending structural breaks, inferred to be faults. This strong conductor is highly prospective for uranium mineralization, and drilling done in 1969 and 1971 has confirmed the presence of graphitic and sulfide-containing pelitic gneisses on the property. Lake sediment samples also collected at Genie during the 2014 exploration program, contained up to 63.3 ppm U, further showcasing the prospectivity of the property. Figure 2: Genie Property Location MapUsam Property Summary: The Usam Project consists of twelve claims totalling 40,041 ha and is located approximately 16 km northeast of Cameco’s Eagle Point Mine (Rabbit Lake Operation). The project has numerous EM conductors that are associated with significant magnetic lows of the Wollaston Domain. While the project is outside the current confines of the Athabasca Basin, the area was overlain by Athabasca sandstones historically. Basement rocks on the property include Wollaston Supergroup metasediments and Archean granitoid gneisses, with highly prospective pelitic to psammopelitic gneisses (including graphitic varieties) making up the largest proportion of the basement rocks. Several north-trending faults related to the Tabbernor fault system cross-cut the property. Previous work on the project includes diamond drilling (12 holes), lake sediment sampling, soil sampling, geological mapping, ground and airborne geophysics, marine seismic, prospecting, and other geochemical sampling, the majority of which was done in the 1980’s and 1970’s. Modern exploration of the property has been limited to geophysics and ground prospecting. As such there is a significant untested potential on the project. Trenching on Cleveland Island uncovered up to 0.31% U3O8 in mineralized pegmatite, and diamond drilling on Gilles Island intersected anomalous uranium, indicating that the basement rocks underling the Usam property are fertile sources of uranium in addition to containing pegmatite- and granite-hosted U-Th-REE mineralization. There are also several sedimentary-hosted base metals (i.e. Cu and Zn) showings on the project and in the surrounding area, which show similarities to the sedimentary-hosted Cu mineralization previously discovered by Rio Tinto and its partners at the Janice Lake Project further southwest in the Wollaston Domain. Figure 3 – Usam Property Location Map CBX/Shoe Property Summary: The CBX property has been recently expanded through staking to include five additional claims adjoining the previously staked CBX and Shoe properties, which have been combined to include a total of seven claims covering 8,777 hectares. The 609 ha Shoe property has remained unchanged, with both CBX and Shoe now consisting of eight non-contiguous claims totalling 9,386 hectares. The new claims lie approximately 6.5 km to 25 km northeast of the Eagle Point uranium mine and cover the northern shore of Wollaston Lake including parts of Cunning Bay. Outcrop exposure on the property is poor, but historical mapping and drilling shows that the newly expanded CBX project is underlain by a mixture of Wollaston Supergroup metasedimentary gneisses, Hudsonian intrusives, and Archean felsic gneisses of the Western Wollaston Domain. Similar lithologies host uranium mineralization at the Rabbit Lake operation, including the Eagle Point deposit, and other uranium deposits in the Athabasca Basin and surrounding regions. The CBX and Shoe properties have had historical exploration, including airborne and ground geophysical surveys, lake sediment, soil, and spruce geochemical surveys, till sampling, prospecting, geological mapping, and a marine seismic survey, but the majority of this work took place in the 1960’s to 1980’s, with limited modern exploration work being carried out on a small portion of the CBX and Shoe properties. Figure 4: CBX/Shoe Property Location Map About Hatchet Uranium Corp.Hatchet Uranium Corp. was incorporated by ValOre on February 7, 2024 and now holds a commanding land position, comprising 97,674 hectares, in the Eastern Athabasca Region of Saskatchewan. About Skyharbour Resources Ltd. To find out more about Skyharbour Resources Ltd. (TSX-V: SYH) visit Skyharbour’s website at www.skyharbourltd.com. Qualified Person (“QP”) The technical information in this news release has been prepared in accordance with Canadian regulatory requirements set out in NI 43-101 and reviewed and approved by Thiago Diniz, P.Geo., ValOre’s QP and Vice President of Exploration. About ValOre Metals Corp. ValOre Metals Corp. (TSX‐V: VO) is a Canadian company with a team aiming to deploy capital and knowledge on projects which benefit from substantial prior investment by previous owners, existence of high-value mineralization on a large scale, and the possibility of adding tangible value through exploration and innovation. ValOre’s Pedra Branca Platinum Group Elements Project comprises 45 exploration licenses covering a total area of 51,096 hectares (126,260 acres) in northeastern Brazil. At Pedra Branca, 7 distinct PGE+Au deposit areas host, in aggregate, a 2022 NI 43-101 inferred resource of 2.198 Moz 2PGE+Au contained in 63.6 Mt grading 1.08 g/t 2PGE+Au. ValOre’s team believes the Pedra Branca project has significant exploration discovery and resource expansion potential. (CLICK HERE to download 2022 technical report* and CLICK HERE for news release dated March 24, 2022). *The 2022 Technical Report entitled “Independent Technical Report –Mineral Resource Update on the Pedra Branca PGE Project, Ceará State, Brazil” was prepared as a National Instrument 43-101 Technical Report on behalf of ValOre Metals Corp. with an effective date of March 08, 2022. The 2022 Technical Report by independent qualified persons, Fábio Valério (P.Geo.) and Porfirio Cabaleiro (P.Eng.), of GE21, commissioned to complete the mineral resource estimate while Chris Kaye of Mine and Quarry Engineering Services Inc. (MQes), was commissioned to review the metallurgical information. The Mineral Resource estimates were prepared in accordance with the CIM Standards, and the CIM Guidelines, using geostatistical, plus economic and mining parameters appropriate to the deposit. Mineral Resources, which are not mineral reserves, do not have demonstrated economic viability, and may be materially affected by environmental, permitting, legal, marketing, and other relevant issues. Mineral Resources are based upon a cut-off grade of 0.4 g/t PGE+Au, correlated to Pd_eq grade of 0.35 g/t, and were limited by an economic pit built in Geovia Whittle 4.3 software and following the geometric and economic parameters as disclosed in the 2022 NI 43-101 Technical Report. On behalf of the Board of Directors,“Jim Paterson”James R. Paterson, Chairman and CEO ValOre Metals Corp. For further information about ValOre Metals Corp. or this news release, please visit our website at www.valoremetals.com or contact Investor Relations at 604.646.4527, or by email at contact@valoremetals.com. ValOre Metals Corp. is a proud member of Discovery Group. For more information, please visit: http://www.discoverygroup.ca/ Neither the TSX Venture Exchange nor its Regulation Services Provider (as that term is defined in the policies of the TSX Venture Exchange) accepts responsibility for the adequacy or accuracy of this release. This news release contains “forward-looking statements” within the meaning of applicable securities laws. Although ValOre believes that the expectations reflected in its forward-looking statements are reasonable, such statements have been based on factors and assumptions concerning future events that may prove to be inaccurate. These factors and assumptions are based upon currently available information to ValOre. Such statements are subject to known and unknown risks, uncertainties and other factors that could influence actual results or events and cause actual results or events to differ materially from those stated, anticipated or implied in the forward-looking statements. A number of important factors including those set forth in other public filings could cause actual outcomes and results to differ materially from those expressed in these forward-looking statements. Factors that could cause the actual results to differ materially from those in forward-looking statements include the future operations of ValOre and economic factors. Readers are cautioned to not place undue reliance on forward-looking statements. The statements in this press release are made as of the date of this release and, except as required by applicable law, ValOre does not undertake any obligation to publicly update or to revise any of the included forward-looking statements, whether as a result of new information, future events or otherwise. ValOre undertakes no obligation to comment on analyses, expectations or statements made by third parties in respect of ValOre, or its financial or operating results or (as applicable), their securities. Photos accompanying this announcement are available at: https://www.globenewswire.com/NewsRoom/AttachmentNg/f5c3fcc5-6703-483e-8780-62526e848cd3https://www.globenewswire.com/NewsRoom/AttachmentNg/9e021720-7cb6-4cd4-88bd-fb660ddbbeaehttps://www.globenewswire.com/NewsRoom/AttachmentNg/0a871315-2064-4f0f-b182-8163e80f949fhttps://www.globenewswire.com/NewsRoom/AttachmentNg/8521e5a0-18da-4705-a8ab-6410f0ff48b4
--------------------------------------------------

Title: The Magnificent 7 trade is struggling — Here's why
URL: https://finance.yahoo.com/news/the-magnificent-7-trade-is-struggling--heres-why-181716458.html
Time Published: 2025-02-10T18:17:16Z
Full Content:
We are experiencing some temporary issues. The market data on this page is currently delayed. Please bear with us as we address this and restore your personalized lists. The Magnificent Seven has turned into the Stupendous One as AI spending fears weigh on sentiment. The usually reliably hot Magnificent Seven trade of Meta (META), Amazon (AMZN), Google (GOOG), Apple (AAPL), Nvidia (NVDA), Microsoft (MSFT), and Tesla (TSLA) has underwhelmed more than one month into 2025. Only one of the big-cap tech components — Meta — has notched double-digit gains out of the box. In fact, shares of Meta have risen for 15 straight sessions through Monday — bringing its year-to-date advance to a stellar (or stupendous...) 20%. Amazon is the only other Mag Seven component to be up on the year to the tune of 5.9%, slightly ahead of the 3.4% increase for the S&P 500 (^GSPC). Alphabet, Apple, Nvidia, Microsoft, and Tesla are all down year to date, with an average drop of 3% based on Yahoo Finance's calculations. Tesla is the worst performer on the year, down 6% as it has been hit with less-than-inspiring sales news from across the world. Tariff concerns have also weighed on the stock, similar to other auto plays like General Motors (GM) and Ford (F). Digging deeper, six out of seven Mag Seven members have reported fourth quarter earnings so far: All but Meta are down since their reports. Alphabet is down the most at 10.4%, as the Street reacted very negatively to its initial 2025 outlook. "Price reactions suggest growing concerns around monetization vs. capex for hyperscalers," said BofA strategist Savita Subramanian in a client note on Monday. To Subramanian's point, the capital expenditure numbers being tossed for 2025 by Big Tech to build out AI infrastructure have been eye-popping — and have caught investors off guard. Collectively, they have the Street worried about whether profit margins for the Mag Seven hit a short-term peak in 2024. Meta, Microsoft, Amazon, and Alphabet are slated to spend a cumulative $325 billion in capital expenditures and investments this year, Yahoo Finance's Laura Bratton reports. This would mark a 46% increase year over year for the four tech stalwarts. Amazon alone sees $104 billion in capital expenditures this year, well above prior analyst forecasts of $80 billion to $85 billion. RBC Capital Markets analyst Brad Erickson warned last week Mag Seven names such as Amazon are "crowded" trades and that the "AI 'spend money to make money' debate will undoubtedly continue." The question now beginning to circulate on the Street is if Mag Seven weakness bleeds into the broader market. If so, it could have an outsized impact on stocks not directly tied to tech. The combined weighting of the Mag Seven within the S&P 500 surged from 21.9% in 2020 to over 30% in 2024, the highest concentration ever recorded, according to data from First Trust. "While the main S&P 500 index has been able to hold up incredibly well in the face of the Mag 7 weakness, should the market get further concerning news on tariffs, and/or inflation, the indices could be susceptible for a much larger decline than we have seen thus far given the overall weakness from the top-weighted names. In addition, should these names remain under pressure that likely puts a 'cap' to the index upside in the shorter-term given their importance," explains 22V Research strategist Jeff Jacobson. A near-term test for the Mag Seven bulls will come with Feb. 26 earnings from Nvidia, says Jacobson. — Brian Sozzi is Yahoo Finance's Executive Editor. Follow Sozzi on X @BrianSozzi, Instagram and on LinkedIn. Tips on stories? Email brian.sozzi@yahoofinance.com. Click here for the latest technology news that will impact the stock market Read the latest financial and business news from Yahoo Finance Apple's latest Powerbeats Pro 2 get a built-in heart rate monitor for $249. Don't rule out a rate hike this year — maybe sooner than you think, prominent economist Torsten Sløk said. OpenAI CEO Sam Altman shot down an unsolicited offer by Elon Musk to purchase OpenAI for $97.4 billion. Gold hovered near all-time highs on Tuesday as the threat of tariffs has sent prices up 10% year to date. Fed Chair Jerome Powell told lawmakers Tuesday that the Fed is not in a hurry to adjust interest rates, reiterating a cautious stance as central bank policymakers digest signs of stubborn inflation and policy uncertainties from the new Trump administration. Tesla's stock is enduring a violent sell-off. Federal Reserve Chair Jerome Powell offered some new assurances Tuesday about payments routed to the central bank from the Treasury Department, his first comments about the fallout resulting from Elon Musk’s access to a sensitive payments system. Citadel CEO Ken Griffin on Tuesday called aggressive trade talk from the new Trump administration" 'huge mistake," producing chaos that in Griffin's view poses an "impediment" to economic growth. (Bloomberg) -- Stanford, Harvard and other top research universities warned of the potential hit to medical care and scientific innovation from the Trump administration’s cut to a type of federal funding from the National Institutes of Health that’s set to go into effect Monday. Most Read from BloombergNice Airport, If You Can Get to It: No Subway, No Highway, No BridgeSin puente y sin metro: el nuevo aeropuerto de Lima es una debacleThe Forgotten French Architect Who Rebuilt MarseilleIn New Orl SpringWorks Therapeutics stock surged over Monday after a report from Reuters that German medical tech giant Merck KGaA could be close to acquiring the company. Sign in to access your portfolio
--------------------------------------------------

Title: Snap Plans $700 Million Junk Offering to Buy Back Convertibles
URL: https://financialpost.com/pmn/business-pmn/snap-plans-700-million-junk-offering-to-buy-back-convertibles
Time Published: 2025-02-10T17:49:17Z
Description: Snap Inc. plans to offer $700 million of junk bonds to repurchase convertible debt, the social-media company said in a statement on Monday.
--------------------------------------------------

Title: Disney dumps two DEI programs as investors pressure company to ax more woke initiatives: SEC filing
URL: https://nypost.com/2025/02/10/media/disney-dumps-two-dei-programs-as-investors-pressure-company/
Time Published: 2025-02-10T17:21:23Z
Description: Disney is reportedly pulling back on its diversity, equity and inclusion policies — the latest major company to walk back the woke initiatives amid pressure from activist investors and the Trump administration. The media giant — which saw its bottom line hurt…
--------------------------------------------------

Title: Crypto News Weekly Recap: Trump’s Tariff War, the Rise of AI Meme Coins, and New P2E Games
URL: https://bitcoinist.com/weekly-recap-trumps-tariffs-ai-meme-coins-p2e-games/
Time Published: 2025-02-10T14:00:47Z
Full Content:
We’re back with our weekly crypto recap. If Trump’s presidency means one thing, it’s that there’ll be no shortage of crypto news. This past week, the newly inaugurated president (along with his immediate family and friends) continued to make headlines in crypto circles. We also witnessed noteworthy developments in AI and gaming. Here’s a recap of what happened in crypto recently: Now, let’s zoom in a little. The crypto market faced turbulence, with Bitcoin ($BTC) briefly touching $92K after Trump announced 25% tariffs on Canadian and Mexican imports. Canada immediately imposed a counter-tariff, whereas Mexican President Claudia Sheinbaum took a more cautious approach. However, there may be more strategic depth to Trump’s tariffs threats than immediate market reactions suggest. Specifically, it could be an attempt to weaken the dollar while maintaining low yields, thus forcing countries to shift from short-term dollar reserves to long-term Treasury bonds. Eric Trump appears to be his father’s main crypto advisor, as evident from his involvement with World Liberty Finance (WLFI) and the official $TRUMP meme coin. Last Monday, the presidential son tweeted ‘it’s a great time to add $ETH’ as the token dipped some 20% (on the back of Trump’s tariff war, that is). $ETH’s price subsequently rose from $2.3K to $2.7K. With an average $ETH purchase price of $3.3K, WLFI currently faces an unrealized loss of approximately $31M. Trump signed an executive order to establish America’s first federal sovereign wealth fund. Senator Cynthia Lummis then tweeted it’s a ‘₿ig deal,’ with the Bitcoin ₿, possibly hinting at an upcoming $BTC investment. This approach would mirror Norway’s sovereign wealth fund strategy, which already has indirect $BTC exposure through investments in companies like MicroStrategy. While Alaska and Texas have been successfully operating state-level sovereign wealth funds for some time, this is an uncharted territory for the federal government. Musk raised eyebrows with his proposal to put the US Treasury on the blockchain – all $1.5T of it. We’re talking about a blockchain that would handle everything from social security checks to federal employee paychecks. However, many are skeptical about the idea. Either way, this transition would force over 3M federal employees to start using blockchain technology and push crypto adoption to levels never seen before. Google CEO Sundar Pichai plans to invest $75B in AI, which he called a ‘small expenditure.’ It’s clear that AI is taking center stage in Google’s strategy, especially given how much of the company’s $95B revenue last year was tied to AI-related services. While Google’s yearly revenue increased 12%, Wall Street seemed jittery about this spending plan, which sent Alphabet Inc. shares down 10%. Capital inflows into the AI sector mean that AI meme coins and AI agent tokens might see more upside this year. In other crypto news, Mythical Games launched a Super Bowl promotion for NFL Rivals, its NFT mobile football game. The two-week event features 30 new player cards from the Kansas City Chiefs and Philadelphia Eagles. In other crypto game news, a new move-to-earn (M2E) fitness app, StepMania, is now available on Telegram. It takes inspiration from StepN, a 2021 M2E game that attracted 5.6M users with its unique mechanic and sneaker NFTs. Trump’s pro-crypto strategy, shared by Eric Trump and Elon Musk, is bullish for the entire altcoin sector. That’s particularly true for AI tokens like MIND of Pepe ($MIND) in light of the current Big Tech AI race. MIND of Pepe is a self-sovereign agent that runs on Ethereum. It can analyze market data, deliver exclusive insights to its token holders, engage in discussions on social media, and even launch its own projects (including crypto games). Currently on presale, one $MIND token costs $0.0032924, but the price will increase in 14 hours. The project has raised $5.6M so far, and early adopters have staked 859B tokens at a 407% APY. After $MIND hits the ground running and lists on DEX, it could surge 10X, especially now that Eric Trump has endorsed the Ethereum ecosystem. This week, all eyes are on Trump’s crypto agenda and the AI sector. The effect of federal policy decisions on market movements is undeniable, so we can only hope the upcoming days will bring good news. Uncertainty forces investors to seek utility-focused projects like MIND of Pepe. Its data-backed market insights and ability to autonomously launch new projects make it one of the best presales in 2025. As always, however, be sure to DYOR and keep a cool head. The current crypto market is extremely volatile, so diversify your portfolio and only invest as much as you can afford to lose. For updates and exclusive offers enter your email. Bitcoinist is the ultimate news and review site for the crypto currency community! Bitcoin news portal providing breaking news, guides, price analysis about decentralized digital money & blockchain technology. © 2025 Bitcoinist. All Rights Reserved.
--------------------------------------------------

Title: Mapping the learning curves of deep learning networks
URL: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012286
Time Published: 2025-02-10T14:00:00Z
Full Content:
There is an important challenge in systematically interpreting the internal representations of deep neural networks (DNNs). Existing techniques are often less effective for non-tabular tasks, or they primarily focus on qualitative, ad-hoc interpretations of models. In response, this study introduces a cognitive science-inspired, multi-dimensional quantification and visualization approach that captures two temporal dimensions of model learning: the “information-processing trajectory” and the “developmental trajectory.” The former represents the influence of incoming signals on an agent’s decision-making, while the latter conceptualizes the gradual improvement in an agent’s performance throughout its lifespan. Tracking the learning curves of DNNs enables researchers to explicitly identify the model appropriateness of a given task, examine the properties of the underlying input signals, and assess the model’s alignment (or lack thereof) with human learning experiences. To illustrate this method, we conducted 750 runs of simulations on two temporal tasks: gesture detection and sentence classification, showcasing its applicability across different types of deep learning tasks. Using four descriptive metrics to quantify the mapped learning curves—start, end - start, max, tmax—, we identified significant differences in learning patterns based on data sources and class distinctions (all p’s < .0001), the prominent role of spatial semantics in gesture learning, and larger information gains in language learning. We highlight three key insights gained from mapping learning curves: non-monotonic progress, pairwise comparisons, and domain distinctions. We reflect on the theoretical implications of this method for cognitive processing, language models and representations from multiple modalities. Deep learning networks, specifically recurrent neural networks (RNNs), are designed for processing incoming signals sequentially, making them intuitive computational systems for studying cognitive processing that involves dynamic contexts. There has been a tradition in the fields of machine learning and neuro-cognitive science to examine how a system (either humans or models) represents information through various computational and statistical techniques. Our study takes this one step further by devising a technique for examining the “learning curves” of deep learning networks utilizing the sequential representations as part of RNNs’ architectures. Just as humans develop learning curves when solving problems, the introduced method captures both how incoming signals help improve decision-making and how a system’s problem-solving abilities enhance when encountering the same situation multiple times throughout its lifespan. Our study selected two distinct tasks: gesture detection and emotion tweet classification, to illustrate the insights researchers can draw from mapping models’ learning curves. The proposed method hinted that gesture learning experiences are smoother, while language learning relies on sudden knowledge gains during processing, corroborating the findings from previous literature. Citation: Jiang Y, Dale R (2025) Mapping the learning curves of deep learning networks. PLoS Comput Biol 21(2): e1012286. https://doi.org/10.1371/journal.pcbi.1012286 Editor: Varun Dutt, Indian Institute of Technology Mandi - Kamand Campus: Indian Institute of Technology Mandi, INDIA Received: July 1, 2024; Accepted: January 13, 2025; Published: February 10, 2025 Copyright: © 2025 Jiang, Dale. This is an open access article distributed under the terms of the CreativeCommonsAttributionLicense, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability: All code used for running simulations, model fitting, plotting, and analysis is available in a GitHub repository at https://github.com/JoyceJiang73/Learning-Curves/. Data for conducting simulations is available from https://github.com/linuxsino/iMiGUE and https://huggingface.co/datasets/dair-ai/emotion. Competing interests: The authors have declared that no competing interests exist. 1 The advantages of using key point data are twofold. First, keypoint data conserves significant computing power; since each second of image sequences (i.e., matrices of pixels) may contain as many as 30 or 40 frames, running deep learning models on image sequences can be prohibitively expensive. Second, it provides better interpretability and understandability. Instead of being possible only through abstract information at the image-frame level, gesture detection can be operationalized as a sequential movement in keypoints across frames [57]. 2 The large batch size for emotion classification is due to each of its input data points using less space compared to the gesture input. 3 The spatial information arising from the gesture, specifically the keypoint coordinates in the gesture dataset, can vaguely help the model distinguish between gesture classes (e.g., hand vs. no gesture). However, this information remains a low-level cue, as the model lacks an understanding of what a complete hand gesture sequence or a no-gesture sequence looks like. Over the past decade, deep learning and neural networks have achieved remarkable performance in prediction and classification tasks in various domains, from machine translation and object recognition, to autonomous driving and reinforcement learning [1, 2]. As powerful representational learning tools, deep neural networks (DNNs) can capture complex patterns in data [1], yet understanding the nature of the information embedded in their multidimensional representations remains a challenge. Researchers have raised concerns about how DNN embeddings represent knowledge and how to holistically interpret these high-dimensional features [3–6]. Over-reliance on such models for decision-making could be detrimental in both research and applied settings due to their complexity and lack of explainability. This is particularly concerning when models rely on biased training data that does not generalize well to target tasks [7, 8]. Without a deep understanding of these models’ underlying properties, they may fail to align with the goals of their human designers. Recently, there has been a combined effort from cognitive science and deep learning to utilize representational learning to address model explainability concerns. These practices have become more prominent due to the success of large-scale models [17], especially large-language models [18]. For example, in the case of language models, examining the internal processes of the BERT Transformer-based architecture has shown that it may recapitulate common natural language processing (NLP) pipelines [19–21]. Chang and Bergen [22] found that the frequency and n-gram structure of word tokens significantly alters the training for language Transformer models learning these words. Inspired by this prior work, this paper outlines a technique for examining the learning trajectories of deep learning models, in particular recurrent neural networks (RNNs). There is historical precedent for our approach, too. McClelland, Rogers and others have studied the underlying knowledge of neural networks by tracking them as they learn [10, 23]. Despite this classic work in cognitive science, it is uncommon to see deep learning models that track progress as a way to unpack what is learned (e.g., going beyond simple RMSE curves; but see also [18], for a counterexample). We term this tracking a “learning curve,” as it resembles research on how human learners process incoming information and improve decision-making through iterations under different situations. A unique benefit with simulation is the possibility to examine many dimensions and measurements of the neural network over time. In the next section, we review recent work on interpreting DNNs and related models. We then introduce our approach based on learning curves. To date, various techniques have been proposed to interpret DNNs. For example, many model interpretability techniques provide task-specific and local explanations, such as saliency maps, attention maps, or Layer-wise Relevance Propagation (LRP), which interpret or visualize the localized influence of a region on the output. These ad-hoc approaches can be unstable, as even a minor change in a single pixel or hyperparameter can substantially affect the local relationships between input signals and output data [24]. Additionally, these local explanations fail to offer a global understanding of whether the selected architecture is well-suited to the task or how the DNN models experience learning. On the other hand, feature attribution methods like SHAP [66] and LIME [67], while conceptually easier to interpret and model-agnostic, tend to work more intuitively with interpretable dimensions typically found in structured tabular data. These methods are less effective for unstructured tasks, such as image and audio data, where feature dimensions are harder to define or interpret. While most techniques emphasize the qualitative interpretation of a model, such as what has been learned or captured by the DNN, the absence of quantifiable measurements makes cross-model comparisons in unstructured tasks particularly challenging [4, 5, 25]. We further provided a systematic description of different model explainability methods, along with their pros, cons, and use cases, as well as a comparison to our proposed approach in Table 1. https://doi.org/10.1371/journal.pone.0313772.t001 In a foundational article on deep learning, LeCun and the colleagues [1] characterized DNNs as representation-learning methods utilizing multilayered large neural network-style models. Representations can be viewed as mental objects capturing semantic properties either observable or unobservable [26]. Different from traditional machine learning models, DNNs display remarkable flexibility and efficiency in encoding lower-level input signals, such as pixels, audio frequencies, or word tokens, into multidimensional vectors at a sophisticated level through multilayered nonlinear transformations [27]. These transformations generate multiple levels of representations that learn hierarchies of features at each layer [1]. The continuous numerical vectors (or hidden vectors) learned at each level are commonly referred to as “embeddings” and serve as dense representations of the original input data. Following this construction, numerous studies have demonstrated the correspondence between DNN-generated and real-world distributed representations among words and sentences [11, 28], speeches [29], images [9], objects [30] and scenes [31, 32]. Representational learning in DNNs offers fundamental contributions to cognitive science, as it can inform how cognitive systems process and organize knowledge, facilitating the comparison of learning processes between humans and machines [33–35]. Recently, several studies in cognitive science and neuroscience have highlighted the importance of integrative modeling between computation, human brains and behaviors [12]. Beyond comparisons of static end-point knowledge, scholars have begun exploring the potential correspondence in learning and information processing between DNNs and human cognitive systems, given that the current performance of DNNs can already approximate human performance across various domains [36–38] (see [18] for a review). For instance, the representations (i.e., embeddings) extracted from multilayered DNNs have shown significant accuracy in predicting neural and behavioral responses in humans throughout the hierarchy of learning and processing. This evidence spans multiple neural-behavioral measurements (e.g., fMRI, EEG, ECoG), modalities (e.g., visual, auditory, and language processing), and model architectures (ranging from simple embedding models like GloVe to more complex neural networks such as RNNs, convolutional neural networks (CNNs), and transformer models (for further details, see [9, 11, 12, 23, 39, 40]). Given the extensive alignment observed between DNN embeddings and neural-behavioral activities, and their presumed meaningful representational mapping with human cognitive systems, an analysis of how they emerge in learning would seem important to understand these relationships. Goldstein and colleagues [11] identify the temporal correspondence between layer-by-layer embeddings in GPT-2 and evolving neural activities in language areas. However, this temporality is restricted to layerwise representations (from low-level to high-level representations) rather than how streams of signals have been received and processed by models or brains [38]. Our aim in this paper is to use temporal analysis in a systematic way by separating and tracking the time course of a network’s learning across classification tasks, thereby enhancing the understanding of the emergence of representations in DNNs. The goal of this study is to unpack the “learning curve” of DNNs through a sequence of hidden representations when the model encounters any temporal processing tasks across its training. To do so, we sample the network’s performance by using its embedding vectors to classify groups of items in its training input. This allows us to map out the progression of the network’s discriminations across these groups – how the network’s internal knowledge, in the form of embedding vectors, evolves during training. The model architecture we focus on is RNNs due to their capacity to model sequential data and time-dependent tasks [41], such as text generation, speech recognition and stock market prediction. Although other deep learning architectures, such as CNNs and Transformers, also have the capacity to process time-series data, RNNs are explicitly designed for processing sequential data, as they effectively capture temporal dependencies through their recurrent connections. CNNs excel in tasks such as image recognition by applying the kernel trick, where convolutional filters are used to extract spatial features from local regions of the input. While CNNs can be adapted to handle sequential data using techniques like 1D convolutional layers, they lack the inherent ability to capture temporal ordering in the data, as they process each segment of input independently. Transformers, on the other hand, rely on self-attention mechanisms to weigh the importance of different input tokens in parallel, allowing them to capture dependencies between distant elements (i.e., tokens). This feature makes Transformers better suited for tasks where relationships between tokens are not strictly ordered in time, and their parallel processing nature is less ideal for learning tasks that require explicit temporal progression. In sum, the strict sequential processing nature of RNNs makes them an intuitive architecture for studying cognitive processing that involves dynamic, changing contexts [42]. In this study, we propose a generalizable interpretability approach that maps the global learning curves of RNNs based on changes in classification performance between embeddings and output across timesteps of the input data. This performance reflects the predictive capacity or the amount of signal captured by the embeddings in predicting the final output. In RNN modeling, each timestep generates an embedding (e.g., in language processing, each timestep represents a word). These embeddings progressively incorporate information from the beginning up to timestep t. By examining changes in classification performance between these embeddings and the output, we can capture how the information evolves over time. To provide clarity, we use the term “learning curve” in this study to denote the holistic approach and intention behind mapping the underlying processing and developmental journey of DNNs. The method we propose separates two parts of the learning curve, one based on overall training, and another based on processing within input items during training. First, the “developmental trajectory” signifies the long-term learning process of DNNs, which is simulated by the increasing number of epochs (i.e., complete passes through an entire training dataset). Second, the “(information) processing trajectory” refers to the momentary accumulation of information across all timesteps (within an epoch), extractable from the performance of the RNN layer (see Fig 1 for a conceptual illustration). We detail each of these further below. The solid curve represents an individual information processing trajectory (across timesteps). For example, as an agent receives more signals over time in one session, its prediction of the gesture increases. The bundle of dotted lines represents a developmental trajectory (across epochs). For instance, this agent improves its ability to predict incoming gestures after repeatedly encountering similar patterns. In a typical binary classification task, the starting point at t=0 is expected to be near 0.50 (and gradually increase to 1.00 across timesteps and epochs) for all epochs because no signals have been provided at the first timestep for solving the underlying task. Hence, all information processing curves in this figure have the same starting point to reflect this pattern. In practice, certain classification tasks might contain structural information even at t=0 (e.g., gesture classification contains spatial information, such as the location of keypoints, at the initial timestep, which could help solve the gesture task from the very beginning). The solid curve represents an individual information processing trajectory (across timesteps). For example, as an agent receives more signals over time in one session, its prediction of the gesture increases. The bundle of dotted lines represents a developmental trajectory (across epochs). For instance, this agent improves its ability to predict incoming gestures after repeatedly encountering similar patterns. In a typical binary classification task, the starting point at t=0 is expected to be near 0.50 (and gradually increase to 1.00 across timesteps and epochs) for all epochs because no signals have been provided at the first timestep for solving the underlying task. Hence, all information processing curves in this figure have the same starting point to reflect this pattern. In practice, certain classification tasks might contain structural information even at t=0 (e.g., gesture classification contains spatial information, such as the location of keypoints, at the initial timestep, which could help solve the gesture task from the very beginning). https://doi.org/10.1371/journal.pcbi.1012286.g001 As Fig 1 illustrates, the resulting visualization consists of learning curves with timesteps on the x-axis and performance on the y-axis. Each epoch contains an information processing trajectory, and the set of curves together forms a developmental trajectory. Due to the multidimensional nature of this plot, while it provides rich qualitative information about the entire model learning experience, it can be challenging for researchers to comprehend all the constructed curves at once. Therefore, beyond the visual presentation and qualitative inspection of the multi-dimensional learning curves, this study further defines four measures: start performance (start, the initial capacity of each information processing), max performance (max, the maximum performance of each information processing), time at max (tmax, when the current information processing reaches the maximum performance), and end – start performance (end – start, the overall performance gain in this information processing) – to facilitate quantitative comparisons across tasks and datasets. Our study particularly focuses on these relatively easy-to-comprehend descriptive statistics (as opposed to more complex metrics like regression coefficients) to simplify the understanding of the already multidimensional and granular nature of the constructed learning curves. To illustrate the generalizability of our approach, we chose two distinct classification tasks: (i) sentence classification and (ii) gesture detection. These tasks differ widely in terms of their modalities. We predicted this would lead to variation in the underlying data generation processes and associated cognitive processing for each modality. In particular, gesture and body movements primarily result from the coordinated contraction and relaxation of muscles, with signals produced at later timesteps derived from the previous timesteps with relatively high autocorrelation [43]. On the other hand, verbal language, being a predominantly semantic modality, exhibits degrees of surprisal and arbitrariness that enhance the cognitive capacity of language processing [44–46]. Therefore, the expected developmental and information processing trajectories will likely exhibit distinguishable patterns across the two different tasks when the DNN system processes them respectively. As we discuss in detail in later sections, this method has a few benefits. Through tracking the learning trajectory of a neural network, researchers can explicitly identify the appropriateness of a model for a given task as well as examine the properties of underlying input signals. This approach could also serve as a standalone visualization to map the accumulation of the underlying signals processed, which can facilitate research on deep learning modeling and signal processing across various modalities. Finally, mapping the learning curve of DNNs has the potential to assist future computational cognitive and neuroscience research and address whether the learning experiences of models also correspond to (or fail to correspond to) the temporal processing in human cognition in addition to the emphasis on static representations in the current literature. In the following sections, we will provide a step-by-step method for visualizing the learning curves of neural networks, illustrate how to holistically interpret signal processing in them and quantitatively compare these curves across two different datasets. This research proposes a model-interpretability method that can extract the learning curve of sequence-based deep learning networks (e.g., RNNs). Inspired by cognitive science, the method measures the learning trajectory and underlying knowledge extracted by such networks. To illustrate the method, we use two temporal tasks: gesture detection and sentence classification as examples. This study therefore demonstrates that the method could inform a range of deep-learning tasks. Our multi-stage pipeline includes three main steps. First, we trained the RNN-LSTM model to generate a sequence of embeddings for a temporal task. Next, we used KNN classifiers to calculate the performance across all extracted embeddings, which we refer to as learning curves (see (2) in Fig 2). Finally, we defined four metrics to quantify these multi-dimensional embeddings for more systematic comparison and significant tests between tasks, classes, the null hypothesis, and development trajectories. The visualization of this multi-stage procedure can be found in Fig 2. Illustrations of the multi-stage pipeline, from extracting embeddings to constructing learning curves and quantifying the curves using four defined metrics. NB: Gesture figures are adapted from Wikimedia Commons [47]. Illustrations of the multi-stage pipeline, from extracting embeddings to constructing learning curves and quantifying the curves using four defined metrics. NB: Gesture figures are adapted from Wikimedia Commons [47]. https://doi.org/10.1371/journal.pcbi.1012286.g002 This study utilized two datasets: the Identity-free Video Dataset for Micro-Gesture Understanding and Emotion Analysis (iMiGUE) by [48] and the Emotion dataset from Hugging Face by [49] to examine the possibility of mapping learning curves for tasks involving temporality. The two temporal tasks (i.e., gesture detection and sentence classification) are distinct in terms of their modalities, lengths, and the steps required to extract and preprocess the features, thereby enhancing the diversity of data to illustrate the learning curve analysis. We detail the preparation of each dataset separately below. The Emotion dataset [49] consists of 20,000 English Twitter messages with six basic emotions (e.g., anger, fear, joy, love, sadness, and surprise) by adopting Plutchik’s [50] wheel of emotions, Ekman’s [51] six basic emotions, and hashtags in tweets. Tweets were annotated through noisy labels and distant supervision introduced by [52]. To prepare the emotion sentences for recognition by the RNN layer, we first applied the “basic English” tokenizer from torchtext to tokenize each tweet. Then, we used GloVe (Global Vectors for Word Representation), a word vectorization technique that does not rely on local word context statistics (local-context information), to vectorize each token in a sentence. GloVe was preferred over other token vectorization techniques like word2vec [53] due to its design to capture the universal meaning of each token/word, rather than the word’s meaning within a specific sentence or context. We opted for GloVe embeddings with 300-dimensional semantic features because it strikes a balance between capturing sufficient information and maintaining computational efficiency [35]. In theory, the input sequences of RNN are not required to have the same length. In practice, these sequences are padded with zeros or trimmed to the same length to optimize the computation in PyTorch. Accordingly, all tokenized tweets were padded to a consistent length of 66 tokens, which corresponds to the length of the longest tweet sample. The shape of each Emotion tweet follows a 300 dimensions × 66 timesteps (see Fig 3). The iMiGUE is a high-quality dataset that contains 18,499 identity-free, ethnically-diverse, gender-balanced samples of 32 psychologically-meaningful micro-gestures (such as scratching an arm, adjusting the hair, or touching an ear [51]). These gestures were all curated from interview clips with athletes at post-match press conferences. Unlike other gesture and emotion datasets, which are typically drawn from staged performances or movie clips, the iMiGUE provides samples of actual gestures from real-life situations. This poses a more realistic, though more challenging, recognition task for deep learning networks [54, 55]. Due to copyright restrictions, the dataset includes only skeleton keypoints (rather than original interview clips) extracted from OpenPose, a multi-person computer-vision system that can simultaneously extract keypoints of the body, hands, face, and feet [56]1. In total, 25 body, 70 facial, and 21 left hand and 21 right hand keypoints were extracted for each frame using OpenPose. Keypoint data were stored in the following format: [x0,y0,c0,x1,y1,c1… ], in which (x, y) represents the coordinate of each keypoint and c indicates the confidence score of each keypoint-coordinate prediction. The confidence score was excluded in the following processing steps. Although the gesture clips have just 39 frames on average, they have a higher standard deviation at 84 frames with a 75th percentile of 72 frames. We therefore set the padded length of the input sequence to 150 frames to ensure that our input data contained sufficient information for training a classifier. Thus, the input size for each gesture clip was 274 units (137 keypoints × 2) × 150 timesteps (see Fig 3). NB: Gesture figures are adapted from Wikimedia Commons [47]. NB: Gesture figures are adapted from Wikimedia Commons [47]. https://doi.org/10.1371/journal.pcbi.1012286.g003 It is important to note that our aim here is not to approach benchmark performance, but rather to examine successful learning. The learning curve analysis will show how that successful learning emerges, and which stimulus discriminations seem to underlie that emergence. Since even the state-of-the-art neural network can achieve only 55% accuracy on this multi-classification task [48], we further grouped the 32 micro-gestures into six categories (body, head, hand, body-head, head-hand movements and an absence of gestures) to ensure that our RNN-LSTM was indeed “learning” when we attempted to map its learning curve. We selected a model architecture (see Fig 4) that is standardized for sequential stimulus processing in the following way. First, we had a batch normalization layer, a primary RNN-LSTM layer, and then a fully connected layer to connect the final dense embeddings with the output classes (for classification). We offer details below. NB: Gesture figures are adapted from Wikimedia Commons [47]. NB: Gesture figures are adapted from Wikimedia Commons [47]. https://doi.org/10.1371/journal.pcbi.1012286.g004 RNN is capable of modeling sequential data and time-dependent tasks [41]. Its architecture represents an iterative function that takes an input sequence (x) and an internal state (h) from the previous timestep (t - 1) to predict the current timestep (t), then updates the state as follows: (1) As the formula illustrates, each timestep t should theoretically reflect the information from 0 to t – 1. We selected the RNN model for gesture detection because it can process temporal information under the assumption that the body movement in each timestep depends on signals in the previous timesteps. While an RNN could leverage the context between elements by maintaining its internal state while processing the entire sequence, the “Vanilla RNN” layer experienced the vanishing-gradient problem during model training [58]. Therefore, long short-term memory (LSTM), which is represented by the function f in Equation 1, was introduced here as an additional state variable, called the cell state, for controlling specific information that needed to be kept or updated while processing the entire sequence [57]. LSTM effectively reduced the vanishing-gradient problem encountered by RNN [58]. The construction of our RNN-LSTM neural network follows common practice in deep learning. First, to enhance training stability and speed up convergence, we applied batch normalization, specifically the BatchNorm1d layer, to the input data at each timestep, with dimensions batch_size × timestep × input_dimension (1 × 150 × 274 for iMiGUE and 1 × 66 × 300 for Emotion). This means that each timestep is treated independently, and normalization is applied across all data points in the batch for each timestep. Specifically, this technique normalizes the inputs to have a mean of 0 and a variance of 1 within each timestep, standardizing the distribution of inputs across the data points in each batch during training. Then, an RNN-LSTM was applied to the normalized input to convert temporal information to a dense embedding at each timestep. An RNN-LSTM with an equal hidden dimension was selected to simplify the tracking of embeddings in the later stage. Finally, a fully connected layer, without any activation, was applied to the embeddings in the last timestep t to predict output labels. To ensure the generalizability of our learning curve mapping approach, we performed 15 rounds of simulation for both gesture and emotion detection tasks, each of which included 15 (6C2) pairwise binary classifications. We conducted 25 repetitions (reps) for each set of simulations (sims) to mitigate the idiosyncrasies of specific processing and developmental patterns we are extracting under each pairwise condition. In total, we collected 25 × 15 × 2 ( reps × sims × tasks) = 750 runs of simulation data. Having an adequate number of simulation runs also enables us to observe clustering tendencies in the convergence and divergence of trajectory patterns across various tasks and different classes. In each simulation, 20% of the shuffled samples were used as the test data, while the remaining data were further split into 80% training data and 20% validation data. While validation data were used for reporting each epoch’s model performance, test data were used for reporting the final model performance on unseen data. The number of epochs indicates the number of times an entire dataset has been passed forward and backward through the neural network. We set the number of epochs to 50 for both tasks, given that most of the variation in learning tends to unfold during the early stages of development. In S3 Text and S4 Text we also illustrate how using 20 and 100 epochs leads to similar observed patterns. Because deep learning algorithms are very sensitive to unbalanced datasets, we applied data augmentation to the training and validation datasets. Specifically, the minority class was up-sampled to match the number of data points in the majority class, which ensured a balanced dataset [59]. Default initiation from PyTorch was used to standardize the model specification across simulations. All simulation runs were trained using the Adam optimizer with a learning rate of 0.001, and the loss function used was the cross-entropy loss for all pairwise classifications. All the other learning rate hyperparameters were kept at their default values. Since deep learning is, computationally speaking, very expensive to train, the model was run on Nvidia RTX 4090 to expedite the processing. We set the batch size to 64 for gesture recognition and 256 for emotion classification to achieve an optimal balance between training speed and performance.2 To map a learning curve for each simulation, we first extracted embeddings of all LSTM timesteps from the hidden layer. We then applied multiple interpretable machine-learning models between these embeddings and the corresponding output labels to understand how the model’s confidence is updated during the LSTM timesteps. We extracted the LSTM array of all timesteps for all batches across all epochs on the test data to ensure that we were extracting embeddings from all developmental stages and the final model was adequately trained on the targeted task. Since we specified the same dimension of the input data for the LSTM hidden layer, for the gesture detection task, we obtained 150 LSTM arrays from 150 timesteps, each array having the size of 1 × hidden_dimension (1 × 274). Similarly, we obtained 66 LSTM arrays in sentence classification, each with a size of 1 × 300. We then stored the corresponding output labels of those arrays as an output array. Those LSTM arrays share one output array since they are embeddings at different timesteps of the same data points. For binary classification, the labels were encoded as 0 and 1. Once the model had been trained and the embeddings extracted, we stored and processed those embeddings in RAM, which has a greater storing capacity than GPU. To approximate the processing curve of a model’s confidence across the LSTM timesteps, this study used a popular machine learning algorithm, k-nearest neighbors (KNN) for identifying embedding “separatables.” This algorithm classifies an object by using a majority vote of its neighbor data points [60, 61]. Because embeddings are seen as a high-dimensional physical (i.e., location-wise) projection of input data (as opposed to a multivariate representation [1]), distance-based models, such as KNN and support vector machine (SVM), are popular choices for examining DNN embeddings in previous studies. Specifically, we applied KNN to the data point (, y), where is the LSTM embedding at t timestep and y is the corresponding output label and calculated the KNN accuracy across all timesteps (information processing trajectory) and all epochs (developmental trajectory), and thus captured the learning curve of the model. These learning curves examine how the LSTM’s embeddings classify the stimulus as it is incrementally presented to the network. Visually, each curve is plotted as the proportion of correct classifications across the item. These constructed learning curves can be analyzed in various ways, either through qualitative interpretation of the visualizations or by applying statistical analysis for quantification and hypothesis testing. In this study, we utilized the four descriptives statistics introduced earlier to assess the multidimensional KKN classification performance of the RNN-LSTM (i.e, how the discriminability of the model is evolving) across its training on two datasets, reconstructing and analyzing the unfolding learning dynamics of these distinct learning tasks. Specifically, for each such curve, we extracted four simple descriptive metrics: start, end - start, max, tmax. In Fig 5, we illustrate these descriptive statistics from an example trajectory of a single classification run for each dataset. From epoch 1 to epoch 50, the LSTM’s embeddings are able to classify successfully over the test item, and we can characterize this success as a change to its performance using four metrics described in the main body of the text (start, end - start, max, tmax). Top: Example item from the sentence task. Bottom: Example item from the gesture task. From epoch 1 to epoch 50, the LSTM’s embeddings are able to classify successfully over the test item, and we can characterize this success as a change to its performance using four metrics described in the main body of the text (start, end - start, max, tmax). Top: Example item from the sentence task. Bottom: Example item from the gesture task. https://doi.org/10.1371/journal.pcbi.1012286.g005 First, we defined the maximum performance (max) and the percentage of time at which that maximum was achieved in the presented item (tmax) over the whole timesteps. We included tmax alongside max because we discovered that max alone is not sufficient to determine performance, especially earlier in training. This is because the network may exhibit unstable performance, dropping across subsequent time slices, which may be indicative of a non-monotonic learning trend seen in related domains [62]. This also suggests the network has learned something about the initial segments of an item, but the later segments wash out its performance as it has not yet encoded these later features. To capture this trend, we use tmax to assess when that maximum was achieved within each information processing trajectory – specifically, the percentage of the time slice at which the observed maximum occurred. In Fig 5 above, we show an illustration of this in the top left. The network achieved a performance of 0.82 on this particular item, but failed to sustain this performance as it dropped to near 0.50 as the sequence unfolds (at epoch 1). In the top right, the performance improves approximately monotonically across time slices (by epoch 50), and tmax is achieved near the final bin of the training item. Additionally, we assess the performance at the start and end of the presented stimulus (start, end). Performance at the start may indicate the relative gains that can be expected from a stimulus item. As shown in the bottom panels of Fig 5, the gesture input already has performance above chance ( ~ 0.80) after the very first segment of the stimulus item. This suggests the model rapidly exploits spatial information in gesture (i.e., the location of keypoint coordinates can already vaguely differentiate gesture classes before sequential movement information is fed into the model). A model that achieves near-perfect performance at start and sustains it does not need to be exposed to the subsequent stimulus. The last measure we use is the subtraction of end - start performance. A high value on this measure suggests the network gets substantial information gains across a test item. For example, even at epoch 50 in the bottom right, the end - start of the gesture item is substantially lower than the simulation trained on the sentence task. These measures can be plotted across epochs. Each learning curve now indicates how an item is being processed across an LSTM’s overall training. The measures are relevant to two timescales in the network’s behavior we conceptualized earlier: developmental (or learning) and information-processing timescales. For example, across epochs, movement along the start measure represents the initial performance at the item’s first time slice at the beginning of each information processing session. The end - start measure within an epoch can describe the relative information gains during each processing session from the item’s full presentation. The max represents the best performance achievable in a session, indicating the network’s current information processing capacity. Finally, if tmax is low, it suggests that the network’s maximum performance of the current session is hindered by subsequent timesteps, implying that additional training may be necessary. We took the output from the KNN classification in Python, described above, and designed a sequence of R scripts to measure, visualize and quantify the trends in these four measures (start, end - start, max, tmax). R’s suite of visualization tools provided a convenient arena within which to view trends across epochs, and in these analysis scripts we also built linear models to statistically test the significance of these trends. All of the scripts in our methods are available at GitHub here: https://github.com/JoyceJiang73/Learning-Curves/. The R scripts only require input of simulation CSV data that contain as fields: binary classification labels, time slice, epoch, and performance measure (e.g., KNN classification performance). We have reported the trained RNN model performance for the average of all pairwise simulations of sentence and gesture classification (epochs = 50) in S1 Text, including metrics such as validation accuracy, validation loss, test accuracy, test loss, recall, precision, and F1. These results confirm that almost all pairwise classifications have been sufficiently trained, indicating that the learning curves are capturing the learned experiences. For each of the four metrics, we conducted a series of significance tests using regression models to assess distinctness across data sources, reshuffled null data, classes, and epochs. Specifically: (1) A linear model was used to assess whether the learning curves of the two datasets differed significantly, (2) A linear model evaluated whether the learning curve for each training classification task significantly deviated from a superimposed (randomly reshuffled) null hypothesis across the four metrics, (3) Linear models were used to assess whether the learning curves for each dataset were distinct by class and across epochs, and (4) An ANOVA test was performed to determine whether class differences contributed more to developmental progression than epoch alone. All the significance tests (Adjusted R-squared) are reported in Table 2, and the corresponding visualizations for each metric are provided in their respective sections. https://doi.org/10.1371/journal.pone.0313772.t002 In Fig 6, we show the LSTM performance at the start of an item across epochs of training. In general, the sentence dataset shows low, near-chance performance, while gesture classifications are already well above chance performance. This chance-level performance for sentence items stays consistent across the entire training period, though the gestural dataset shows some improvement. For the gesture dataset, this suggests that the network has some information about a classification before much of a training item is even shown to the network. It would indicate that gestural data has spatial information in the point coordinates of the body and is exploited by the network at the very first time bin. With language, it takes time for word embedding vectors to be integrated in the network. To confirm these trends, we tested a linear model that predicted start performance by training data, showing that dataset accounted for about 98.14% (p < .0001) of the variance seen in Fig 6. The classification of the gesture data accounts for 95.51% (p < .0001) of variance internally to that dataset, whereas for sentence classifications accounts only for 10.28% (lower but also significant, p < .0001). The learning curves reveal that the first word of a language task has low diagnostic accuracy for a classification, but the spatial variance over gestural classifications is much more informative. The start performance for sentence null is the only metric that shows no significant difference between learning from ordered sentence classes and reshuffled data (among all significance tests; = 0.0001, p < .0879). This is expected, as all sentence binary classifications begin with a consistent initial performance of 0.50 (random guessing for binary classification), which remains unchanged even with randomly reshuffled sentence data. In contrast, the start performance significantly differs for ordered gesture classes versus reshuffled data because the various gesture pairs contain different levels of spatial information ( =0.0824, p < .0001), affecting their initial performance. Start metric over epochs, indicating that in the sentence task, the classification at the first time slice remains stable near change, whereas the LSTM’s embeddings for the gesture task are distinct across stimulus types, and also show slightly more improvement over training (while already being well above change relative to the sentence task). Start metric over epochs, indicating that in the sentence task, the classification at the first time slice remains stable near change, whereas the LSTM’s embeddings for the gesture task are distinct across stimulus types, and also show slightly more improvement over training (while already being well above change relative to the sentence task). https://doi.org/10.1371/journal.pcbi.1012286.g006 Curiously, if one only investigated maximum performance across training, these classification tasks could be regarded as relatively similar in their behavior. As shown in Fig 7, both sentence and gesture datasets yield a classification performance that is high, between 0.75 and 1.0 depending on the classification. In the gesture dataset, there are more “difficult” classifications, shown by outliers in max performance across training. This can be helpful in diagnosing representational challenges in the network’s training, marking what pairs of training items may be more difficult to distinguish than others. Again, as with the start measure, the max performance shows greater variance associated with classifications in the gesture case ( =0.8250, p < .0001) than the sentence case ( =0.2736, p < .0001). The difference between these two datasets is not as pronounced as in the start measure, as a linear model shows that only 2.11% (p < .0001) of the variance is associated with the dataset in a linear model predicting max performance observed within a trial. Despite the small value, it is nevertheless significant and driven by the relatively higher performance in the sentence task. The max performance also differs for both sentence ( =0.0042, p < .0001) and gesture tasks ( =0.0722, p < .0001) when the network learns from ordered datasets compared to randomly shuffled ones. The sentence conditions appear to pick up more spurious signals, ultimately approaching 1.00 as the epochs increase, while the gesture condition remains consistently unlearned across epochs. The learning pattern for tmax across epochs (see next section) provides additional insights into these trends. Both sentence and gesture tasks show an increase in max performance over training epochs. On average this asymptotes near perfect performance but can vary depending on binary classification. Both sentence and gesture tasks show an increase in max performance over training epochs. On average this asymptotes near perfect performance but can vary depending on binary classification. https://doi.org/10.1371/journal.pcbi.1012286.g007 We would expect that completed training in the LSTM should show that maximum performance should appear near the end of an item, as this would suggest that the network has extracted useful information in the whole presentation. Indeed, this is indicated by lower time-at-max values earlier in the training. During the first few epochs, the maximum value occurs proportionally earlier in a training item, similar to the example shown above in Fig 5. However in both sentence and gesture datasets, networks slowly extract features across the stimulus items for classification, as shown in Fig 8. The time gradually rises for all classifications, though it can also vary widely and tends to be more irregular in gesture. In a linear model predicting tmax from data source, sentence and gesture are only slightly different, with 0.34% (p < .0001) of the variance accounted for. Classifications in sentence and gesture both relate significantly in a linear model predicting time at max, with sentence classifications accounting for 9.51% (p < .0001) of the variance and gesture classifications 24.95% (p < .0001). Interpreting tmax alongside max, we confirm that the learning experience of a language task is more reliant on spurious signals for reshuffled data compared to ordered data, as max continuously increases to reach 1.00 while tmax remains sporadic. The gesture condition remains consistently unlearned across epochs for reshuffled data, as the max performance can occur at any point along the processing trajectory (any timestep). Both tmax values indicate that no meaningful delayed signals were developed when the network was trained on superimposed reshuffled data. The time at max differs significantly for reshuffled versus ordered data in both sentence ( =0.0120, p < .0001) and gesture ( =0.0058, p < .0001) conditions. Both sentence and gesture tasks show an increase in the proportion of time at which maximum performance is observed. Indicative of information gain, this rises in both – suggesting the LSTM comes to better integrate the full test items for its performance. However this is much more orderly in the sentence task than the gesture task. Both sentence and gesture tasks show an increase in the proportion of time at which maximum performance is observed. Indicative of information gain, this rises in both – suggesting the LSTM comes to better integrate the full test items for its performance. However this is much more orderly in the sentence task than the gesture task. https://doi.org/10.1371/journal.pcbi.1012286.g008 Finally, we wish to get a sense of how the LSTM models are extracting information within an information processing session. One way to do this is to discern how much higher its performance is at the end of a session compared to its performance initially. This is where we see the greatest difference between the datasets. As shown as Fig 9, sentence shows substantial information gain across stimulus items. By the end of training, the start and end of trials involves considerable difference, and the positive value of this difference shows that the network improves its performance significantly across presentations. With gesture, the situation is quite distinct. In most classifications, there is indeed a rise. But because gesture is informative even at the first time bin of an item, there isn’t much information gain remaining. Indeed even the highest instances of this value hover near 0.10. Therefore, while performance on gesture is higher at first, the network may find the task more difficult in integrating that spatial information over time to improve performance. As in the start measure, the difference between datasets is quite large. When data source is used to predict the end - start measure, 39.23% (p < .0001) of the observed variance in this measure can be associated with data source, with sentence associated with much higher information gain than gesture results. When looking at each data source separately, only 13.01% (p < .0001) of the measure is associated with the classification pairs in sentence. In the gesture dataset, this association is 38.08% (p < .0001), showing much higher contribution of the class differences in gesture. Both sentence’s (R2 =0.0027, p < .0001) and gesture’s (R2 =0.0340, p < .0001) end-start values differ significantly between ordered and reshuffled data. Consistent with max performance gradually reaching 1.00, the information gain of the sentence network increases incrementally from 0.00 to 0.50 when trained on reshuffled data. Combined with insights from tmax, we suggest that although learning may appear to occur even with reshuffled data, it is likely driven by spurious signals picked up during the processing session. Additionally, the end-start performance for gesture gradually improved from highly negative to slightly negative but was not able to reach 0.00 during the early epochs (e.g. epochs smaller than 50), which explains the flat max performance for the reshuffled gesture condition. The information gain from start to end of stimulus item shows a stark difference between datasets. The sentence task shows that the LSTM’s performance rises as it integrates data from start to end. With gestures, the story is more complicated, showing less improvement overall, and in one classification task, it is almost entirely unlearned. The information gain from start to end of stimulus item shows a stark difference between datasets. The sentence task shows that the LSTM’s performance rises as it integrates data from start to end. With gestures, the story is more complicated, showing less improvement overall, and in one classification task, it is almost entirely unlearned. https://doi.org/10.1371/journal.pcbi.1012286.g009 Recognizing the importance and challenges involved in systematically unpacking the internal representations of DNNs, this study introduced a multi-dimensional quantification and visualization approach, “learning curves,” which can capture two temporal dimensions of a model learning experience. First, it captures the “information processing trajectory,” how the network is doing as it processes test items. Second, it captures the “developmental trajectory,” describing how this processing is changing over training epochs. The former represents the influence of incoming signals on an agent’s decision-making, which is operationalized by the timestep within a single epoch of an RNN. The latter conceptualizes the gradual improvement in an agent’s decision-making abilities throughout its lifespan, operationalized by the iteration of epochs. The learning curve approach we illustrate in our two datasets shows that we can quantify and qualitatively investigate both of these dynamics within the same analysis, utilizing four descriptive metrics: start, end - start, max, tmax. Based on a series of significant tests (see Table 2), we first demonstrated that there is a data source difference between sentence and gesture classification in the overall learning patterns across all four metrics (all p’s < .0001). Additionally, the learning experiences of both datasets’ ordered classifications are distinct from their respective reshuffled null baselines, except for the start of the sentence classification. This is because both ordered (aggregated across all classes) and unordered binary sentence classifications will have an initial classification performance of 0.50 due to random guessing. This is corroborated by the significant test of the start metric for sentence class and gesture class: sentence pairwise classifications show very minor differences, with R-squared 0.10 (p < .0001), whereas gesture classes show much greater divergence, with R-squared above 0.90 (p < .0001), due to the initial spatial information contained in gesture coordinates, even at timestep 0, as mentioned in the previous results section. The significant difference between the reshuffled null baseline for ordered vs. unordered gesture classification is also due to different gesture class pairs having different prior spatial information, deviating from the 0.50 random guess scenario observed in sentence classification. The modality distinction is further evident in the end - start metric (where sentence classification shows a much larger information gain than gesture classification, with gesture’s information gain is more gradual). Finally, the ANOVA test capturing the R-squared difference between epoch × class versus epoch-only indicates significant distinctions between pairwise classes and not being distinguished by class (all p’s < .0001). This reveals that within the same multiclassification task, certain classes can be more difficult or easier to separate. Taken together, based on the analysis of these four measures derived from the learning curves across two distinct datasets, we highlight three insights gained from mapping these curves: non-monotonicity, pairwise comparisons, and domain distinctions. First, we observed a non-monotonic trend in the learning experiences of DNNs. In other words, learning does not always show an increasing monotonic improvement because networks sometimes reveal temporary decrements in their performance across training. This is a characteristic recognized in previous literature as a key advantage for their advancement in addressing the most challenging tasks. For example, in applying neural network models to language, the presence of non-monotonic learning was taken as evidence that networks “reorganize” their knowledge as they learn. This may mean networks are acquiring more efficient representations of a problem space and the drop in performance is indicative of a transition into that more efficient representation. Classically, in the case of models learning language, they show a decrement in performance when they “discover a rule” in grammars [68] Our results are a testament to the observation that both representational consolidation and “catastrophic forgetting” remain as important issues in DNN learning [69]. Specifically in our results, we found that RNNs exhibit different preferences for early versus late cues when addressing various sequential tasks. For instance, in the sentence task, we noted a more pronounced performance improvement occurring in the later stages of information processing (i.e., model development). In contrast, gesture learning tends to show quicker progress, with more variability across epochs and repetitions of simulations, suggesting that DNNs tend to rely on shortcuts, such as naive cues related to keypoint coordinates, for gesture classification, rather than focusing on high-level movement sequences. This shortcut-based “learning” also is evident in the higher initial performance (start) for gesture classification ( > 0.75). On the other hand, sentence classification begins at around 0.50 (the at-chance probability) and exhibits greater performance gains in later epochs, indicating that the models classify based on high-level semantic sequences. Additionally, although multiclassification tends to exhibit collective model performance, our between-class pairwise comparisons reveal the presence of outliers within multiclassification. For example, “joy/fear”, “surprise/fear”, and “sadness/fear” demonstrate higher information gains across epochs compared to other emotion pairs, suggesting that these classes are further apart from each other. The “body/head” classification appears to experience learning challenges, possibly because these two movements have difficulty being completely separated, as the head’s movement in naturalistic data may inevitably coincide with that of the body due to joint coordination. The combination of learning curve mapping and instance measures thus serves as an effective approach for “auditing” representations in multiclassification problems. The proposed pipeline improves model explainability beyond holistic evaluation of classification performance and ad-hoc attention visualization by unpacking pairwise class learning patterns to reveal any pairs that are unsuccessful in being discriminated or involve delayed knowledge gain. This granular examination allows modelers to better investigate a model’s appropriateness for the underlying task, as well as the properties of the processed input signals, reaffirming the value of our learning curve conceptualization. With these findings, it is tempting to infer that there are domain distinctions among different modality classification tasks. Gesture learning, for instance, may rely on autocorrelated signals (as body movements result from the coordinated contraction and relaxation of muscles), potentially emphasizing spatial semantics as early cues 3, while language learning relies on higher degrees of surprisal, irregularities and arbitrariness, as suggested by previous literature (see [43–46]). These domain distinctions are valuable to cognitive science in understanding how humans process and distinguish between various modes of communication, shedding light on the neural mechanisms underlying the flexibility and adaptability of the human mind when processing different forms of information and communication modalities. Though it is intuitive and tempting to explain these domain distinctions here, we cannot yet assert that these trends would hold for all sentence or gesture (keypoint) classification tasks, only the ones we investigate here. However the learning curve results would seem to align with an intuition of how linguistic symbols would be sequentially integrated into a neural network in contrast to the highly auto-correlated spatial information contained in gesture performance. Still, we cannot infer a broad generalization about “language vs. gesture” and only leave it as a potential path for future investigation. Indeed, this may be an additional benefit of a method like the one we present here. Learning curves could finely ascertain these domain distinctions, and expanding the set of data to test may permit generalization in future work. This may have theoretical implications itself. The learning curve analysis may provide information about the distinct sources of information from varied modalities. When neural models (or human brains, presumably) integrate distinctive sources of information, they may strengthen understanding of complex multimodal data by leveraging their unique information-processing and developmental benefits. This study provides conceptual and operational illustrations of applying learning curve methods to RNNs, though both the illustration and the proposed method come with certain considerations. First, we selected two distinct datasets to demonstrate how learning curves offer insights into different data and modality learning experiences. While illustrating significant domain distinctions, our study is limited in making broad generalizations between “language vs. behavioral.” Future cognitive and behavioral research interested in such conceptual generalization can apply our proposed method to various distinct language tasks (e.g., language translation) and behavioral datasets (e.g., facial expression detection) to achieve broader generalizations across different modalities. Additionally, as highlighted at the beginning, our method could have implications for multimodal DNN modeling, given that our simulations utilize datasets with distinct modalities. For the current scope, we focus on how our proposed method facilitates the understanding of distinguishable modalities as an initial step, while future work can explore adopting this method for multimodal DNNs and datasets (e.g., audiovisual emotion classification) to assess how multimodal learning differs from unimodal learning. From a methodological standpoint, as detailed in the model explainability technique comparison (Table 1), our proposed method is currently limited to temporal tasks and DNN architectures, specifically RNNs. However, it has the potential to be relatively easily adapted to various tasks, modalities, multiclassification, and different performance measures, beyond the binary classification used in this study. We have provided illustrations of the multiclassification learning curve based on sentence classification in S2 Text. Additionally, because our method generates measures at every timestep and epoch, it may be more computationally expensive compared to other explainability methods. However, this arises from the tradeoff between obtaining a more granular measure across all training steps versus only obtaining snapshot visualizations of a single neural network layer or attribution scores for all input features. As documented in the DNN explainability comparison (Table 1), each technique has its strengths and considerations, and researchers can exercise discretion in choosing the appropriate method. The learning curve method excels in providing a more systematic understanding of the model’s learning process across models and modalities. There is a long tradition in cognitive science and computational neuroscience of examining the internal representations of models [9–12]. One reason for this is to determine if a model’s features or processes reflect processes of the human mind. Such models can be informative for inferring properties of human mental processing, and so have direct theoretical implications. For example, Elman [13] and others [14] showed that RNNs can learn patterns sufficiently complex to resemble human grammar. By examining the internal activations of these recurrent networks, they showed that these systems are driven by graded, statistical features. Words are not discrete “symbols” but scalar vectors conditioned by linguistic context in time [15]. This was taken to challenge theories that see language as a purely abstract and symbolic recursive process [16]. Inspired by prior work, the learning curve method we proposed uses temporal mapping (information processing and developmental trajectories) to help modelers more comprehensively understand the underlying learning and decision-making processes of a complex model architecture without delving into the intricacies of interpreting its internal representations directly [23, 38]. This kind of systematic and quantitative approach has recently gained popularity in both computational cognitive science and deep learning communities [5, 38, 63–65], as it facilitates multidimensional comparisons across models and modalities, which were previously seen as challenging for DNN-like models. The current study illustrates multiple techniques for analyzing model learning experiences and highlights three insights across different communication modalities based on these analyses. Future studies can utilize this learning curve mapping approach to enhance model interpretability studies by evaluating a model’s appropriateness for the task at hand, examining the properties of the underlying input signals, and assessing the model’s alignment (or lack thereof) with human learning experiences, which is also a critical consideration for computational cognitive science and neuroscience research. Sentence and gesture RNN-LSTM model performance. (PDF) Table A. Average Performance metrics for sentence classification across all simulation runs (epochs = 50). (PDF) Table B. Average Performance metrics for gesture classification across all simulation runs (epochs = 50). (PDF) Table C. Average performance metrics for 10 examples of gesture classification across all simulation runs (epochs = 100). https://doi.org/10.1371/journal.pcbi.1012286.s001 (PDF) Illustrations of multiclassification results for sentence classification (epochs = 20). https://doi.org/10.1371/journal.pcbi.1012286.s002 (PDF) Illustrations of sentence and gesture simulations across 20 epochs. https://doi.org/10.1371/journal.pcbi.1012286.s003 (PDF) Illustrations of gesture simulations across 100 epochs. https://doi.org/10.1371/journal.pcbi.1012286.s004 (PDF) We thank the organizers, panelists, and audience for allowing us to present the initial version of our work at the 53rd Annual Meeting of the Society for Computation in Psychology (SCiP) and for sponsoring the registration fee. We are also grateful to the reviewers from the 45th Annual Meeting of the Cognitive Science Society for their comprehensive review and invaluable feedback. Additionally, we thank Hongjing Lu, Jungseock Joo, and Elisa Kreiss for their insightful feedback on the theoretical framework and methodologies.
--------------------------------------------------

Title: McDonald's earnings miss due to E. Coli outbreak and a decline in consumer spending
URL: https://qz.com/mcdonalds-q4-2024-e-coli-inflation-shamrock-shake-1851758426
Time Published: 2025-02-10T13:50:00Z
Full Content:
McDonald’s customers aren’t “lovin’ it.” The fast-food giant missed Wall Street’s expectations for the fourth quarter, largely due to the lingering effects of an E. coli outbreak and ongoing inflation pressures. The October incident sickened hundreds of customers and caused a massive drop in foot traffic, leading to a decline in same-store sales. In early trading, McDonald’s shares were slightly up. “Our fourth-quarter performance reflects the impact of the food incident,” CEO Chris Kempczinski said during the company’s Feb. 10 earnings call. The Illinois-based company reported revenue of $6.39 billion, missing Wall Street’s forecast of $6.44 billion in revenue, while earnings per share came in at $2.80 — meeting expectations. Additionally, U.S. same-store sales fell 1.4% year-over-year, surpassing the Street’s modest projection of 0.6%. The impact of the E. Coli outbreak, which began in late Oct. 2024, has started to reveal the toll it took on McDonald’s performance. Citi analyst Jon Tower said in a research note that he believes the outbreak marked the company’s “low point” and sees the coming quarter as “setting the table” for a recovery. However, inflation remains a challenge and is expected to persist in the near future. Jan. 2025 was another tough month for McDonald’s, despite optimism surrounding its McValue platform. BTIG analyst Peter Saleh noted that severe weather conditions across the U.S. complicated efforts to determine whether the value menu had increased foot traffic. In a note covering the restaurant industry, Bank of America set a 12-month price target of $312 for McDonald’s, reflecting steady growth expectations despite the challenges of its franchised business model. In Nov. 2024, the company announced a $100 million investment to support marketing efforts and franchisee operations. About 95% of U.S. locations are run by franchisees. BoA highlighted both upside and downside risks: Stronger-than-expected sales and cost savings could boost McDonald’s stock, while slow sales and margin pressure from inflation could weigh on its performance. According to foot-traffic analytics firm Placer.ai, inflation likely contributed to dips in McDonald’s visits, while budget-conscious consumers flocked to limited-time offers and special releases. The company has capitalized on this trend, driving significant foot traffic with the June launch of its $5 Meal Deal. Additionally, the Chicken Big Mac, introduced in Oct. 2024, drove a 7.2% increase in visits during its first week. The promotions help McDonald’s maintain its relevance. The company recently launched a “McValue” menu, expanding on its $5 meal deal. It’s also bringing back the Shamrock Shake. Other nostalgic promotions, such as collector’s cups relaunched in Aug. 2024, have proven extremely successful. The cups were so popular that customers resold them on third-party websites such as eBay for as much as $100 each. Elsewhere, McDonald’s also made headlines in Oct. 2024 when President Donald Trump staged a cook shift at a Pennsylvania location. The company quickly clarified it had no involvement in the event, noting that it was not “red or blue, but golden.” In Jan. 2025, McDonald’s announced it would scale back its Diversity, Equity, and Inclusion (DEI) efforts, joining the likes of Google, Meta, and Walmart — but not Costco. Our free, fast, and fun briefing on the global economy, delivered every weekday morning.
--------------------------------------------------

Title: Meta set to notify workers impacted by its performance-based cuts
URL: https://www.businessinsider.com/meta-notifying-workers-impacted-performance-based-cuts-2025-2
Time Published: 2025-02-10T13:24:15Z
Full Content:
Welcome back! President Donald Trump said that he plans to impose 25% tariffs today on imports of steel and aluminum. Gold hit a record high following the news, while some related currencies softened against the dollar. In today's big story, Meta employees impacted by its performance-based cuts are getting notified today. What's on deck Markets: Palantir and retail investors are a match made in heaven. Tech: Consulting executives told us the prompts they use to get the most out of AI. Business: The billionaire heirs desperate to give away their money. But first, a "reduction of force." If this was forwarded to you, sign up here. The day after the Super Bowl is always tough at work, but it's about to be particularly hard for some Meta employees. Today Meta is starting to notify workers impacted by its performance-based job cuts. Less than a month ago, CEO Mark Zuckerberg told employees the tech giant was firing 5% of its entire workforce to speed up how quickly it got rid of low performers. (He wasn't that blunt, but the point stands.) Business Insider's Jyoti Mann got the internal memo detailing how the cuts will work. When it's all said and done, some 4,000 Meta workers are expected to be shown the door. Jyoti, Pranav Dixit, and Hugh Langley spoke to some employees about the vibes inside Meta leading up to the cuts — "feels like living in a George Orwell novel," one worker said — along with how they play into Meta's broader reorganization efforts. The layoffs come as Meta has made some other seismic shifts. From overhauling its content moderation system and nixing DEI programs to doubling its investment in AI, Meta hasn't wasted time in what Zuckerberg has said will be an "intense year." Coincidentally, Meta employees are having their own issues with content moderation. Some workers have questioned Meta's removal of posts and comments from its internal forum. Meta's Community Engagement Expectations restrict mocking protected categories like race, but some employees accused the tech giant of censoring valid discussions. Internal drama isn't the only problem Meta is facing. The fight for AI dominance is heating up and getting more expensive. Meta expects its capital expenditure to be as high as $65 billion this year, about double what it spent in 2022. Meta's push for open-source AI models also contradicts what some of its competitors are doing, which could either boost it or backfire. A jump in delinquencies in a corner of the mortgage market suggests first-time homebuyers may be struggling. 1. Palantir + retail investors = big returns. CEO Alex Karp doesn't fit the Wall Street mold, so it's not surprising his approach to investors doesn't either. Instead of the IPO route, Palantir went for a direct listing, and — during earnings calls — Karp often prioritizes retail investor questions. These mom-and-pop investors are reaping the rewards, with the stock up more than 47% this year. 2. The White House's opening pro-crypto move. AI and crypto czar David Sacks announced on Tuesday that the Trump administration has prioritized a federal stablecoins bill. Stablecoins, which are cryptos pegged to fiat currencies, could help ensure US dollar dominance, a crypto expert told BI. That's because about 98% of the stablecoins are tied to the greenback. 3. The investor gold rush amid Trump-era policies. Trump's administration has spurred trade and geopolitical uncertainty, so investors are flocking to the yellow metal as a safe haven. Gold hit a record high today after Trump on Sunday threatened steel and aluminum tariffs. Ahead of Trump's comments, IGN said it had expected gold to reach $3,000 an ounce this quarter. 1. Rise of the (VC-backed) robots. A few startups are building autonomous robots, with some US companies securing up to $150 million in funding. The robots use AI and spatial intelligence to do everything from making coffee to helping with surgery. BI found 12 businesses to watch. 2. X has a new adtech partner. Elon Musk's company is working with Magnite, a supply-side platform that helps publishers manage and sell advertising. X has been working to reverse the losses it took when many advertisers left the platform in 2022, though Musk's beef with them is still going strong. 3. How McKinsey and the Big Four use AI. BI asked execs at five big consulting firms — McKinsey, PwC, KPMG, EY, and Deloitte — their best tips for using AI in everyday work. They told us their favorite prompts and advice for getting the most out of the tech. 1. These wealthy heirs want to divest — from themselves. One conference is helping rich — and mostly young — people grapple with their identities and redistribute their funds to causes that matter. The up-and-coming millionaires and billionaires are also finding community (albeit a small one) along the way. 2. Are we making eggs-cuses for all these price hikes? Most people know that egg prices have skyrocketed lately, in part because bird flu has decimated the egg-laying hen population. But when customers are primed to expect price hikes, some retailers can hike prices beyond what's really necessary. Are retailers surcharging to stay afloat, or is there a bit of fowl play at work? 3. RTO FAQs. If you're one of the tens of thousands of Americans recently affected by an order to return to the office, fear not. BI has assembled a guide to help you prepare, including commute planning and deciding what to wear. Every song on Kendrick Lamar's Super Bowl halftime show setlist. Jump to
--------------------------------------------------

Title: CNBC Daily Open: Enthusiasm over Trump and AI appears to be waning
URL: https://www.cnbc.com/2025/02/10/cnbc-daily-open-investors-appear-less-enthusiastic-over-trump-and-ai.html
Time Published: 2025-02-10T07:30:01Z
Description: Though artificial intelligence and Trump are still the main characters of the stock market, they are steering it in a direction different from December.
--------------------------------------------------

Title: Facebook at 21: a lesson in patience and profits
URL: https://www.irishtimes.com/business/2025/02/10/facebook-at-21-a-lesson-in-patience-and-profits/
Time Published: 2025-02-10T06:00:00Z
Full Content:
Facebook celebrated its 21st birthday last week and investors are in party mood. Already up 18 per cent in 2025, Meta gained 13 days in a row – the longest winning streak in its history. Stunning numbers are nothing new for Facebook. Begun in Mark Zuckerberg’s Harvard dorm in 2004, the company went public in 2012 and, after enduring a few tricky initial months, subsequently went skyward. A $10,000 (€9,633) investment in Meta at its May 2012 price would be worth $184,200 today, notes Bespoke Investment. Still, buying the stock is one thing; holding on is another. Bespoke notes that between September 2021 and November 2022, Meta lost three-quarters of its value. Chastened, Zuckerberg reined in the spending on the so-called metaverse. Soon after, ChatGPT came along, the AI boom began, and Meta soared nearly 800 per cent. Still, it’s human nature to cut your winners and bank profits, so holding on during a bull market can be as difficult as during a bear market. As Meta’s history proves, the hardest part of investing isn’t picking winners; it’s having the stomach to hold them. © 2025 The Irish Times DAC
--------------------------------------------------

Title: DeepSeek, Alibaba & The AI Race
URL: https://www.luxuo.com/business/deepseek-alibaba-the-ai-race.html
Time Published: 2025-02-10T04:01:21Z
Full Content:
With China’s DeepSeek challenging Silicon Valley’s dominance and AI’s influence extending from policy-making to consumer behaviour, the AI race is no longer about innovation — it is about who wields the power. As AI-powered personal assistants become deeply embedded in the daily lives of large corporations, businesses and individuals alike, the balance between convenience and control is shifting. From China’s DeepSeek to the West’s AI race, artificial intelligence is no longer just a tool — it is an omnipresent force shaping global surveillance, economic power structures, and the future of digital autonomy. As AI systems become more advanced and widely available — often for free or at a fraction of the cost — what is their real value and if AI developers are the ones truly benefitting, what does that mean for the rest of us? Read More: The Influence of AI in the Interior Design Industry Chinese AI model DeepSeek, which recently took Silicon Valley by surprise and became one of the most downloaded apps in the United States has raised security concerns due to a “hidden code” that allegedly transmits user data to the Chinese government, according to cybersecurity experts. Researchers from Feroot Security, a Canada-based cybersecurity firm, discovered that DeepSeek’s browser-based version not only collects user information but also generates a unique digital “fingerprint” to track activity across multiple websites. Read More: The Influence of AI in the Interior Design Industry Launched in January 2025 as a free, open-source platform, DeepSeek sent shockwaves through the tech industry, triggering over USD 1 trillion in stock market losses and threatening the dominance of U.S. leaders such as Nvidia. Its sudden rise has cast doubt on the perceived dominance of well-funded U.S. AI companies like OpenAI, which has reportedly invested billions in similar technologies. As the South China Morning Post reports, Hangzhou-based DeepSeek rose to worldwide fame last month after it released the V3 large language model (LLM) and R1 reasoning model, which the company said showed capabilities comparable to products from ChatGPT creator OpenAI, despite being developed at significantly lower costs. DeepSeek was founded by entrepreneur and businessman Liang Feng in 2023. By offering an advanced AI tool for free — despite its relatively low reported development costs — DeepSeek is reported to have unsettled Silicon Valley and challenged assumptions about American leadership in the AI space. The AI landscape is no longer a Silicon Valley monopoly as for years, Silicon Valley dictated the pace of AI development, but that dominance is slipping. The rise of Chinese firms — particularly DeepSeek — has accelerated global competition, leading to an unprecedented shift in technological power. No longer is the race confined to a handful of Western tech giants — now, Eastern AI firms are outpacing them in speed, efficiency, and sophistication. Despite DeepSeek’s rise, the Wall Street Journal reported that Amazon, Google, Microsoft and Meta continue to “pour billions into artificial intelligence”. The report states that Microsoft, Google, and Meta Platforms are projected to collectively invest over USD 215 billion in capital expenditures for their current fiscal years, marking an annual rise of more than 45 percent. Meanwhile, Amazon.com — while withholding a full-year forecast — indicated that its total capex across various divisions is set to exceed USD 100 billion, with the bulk of this increase being directed towards AI development. These figures reflect the growing intensity of the AI race, despite investor concerns surrounding the influence of China’s DeepSeek and the uncertainty over whether these major U.S. firms will see a return on their massive investments. Investors are particularly unsettled by DeepSeek’s ability to replicate much of the functionality of leading American AI systems, all while spending less and utilising fewer, less powerful chips, as claimed by its Chinese creators. However, U.S. company leaders remain confident, highlighting their own technological progress and arguing that reduced costs will drive AI adoption and boost demand for cloud services, which AI heavily relies on to function. So why do Western AI models struggle with inferential reasoning compared to their Eastern counterparts? AI development in countries like Russia and China often take a more centralised, state-backed strategy. This support includes access to vast amounts of data and resources, which can help hone the models to specific needs, including advanced reasoning. Western companies may not have the same access to data and government-driven initiatives that prioritise AI development. Then there is the discussion of public versus private sector approaches. Different countries are taking varied approaches to AI regulation, with the public and private sectors playing distinct roles. In China, for instance, the government leads AI development, driving a centralised strategy that ensures national interests are prioritised. In contrast, the West follows a more fragmented, market-driven approach, where tech giants hold significant sway. This highlights how each model presents its own set of challenges and opportunities for growth and governance in the AI rat race. DeepSeek’s latest iteration has redefined the limits of AI-powered assistants. Unlike standard chatbots, which rely on predefined responses, DeepSeek anticipates user needs before they even ask. It analyses past interactions, contextual nuances, and even subconscious patterns to refine its responses. Unlike ChatGPT, which still struggles with complex inferential reasoning, DeepSeek has developed a more sophisticated approach — one that does not just respond but understands intent between the lines. This raises a critical question: while Western AI models prioritise accessibility and ethical constraints, Eastern firms are refining AI to a near-intuitive level. Will this give them a strategic advantage? And what does this level of sophistication mean for privacy and control? However, despite once being a frontrunner in advanced AI models, DeepSeek has now been dethroned by an even more sophisticated competitor in Alibaba. According to a report by USA Today, Alibaba’s release of its Qwen 2.5-Max artificial intelligence model on the first day of the Lunar New Year highlights the intense pressure exerted by DeepSeek’s recent success. Qwen 2.5-Max — according to Alibaba’s cloud unit — surpasses both DeepSeek-V3 and other leading models like GPT-4o and Llama-3.1-405B in performance. This move follows DeepSeek’s disruptive impact, particularly after its DeepSeek-V2 model, which triggered a price war in China by being open-source and exceptionally low-cost. In response, Chinese firms like ByteDance, Baidu, and Tencent have quickly updated their own models to stay competitive, further intensifying the AI arms race within China and globally. Read More: The OpenAI Debate and the Ramification of Improper AI Governance AI is already shaping human behaviour in ways most people do not realise. From a commercial aspect with AI-written books and deepfake models used for branded commercials to automated journalism, artificial intelligence is not just assisting creativity — it is replacing it. The question is no longer whether AI will generate content but whether we will be able to distinguish human work from machine-generated output. Most users engage with AI tools without realising they are feeding them data — data that becomes fuel for even more advanced learning systems. Governments and corporations are increasingly aware of AI’s power, leading to rising restrictions and tighter control. AI’s presence is often unseen — tracking, predicting, and influencing decisions without users’ explicit awareness. The rise of AI platforms like DeepSeek has sparked new conversations around digital sovereignty. With AI models and data being controlled by a select group of tech giants, the question of who owns and governs digital resources becomes ever more pressing. As seen with the United States’ Silicon Valley, nations that develop and control their own AI technology may gain unprecedented power, influencing global economic and political dynamics. Read More: Is Cinema Preparing for An AI Takeover? With Oracle debuting its AI agents, the next phase of the AI race is here. These self-learning systems are not just programmed to respond; they adapt, evolve, and outthink human-driven decisions. The race is not about creating better chatbots — it is about who can build AI systems that function independently of human intervention. DeepSeek’s latest model faced immediate restrictions from hundreds of companies and government agencies. A newly proposed law in the U.S. could impose hefty fines or even prison sentences on individuals who use the Chinese AI app DeepSeek. The bill — introduced by Republican Senator Josh Hawley — seeks to ban U.S. citizens from contributing to the advancement of artificial intelligence technology in China. It also aims to restrict the importation of AI-related technology or intellectual property developed within the country. If passed, the legislation would enforce strict penalties, including fines of up to USD 1 million for individuals and as much as USD 100 million for businesses. Those found in violation could also face up to 20 years in prison. It is not all doom and gloom, despite the fear-mongering surrounding AI, its real impact is already proving to be transformative. From streamlining business operations to ensuring fairer decision-making in industries like insurance and governance, AI is stepping in where human limitations exist. Rather than a threat, AI could be the key to a more efficient, equitable, and forward-thinking world. One such argument is that AI is not replacing human intelligence but rather amplifying it. Insurance, healthcare and the finance industry could benefit from some element of AI intervention. With insurance, AI can be used to analyse patterns in claims to identify potential fraud while AI-powered automation could speed up claims approvals, reducing paperwork and human errors. As it stands, chatbots already handle routine inquiries, AI could make machine learning models assess customer data to offer personalised policies, improve efficiency and reduce wait times. From the administration angle, AI could reduce paperwork and improve patient data management within the healthcare sector while AI-driven models can predict stock market trends and optimise trading strategies within finance and banking. AI has the potential to meet growing demands that are no longer humanly possible to fulfill at a large scale. AI also has the potential to eliminate human biases in vetting processes, from loans to medical diagnostics. AI automated systems could ensure consistency and prevent human errors or prejudices alongside faster, data-driven decisions that benefit businesses and consumers alike. AGI stands for artificial general intelligence and refers to machines that are able to apply their knowledge and learning ability to a wide range of tasks. Most AI today is “narrow” or “specialised” and designed for specific tasks. True generalised AI will have the capacity to think, act and behave far more closely to the way we do across tasks involving creativity and complex problem-solving. Instead of resisting AI, embracing it for good could be the biggest opportunity of our generation. The real failure is not AI — it is the lack of vision on how it is implemented. Smart systems are helping governments make better infrastructure and policy decisions. The debate of AI ethics has been ongoing for decades and it is time to focus on solutions instead of fueling paranoia. AI is not the problem — how we choose to use (or misuse) it is. Read More: For Better or Worse: Here Is How AI Artist Botto is Reshaping the Art Industry The AI race is no longer about innovation alone — it is about control. With each advancement, AI developers are not just creating better tools; they are designing systems that learn from us, adapt, and evolve. While AI is being given away for free or at minimal cost, the real currency is data, influence, and power. There should also be some importance placed on having diverse teams in AI development as AI systems are shaped by the biases and perspectives of those who create them. Without a diverse group of developers, there is a risk that AI may perpetuate social inequalities or missed opportunities for more inclusive solutions. The real failure is not the implementation of AI itself but rather the profit-driven rush to dominate it. It is a scene all too familiar — when left unchecked, technological advancements prioritise corporate gains over public good. Governments should have systems in place to ensure AI serves society, not just those who control it. Yet, this has been an ongoing discussion for at least the last 30 years or so — warning of automation, bias, and unchecked surveillance — only to find ourselves repeating the same cycle. For more on the latest in business reads, click here.
--------------------------------------------------

Title: What DeepSeek? Big Tech keeps its AI building boom alive.
URL: https://www.thestar.com.my/tech/tech-news/2025/02/10/what-deepseek-big-tech-keeps-its-ai-building-boom-alive
Time Published: 2025-02-10T02:39:00Z
Full Content:
Monday, 10 Feb 2025 Construction for a new Microsoft data centre in East Wenatchee, Washington on Nov 3, 2024. An apparent breakthrough in efficiency from the Chinese start-up did not make tech’s biggest companies question their extravagant spending on new data centres. — ©2025 The New York Times SEATTLE: Wall Street went into panic mode about two weeks ago after the Chinese startup DeepSeek released an artificial intelligence system that appeared to be radically more efficient than what its American competitors had built. The investors who had pumped trillions of dollars into tech stocks over the past few years worried whether the tens of billions of dollars that tech companies were spending on new data centres suddenly looked like comic overkill. But the biggest tech companies made clear in recent earnings reports that they believe there may be no such thing as overkill when it comes to new data centres. Amazon on Thursday implied that its capital expenditures – a figure that includes data center construction and other items like warehouses – could top US$100bil (RM443.93bil) this year. Microsoft said its spending could surpass US$80bil (RM357.63bil). Alphabet said it would spend US$75bil (RM335.28bil), and Meta reaffirmed plans to have capital spending hit as much as US$65bil (RM290.57bil). Combined, they could spend roughly US$100bil (RM443.93bil) more than last year on these projects. Executives urged patience. The problem right now, they said, is that customers want more AI than the companies can supply. And the only way they can meet demand is to build as much as they can as quickly as they can. “Whenever I see someone else do something better, I say, ‘Ugh, we should have done that’,” Meta CEO Mark Zuckerberg told employees at a companywide meeting last week, according to a recording obtained by The New York Times. “Competition is good, but we need to make sure that we win.” Here are some key points to understand this spend-happy moment for tech: Tech companies need more data centres than they have. Many of the companies say they’re constrained by the supply of chips, land and power needed to build data centres, and are racing to get more of them open. Microsoft, Alphabet and Amazon all said they could have had higher cloud computing sales if they had the capacity. Cloud services are the typical way AI is delivered to customers. Alphabet saw “demand that exceeds our available capacity”, Anat Ashkenazi, Alphabet’s finance chief, told investors. “So we’ll be working hard to address that and make sure we bring more capacity online.” Microsoft has been saying that it has been constrained for a while, and previously told investors that the pressure would ease early this year. But this past week, when it reported its latest earnings, executives told investors that it might take until summer to get enough capacity up and running to meet the full demand. Its stock fell about 5% in after-hours trading after the report. They say greater efficiency will expand the use and demand for AI. While many people think about data centres as the enormously expensive, power-hungry places where advanced AI systems are developed, they are also where AI is deployed. Those are two different steps: training a model that underpins ChatGPT, versus asking ChatGPT for a recipe suggestion. Deploying AI is known as “inferencing” in the industry; it is where the tech companies increasingly say their businesses will boom. As costs come down, “AI will be much more ubiquitous,” Microsoft CEO Satya Nadella told investors this past week. Amazon CEO Andy Jassy told investors on Thursday that while a world where every app was infused with AI could be hard to fathom, “this is the world we’re thinking about all the time.” That vision, he said, has inferencing at its core. He argued that lowering the costs of inferencing would follow the pattern of previous technological trends: As the systems become less expensive to deploy, Jassy said, customers will “get excited about what else they could build that they always thought was cost-prohibitive before, and they usually end up spending a lot more in total.” The companies say they have to think about the long haul. Cloud providers are used to giving customers the illusion of endless supply, which means they must juggle having just enough data centres online to stream the video you want or answer your chatbot query. But they also can’t build too far in advance, locking up billions of dollars that could be deployed elsewhere. Balancing those two – particularly when securing land, chips and power for data centres can take years – is one of the enormous challenges the companies face. Executives have argued that they can adapt how they use the investments, between building and deploying AI models, and between serving their own core business and those of customers. Nadella said Microsoft’s infrastructure was “pretty fungible”. Ashkenazi said Google was also flexible. It could, for example, “repurpose capacity” to serve Google Search instead of cloud customers. Zuckerberg said Meta was studying DeepSeek and the ways it created efficiencies, but that investing heavily in data centres would be a strategic advantage against a small and nimble competitor. “We serve a billion-plus people – that’s just a lot of people, so more and more of the fleet is going toward running inference,” he told employees. Regardless of the explanation, cutting into profits – even the gaudy profits of tech’s biggest companies – is unlikely to thrill investors. Every company saw its share price fall after its earnings report. – ©2025 The New York Times Company Thank you for your report! Copyright © 1995- Star Media Group Berhad [197101000523 (10894-D)] Best viewed on Chrome browsers. We would love to keep you posted on the latest promotion. Kindly fill the form below We hope you enjoy this feature!
--------------------------------------------------

Title: CNBC Daily Open: Markets aren’t as enthusiastic over Trump and AI as they used to be
URL: https://www.cnbc.com/2025/02/10/cnbc-daily-open-markets-arent-as-keen-on-trump-and-ai-as-before.html
Time Published: 2025-02-10T00:48:53Z
Description: While AI and Trump are still the main characters of the stock market, they are steering markets in direction different from December.
--------------------------------------------------

Title: Sometimes even 'bad press' is bad and these 10 tech rebrands prove it
URL: https://www.androidpolice.com/worst-tech-rebrands-list/
Time Published: 2025-02-10T00:37:10Z
Full Content:
We love to hear about innovations in consumer technology that will "change the world." Often, the word "disruptive" is used to describe such innovations. But what happens when the disruption isn't to the world's status quo but to those attempting to disrupt it? You get gaffs like Airbnb's logo rebranding in 2014. Its Bélo symbol was involved in multiple controversies. Sometimes, a rebrand isn't as apparent as a new logo. Instead, it's a change in direction, like when Apple launched FCPX and alienated much of its professional audience. Recently, Microsoft rebranded Bing Chat, making it Microsoft Copilot, and experienced a failure. That got us thinking about other hilarious historical fails. Let's take a trip down nightmare memory lane to revisit the ten worst tech rebrands of all time. Are users really the focus anymore? Netflix started an online DVD rental service. When streaming was introduced in 2007, it was part of the fee you paid for DVD rentals. However, Netflix CEO Reed Hastings saw the future of online video consumption and had a plan. Hastings announced in July 2011 that Netflix would separate the two services. What was once $10 for DVD rentals and unlimited streaming would now be $7.99 for DVD rentals or streaming and $15.98 if you wanted both. Two months after the announcement, Netflix stock lost half its value. Hastings announced a new DVD rental service in September 2011 called Qwikster. Users needed to sign up for a new, separate account, but consumers weren't having it. So, Hastings and company shuttered that service one month later. "Fail fast," indeed. A new kid on the block tried to corner the mobile payment market in 2013. AT&T, Verizon, and T-Mobile backed the mobile wallet app, Isis. The app was intended to be its own payment vendor instead of a place to hold your credit and debit cards. But the timing for that name was horrible. The terrorist organization, Islamic State of Iraq and al-Sham, aka ISIS, made global headlines, making it challenging for those searching the internet for information about the mobile wallet Isis. Isis was rebranded to Softcard to make it search-friendly and to distance the brand from the terrorist organization. Isis Mobile Wallet/Softcard never took off. Still, you may be using remnants of its IP today because Google bought portions of Softcard and relaunched Google Wallet as Android Pay. An enterprising tech journalist, TechCrunch's Michael Arrington, regaled the world in 2008 with his vision of the casual computing future: the CrunchPad. Originally slated to retail for $200, the Singapore development studio Fusion Garage brought the tablet to market without Michael Arrington at the helm as the JooJoo. The JooJoo launched for $500 days before the iPad. "JooJoo" is the phonetic pronunciation of "juju," derived from the West African word for "luck." Launching before the iPad was bad luck because the iPad was a better device in terms of hardware and its app store ecosystem, which ensured the name rebrand wasn't the only reason the JooJoo would fail. Oxford University Press began printing in 1669 and is one of the most lauded producers of dictionaries. It published its first dictionary in 1884. But what happens when a brand with a centuries-long pedigree wants to bring its look into the 21st century? Like "The Shack," Oxford's modernizing wasn't for the better. Oxford University Press replaced its classic blue logo in 2014 with a modernized logo that it hoped would help it stand out and beat the competition. It did when it introduced its new Beats Words By Dre logo for Oxford Dictionaries. It looks familiar, right? When it brainstormed this, it didn't forget about Dre. I'm a fan of the artist enclave, DeviantArt. It's easy to get lost in the rabbit hole of genres of fan art, photography, literature, and more, but the logo you see today wasn't around before 2014. That year, it hired global independent design studio Moving Brands to help it move into the future as DeviantArt, presenting a new site layout to its users, called "deviants," and introduced a mobile app. With all of that came a logo redesign. It changed from its "DA" initials logo (top) to one with slices (bottom), and it was definitely "artistic." I'm still trying to determine if the green element is a step stool or a martial arts weapon called a "Tonfa." Microsoft 365 Office is the most widely used productivity suite in the corporate world. Its user base grew by 200 million in 2019, so there are many eyes on the company's brand. That means small things can become a big deal. Such is the case with the now viral name change and logo redesign for Microsoft 365 Office, now Microsoft 365 Copilot. Copilot is Microsoft's foray into artificial intelligence, and since AI is the future, I'm not mad about the name change. The hot takes came fast and furious when everyone saw the new logo, which had the M365 tag strewn across it so small it was almost illegible. Microsoft is all-in on its AI initiative, raising the subscription price for the first time in 13 years and integrating Copilot into the base plan instead of charging extra for a Copilot Pro subscription. Maybe it should've paid a bit extra to have that logo redesigned. Do more with Copilot Tech and science fiction go hand-in-hand. Star Trek communicators are now smartphones. "The Demolition Man" featured driverless cars, and now Waymo's on the road. Some movies seem to have a prescience about what's to come, and the folks in marketing at what was once known as the Sci-Fi channel wish they could've tapped into that quality before rebranding to SyFy. In Poland and other countries around the world, "Syfy" is synonymous with the STI syphilis. That's probably not what you want consumers to associate with your brand, but it wasn't enough to make SyFy rethink its redesign. The name remains today, and it is still ridiculed. I guess there's truly no such thing as "bad press." Radio Shack's move in 2008 to rebrand to "The Shack" to survive the recession in the United States didn't work out well. It tried to remain relevant and appeal to younger consumers but alienated its long-time DIY customers. The odd thing was that it never fully committed to the rebrand. The marketing referred to the company as "The Shack," while the logo had both names, "RadioShack The Shack," making it confusing. When a stoic Morpheus speaks those words to Neo, it references another virtual world, the fictional "Matrix." The issue in is the same: How do you communicate the virtual world's value without showing someone what it is? Even then, how easy is it for the average user to conceptualize what exactly Mark and the Facebook fam are trying to do as they drive folks to the Metaverse? The most concerning issue with the rebrand is that while the age demographic with the most users in the US is between 25 and 34, users who spend the most time on Facebook are between 55 and 64. How well are they receiving the new branding and the aspirational ideals put forth by Mark Zuckerberg? Is "Meta" too meta for Meemaw and PawPaw? Learn about Meta AI and how you can use this free, open source artificial intelligence to chat, create images, and more Picture this: Twitter was so widely used that it became the "go-to" platform for those fighting oppression internationally during social upheaval and protest. Traditional media organizations quote its content multiple times daily in broadcast news reports and more. Now picture this: Elon Musk buys the platform and runs into trust issues with that audience, so much so that many advertisers drop Twitter. Some major brands with an active presence also shut down their Twitter accounts. According to analysts, 32 million users left after Musk's $44 billion purchase. Amidst all that, and despite Twitter's powerful name recognition, Musk changed the branding. Elon Musk's rebranding of the Twitter platform to "X" is the worst tech rebrand in modern times. Almost 18 months after the announcement, nearly everyone still refers to his platform as "X (formerly Twitter)." Not to mention, sending someone an "x" sounds weird: "Hey! I'll X you that info in a bit." Writing an article like this is tough because many involved in these decisions take huge risks, and I applaud that. It's easy to get stuck in the paralysis of analysis, and it's also challenging to catch lightning in a bottle and come up with a rebrand that resonates with consumers and makes tech cool again. Let these ten examples be cautionary and educational, showing you what not to do the next time you're thinking of changing things up. We want to hear from you! Share your opinions in the thread below and remember to keep it respectful. Your comment has not been saved EDIT: I see my reply to you, Tshaka, was deleted. Was it because I pointed out where you plagiarized this article, with source? Or because I answered you directly? You can still find my response under my profile name. Don't steal others' work. And stop with the Elon hit pieces; it's lazy and tiresome, albeit that is the current state of tech "journalism." ORIGINAL POST: Liberal-slanted tech journalism trashing Twitter/X rebrand? (Even though "Mastodon" is the symbol of the Republican Party and Bluesky is a direct "Twitter" rip-off?) I'm completely shocked... 🙄 wait which parts are "liberal-slanted", i am def missing something to be fair, man, the company lost millions of users and billions in value by ditching a name so significant that its branded verb, "tweet", made it into the dictionary in just 5 years and people around the world still call it twitter be honest, it was an objectively bad rebrand by any metric Show me where I politicized anything in my abbreviated synopsis of everything that has happened with X (formerly Twitter) since Musk purchased it. Go ahead; I'll wait. Mastodon and Bluesky are not rebrands, so your parenthetical is a non-sequitur. Samsung's best-selling flagship in years Plus Pixel 9 Pro problems paint a predictable picture The graveyard grows At least you still get fast, free shipping. Oh, wait Saving money streaming is harder But good luck finding the newcomer stateside
--------------------------------------------------

Title: The pros and cons of buying Betashares Nasdaq 100 ETF (NDQ) units this month
URL: https://www.fool.com.au/2025/02/10/the-pros-and-cons-of-buying-betashares-nasdaq-100-etf-ndq-units-this-month/
Time Published: 2025-02-09T21:22:13Z
Description: This fund has a lot going for it. But is the valuation too high? 
The post The pros and cons of buying Betashares Nasdaq 100 ETF (NDQ) units this month appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: What DeepSeek? Big tech keeps its AI building boom alive
URL: https://economictimes.indiatimes.com/tech/technology/what-deepseek-big-tech-keeps-its-ai-building-boom-alive/articleshow/118080967.cms
Time Published: 2025-02-09T02:31:25Z
Full Content:
Artificial Intelligence(AI) Java Programming with ChatGPT: Learn using Generative AI By - Metla Sudha Sekhar, IT Specialist and Developer Artificial Intelligence(AI) Basics of Generative AI: Unveiling Tomorrows Innovations By - Metla Sudha Sekhar, IT Specialist and Developer Artificial Intelligence(AI) Generative AI for Dynamic Java Web Applications with ChatGPT By - Metla Sudha Sekhar, IT Specialist and Developer Artificial Intelligence(AI) Mastering C++ Fundamentals with Generative AI: A Hands-On By - Metla Sudha Sekhar, IT Specialist and Developer Artificial Intelligence(AI) Master in Python Language Quickly Using the ChatGPT Open AI By - Metla Sudha Sekhar, IT Specialist and Developer Office Productivity Zero to Hero in Microsoft Excel: Complete Excel guide 2024 By - Metla Sudha Sekhar, IT Specialist and Developer Marketing Performance Marketing for eCommerce Brands By - Zafer Mukeri, Founder- Inara Marketers Finance A2Z Of Money By - elearnmarkets, Financial Education by StockEdge Astrology Vastu Shastra Course By - Sachenkumar Rai, Vastu Shashtri Marketing Modern Marketing Masterclass by Seth Godin By - Seth Godin, Former dot com Business Executive and Best Selling Author Data Science SQL for Data Science along with Data Analytics and Data Visualization By - Metla Sudha Sekhar, IT Specialist and Developer Strategy Succession Planning Masterclass By - Nigel Penny, Global Strategy Advisor: NSP Strategy Facilitation Ltd. Web Development A Comprehensive ASP.NET Core MVC 6 Project Guide for 2024 By - Metla Sudha Sekhar, IT Specialist and Developer Artificial Intelligence(AI) AI and Analytics based Business Strategy By - Tanusree De, Managing Director- Accenture Technology Lead, Trustworthy AI Center of Excellence: ATCI Marketing Digital Marketing Masterclass by Pam Moore By - Pam Moore, Digital Transformation and Social Media Expert Office Productivity Mastering Microsoft Office: Word, Excel, PowerPoint, and 365 By - Metla Sudha Sekhar, IT Specialist and Developer Marketing Digital marketing - Wordpress Website Development By - Shraddha Somani, Digital Marketing Trainer, Consultant, Strategiest and Subject Matter expert Web Development Mastering Full Stack Development: From Frontend to Backend Excellence By - Metla Sudha Sekhar, IT Specialist and Developer Finance Financial Literacy i.e Lets Crack the Billionaire Code By - CA Rahul Gupta, CA with 10+ years of experience and Accounting Educator Artificial Intelligence(AI) AI-Powered Python Mastery with Tabnine: Boost Your Coding Skills By - Metla Sudha Sekhar, IT Specialist and Developer 5 Stories 7 Stories 9 Stories 9 Stories 8 Stories 6 Stories Things LTI-Mindtree’s incoming CEO needs to fix before growth takes off Rate cut is good news for borrowers. But what about bank stocks? Why your loan rate is rising despite RBI rate cut Raising a glass to the future, alcobev looks to end FY25 on a high RBI Governor delivered something better than a rate cut Stock Radar: Kotak Mahindra Bank hits fresh 52-week high in February; will the rally continue? Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Slideshow Private Companies Top Definitions Top Commodities Top Prime Articles Top Story Listing Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Fears Over Tariffs And AI Spending Overwhelm Better Earnings
URL: https://www.forbes.com/sites/bill_stone/2025/02/08/fears-over-tariffs-and-ai-spending-overwhelm-better-earnings/
Time Published: 2025-02-08T15:52:41Z
Full Content:
Earnings season continues this week, with notable economic releases for inflation and consumer ... [+] spending. However, with President Trump threatening reciprocal tariffs, fundamental data could take a backseat to politics. With almost two-thirds of the fourth-quarter earnings season complete, the pace slows slightly with 77 S&P 500 companies scheduled to report. Seventy-seven percent of S&P 500 firms reported better-than-expected earnings for the quarter. Two of the Magnificent 7 reported earnings last week, with only Nvidia (NVDA) remaining on February 26. The Magnificent 7 consists of Microsoft (MSFT), Meta Platforms (META), Amazon.com (AMZN), Apple (AAPL), Nvidia (NVDA), Alphabet (GOOGL) and Tesla (TSLA). S&P 500 Earnings Season Despite robust earnings growth and a noisy but solid jobs report, concerns about President Donald Trump’s threatened additional tariffs and some worries about artificial intelligence spending caused some market consternation. The S&P 500 fell 0.2% for the week, but the Magnificent 7 lost 2.8%. Market Returns According to FactSet data, industrials, consumer discretionary, and healthcare contributed most significantly to last week's earnings growth improvement, while the industrials sector was the largest detractor. Uber Technologies (UBER) was the most significant contributor to industrials, with Amazon.com (AMZN) being the most critical contributor to the consumer discretionary sector. Within healthcare, earnings beats by Pfizer (PFE) and Bristol Myers Squibb (BMY) increased the earnings growth rate. The financial sector is expected to show the most rapid year-over-year growth rate in the S&P 500, followed by communications services and consumer discretionary. The energy sector is at the bottom, with a forecasted almost twenty-eight percent decline in year-over-year earnings. Estimated Earnings By Sector Sales growth is closely tied to nominal GDP growth, which combines after-inflation economic growth (real GDP) with inflation. At this point in the earnings season, sales growth at 5.2% exceeds expectations, with the tailwind from 5% year-over-year nominal GDP growth. Estimated Sales By Sales Due primarily to the robust earnings growth for financials, communication services, and consumer discretionary, the blended earnings performance has significantly outperformed expectations at the end of the quarter. Combining actual results with consensus estimates for companies yet to report, the blended earnings growth rate for the quarter is at +16.4% year-over-year, above the expectation of +11.9% at the end of the quarter. S&P 500 Earnings Summary Because these companies are a critical driver of earnings growth and a significant percentage of the S&P 500’s market capitalization, the Magnificent 7 are the group to watch this earnings season. Alphabet (GOOGL) and Amazon.com (AMZN) reported last week. While earnings growth was robust for both, it was below very high expectations, and the stocks were sent lower for the week. Separately, investors are questioning whether both companies' increased AI infrastructure spending will yield a good return on investment. Looking at Alphabet specifically, their cloud business revenue, including AI infrastructure demand, grew by 30% year-over-year, but that slowed from the 35% growth in the previous quarter. Notably, the company said capacity constraints, not demand, were the issue. The company expects to spend about $75 billion in 2025 on additional AI infrastructure, an increase of 43% over 2024. Aside from focusing on AI, Alphabet’s core businesses, search and YouTube, performed well in the quarter. Nvidia (NVDA) is the final group member to report earnings, scheduled for February 26. The introduction of DeepSeek led to worries that demand for their AI chips might falter, so the stock had been down over 20% from its peak until last week’s 8% rally. Alphabet and Amazon announced significant increases in capital spending to meet AI capacity demands, which helped reduce worries that a decline in demand for NVIDIA’s chipsets could be imminent. Notable companies reporting this week are McDonald’s (MCD), Coca-Cola (KO), Humana (HUM), Kraft Heinz (KHC), Applied Materials (AMAT), and Airbnb (ABNB). Magnificent 7: Q4 Earnings Growth It was a noisy monthly jobs report on Friday. Non-farm payrolls grew by 143,000 in January, which was below consensus. More recent months were revised higher but offset by downward revisions to farther out months. Both household employment measures are distorted by a much higher than typical adjustment for immigration-related adjustments. These adjustments also helped send the unemployment rate down to 4%. The workweek fell to a 5-year low of 34.1 hours, and hourly earnings rose more than expected. Though the government said that the numbers showed no impact from the L.A. fires and weather conditions, it seems likely that there was an impact, so it's best to take this month’s job data with a grain of salt. Monthly Job Growth Looking at other labor market data for some clarity, initial claims for unemployment benefits remain low, so there is no sign of a collapse in the job market. Initial Jobless Claims Given the mixed and noisy monthly jobs report, markets and the Federal Reserve (Fed) will wait for more straightforward data to make any decisions about the economy. The Fed is expected to be on the sidelines until mid-year when it might resume short-term interest rate cuts. Regarding the U.S. economy, the crucial releases will be Wednesday’s consumer inflation (CPI) and Friday’s retail sales numbers. CPI is expected to hold steady at an above-target 2.9% rate, but the details will be more critical with housing costs and services inflation closely watched. Retail sales are expected to slow from December’s rapid pace, but the U.S. consumer is the lifeblood of the economy. U.S. CPI Inflation Estimate Earnings season slows, but plenty of important companies are reporting this week. Fundamental data could take a backseat to politics. President Trump announced he would roll out reciprocal tariffs this week to match the levies other countries put on U.S. imports. Markets would probably see these as positive relative to the original thought of a 10% to 20% global tariff, but it would almost certainly add the European Union to the targeted regions. The duration of the higher tariffs, retaliatory action by impacted countries, the strength of the U.S. dollar, and the ability to switch to domestically produced or imports from non-impacted countries will impact any price increases or demand shocks caused by the tariffs, so the ultimate result is impossible to know. In any case, markets dislike uncertainty, so tariff news should add to volatility. Disclosure: Glenview Trust holds many stocks mentioned in this article within its recommended investment strategies.
--------------------------------------------------