List of news related to Meta stock price META:

Title: 'Full hit from China': Analysts are staying bullish on Nvidia ahead of earnings, but tariffs loom large
URL: https://www.businessinsider.com/nvidia-q1-earnings-preview-nvda-stock-price-tariffs-china-outlook-2025-5
Time Published: 2025-05-27T16:38:59Z
Full Content:
Nvidia will report fiscal first-quarter earnings on Wednesday after the closing bell. The impact of President Donald Trump's tariffs on Nvidia will be in focus, as will the company's shrinking market share in China. The overall outlook for Nvidia earnings on Wall Street is generally upbeat, but uncertainty looms in the second half of 2025. The earnings-release time is shortly after 4 p.m. ET, and its conference call with analysts will start around 5 p.m. Nvidia's stock was up 0.9% year-to-date through Tuesday's close, slightly outpacing the S&P 500's 0.7% increase. Nvidia announced a partnership with Saudi tech firm Humain on May 13 to strengthen sovereign AI infrastructure and elevate the country's position as an AI developer. The chipmaker's stock surged to its highest valuation since February following the announcement. Nvidia said in an announcement that the first phase of the 500 megawatt buildout will be a deployment of 18,000 Nvidia GB300 Grace Blackwell AI chips. Nvidia defines "sovereign AI" as a nation's capability to produce AI with its own resources, the chipmaker wrote on its website. It's an area that Huang has emphasized multiple times. "It codifies your culture, your society's intelligence, your common sense, your history — you own your own data," Huang at the World Governments Summit in Dubai last year. — Ana Altcheck Huang announced at the tech show Computex on May 19 that Nvidia is looking at the Beitou Shilin area in northern Taipei for a new Nvidia office named "Nvidia Constellation." Taipei's mayor Chiang Wan-an, who showed up at Huang's keynote, said in an interview following the announcement that the city government welcomes the chipmaker's office and will provide assistance as needed. Huang, who was born in Taiwan, also announced a partnership with TSMC to build an AI supercomputer in Taiwan. The project will be a joint effort between the Taiwanese government and Taiwan manufacturing firm Foxconn. — Ana Altchek At GTC this year, Jensen Huang announced plans for Nvidia to build a quantum research lab in Boston. "It will likely be the most advanced accelerated hybrid quantum computing research lab world," Huang said. "And it's going to be located in Boston so that we can partner with Harvard and MIT." Wednesday's earnings call is the company's first since the announcement, and the quantum sector has been booming. Some notable startups include D-Wave, whose stock has risen over 138% in the last 30 days as of May 27; IonQ, whose shares have risen over 67% in the same timeframe; and Rigetti Computing, whose stock has increased by more than 53% in the last month. As tech giants, including Nvidia, expand their quantum offerings, investors are keen on tangible road maps for their progress in the burgeoning field. Expect to see those plans mentioned more often going forward. — Katherine Tangalakis-Lippert Angelo Zino, a senior equity strategist at CFRA Research, said the outlook for Nvidia had "considerably improved" in recent weeks, pointing to progress between the US and China in securing a framework trade deal and the US getting rid of its AI diffusion rule, which would have imposed restrictions on Nvidia's chip sales to China. Nvidia's recent Saudi Arabia deal also gives the company "strategic importance" in the US's trade negotiations with other countries, Zino wrote in a note to clients. "We believe NVDA's content growth story in data centers will extend through at least 2027, supported by its product pipeline," Zino said. "While quarterly variability is a risk, we view any pullbacks as enhanced buying opportunities given the better policy backdrop/customer visibility," he added. It's the first earnings call for the company since US lawmakers said in April they were investigating how the Chinese AI company DeepSeek may have gotten Nvidia chips. House Representatives said in a report that it appeared DeepSeek had used 60,000 Nvidia chips even though US sanctions limited the sale of the company's tech in China. The lawmakers also accused Jensen Huang of directing the design of Nvidia chips to get around US export controls. The company said in a statement at the time that "the US government instructs American business on what they can sell and where — we follow the government's directions to the letter." — Kelsey Vlamis While earnings for Nvidia should be "fine" for the first quarter, guidance will likely be lowered in the second quarter to account for a hit to the company's revenue in China, analysts estimated. "With regard to the F2Q26 we are slightly lowering estimates to account for a full hit from China," the firm wrote in a note this month. Still, demand for Nvidia's products will likely remain on a solid trajectory in the coming years, the firm said, pointing to Nvidia's recent deal with Saudia Arabia. The research firm reiterated its "buy" rating on the stock and issued a price target of $150 a share, implying 11% upside from current levels. Nvidia's almost 25% share price jump this past month has come hand-in-hand with several announcements that place its chips at the center of vast new AI data centers. During a trip to Saudi Arabia this month, Jensen Huang revealed a deal to ship 18,000 of Nvidia's top chips to the country. Last week, the FT reported that Oracle will buy $40 billion worth of chips from Nvidia for a US data center. Any sign that more deals are coming down the line could reassure investors that huge revenue growth in the chip giant's lucrative data center business is still achievable. — Hasan Chowdhury Nvidia's stock is up 25% in the last month. In an earnings preview note, Jefferies analysts led by Blayne Curtis wrote that Nvidia's stock "has been running over the past few weeks" following an easing of AI restrictions by the US government and recent announcements of AI projects with the United Arab Emirates and Saudi Arabia. The analysts wrote that they don't expect these developments to help today's results or guidance for the current quarter. The main drag on earnings will likely come from the Trump administration's restriction on selling H20 chips to China. Still, the analysts expect quarter-on-quarter growth to be viewed positively by investors and that Nvidia's supply chain issues will be resolved. The networking business should also pick up, alleviating some investor concerns around potential market share loss in that sector, the analysts wrote in last week's note. — Shubhangi Goel Restrictions on chip sales in China will likely remain an "overhang" on Nvidia until new rules are implemented, analysts at DA Davidson wrote. New restrictions on chips sold in China could also come "any day," they added, despite the Trump administration recently rolling back Biden's AI diffusion rule. The firm reiterated its "neutral" rating on the stock and issued a $120 price target, which they said was dependent on factors like how regulation in China will shake out and if some of Nvidia's neocloud customers face borrowing challenges. The price target implies 11% downside from current levels. In April, President Donald Trump placed restrictions on exports of Nvidia's H20 chips to China. The chipmaker said it expected a $5.5 billion write-down in its first-quarter earnings as a result of the new restrictions. During the Computex Taipei tech conference in Taiwan on May 21, Huang said the chipmaker's market share in China dropped to 50%, down from 95% four years ago. He added that Chinese companies have benefited from US chip export rules, and they have been "a failure." The chipmaker is reportedly planning to begin mass production for a less powerful AI chipset for China at a lower price as early as June, Reuters reported on Monday. — Ana Altcheck Piper Sandler said it expects Nvidia to miss on revenue for the first quarter. That's due to uncertainties related to the US economy, tariffs, and the ban on its H20 chip in China. "All in all, we think that NVDA is poised to be flat to down into the print this week," analysts wrote. Still, the firm sees Nvidia benefiting from a strong second half of the year. That's due to strong capital expenditures from other tech firms and an improving macroeconomic backdrop. "We advise investors to weather the uncertainty and stay long the stock as this is likely largely the last wave of negative news for NVDA this year." The firm reiterated its "overweight" rating on Nvidia and issued a $150 price target, implying 11% upside from current levels. A concern often raised by Nvidia investors is the small pool of customers it has depended on to generate billions of dollars in revenue. A handful of Big Tech firms have accounted for more than half of Nvidia's data center revenue as the AI boom has taken shape. But Nvidia has shown signs that its days of depending on West Coast tech firms may soon be over. Deals to sell its chips to nation states with plans to build sovereign AI capabilities have been picking up. Investors could look to get a clearer roadmap on plans to widen its customer base. — Hasan Chowdhury Nvidia should post a "modest" sales beat in the first quarter, but analysts at Bank of America said the company's guidance for the current quarter could be "messy. " They pointed to the impact of tariffs on Nvidia's shipments from China. The bank estimated that Nvidia could see a $4 billion to $5 billion headwind on shipments from China in the second quarter. Sales guidance for the second quarter could be revised down to $41 billion, down from the $46 billion consensus estimate. BofA estimated that Nvidia's total sales for the 2026 fiscal year could be around 6% below consensus, while earnings could be around 10% below consensus. "Despite these near-term headwinds, we maintain Buy on NVDA, a top sector pick given its unique leverage to the global AI deployment cycle and possibility for China sales recovery on new redesigned/compliant products later in the year," analysts wrote. The bank has a price target of $160 a share, implying 19% upside from current levels. First quarter Revenue estimate: $43.32 billion Second quarter Full year 2026 Where Big Tech secrets go public — unfiltered in your inbox weekly. Sign up chevron down icon An icon in the shape of an angle pointing down. Jump to
--------------------------------------------------

Title: Introduction to Sketching with Code
URL: https://www.creativeapplications.net/tutorial/introduction-to-sketching-with-code/
Time Published: 2025-05-27T14:03:36Z
Full Content:
This article first appeared on kyndinfo.notion.site. For more more information, updates, and additional resources, please see here. I’ve been wanting to write books that are like introductory science texts but not superficial. Books that go beyond pop science explanations but aren’t dry and dense like academic texts. Take quantum physics as an example. A pop science book might explain that the world is made of tiny particles, even smaller than atoms, and that these particles are also waves. Very intriguing. But after reading it, you realize that you still don’t actually understand how to make sense of wave-particle duality, how it really works, or what it means mathematically. So I turn to a college-level book, thinking, “This will give me the answers.” Instead, I find a wall of formalism. Dense derivations, little motivation, and an assumption that I already know what’s important. These books explain the details, but they don’t help me much, not having a real math or science background. Having said this, I’ve been struggling to write a book or article that’s truly for beginners. Many of my past articles were written for relative beginners, often based on lectures, workshops, or explanatory documents I’ve shared with colleagues and friends. I’ve been trying to keep them more engaging rather than feel like dry textbooks. But at the same time, they are not really for beginners. Many of them assume a certain level of prior knowledge, since it is a massive effort to build up all the knowledge from scratch. There are already great resources out there. For example, I don’t think I should write yet another book on vector math, and I’m not the best person to do that. So after contemplating this, I’ve decided to let go of the idea of perfect thoroughness or consistency, at least for now. Chasing that feels like an overwhelming amount of work without yielding much more of what the world might be missing. Instead, what I find could be more fun and uniquely valuable is externalizing the maps in my mind. The overviews, the relationships between key concepts, and the motivations behind them. As always, these are also an exercise for myself. They are like notes to organize and structure ideas, which might one day evolve into a more structured book. At the same time, I’m trying to create something that helps readers orient themselves better. To provide a sense of where things fit, what’s important, and what gaps they might need to fill in from somewhere else. There are 8 sections in this article covering various perspectives. You don’t have to read them in order—use the navigation menu on the left (right for mobile) to jump to any section you like. FAQs Mini License? Can I use the contents? Yes, please! Anything from personal to educational. I’d appreciate if you can let me know where you have used them, but it’s not required. Assume CC BY 4.0 unless otherwise noted. Use at your own responsibility. Read more How can I follow the updates? Follow me on Bluesky, Threads or X/Twitter Is there an index page? Yes, here. We think we naturally know what space is, but do we really? In an everyday sense, space feels like a void for objects and matter to fill. We are all in space and move within space. It seems like a backdrop that exists independently of everything inside it. That’s what Newton thought too. He wrote: Absolute space, in its own nature, without regard to anything external, remains always similar and immovable. But Newton also thought that even if absolute space exists, it is practically inconceivable to us. Where in the absolute are we? We’d never know. So he continues: Relative space is some movable dimension or measure of the absolute spaces; which our senses determine by its position to bodies. What he is saying is that we can only measure position relative to something. When we walk, we know we move relative to the ground, or the Earth. The revolution of the Earth is the movement relative to the Sun. Then the Sun or the solar system moves relative to the galaxy. From a relativistic perspective, both geocentrism and heliocentrism can be seen as correct in their own reference frames, but neither is absolutely correct. It’s just that heliocentrism provides a much simpler model for the motion of planets. However, there is a huge gap between those who only believe in geocentrism and those who discovered heliocentrism, or dogmatists and inquirers. “Orb – on the movement of earth” is a great historical fiction on this topic. We want to draw stuff on a computer. To do that, we need to have three things: an origin (a reference point to measure the position relatively from), axes along which we measure distances, and a unit to define what distance corresponds to a number 1. In theory, you can pick them freely, but most software has its own default coordinate system. For 2D drawing, like Photoshop, Illustrator, Figma, P5.js, and openFrameworks, the origin is usually the top-left corner of a canvas, and axes are horizontal (x) and vertical (y). The unit is usually a pixel or the device unit. For example, on a Retina display Mac, 1 point corresponds to 2 or 3 (or other numbers depending on your settings) physical pixels. This factor is set so that the numbers you choose appear about the same size perceptually regardless of the actual resolution of the device. A bit more about “perceptually the same size” : for example, text of size “14” is usually rendered smaller on mobile devices than on a laptop, and much bigger on a TV, because these devices are designed to be seen from different viewing distances. There is no perfect formula for this, but most manufacturers converge in the same range, so you don’t have to worry too much about each device separately. I said “default” because in many cases, you can manipulate the space, or rather the coordinate system. In design tools, you can drag the canvas and zoom in and out. Programming frameworks often provide methods to translate, scale and rotate the space, such as p5.js’s translate(), scale(), rotate() functions, or you can write your own functions to process the coordinates. You can also skew the space. Axes are usually selected to cross at 90° but this isn’t necessary. As long as they are independent, meaning they are not parallel, you can express a position uniquely by selecting one value for each of the axes. The demo below from the Mapping page demonstrates affine transformation, which is a simple method to enable these kinds of coordinate transformations. Once you have a coordinate system, you can move things around in this space. Newtonian physics is the perfect stating point. In Newtonian physics, you start from a position, then apply velocity to move it, and apply acceleration to change the velocity. All of these are expressed by vectors, which is basically just a few numbers (called components) put together. Expressing them as vectors makes it much easier to handle them, as opposed to manipulating the values per axis, and there are several very useful operations defined for vectors that you cannot avoid because they’re too good. Newton and VectorsVector Operations Of course, what comes after 2D is 3D. Moving up one dimension is just simple. You define a position is a vector with 3 components instead of 2. Do the same for the velocity, acceleration, etc. as long as they are defined as vectors, the math stays basically the same. What is different, and can be a lot more challenging, is rendering. Because your canvas is 2D, you cannot just place things defined in a 3D space on it. You need to translate the points in 3D space to 2D space somehow. This is called projection, which dates back to the 15th century during Brunelleschi’s time. Projection and 3D Rendering A post shared by kyndinfo (@kyndinfo) Also, to make things look 3D, lighting or shading (basically the same thing — you add light to the scene, which creates shade too) plays an important role. Without light and shade, a sphere would look like just a circle. Shadows casting from one object onto another will clarify their spatial relationship. Light It is your choice of style though. You can choose to draw your scene flat and cartoonish. You can go up to higher dimensions. No matter how much you go up, vectors will help you keeping the math the same. Thinking about higher dimensions not usually for graphics, but any data that can be represented with multiple numeric parameters are thought as a vector in multi dimensional space, with each parameter corresponding to a single axes. Then you can apply familiar concepts and operations to the data, such as addition, multiplication, calculating distance and angle, etc. In fact, large language models (LLM) represent concepts like “dog,” “human,” “run,” “happy,” etc. as points in a mega-multidimensional space (like, 3072 dimensions), and you can do math between them to, for example, measure conceptual distance or similarity between two words or sentences. Getting familiar with this is critical for understanding how current AI models work. Dimensions Colors can be represented with a series of numbers, such as R, G, and B. So they are vectors, and they live in a space. In fact, people have invented many different spatial models to represent colors “accurately” for different purposes. Imagining the relationship of colors spatially helps grasping them holistically and intuitively. Changing the hue is rotation, brightening is ascending, etc. Each different color model organizes colors differently, offering different mental models. Choosing the right model for your purpose can be critical for making your work easy and look good. Colors and Numbers In a space, you can measure distances between points. The most common way is the Euclidean distance, which is basically the length of the straight line between two points. In math, there are various kinds of spaces and some of them don’t have the concept of distance. But that is not the only distance we care about. How do you measure the distance from Tokyo to São Paulo? What you want is probably not a straight line distance between two points, but a curved length along the surface of the Earth. This is called spherical geometry. How about a checkerboard? How do you measure distances between two squares? Perhaps the number of moves it takes? Various distances The concept of distance becomes important in various cases, for example in finding the shortest path to move characters along in game design, or dividing territories based on the distances. Visualizing Distances In computer graphics, functions that returns the distance from a point to the surface of object, called signed distance functions, or SDF are often used for 3D rendering. Signed distance functions So, what is space, after all? The answer depends. For most of our purposes when sketching something with code, Newtonian or Euclidean spaces provide the most intuitive and familiar way of thinking about space — flat, predictable, and measurable with straightforward distances and angles. But this is neither the only space nor the true shape of space. It’s not the true shape of space because Einstein’s general relativity describes spacetime as a curved, four-dimensional manifold. Meanwhile, quantum physics reveals that at the smallest scales, space and time behave in probabilistic, discontinuous, and often counterintuitive ways. Scientists have shown that our naive, everyday understanding of space and time breaks down at both the micro and macro scales. It’s not the only space because mathematicians have been generalizing and expanding the concept of “space.” Riemannian geometry broadens the idea of geometry to include uniform and non-uniform curved spaces, including Euclidean, spherical, hyperbolic, and other geometries we touched upon in the distance discussion above. Space isn’t even just about distance or geometry. Topology focuses on how shapes are connected, regardless of size or distance. Mathematics even defines spaces that aren’t intuitively spatial at all, like the space of all polynomials of a certain degree, where the focus is on the coefficients rather than the shape or a curve. If you’re interested, go forward and explore these fascinating concepts. Color is a fascinating topic that lies between science and human perception. Physically, we know how humans perceive colors. We have photoreceptors called cones in our eyes that react to different frequencies of light, and the levels of stimuli to these three types of cones determine what color we see. The coverage of each type of cone roughly matches with the spectrum of red, green, and blue; that’s why RGB displays can show us various colors even though they have only three colors of light-emitting elements. Spectrum and Cones However, this model doesn’t necessary explain what we actually experience. For example, the violet and red at both ends of the spectrum are actually continuous, with purple in between them. We see different brightness depending on the hue even when the amount of physical light is the same (in general, yellow looks much brighter than blue). We even associate different feelings, emotions, and meanings to the colors. People have been creating different models to describe colors. These are roughly divided into two categories: ones that are mainly derived from physical properties, and ones are more based on the ways we normally describe colors. The first challenge in systematizing colors is that different people have different responses to the spectrum of colors. To find a sort of average, called the standard (colorimetric) observer, an experiment called Wright-Guild color matching was conducted. The basic idea is that the participant is presented with a monochromatic color, which is a color with a single frequency of light, and asked to reproduce the same color by mixing three primary colors (red, green, blue) to match with a target color. The tricky part is that the three physical lights can’t reproduce every color. The participants could match a lot of the target colors just by adjusting the brightness of each primary. But for some colors, they couldn’t get it right unless they actually subtracted one of the primary lights. There isn’t such a thing called negative light, so they solved this by adding the “negative” light to the target color side instead of the mixing side. To better understand this, look at the chart above again. Because each cone covers a relatively broad range, there is no light that can stimulate them independently. There are inevitably certain balances of cone stimulations that can’t be achieved using just a combination of three lights. They eventually transformed the data into something called the CIE 1931 XYZ color space, which is basically a remapping of the RGB measurements to get rid of the negative values by picking new “primaries” that were more imaginary concepts than actual lights. CIE Color spaces Most color models today are based on this CIE color space, meaning that the values in a system can be converted to values in XYZ. The RGB color model that we use on computers today completely fits within the XYZ color space (because by definition, XYZ space has all the human-perceivable colors and even more). Some other ways of describing colors are based on the ways we talk about colors. When we see colors, we say which kind of colors they are (reddish, yellowish), how bright or dark they are, how saturated or vivid, or dull or grayish, etc. The Munsell color system is a system to break down colors by three attributes: hue (such as red, blue, and green), value (lightness), and chroma (vividness or purity of the color). The many color models we use on computers are similar to Munsell’s model, but are designed to be able to be mapped to the RGB values on the screen and thus to XYZ values. Various Color Models These models allow us to compute colors. For example, we can add, subtract, multiply, or rotate colors. What operations make sense and the results you get vary from model to model, and it is important to select a model for your purpose. Computing Colors Being able to process colors to get the desired tone is a critical skill in image and video production. Color Processing Mixing colors is its own deep world. There are so many ways to mix colors. If you are familiar with tools like Photoshop, you probably know there are a bunch of “blending modes” such as add, multiply, screen, overlay, etc. These are different ways to calculate a new color based on the input colors. Most of them are defined with RGB color model. Using the other models can yield different results. Mixing colors in the real world is even more complex, because the color mixing in the real world is the result of various materials reflecting, refracting, and absorbing light. There isn’t a single formula to solve this, but the Kubelka-Munk model is known for emulating real paints consisting of a medium and different colored particles inside. Mixing Colors Light striking the retina lets us see colors. To fully understand how colors happen to us, we must understand how light travels from its source to our eyes, bouncing off surfaces while reflecting and refracting along the way. Reflection and Refraction When the light hits an object, the object absorbs some of the light and reflects the others. On transparent objects, some also passes through. Different materials behave differently. If the material absorbs red and yellow spectrums of light more than blue, then the object will look blue. Rough surfaces will scatter the light in all different directions, thus you don’t see clear reflections of other objects on them, while smooth surfaces will mirror the surroundings more clearly. There are many different methods to simulate this process, from very rough to more accurate approaches. Studying these methods can help you understand how we see colors on objects and better control the colors and appearance of your rendering. Illuminating objects The last section is just an example of using these theories in practice. This installation for KAF Digital space mixes several methods discussed above to fill the space with colors that have the more meditative quality of Fangor’s oil painting, appearing less digital and flashy. A post shared by kyndinfo (@kyndinfo) Filling Space with Colors One of the most fun parts of sketching with code is that it’s not static. Let’s draw both in space and time. How do you move a thing? Just push it. Sir Isaac Newton basically said that things stay at rest in the same position, or keep moving at the same velocity until you apply force to accelerate them. Position, velocity, acceleration, and force are the most fundamental concepts to understand physical motion. Emulating these concepts is extremely simple: frame by frame, just add force to acceleration to change it, then acceleration to velocity, then velocity to position. Newtonian mechanics Position, velocity, acceleration, and force are in relationships of differentiation and integration. Differentiation is the rate of change, and integration is the accumulation of changes. For example, velocity is a quantity that describes how much position will change at a certain moment, and position is the result of velocity being applied and accumulated over time. These subjects might have been annoying in math class. But the foundational ideas are quite simple, and luckily in sketching motion with code, you usually don’t have to solve any equations, but instead can use a very rough approximation called numerical integration, which breaks down the changes into simple additions per small amount of time. You probably have been using this method already without knowing it, for example, the Newtonian physics simulations above are based on a numerical approach instead of mathematically accurate methods. DifferentiationIntegration The proper way to solve differentiation and integrations by solving equation is called analytical solution. Then the approximation by dividing the changes over a small amount of time is called numerical approximation. As mentioned above, the latter is the most common and actually more useful in so many cases, but understanding the relationship between these two approaches is important since both have their own merits and drawbacks. The numeric approach is preferred not just because it’s easier, but because it’s also more flexible. Solving motion through equations is like predicting the future. Once solved, you know exactly what will happen at any moment. But it’s often difficult or impossible to account for all variables. Even calculating the movement of three objects drawn by gravity becomes unpredictable. Any system involving external factors, like human interaction, cannot be determined in advance. Since the numeric approach calculates changes bit by bit (very often frame by frame), it is pretty easy to react to unpredictable changes by updating parameters in real time. The prediction of the gravitational motion of three objects, made popular by Cixin Liu’s bestseller The Three-Body Problem, is called the Three-Body Problem. While solving the two-body problem (like the Earth and Moon) is relatively straightforward and has a neat, predictable solution, the three-body problem is usually chaotic and unpredictable. A common drawback with numerical methods is their inaccuracy. Because these methods approximate smooth motion through discrete changes, the results can go slightly off from physically accurate motion. There are several ways to minimize this problem. The most popular approaches are called the Improved Euler method and Runge-Kutta method (the Euler method is actually a special case of Runge-Kutta). Continuous Time and Discrete Time Simulations are, by nature, hard to predict precisely, because that’s the point. They’re meant to create or mimic phenomena that can’t be fully described in advance. We simulate weather, for example, because we don’t know exactly what will happen. But this can be a problem when you want more direct control over how things move, where they end up, and when. Also sometimes, you may even want to break the laws of physics. Great animators and motion designers often exaggerate movement and timing, bending or ignoring realism to create something more expressive or impactful. A very common technique in motion design is called tweening, short for in-betweening. You start from defining a start and end state, then fill the intermediate frames by interpolating between them. The word comes from the days when animators hand-drew the frames that filled the gap between key poses (many still do — some expressions simply can’t be captured any other way). In computer animation, we often use easing functions to control these transitions. Easing functions connect one value to another, changing the pace in between, like slowing in, speeding out, bouncing, etc. In motion design, you can think of an easing function as a kind of mapping between time and another value, such as position, scale, or opacity. Interpolation and Animation Things don’t just move freely in the real world. On Earth, everything is bound by gravity. Our body movement is constrained by the structure of bones and muscles, but this constraint is what lets us move around and lift things using our bones as levers. Adding constraints to objects is a way to add reality to the motion. Verlet Integration is a way to create simple physics simulations by connecting dots with sticks which constrains the distance between two points. Though simple, this technique is very flexible, and by combining multiple points and sticks, you can create various kinds of objects from rigid bodies to soft string-like objects. Building Ragdolls To control movement more precisely and intentionally, you might want to study kinematics. Forward kinematics and inverse kinematics are common techniques used in robotics, games, and so on to describe the motion of jointed parts, such as mechanical arms or human bodies. Both methods handle structures made of multiple segments and joints to connect them. Forward kinematics starts from the root of the connection and propagates the posture going forward to the end. You directly manipulate the rotation of each joint, and that will determine the overall movement. Kinematics Inverse kinematics is more goal-driven. You define where you want the endpoint to be, then the algorithm figures out the rotation of each joint that makes the desired result happen. Kinematics Motion of objects and life in the real world is often unpredictable, and you can always introduce some randomness to your design. The demo below shows four random walkers with different methods to generate random values. In real applications, you might want to combine more predictable methods like above with some randomness. Depending on your goal, play around and find a sweet spot. Taming Randomness While we’ve explored how to move objects and characters, there’s one more crucial but invisible actor to consider: the camera. How you frame the scene and position the viewpoint dramatically shapes the final result. Camera in Action When you sketch with motion, you’re not just placing things in space but you are giving them life in time. Whether you follow the rules of physics, break them for expressiveness, or invent your own systems, motion opens a lot possibilities for exploration and creativity. If you’re curious, keep experimenting and see where the next movement leads. A post shared by kyndinfo (@kyndinfo) Human perception has evolved over a long time to give us useful information about the world. But what we perceive is only a little sliver of what’s actually out there. For example, light, or electromagnetic waves more broadly, spans a vast range of frequencies, but we can only see a small part of that spectrum. The reason we probably settled on this specific range is that it was good enough to detect and distinguish everyday objects, plants, and animals, helping us survive and navigate our environment. Another reason is more physical: many parts of the electromagnetic spectrum, like X-rays or gamma rays, tend to pass through objects or scatter unpredictably. That makes it difficult, if not impossible, for animals to evolve organs that can capture them. The way we perceive different physical phenomena doesn’t always match their actual physical nature. Light and sound are both waves, but we almost never experience them as vibrations (except for very low-frequency sounds). In both cases, frequency affects what we perceive, but in very different ways. Sound frequencies are perceived as pitch, and we describe them as “high” or “low.” On the other hand, different frequencies of light appear to us as different hues of colors without any sense of high or low frequency. Studying perception is fascinating because every person’s experience of the world is different. No one else sees exactly the same colors you see. Your perception exists only within you and doesn’t exist without you. For this reason, the study of perception can’t be purely objective. We have to ask people how they feel. In fact, the color models we use today are based on data collected from many people comparing colors and describing how similar or different they appear. Some people may extend this to say the world doesn’t exist without you, or it is impossible to understand someone else. Let’s not go there yet. How we perceive and use color is shaped by many different factors. First, our biology. As we touched upon earlier, we perceive different light frequencies as different hues, not as a one-dimensional gradient from red to violet, but arranged in a circle. This is because we perceive colors based on the balance between three receptors (cones) for different ranges of frequencies, each roughly corresponding to red, green, and blue. The actual light is a mixture of different frequencies. If a light contains more blue and red but less green, we perceive it as purple, a color that doesn’t actually exist in the spectrum itself — below red is infrared, and above violet is ultraviolet. This reminds us that color might be more about what’s happening inside us than something intrinsic to the light itself. Colors are also cultural and social. Despite not knowing exactly what other people are seeing, we tend to generally agree on how we see colors. We can think of this in terms of semantics, such as red or yellow-and-black stripes for danger, to feelings and aesthetics like “blue feels cool” and “green is relaxing,” to what feels pleasing versus jarring. These associations likely started from nature, where certain objects and phenomena have specific frequencies, and then were influenced by cultural aspects, as certain colors are used repeatedly in specific ways to reinforce our associations. For example, unripe fruits are usually green, and they turn yellow, orange, or red when they are ready to eat, and we’ve been reinforcing these associations by using these warm colors on food packages, signboards, etc. Our view on colors has also been significantly influenced and constrained by our tools. People have been working hard to reproduce and create various colors—from using plant-based dyes, to making expensive gemstones into paints, chemical and synthetic compounds, to analog and digital displays, and even nanotechnologies to create structural colors (such as iridescence). As colors became more available, they got more involved in our culture and even economies, creating trend cycles of colors to drive consumption. The fact that people don’t see colors in the same way cannot be emphasized enough. It is estimated that there are approximately 300 million people in the world with color vision deficiency. I’d even hesitate to call it a ‘deficiency’—it’s really just a different way of seeing. We often use charts like the one below to simulate different perceptions (the demo is emulating deuteranopia). This probably does a good job of illustrating where people may have difficulty distinguishing colors, but this is not representing the world they’re seeing. It’s not true to say the world appears mostly yellow and blue to people with deuteranopia. Representing someone’s vision for someone else is impossible, but at least understanding how we see colors and how to manipulate them systematically can help us imagine and approach this challenging and fascinating problem. Computing Colors All of this shows that color perception is a complex mix of physical phenomena, evolutionary quirks, instinct, culture, and tools from pigments to pixels. And all of it is, in a way, somewhat arbitrary. It’s just the way the world and we humans happened to turn out. Depending on the angle you look from, you can describe color in many different ways. That’s why we have so many color systems and models. We are only scratching the surface of this topic in Colors and Numbers, but there are tons more that we can discuss. I recommend the following book for the history of color theories and their use. The History of Colour: A Universe of Chromatic Phenomena This comprehensive, beautiful book delves deep into the complex but fascinating story of our relationship with colour throughout human history. Amazon Sound is also a wave. Unlike color, we can actually feel its frequency more directly, though we don’t always notice it as physical vibration. Lower frequencies feel “low,” and higher frequencies feel “high.” This height of sound is called pitch. Interestingly, we don’t experience pitch as just a straight line—it feels more like a spiral. For example, if you start at 440 Hz (the A note) and go up, you’ll eventually hit another A at 880 Hz at exactly double the frequency. It’s not the same sound, but we hear it as the same note, just in a higher octave. Western music typically divides the octave is into 12 pitches, each spaced evenly in terms of frequency ratio. Other musical cultures divide the octave differently. Some use more notes, some fewer, but the concept of the octave itself shows up pretty much everywhere. Pitch and Frequency Not just pitch, we can also tell sounds apart based on their timbre, which is like the texture or color of a sound. That’s how you can tell apart different sounds, like a bird chirping, car horns, or instruments like piano and trumpet. In Japanese, timbre is called 音色 (on-shoku), which literally means “sound color.” Both pitch and timbre are tied to the physical characteristics of the thing that’s making the sound. Let’s take a guitar string as an example. The pitch depends on its length and weight. The longer or heavier the string, the lower the pitch. But the sound a string makes isn’t just one pure frequency. When you pluck it, the string vibrates in many ways at once. The main pitch is called the fundamental frequency, but parts of the string also vibrate in smaller sections—halves, thirds, quarters, etc. These create harmonics or overtones, which are simple multiples of the base frequency. This happens with any object, though the mix of frequencies might be a lot messier. Just like colors are usually a blend of frequencies, sounds are too. But unlike color, where a certain frequency always looks like the same hue (e.g., 440 THz light always looks red), sound doesn’t work that way. A guitar sounds like a guitar no matter what note it plays, because its timbre comes from the relationship between the overtones, not the absolute frequencies themselves. That’s why we can tell a violin from a flute, or recognize someone’s voice, or know what kind of object made a noise, based on how that sound’s overtones are structured regardless of the frequency. You can sing a high note or low note, and you will still sound like you. It is interesting to think about this from an evolutionary standpoint. Our hearing has evolved to react to frequency ratios probably because that was useful in identifying different sources, people, animals, objects, etc. This sensitivity also lets us enjoy the different tones of sound and the relationships between different pitches of sound, which is known as harmony in music. Understanding wave properties helps us better comprehend the relationship between physical vibrations and the sounds that we hear, and sketch ideas playing around with and visualizing sound. Sound visualizationSine waves and Additive SynthesisFourier Series Something that I think is understated, but has been a very useful insight for sketching and designing things, is that the relationship between physical quantities and their impact on human perception is logarithmic in many cases. For example, the strength of the light we feel is depends on the number of photons our eyes catch. But twice the photons doesn’t mean it feels twice as bright. Physically, a sunny day outdoors can have 200 times more light than a well-lit office. But does it feel 200 times brighter? Probably not. The same goes for sound. Loudness is related to the energy of the sound, which is directly related to the amplitude of the sound wave. But again, the relationship to the way we feel is not direct. A subway might have 100 times more sound energy than someone talking, but it doesn’t feel 100 times louder. This can be seen in a couple of ways. Life had to survive across wildly different conditions, like daylight and moonlight. So our perception systems likely evolved to compress all that information into a manageable scale. The other way to look at it is that when numbers are huge, small changes matter less. Imagine you’re stuck in traffic — a 10-minute delay feels huge when your trip is only 15 minutes, but it doesn’t matter as much if you’re going for a 5-hour drive. Similarly, adding a candle doesn’t help much when you’re already under the sun, while it can make a huge difference when you’re in a cave. Physical values and human perception Humans could have evolved more sensitive eyes like cats to see in darkness, but we haven’t. Perhaps our ancestors were not as nocturnal as cats, or they weren’t nocturnal because they didn’t have cats’ eyes. Either way, we managed to survive.There is an important nuance here. Life doesn’t evolve with purpose. A statement like “giraffes acquired long necks in order to eat leaves high up” is misleading. The reality is more like something random happened to the genes, and that change happened to give the genes themselves a better chance to be passed on to following generations. This taps into another important topic. Resolution is how detailed something can be represented or perceived. There is a certain limit to our senses regarding how small a difference we can detect, or how finely we can distinguish things from one another, and various technologies actively exploit these limitations to deliver experiences. For example, computer displays are made of small dots. But when these dots are small enough — somewhere around 300 DPI（dot per inch) at reading distance, or a viewing angle of 1 arcminute (1/60 degree) — our eyes and brain stop seeing the dots and start to see images and characters. Movies have only 24 frames per second, but that’s enough to make us believe they are smooth and real. The majority of digital audio uses 44,100 samples per second, which means a second of sound is represented by 44,100 numbers. This rate was selected because it is enough to reproduce sound up to 20,000 Hz, which is about the upper limit of pitch that humans can hear. If humans had different capabilities, these technologies could have been designed differently. An interesting part is even when there isn’t enough resolution, our brain often try to see continuity by complementing the missing details. Early monitors were much more coarse and the pixels were far more visible. But people were accepting the quality, and there is even appreciation and nostalgia for the low resolution graphics aka 8-bit style. Animations, especially cartoonish ones, can utilize a lower frame rate to add unique effects. Sometimes, letting the brain fill in the gaps is better than trying to draw everything in detail and having the illusion fall apart by having discrepancies here and there. When we sketch with code, we are always engaging with how humans perceive things. Our senses are full of quirks. They compress, filter, and interpret the raw physical world in different ways to make sense of it. Understanding how human perception works helps us see things and make things from different perspectives. It feels fascinating and freeing that when it comes to human perception, there’s no one right model, just many ways to look at things. Embrace the differences and complexity, and have fun. Humans are wired to spot patterns. Our ancestors survived by recognizing various footprints to avoid danger and find food, spots and dots on fur, rotational patterns of petals, cycles of the sun, moon, stars and repeating seasons, tides, and weather. Repeating patterns are fundamental to design and art. We see them everywhere, like in the repeating columns of ancient buildings, brick patterns, glass tiles on skyscrapers, and street pavements. Checkers, stripes, and other repeating motifs appear universally in decoration and textiles. Musicians create structure through rhythms, phrases, and chord cycles. Patterns are powerful vocabulary. Following patterns makes things predictable. It is often relaxing and comforting, but intense rhythm can also bring energy and even trance. Breaking a predictable rhythm creates tension, drawing people’s attention, and going back and forth between tension and familiar patterns is a way to drive a song or a story. At the basis of patterns is repetition. We recognize patterns because the same thing happens again and again. There are many methods to create and handle repetitions mathematically and programmatically. For example, modulo is basically a mathematical way of saying “count to n then go back to one and repeat,” which is just dividing a number by another number and taking the remainder. This is common in everyday life, like the time on a clock or the days of a week, and is a basis for many repeating patterns, such as repeating tiles and musical rhythms below. This can be easily expanded to repetition of continuous values. Repetition The sine wave is another fundamental building block for repeating patterns. A mind-blowing fact is that with an infinite set of sinusoidal waves, each with its own frequency, amplitude, and phase, you can synthesize any pattern in theory. This idea is called the Fourier series for repeating patterns and the Fourier transform for ones that don’t. They are so universal and useful that you’ll bump into them in many places, such as audio and signal processing, and a whole lot of physics. Sine waves and Additive SynthesisFourier Series Tiling is one of the most obvious visual patterns. By repeating the same elements in a space, or dividing a space based on some rules, we can create various interesting patterns that can continue forever. The patterns can be simple repetition of the same geometric shapes to something more unique and interesting like M. C. Escher’s drawings. There are patterns that are not exactly regular and predictable like Penrose tiles or Truchet tiles. A Voronoi diagram is a method to create more organic patterns. Cells in Voronoi diagrams are all different, but humans can still see that they follow the same rules or a “pattern.” Just like when you look through a microscope, no cells in nature are exactly the same, but we can intuitively see they are the same thing regardless. Instead of dividing space, we can create musical rhythm by dividing time. Even within the same tempo and time signature, time can be divided differently, and the way you divide dictates the “feel” of the rhythm. Experienced musicians have a finely tuned resolution of time, meaning they can feel this division more granularly and accurately and precisely position their notes. Many players also adjust the timing slightly off from the exact timing naturally or intentionally to create add more nuances, such as heavy or bouncy rhythms. Resolution – Rhythm Click on “Run Pen”, then click one more time to play the sound. Music has many layers of repetition beyond basic drum loops. From guitar riffs that stick in your head to rhythmic and melodic patterns of phrases, song structure is built by repeating different sections, such as verses, bridges, and choruses. There is so much more that I can write here that we can talk about. Again, nothing follows a perfectly regular pattern. Randomness is a great tool that can help make our sketches more interesting and natural. Randomness is, by definition, hard to predict, but that doesn’t mean it’s uncontrollable. There are many different ways to “shape” randomness for your purpose. Let’s take a die as an example. A single throw of a fair die is completely unpredictable. The probability for all possible results is equal. But if you throw it twice and sum the results, you will get more 7s than 12s. The more times you throw a die, the more the sum or average will converge to a certain expected value. This is called the Law of Large Numbers. You can also apply some function to random results to adjust the distribution. For example, if you take a square of a random floating number from 0.0 to 1.0, you get this shape. Can you see the difference between these two cityscapes? These are made by using the exact same regular grid with two different functions that give them subtly different impressions. In most cities, building heights in the suburbs show little variation because nearly everything is only a few stories tall. Downtown, though, the range is much wider. While many buildings are still modest in height, a handful of skyscrapers rise far above the rest. You can think of this as a “pattern” that can repeat across many cities. Taming Randomness Noise functions are methods to get randomly but continuously changing, like the up and down of terrain, the change of temperature, or the direction an animal wanders. These are all unpredictable, but unlike dice rolls, they look smoothly connected and values next to each other seem to be somehow related. There are many noise function algorithms, but most major ones share a simple idea of smoothly connecting random numbers. So the values are random, but if you pick a point close to another, they return relatively close values, and there is no abrupt jump, or discontinuity between them. Noise functions are often layered by adding them together at different scales to create a more natural look. Imagine a mountain range. From far away, you see its jagged silhouette. As you hike closer, you start to see smaller ridges, and up close, you notice every rock face has tiny cracks and ledges that are similar to larger forms. It is quite a common pattern in nature for things to repeat similar shapes at many levels of zoom. You can find this in rolling clouds, breaking waves, cracked desert ground, leaf veins, and the shape of coastlines. Taming RandomnessReading a Noise FunctionDrawing Landscape This common pattern is called fractal. A fractal is a pattern or shape that repeats the same structure at different scales, in such a way that a part of the shape resembles the whole, and a sub-part of a part resembles the larger part. Layering of noise functions can be seen as a way to emulate the fractal nature of the real world with a finite number of computations (for example, the landscape demo above is made of 9 iterations of the same noise). Some mathematically defined fractals, such as the Mandelbrot set or Julia set, have literally infinite details. The real world is messy with infinite detail. When we see or hear things for the first time as a baby, we can’t make sense of them because they are like a raw stream of data with no structure. But as we keep experiencing similar things appear and occur, our brain starts to grasp patterns. We no longer track every speck of bark on a tree, but we just think “tree.” This is a shortcut to let us deal with overwhelming complexity and act quickly. It works the other way around, too. When information is missing, like in a blurry photo or garbled phone call, our brain tries filling the gaps with its library of familiar patterns. Resolution Does this sounds like data compression? Yes, many technologies like ZIP, JPEG, or MP3 borrow the basically same trick. They hunt for repeated structure in data, toss the redundant bits, and keep a smaller blueprint. Decompressors rebuild the original (or close enough) by reading that blueprint plus a few rules, just like our brain filling in a pixelated image. I wanted to study and write an article about information theory and data compression. I might, someday. And AI image generation. The foundational idea of diffusion models is to show the AI the process of adding noise to an image until it becomes unrecognizable, and have it learn the backward process to re-imagine plausible details of images from noise. “Pattern” is a broad word. We started from simple repetitions and reached our recognition of the noisy world. At a meta level, patterns what let us make sense of the world. They are so crucial for us to grasp and organize information. But at the same time, it is also important to remember that leaning to match on patterns can make us overlook the details that matter, sliding into stereotypes and snap judgments. We need to free ourselves from familiar patterns sometimes. A fun and useful practice is to jump between levels. Spotting big patterns while appreciating the subtle differences, nuances, and noise without falling into excessive generalizations. Master that dance and you’re using your built‑in compressor to its fullest, while staying aware of both its strengths and its traps. Yes I’m taking about both life and art. Interpolation is about estimating the values that lie between the points you already know. When you’re moving a character from point A to B, you interpolate between them so you know where the character is at a given time. You know the stock price in May and July, so you interpolate between them to estimate what the price was in June. The idea is so simple, and applicable to so many things. Let’s start from the simplest case where the value changes at the same rate all the way. Suppose that point P moves from one point A to another point B, and the variable t is the ratio how much P has moved between them. If t = 0, P is in the same position as A. If t = 1, P is in the same position as B, and if t = 0.5, P is exactly at the midpoint between A and B. This can be expressed as follows. A, B, and P can be either numbers or vectors. Interpolation and Animation – Linear Interpolation If we interpolate between points in 2D, we can draw a line segment. But things don’t always change at the same pace. To manipulate the rate, we use different functions called easing functions. Easing functions are not a clearly defined category, and they can be any function that can map a range of values (usually 0 to 1) to the same range continuously, keeping the start and end the same. MappingEasing functions On a one dimensional line, you can draw only a single path between two points, but in higher dimensions, like 2D or 3D, there are infinite ways to connect two dots. Defining a path between points in flexible but precise way is very crucial for computer graphics to draw lines and shapes. One of the most common methods called Bézier curve is basically to repeat linear interpolations multiple times. The demo below illustrates this idea. Bézier and Spline There are many different methods to draw curved lines similar to Bézier curves but with more advanced features. For example, B-spline introduces additional parameters to adjust the weights of each point to add more nuanced control over the line’s shape. Bézier and Spline – B-Spline When we draw curves, we often care how smooth they are. What smooth means can vary, but to deal with a specific kind of smoothness, there is a very useful mathematical concept called continuity. To measure how smooth a curve is in a mathematical sense, you can take the derivative of the function that represents the curve. The first derivative shows the rate of change of position, and the second derivative shows the rate of change of the rate of change. An algorithm called natural spline is designed to guarantee C2 Continuity at anywhere through a curved line goes through multiple points. While a natural spline is mathematically smooth, it may not always look like the smoothest line to human eyes. The Hobby curve algorithm minimizes curvature at each point along the line, making strokes that look like Henri Matisse’s stroke drawn with a very long brush. Controlling curves For connecting lines or interpolations smoothly, Hermite interpolation, or so-called smoothstep functions are often used. When t moves from 0 to 1, these functions can adjust the rate of change to make the derivative at the beginning and the end be zero. The first formula guarantees C1 continuity and the second one is for C2, the last one is the general form for any level of continuity. The formulas themselves are not very intuitive, but if you look at the graphs below, you can visually confirm that the rate of change becomes zero. The graph to the left shows the first derivative, and the one to the right shows the second derivative. For example, these demos for the noise functions from the Taming Randomness page use the Hermite functions. Instead of trying to control the interpolation precisely, there are methods to use algorithms to figure out the exact way to interpolate between points. This is particularly useful when there are multiple data points spread across a space. RBF interpolation is one of those methods. It solves for weights automatically and produces a smooth surface that passes through however many points you throw at it, even in higher dimensions. Take a look at the demo of RBF interpolation in 2D space. You can drag white points to play around. Continuous Mapping Interpolation is not just about moving things and drawing lines. Colors are another good example. You can mix two or more colors by interpolating between them. Your result can be different depending on which color model you use. You could even draw a curved line in a color space to get an interesting gradient. You might have seen a menu to select (re)sampling method when you are resizing an image in graphics software like Adobe Photoshop. These software programs take colors from the closest pixels and interpolate between them. In case of Photoshop, “Nearest Neighbor” means no interpolation (just pick a color from a single closest pixel), “Bilinear” linearly blends the 2 × 2 neighborhood, “Bicubic” uses a cubic function to give it C1 continuity. Mixing Colors As you might have noticed already at this point, you can interpolate between anything that can be represented as vectors. By seeing things through the lens of concepts like vectors and interpolation, we can treat many things in the same way (that is the power of abstraction). Interpolation is indeed a powerful and versatile technique to add to your toolbox. There is a very similar and related concept called extrapolation. As the name suggests, extrapolation is to extend a line beyond the ends. Extrapolation are used to predict values beyond the boundary of known data points (predict the future!). It’d be interesting to explore this topic too if you are interested. Waves are everywhere. I suppose the word originally meant just waves on the water surface, like the ocean or a lake. However, there are many other things that exhibit characteristics of waves, such as sound, earthquakes, electromagnetic waves, and light. In terms of design and graphics, wavy shapes are one of the most common patterns and are often seen in fashion, architecture, and various art and craft pieces. Wave Mathematically, the sine wave is the most fundamental wave shape. It represents the position of a point on a unit circle as the center angle changes. A sine curve can be drawn as a result of a physical object following the spring equation (Hooke’s law, F=-kx). Basically, this means if you vibrate a thing attached to a spring and plot its position over time, you get a sine curve. It is fascinating that these seemingly very different methods result in the exact same curve, which indicates that there’s something special about the sine curve (yes, it is). Waves propagate and spread in space. Many waves are mechanical, meaning they are caused by physical materials pushing against each other. When you pluck a guitar string, it vibrates and pushes the air around, which pushes the air next to it, and so forth. Sound is propagated across a room as oscillations of air pressure this way. Earthquakes, ocean waves, etc., all belong to this category. Direction of the wavesVibration and Propagation Electromagnetic waves are a different class of wave that needs no material medium at all. Electromagnetic waves have properties of both particles (photons) and waves. Light, radio, microwaves, X‑rays, etc. are the same phenomenon at different frequencies. What oscillates is simply the electric (E) and magnetic (B) fields themselves. What is light? Even without a medium, electromagnetic waves still obey the same wave physics, such as reflection, refraction, interference, diffraction. The concept of “wave” is an abstraction to capture these common characteristics between different things and phenomena. It’s not that they behave the same because they are waves, but we call them waves because they follow the same rules. For example, electromagnetic waves are diffractive, meaning that they can go around behind obstacles just like waves on the surface of water. Thanks to diffraction (along with reflection and scattering) we can watch TV or use cellular phones even when there is an obstacle between the antenna and the devices. What is light? Electromagnetic waves between about 400–800 terahertz are what we normally think of as light, and they are visible to human eyes. Depending on the frequency of light, we see different colors. Understanding how different frequencies of light interact with objects before they reach our eyes is crucial for thinking about our vision and how we model these phenomena to create graphics. Spectrum and ConesLight Sound is also a wave. When you clap your hands, pluck a string, or hit a drum, it makes the air vibrate. In the case of light, the frequency corresponds to the color. In sound, the frequency corresponds to the pitch, how high or low it sounds, and the amplitude affects the loudness. “Seeing” the sound wave helps us understand the sound a lot. For example, you can see how different frequencies of sine waves sound, or how various sounds from different instruments look. Sound visualization Musical intervals are based on frequency ratios. The equal temperament commonly used in Western music divides an octave evenly into 12 semitones. No matter where you start on the piano keyboard, the ratio of the frequency from one note to its right neighbor (e.g., C to C#) is 21/12. With the semitone as a unit, we can think of the distance between notes as a simple number relationship. Interestingly, we don’t experience pitch as just a straight line—it feels more like a spiral. For example, if you start at 440 Hz (the A note) and go up, you’ll eventually hit another A at 880 Hz at exactly double the frequency. It’s not the same sound, but we hear it as the same note, just in a higher octave. Upon these notes, we can construct different harmonies. It is all about ratio and relative relationships, and depending on how you lay them out vertically (play at the same time) or horizontally (play in sequence), you can evoke totally different feelings from the same sets of notes, or frequencies. In the slightly more song-like example below, the chord (F major 9th) chord is repeated every second time. Notice how the same chord can sound different depending on the preceding chord. Pitch and FrequencyNumbers and IntervalsThe secrets of sound (for kids) Other than the pitch and loudness, sound also has timbre, which is like the texture or color of a sound. That’s how you can tell apart different sounds, like a bird chirping, car horns, or instruments like piano and trumpet. Timbre comes from the fact that most of the sound don’t consists of a single frequency, but rather a mix of many different frequencies. And the balance between these different frequencies determines the timbre. Let’s take a guitar string as an example. When you pluck it, the string vibrates in many ways at once. The main pitch is called the fundamental frequency, but parts of the string also vibrate in smaller sections—halves, thirds, quarters, etc. These create harmonics or overtones, which are simple multiples of the base frequency. We can visualize this too. In the demo below, you can see the shape of the wave changes as we add extra sine waves with different frequencies and amplitudes (the radius of the circle), in this case, getting closer to a so-called square wave. Sine waves and Additive Synthesis I wish I had added sound playback to these demos. I may try later. You can compare the sound of square wave vs. sine wave on this Wikipedia page: https://en.wikipedia.org/wiki/Square_wave_(waveform) This bring us to the favorite topic of synthesizing everything with sine waves. If you have an infinite set of sinusoidal waves, each with its own frequency, amplitude, and phase, you can synthesize any periodic pattern in theory. This idea is called the Fourier series for repeating patterns and the Fourier transform for ones that don’t. For example, an arbitrary shape, like an alphabet can be written as a Fourier series, or a series of waves. The Fourier transform is widely used in various fields such as sound processing, signal processing, and image compression. By applying Fourier transform to sound data, we can determine the amount of different frequencies contained within it. This lets us manipulate the data by reducing or increasing a certain range of frequencies. Sine waves and Additive SynthesisFourier Series The Navier-Stokes equations describe the properties and behavior of media such as water and air, including mass conservation, momentum conservation, and viscosity. This is a fundamental theory that explains how waves work in these media, but that’s not all. In computer graphics, these equations are often used for fluid simulation. Understanding and simulating waves using the Navier-Stokes equations can be a fun and interesting research topic. Fluid Simulation Shapes seem like an obvious topic. Since they are ubiquitous, we take them for granted. Many tools provide ways to draw shapes, and many of them are pretty simple, like these functions in p5.js. But as soon as we look more closely, things get complicated. In fact, there are infinite ways to define and draw shapes, and most of the shapes in the wild, from industrial products to nature, have so many nuances. Let’s start with geometric shapes that are (relatively) easier to define mathematically and handle with code. It is easy to draw line segments, circles, etc. in various environments including HTML canvas. However, it is not so easy to draw figures like those in geometry textbooks. Even a simple operation such as drawing a circle around the intersection of two straight lines requires a bit of calculation. The ability to draw a picture and the ability to handle geometry mathematically are two different things. Of course, there are many well-designed libraries out there, but it is also a good learning experience to implement basic functions by yourself. Well it was at least for myself. Geometric Drawings Understanding basic shapes gives us a foundation for drawing many other more complex shapes. You might have experience in drawing shapes with just a compass and ruler in math class. It is sometimes fun to limit your tools and play within the constraints. It is fascinating to see we can create pretty complex shapes just with circles and lines. Geometric Drawings – Other drawings Geometric shapes are widely used in our design. Repeating them in patterns is a major vocabulary of decorative and graphic design. Since ancient times, people have been crafting intriguing patterns by repeating graphic elements or organizing space based on specific rules. Tiling Not just for directly drawing shapes, we often use geometric shapes as proxies for more complex shapes as well. A good example of this is collision detection in games and 3D simulations where we want to detect object intersections. We usually use basic shapes like circles, triangles, rectangles, and combinations of them to approximate complex shapes at different precisions as needed. If you’re making a NES game, Mario can be just a rectangle. If you’re making a 3D fighting game, you might want boxes to represent a character’s body, upper and lower arms, fists, etc. separately. A human can tell if two figures overlap or not at a glance. But it is difficult to solve this generally with a computer, and different problems often require different tricks. Various methods of checking whether figures overlap are used to detect collisions between objects in physics simulations and games. You may rely on the functions of existing tools and libraries in actual projects. But knowing the basics will not only help you understand these tools but will also enable you to quickly implement only the functions you need yourself. Detecting Collision The linked page above (Detecting Collision) only covers collision detection in 2D. 3D collision detection can be much more intricate, but understanding in 2D can extend and help a lot in 3D too. Below is a simple reference I found for 3D. https://developer.mozilla.org/en-US/docs/Games/Techniques/3D_collision_detection?utm_source=chatgpt.com While straight lines are much easier to handle, curves are the norm on Earth. Nature doesn’t have many straight lines. Almost nothing goes straight, except for light. Curves are obviously a very essential tool for sketching. However, unlike straight lines, which can be defined with just two points and a linear formula, curves have infinite possibilities and are much harder to handle. There have been many different methods of defining or drawing curves, each with its own advantages, disadvantages, and constraints. Cumulative Curves One of the most common ways that curves occur in the physical world is through cumulative motion or buildup, meaning a point starts moving from one location and changes its course over time. This can include freehand drawing, paths followed by insects, animals, vehicles, and the growth patterns of plants and other organisms. While the cumulative approach is straightforward and versatile in theory, it is often not the best for many practical use cases where you want more precise control over the overall shape and details, such as in drawing or design tools. These tools usually adopt parametric approaches. In the parametric approach, a shape or curve is defined mathematically using one or more parameters. By varying these parameters, you can generate points that make up the shape. For example, if the parameter is called t, you can get all the points from beginning to end by moving t within a certain range. Parametric Approaches Among parametric approaches, there is a category of methods that are designed to define curved lines freely and precisely with multiple control points. These methods are widely used in design tools such as Adobe Illustrator or Figma, CAD, 3D modeling tools, and others. A spline curve is a mathematically defined curve that smoothly passes through or is close to a series of control points. It is like a big family, and there are many variations including Bézier curves, B-spline curves, NURBS (Non-Uniform Rational B-Splines), Cardinal Splines, Catmull-Rom Splines, Hermite Splines, etc. Bézier and Spline When we talk about shapes, it is important to discuss their quality, not just methods, since after all, we are sketching to express something. Curves can have various visual characteristics. For example, some curves might look more organic and natural, while others might appear more geometric and mechanical. How can we create these different looks of curves? There is another very different category of defining shapes than drawing outlines. “Signed distance functions”, or SDF sounds scary but it is not too crazy to understand. A SDF is a function that can tell you how far a point is from a surface of a shape, say a sphere (usually in Euclidean space) For example the distance of a point of from the surface of a sphere can be expressed as below where p is the coordinate of the point, c is the center of the sphere, and r is the radius: Signed distance functions This is very interesting because SDF is the technique used as basis for many mind-blowing 3D graphics demos with really small amount of code you often find on ShaderToy. Using this method in 3D, you can create different shapes pretty flexibly with a relatively short amount of code using shaders. Projection and 3D Rendering Reading “Raymarching – Primitives” Shaders and 3D rendering have a very steep learning curve. There isn’t a common or obvious way to build knowledge step by step, and it takes learning from multiple resources and examples. Shadertoy is one of the best resources with tons of cool demos with publicly available code. However, understanding this demo code can often be daunting. There are many techniques to grasp, and it’s often difficult to understand what’s happening in the code at first glance. There isn’t really a shortcut here (as far as I know). Studying demo codes step by step, a little by little is the best way. Two following pages are my attempt to help with the journey. Drawing Landscape The landscape example just above shows a lot more intricate and organic shape than other shapes we have been seeing on so far. Many naturally occurring shapes follow a pattern called a fractal, where a shape repeats its structure at different scales. In fractals, parts of the shape resemble the whole, while smaller parts resemble the larger parts. This property, called self-similarity, is key to modeling many complex structures and patterns found in nature, such as coastlines, mountains, clouds, and tree branches. Objects and lives in nature often to form fractal-like structures in the process of building themselves, where similar patterns recur at progressively smaller or larger scales. Understanding this principle can help us sketch them. Fractal Natural shapes are also unpredictable. The ups and downs of terrain, the changes in temperature, or the directions animals wander towards all seem random, but they look smoothly connected too, and values next to each other seem to be somehow related. Noise functions are the go-to methods for this, and we often overlay multiple layers of noise functions in a fractal manner (scale and add) to get desired complexity. Taming Randomness Reading a Noise Function As I said in the beginning, there are infinite ways of defining and drawing shapes. A couple of major topics I didn’t touch upon are: That said, if you’re interested, I recommend building basic tools yourself to learn the fundamentals. Even trying to render a few polygons can provide good insights (hint: project 3D points to 2D screen, calculate normals using cross product of vectors, and apply lighting). All of these sections were originally published as introductory articles, or maps of things in my mind, to provide overviews of different topics in Sketching with Code from different angles. Most of the contents is from Sketching with Math and Quasi Physics (2019), and the other workshop materials or self-educational notes. The majority of them were originally in English, but some were in Japanese first. The translations were done with the help of AI in both directions. I used p5.js for most of the code samples, unless otherwise noted. However, I tried to write them in a way that can be applied to any environment as much as possible. I am not a mathematician or computer engineer. This is simply a compilation of ideas that I have found useful from the perspective of a visually oriented person. If you notice any mistakes, have better examples, or know of links that would aid in learning, please share them with me. In this context, I am less interested in show-off demos, so please refrain from providing tweet-length shaders or similar content. Hope you enjoyed these intros. For more, see https://kyndinfo.notion.site/Sketching-with-Math-and-Quasi-Physics CreativeApplications.Net [CAN] is a community of creative practitioners working at the intersection of art, media and technology. Since 2008, CAN has been at the forefront of innovation––facilitating and contributing to the conversation about culture, society and critical making. CAN is also known for uncovering and contextualising noteworthy work featured on the festival and gallery circuit, executed within the commercial realm or developed as academic research. From online and offline publications to live events, CAN’s initiatives have played an instrumental role in the ideation, development and critique of a multitude of computational tools, projects and collaborations — always promoting critical dialogue, disciplinary interstices, knowledge sharing and feedback + response in diverse media. Discord / X (Twitter) / Instagram / BlueSky / Mastodon / Youtube / Facebook / GitHub / RSS (Members)
--------------------------------------------------

Title: Vulnerability Summary for the Week of May 19, 2025
URL: https://www.cisa.gov/news-events/bulletins/sb25-147
Time Published: 2025-05-27T12:13:52Z
Full Content:
An official website of the United States government Here’s how you know Official websites use .gov A .gov website belongs to an official government organization in the United States. Secure .gov websites use HTTPS A lock (LockA locked padlock) or https:// means you’ve safely connected to the .gov website. Share sensitive information only on official, secure websites. Free Cyber ServicesSecure by design Secure Our WorldShields UpReport A Cyber Issue Search Free Cyber ServicesSecure by design Secure Our WorldShields UpReport A Cyber Issue The CISA Vulnerability Bulletin provides a summary of new vulnerabilities that have been recorded in the past week. In some cases, the vulnerabilities in the bulletin may not yet have assigned CVSS scores. Vulnerabilities are based on the Common Vulnerabilities and Exposures (CVE) vulnerability naming standard and are organized according to severity, determined by the Common Vulnerability Scoring System (CVSS) standard. The division of high, medium, and low severities correspond to the following scores: Entries may include additional information provided by organizations and efforts sponsored by CISA. This information may include identifying information, values, definitions, and related links. Patch information is provided when available. Please note that some of the information in the bulletin is compiled from external, open-source reports and is not a direct result of CISA analysis. Back to top Back to top Back to top Back to top We recently updated our anonymous product survey; we’d welcome your feedback.
--------------------------------------------------

Title: Ad Tech Briefing: Google’s AI updates are portent of antitrust cases to come
URL: http://digiday.com/media-buying/ad-tech-briefing-googles-ai-updates-are-portent-of-antitrust-cases-to-come/
Time Published: 2025-05-27T04:01:00Z
Full Content:
Save 35% on an annual Digiday+ membership. This Ad Tech Briefing covers the latest in ad tech and platforms for Digiday+ members and is distributed over email every Friday at 10 a.m. ET. More from the series → Alphabet’s AI advancements continue to wow Wall Street, but they’re raising eyebrows with regulators in Washington, D.C., and last week’s launches at Google Marketing Live should cause advertisers on Madison Avenue to reserve judgment. Despite several tussles with the Justice Department and hand-wringing over the fate of third-party cookies, the company’s stock price popped last week, as the markets gauged Alphabet’s revenue plans following the Google I/O and Google Marketing Live announcements. The Google Marketing Live updates include several AI-powered advertising products and capabilities across Search and YouTube (see below). The offerings focus on simplifying creative production, enhancing targeting precision, heralding predictive intelligence, with Google’s Vidya Srinivasan, vp and general manager, ads & commerce, Google, claiming they can “remove the guesswork and drive measurable impact.” Key product launches and capabilities include: These product updates reflect Google’s broader strategy to embed AI across the advertising lifecycle — from discovery and creative to delivery and performance — making ads more relevant, actionable, and efficient. When Google speaks, the entire advertising industry should listen, but as several disclosures in the antitrust trials of the last two years show, this should never be in an obsequious manner. Several Digiday sources attempted to read between the lines of Google’s product announcements last week, and concluded that it is trying to define measurement and control optimization as the AI wars with Big Tech continue. This is the playbook that led to its colossal success, and some fear history may repeat itself. Nathan Woodman, founder of Proof-in-Data, observed how the two key innovations, i.e., new AI formats enhancing user experience and measurement tools like AI Max, although he warned the tools may prioritize Google’s revenue growth over advertisers’ success. “There’s a lot of people who have a problem with handing the keys over to Google,” said Woodman. “In many cases, you get the people inside an organization doing the media buying, they look at the dashboard, they look at the screen… they can lose a critical eye as to what’s happening.” According to Woodman, who formerly held senior roles at several digital media agencies, this can lead to “a GPS-effect,” a phenomenon whereby “the GPS is telling you where to go, but you don’t really look around.” Several sources noted that while the potential for the latest announcements to help marketers is clear, but questioning the provenance of these news systems is critical, as “blindly following the GPS is detrimental.” Key questions to ask Google about its AI systems Digiday sources advised marketers to interrogate the below issues when considering the latest Google AI tools. This is because Google’s strategy involves integrating AI agents to control budgets, bids, planning, and creative decisions, and Google has a proven track record for favoring its own systems. Robert Webster, CEO of TAU Marketing Solutions, noted how Microsoft and Google are positioning their tools to control the web, with this competition likely to prove a concern at Google HQ as many doubt its ability to monetize search in the AI-era compared to its earlier fortunes. “What everyone is fighting to be is the orchestration layer… where you use their agents to do things,” he added. “Microsoft’s hook is that everyone has a license for Word and Excel, so you’ll use them to do it. Google’s hook is that you spend most of your money with Google, you’ll want to use their agents to connect to Google Ads and your data seamlessly.” Webster, formerly a senior executive at WPP’s media investment arm, notes that this AI arms race should cause concern among regulators — Bloomberg likewise reported such DOJ scrutiny last week — with the concerns raised about Google’s AI an echo of those documented in the most recent antitrust case. “From a DOJ point of view, isn’t that a control game… these agents will do things like decide budgets, decide bids, decide planning, and what creative to run,” observed Webster. “If they’re built on Google systems, will Google be able to tweak them to help Google?” “Everything is broken, because it’s designed to be that way… There’s a lot of people out there who are just afraid to ask the right questions, and AI is only going to intensify that.” – A veteran media agency exec discusses how history seems to be repeating itself at the recent Digiday Programmatic Marketing Summit, where agency execs grumbled over measurement and attribution. WTF just happened to the IAB Europe’s TCF — and what does it mean for targeted ads in the EU? Perplexity is burning through cash, quickly While the AI-powered search engine has been able to increase its subscription revenue and users in the past 12 months, that growth has come at a large cost. According to The Information, Perplexity generated $34 million in revenue in 2024, but burned through $65 million in cash, which was spent on cloud servers and AI models which power much of its answers. Meta’s attempt to get its antitrust case thrown out failed The FTC’s battle against Meta kicked off on April 14, but Meta has since tried to have the case thrown out. However, on May 20, Judge Boasberg denied Meta’s motion, which asked him to rule against the FTC before it had even launched its defense case, according to The Verge. As TikTok’s U.S. existence is still in limbo, Shop staff are being laid off U.S. TikTok Shop staff were told to work from home on May 21, as they waited to receive emails regarding “difficult decisions,” which everyone understood to mean job cuts, according to Bloomberg. The layoffs follow leadership changes, including those in TikTok Shop which saw Mu, a former exec of Douyin’s (TikTok’s Chinese counterpart) e-commerce arm, take over the platform’s U.S. operations. ‘Marching off a cliff’: Developers at Microsoft Build question their future relevance’ Microsoft Build presenters celebrated the company’s latest advancements in AI, including multi-agent AI orchestration in Copilot Studio, the NLWeb protocol for natural language web interaction, and the integration of xAI’s Grok models into Azure AI Foundry. All these emphasize a shift toward an open agentic web. However, as Semafor’s Rachyl Jones noted, the mood on the floor was not so bullish, noting how one attendee asked: “Is there going to be a Build 2035, or will there not be any more developers?” At the three-year-old media agency, Mediaplus, the operating philosophy is about personal touch and being a side-by-side partner to clients. Canvas’ full-funnel offering emphasizes data-side and performance chops in a way that balances the need for upper-funnel wisdom and lower-funnel insights Pundits believe Google’s antitrust remedies will produce a similar result; optimists eye opportunity for independents. Get access to tools and analysis to stay ahead of the trends transforming media and marketing Visit your account page to make changes and renew. Get Digiday's top stories every morning in your email inbox. Follow @Digiday for the latest news, insider access to events and more.
--------------------------------------------------

Title: Grabar Law Office Investigates Claims on Behalf of Shareholders of DoubleVerify Holdings, Inc. (NYSE: DV); NAPCO Securities Technologies, Inc. (NASDAQ: NSSC); Pacira Biosciences, Inc. (NASDAQ: PCRX); and Treace Medical Concepts, Inc. (NASDAQ: TMCI)
URL: https://www.globenewswire.com/news-release/2025/05/27/3088306/0/en/Grabar-Law-Office-Investigates-Claims-on-Behalf-of-Shareholders-of-DoubleVerify-Holdings-Inc-NYSE-DV-NAPCO-Securities-Technologies-Inc-NASDAQ-NSSC-Pacira-Biosciences-Inc-NASDAQ-PCR.html
Time Published: 2025-05-27T01:54:00Z
Full Content:
May 26, 2025 21:54 ET | Source: Grabar Law Office Grabar Law Office PHILADELPHIA, May 26, 2025 (GLOBE NEWSWIRE) -- DoubleVerify Holdings, Inc. (NYSE: DV): If you have held DoubleVerify Holdings, Inc. (NYSE: DV) shares since prior to November 10, 2023, and would like to learn more about the investigation and your rights, please visit https://grabarlaw.com/the-latest/doubleverify-shareholder-investigation/, contact Joshua Grabar at jgrabar@grabarlaw.com, or call us at 267-507-6085. Grabar Law Office is investigating whether officers and directors of DoubleVerify breached their fiduciary duties owed to the company. Why? As alleged in an underlying securities fraud class action complaint, DoubleVerify, via certain of its officers, failed to disclose that: (a) DoubleVerify’s customers were shifting their ad spending from open exchanges to closed platforms, where the Company’s technological capabilities were limited and competed directly with native tools provided by platforms like Meta Platforms and Amazon; (b) DoubleVerify’s ability to monetize on Activation Services, the Company’s high-margin advertising optimization services segment, was limited because the development of its technology for closed platforms was significantly more expensive and time-consuming than disclosed to investors; (c) DoubleVerify’s Activation Services in connection with certain closed platforms would take several years to monetize; (d) DoubleVerify’s competitors were better positioned to incorporate AI into their offerings on closed platforms, which impaired DoubleVerify’s ability to compete effectively and adversely impacted the Company’s profits; (e) DoubleVerify systematically overbilled its customers for ad impressions served to declared bots operating out of known data center server farms; (f) DoubleVerify’s risk disclosures were materially false and misleading because they characterized adverse facts that had already materialized as mere possibilities; and (g) as a result of the above, Defendants’ positive statements about the Company’s business, operations, and prospects were materially false and/or misleading or lacked a reasonable basis. What You Can Do Now: Current DoubleVerify shareholders who have held DoubleVerify shares since prior to November 10, 2023, can seek corporate reforms, the return of funds back to the company, and a court approved incentive award at no cost to them whatsoever. If you would like to learn more about this matter, you are encouraged to visit https://grabarlaw.com/the-latest/doubleverify-shareholder-investigation/, contact Joshua H. Grabar at jgrabar@grabarlaw.com, or call 267-507-6085. NAPCO Securities Technologies, Inc. (NASDAQ: NSSC) Shareholder Class Action Survives Motion to Dismiss: If you are a Current NAPCO Securities Technologies, Inc. (NASDAQ: NSSC) shareholder who has held NAPCO shares since prior to November 7, 2022, you can seek corporate reforms, the return of funds back to the company, and a court approved incentive award at no cost to you whatsoever. Click here to join or learn more: https://grabarlaw.com/the-latest/Napco-shareholder-investigation/, contact Joshua H. Grabar at jgrabar@grabarlaw.com, or call 267-507-6085. Why? Key allegations in a recently filed securities fraud class action complaint have survived a motion to dismiss. That complaint alleges that NAPCO Security Technologies, Inc. (NASDAQ: NSSC), through certain of its officers and directors, made materially false and/or misleading statements and/or failed to disclose that: (1) NAPCO failed to address any material weaknesses with internal controls regarding cost of goods sold ("COGS") and inventory; (2) NAPCO downplayed the severity of material weaknesses regarding their internal controls; (3) NAPCO’s unaudited financial statements from September 30, 2022 to the present included “certain errors” such as overstating inventory and understanding net COGS, resulting in overstated gross profit, operating income and net income for each period; (4) as a result, NAPCO would need to restate its previously filed unaudited financial statements for certain periods; and (5) as a result, Defendants’ statements about its business, operations, and prospects, were materially false and misleading and/or lacked a reasonable basis at all relevant times. On April 11, 2025, the federal court determined that key allegations would survive Defendants’ motion to dismiss the complaint. “Plaintiffs have adequately stated Exchange Act claims by pleading scienter [knowledge of wrongdoing] through defendants’ unusual stock sales and by plausibly alleging loss causation between the corrective announcement and stock price drop. Plaintiffs have also stated Securities Act claims against NAPCO and the underwriter defendants.” “Taking the well-pleaded facts as true, there is no question that plaintiffs have adequately pled scienter. First, the stock sales were highly unusual in timing and amount. As to amount, the total proceeds of over $108 million from stock sales by the officer defendants weigh in favor of a motive. . . . And the officer defendants sold hefty percentages of their holdings – 48.5% for Soloway and 45.5% for Buchel.” What To Do Now: If you have held NAPCO shares since before November 7, 2022 and would like to learn more about this matter, please visit https://grabarlaw.com/the-latest/Napco-shareholder-investigation/, contact Joshua H. Grabar at jgrabar@grabarlaw.com, or call 267-507-6085. You can seek corporate reforms, the return of funds back to the company, and a court approved incentive award at no cost to you whatsoever. $NSSC #NAPCO Pacira Biosciences, Inc. (NASDAQ: PCRX): If you have held Pacira Biosciences, Inc. (NASDAQ: PCRX) shares since prior to August 2, 2023, and would like to learn more about the investigation and your rights, please visit https://grabarlaw.com/the-latest/pacira-shareholder-investigation/, contact Joshua Grabar at jgrabar@grabarlaw.com, or call us at 267-507-6085. Grabar Law Office is investigating whether certain officers and directors of Pacira breached their fiduciary duties owed to the company. Why: A recently filed securities fraud class action complaint alleges that Pacira, via certain of its officers, disseminated materially false and misleading statements and/or concealed material adverse facts concerning the validity and scope of the Company's patents. On August 9, 2024, Pacira announced that the New Jersey District Court had invalidated its ‘495 patent. This announcement surprised investors and analysts alike as they reacted immediately to the revelations, and the price of Pacira’s common stock declined dramatically. What You Can Do Now: Current Pacira shareholders who have held Pacira shares since prior to August 2, 2023, can seek corporate reforms, the return of funds back to the company, and a court approved incentive award at no cost to them whatsoever. If you would like to learn more about this matter, you are encouraged to visit https://grabarlaw.com/the-latest/pacira-shareholder-investigation/, contact Joshua H. Grabar at jgrabar@grabarlaw.com, or call 267-507-6085. Treace Medical Concepts, Inc. (NASDAQ: TMCI) If you have held Treace Medical Concepts (NASDAQ: TMCI) shares continuously since prior to May 8, 2023, you can seek corporate reforms, the return of funds back to the Company, and a court approved incentive award at no cost you. Visit https://grabarlaw.com/the-latest/treace-shareholder-investigation/, or contact Joshua H. Grabar at jgrabar@grabarlaw.com or call 267-507-6085 to learn more.Why? A recently filed securities class action complaint alleges that, Treace Medical Concepts, Inc. (NASDAQ: TMCI), via certain of its officers, made materially false and/or misleading statements and failed to disclose adverse facts about the Company’s business, operations, and prospects. Specifically, the Complaint alleges Defendants failed to disclose that: (1) competition impacted the demand for and utilization of its primary product, the Lapiplasty 3D Bunion Correction System; (2) as a result, Treace Medical’s revenue declined, and the Company needed to accelerate its plans to offer a product that served as an alternative to osteotomy (a surgical procedure involving the cutting and realignment of a bone to improve its position or function); and (3) Defendants’ positive statements about the Company’s business, operations, and prospects were materially misleading and/or lacked a reasonable basis. What You Can Do Now: Current Treace shareholders who have held Treace shares since prior to May 8, 2023, can seek corporate reforms, the return of funds back to the company, and a court approved incentive award at no cost to them whatsoever. If you would like to learn more about this matter, you are encouraged to visit https://grabarlaw.com/the-latest/treace-shareholder-investigation/, contact us at jgrabar@grabarlaw.com, or call 267-507-6085. #Treace $TMCI Attorney Advertising Disclaimer Contact:Joshua H. Grabar, Esq.Grabar Law OfficeOne Liberty Place1650 Market Street, Suite 3600Philadelphia, PA 19103Tel: 267-507-6085Email: jgrabar@grabarlaw.com PHILADELPHIA, May 21, 2025 (GLOBE NEWSWIRE) -- AppLovin Corporation (NASDAQ: APP): Grabar Law Office is investigating claims on behalf of shareholders of AppLovin Corporation (NASDAQ: APP). The... PHILADELPHIA, May 19, 2025 (GLOBE NEWSWIRE) -- Maison Solutions Inc. (NASDAQ: MSS) Class Action Survives Motion to Dismiss: Grabar Law Office is investigating claims on behalf of shareholders of...
--------------------------------------------------

Title: Last Chance Memorial Day Deals on Home Security Cameras and Video Doorbells
URL: https://me.pcmag.com/en/home-security-cameras/30081/last-chance-memorial-day-deals-on-home-security-cameras-and-video-doorbells
Time Published: 2025-05-26T20:52:16Z
Full Content:
It's always the right time to add to or refresh your home security devices, but it's so much more right when it's on sale. We're still seeing major discounts on smart video doorbells and indoor and outdoor security cameras, even after the Memorial Day holiday. Whether you want to capture HD videos of people at your front door, check on your pets whereabouts during the day, or find out whether you received a package delivery (maybe for a security camera, how meta), these top-rated security cams will get the job done and save you some money, too. Don't forget to check out our round-up of the Best Memorial Day tech sales for all of the best deals still out there. The TP-Link Tapo Indoor/Outdoor Home Security Wi-Fi Camera C120 is a versatile camera that can work indoors or outdoor, but because of the corded power source, in may be optimal for indoor use. But even the limitation of a power cord couldn't prevent this camera from being one of our favorite security devices and now It's currently 38% off. We love the sharp 2K video with strong color night vision, smart motion and sound alerts, local and cloud video storage, built-in spotlights, and voice control support. In his review, our expert, John R. Delaney, said, "If you’re in the market for a feature-rich security camera for indoor or outdoor use but don’t want to spend a bundle, the TP-Link Tapo Indoor/Outdoor Home Security Wi-Fi Camera C120 deserves a spot on your shortlist." It's no wonder why it's scored an "Outstanding" rating. One of the earliest pioneers in video doorbells is of course pretty great at making indoor cameras too. The Ring Indoor Cam is an affordable, two way audio with 1080p video resolution that offers cloud storage, and color night vision. While there is no local storage option, the camera does work with Alexa and has excellent phone controls while in app. Our expert gave the cam, an "Excellent" rating saying, "The Ring Indoor Cam is easy to recommend, especially if you already use other Ring products to protect your home." Easier still, thanks to the 43% discount. You can also get the Ring Indoor Cam at Best Buy for $34.99. The Ring Floodlight Cam Wired Plus is a hybrid camera that doubles as an excellent set of floodlights. With sharp HDR video quality, color night vision, cloud storage, Wi-Fi connectivity, and bright 2,500 lumens of lighting, the Ring Plus delivers in both of its primary features. Using your existing home wiring, the hybrid security device can deliver non-stop power. In our review our expert said the Wired Plus is a "worthy addition for enhancing your outdoor security" and gave it an "Excellent" rating. Right now it's 33% off. You can also get the Ring Floodlight Cam Wired Plus at Walmart for $119.99. The Eufy Security SoloCam S340 is a great, sharp camera that offers night vision, local video storage, has no blind spots thanks to a mechanical pan and tilt feature and cane operate on battery or solar power. With a built-in spotlight, 3K resolution video, and a three-month battery life, the Eufy is a serious powerhouse when it comes home security. Our called the cam, an "excellent choice for monitoring your yard" giving it an "Editor's Choice" rating in their review. Go and save yourself $40 on this one today. Built with the same features that various, more expensive cameras supply, the Wyze Cam OG is also one of the most affordable indoor/outdoor cams. The tiny, small cube cam offers HD video, weatherproof capability, and it has a built in spotlight. With local and cloud storage options, our expert gave the camera our "Editor's Choice" award saying, "The supremely low-cost Wyze Cam OG indoor/outdoor security camera delivers 1080p video, [and] doesn't skimp on features." Right now, it's at an even lower than normal price of $24. The Ecobee Smart Doorbell Camera (wired) is a surprisingly quality security device with the ability to synch to any home smart system (Amazon, Alexa, or Apple). The Ecobee Camera delivers a head-to-toe view of your front door, 1440p camera clarity with 175-degree field of view and 8x digital zoom. The Ecobee app, even without a subscription, allows two-way talk on the camera, but with a monthly sub, you can also unlock person, and package detection. With a three-year warranty right out of the box, a 38% off discount, plus an "Excellent" rating from our expert, you'll be hard pressed to find a deal better than this. You can also get the Ecobee Smart Video Doorbell Camera (Wired) at the Home Depot for $99.99. The Eufy Video Doorbell E340 features two cameras (one for visitors, the other for packages), both at a sharp 2K resolution and requires no subscription. while other video cam companies offer a paid sub to access a library of videos, Eufy gives many of those same options for free including alerts, face recognition, and more. It's why our expert gave the cam an "Excellent" rating, stating, "it offers free video storage and smart alerts, features many competitors charge extra for." And right now, you can it this great video doorbell for 20% off. You can also get the eufy Security Video Doorbell E340 at Walmart for $119.99. The Arlo Video Doorbell is "an all-around solid performer," and it's currently 54% off. The second gen version from Arlo offers a simple is a wireless installation that delivers great video quality, smart detection, and a rechargeable battery. While a subscription is needed to record video, it still earned a "Best of the Year" nod in 2024. In our review, our expert note, "Starting at $79.99, it’s one of the more affordable wireless video doorbells out there, and its excellent performance earns it our Editors' Choice award. You can also get the Arlo Video Doorbell (2nd Gen) at Best Buy for $59.99. We’re still finding Memorial Day Deals everywhere, from retailers like Amazon and Walmart to top brands like Apple, HP, and Dell. And don’t forget to check out all of the Memorial Day Deals Under $100 and Under $50. But, if you’re looking for something more specific, we’ve rounded up the following holiday deals for you: PCMag is obsessed with culture and tech, offering smart, spirited coverage of the products and innovations that shape our connected lives and the digital trends that keep us talking.
--------------------------------------------------

Title: Last Chance Memorial Day Deals on Home Security Cameras and Video Doorbells
URL: https://uk.pcmag.com/home-security-cameras/158254/last-chance-memorial-day-deals-on-home-security-cameras-and-video-doorbells
Time Published: 2025-05-26T20:52:16Z
Full Content:
PCMag editors select and review products independently. If you buy through affiliate links, we may earn commissions, which help support our testing. It's always the right time to add to or refresh your home security devices, but it's so much more right when it's on sale. We're still seeing major discounts on smart video doorbells and indoor and outdoor security cameras, even after the Memorial Day holiday. Whether you want to capture HD videos of people at your front door, check on your pets whereabouts during the day, or find out whether you received a package delivery (maybe for a security camera, how meta), these top-rated security cams will get the job done and save you some money, too. Don't forget to check out our round-up of the Best Memorial Day tech sales for all of the best deals still out there. The TP-Link Tapo Indoor/Outdoor Home Security Wi-Fi Camera C120 is a versatile camera that can work indoors or outdoor, but because of the corded power source, in may be optimal for indoor use. But even the limitation of a power cord couldn't prevent this camera from being one of our favorite security devices and now It's currently 38% off. We love the sharp 2K video with strong color night vision, smart motion and sound alerts, local and cloud video storage, built-in spotlights, and voice control support. In his review, our expert, John R. Delaney, said, "If you’re in the market for a feature-rich security camera for indoor or outdoor use but don’t want to spend a bundle, the TP-Link Tapo Indoor/Outdoor Home Security Wi-Fi Camera C120 deserves a spot on your shortlist." It's no wonder why it's scored an "Outstanding" rating. One of the earliest pioneers in video doorbells is of course pretty great at making indoor cameras too. The Ring Indoor Cam is an affordable, two way audio with 1080p video resolution that offers cloud storage, and color night vision. While there is no local storage option, the camera does work with Alexa and has excellent phone controls while in app. Our expert gave the cam, an "Excellent" rating saying, "The Ring Indoor Cam is easy to recommend, especially if you already use other Ring products to protect your home." Easier still, thanks to the 43% discount. You can also get the Ring Indoor Cam at Best Buy for $34.99. The Ring Floodlight Cam Wired Plus is a hybrid camera that doubles as an excellent set of floodlights. With sharp HDR video quality, color night vision, cloud storage, Wi-Fi connectivity, and bright 2,500 lumens of lighting, the Ring Plus delivers in both of its primary features. Using your existing home wiring, the hybrid security device can deliver non-stop power. In our review our expert said the Wired Plus is a "worthy addition for enhancing your outdoor security" and gave it an "Excellent" rating. Right now it's 33% off. You can also get the Ring Floodlight Cam Wired Plus at Walmart for $119.99. The Eufy Security SoloCam S340 is a great, sharp camera that offers night vision, local video storage, has no blind spots thanks to a mechanical pan and tilt feature and cane operate on battery or solar power. With a built-in spotlight, 3K resolution video, and a three-month battery life, the Eufy is a serious powerhouse when it comes home security. Our called the cam, an "excellent choice for monitoring your yard" giving it an "Editor's Choice" rating in their review. Go and save yourself $40 on this one today. Built with the same features that various, more expensive cameras supply, the Wyze Cam OG is also one of the most affordable indoor/outdoor cams. The tiny, small cube cam offers HD video, weatherproof capability, and it has a built in spotlight. With local and cloud storage options, our expert gave the camera our "Editor's Choice" award saying, "The supremely low-cost Wyze Cam OG indoor/outdoor security camera delivers 1080p video, [and] doesn't skimp on features." Right now, it's at an even lower than normal price of $24. The Ecobee Smart Doorbell Camera (wired) is a surprisingly quality security device with the ability to synch to any home smart system (Amazon, Alexa, or Apple). The Ecobee Camera delivers a head-to-toe view of your front door, 1440p camera clarity with 175-degree field of view and 8x digital zoom. The Ecobee app, even without a subscription, allows two-way talk on the camera, but with a monthly sub, you can also unlock person, and package detection. With a three-year warranty right out of the box, a 38% off discount, plus an "Excellent" rating from our expert, you'll be hard pressed to find a deal better than this. You can also get the Ecobee Smart Video Doorbell Camera (Wired) at the Home Depot for $99.99. The Eufy Video Doorbell E340 features two cameras (one for visitors, the other for packages), both at a sharp 2K resolution and requires no subscription. while other video cam companies offer a paid sub to access a library of videos, Eufy gives many of those same options for free including alerts, face recognition, and more. It's why our expert gave the cam an "Excellent" rating, stating, "it offers free video storage and smart alerts, features many competitors charge extra for." And right now, you can it this great video doorbell for 20% off. You can also get the eufy Security Video Doorbell E340 at Walmart for $119.99. The Arlo Video Doorbell is "an all-around solid performer," and it's currently 54% off. The second gen version from Arlo offers a simple is a wireless installation that delivers great video quality, smart detection, and a rechargeable battery. While a subscription is needed to record video, it still earned a "Best of the Year" nod in 2024. In our review, our expert note, "Starting at $79.99, it’s one of the more affordable wireless video doorbells out there, and its excellent performance earns it our Editors' Choice award. You can also get the Arlo Video Doorbell (2nd Gen) at Best Buy for $59.99. We’re still finding Memorial Day Deals everywhere, from retailers like Amazon and Walmart to top brands like Apple, HP, and Dell. And don’t forget to check out all of the Memorial Day Deals Under $100 and Under $50. But, if you’re looking for something more specific, we’ve rounded up the following holiday deals for you: PCMag.com is a leading authority on technology, delivering lab-based, independent reviews of the latest products and services. Our expert industry analysis and practical solutions help you make better buying decisions and get more from technology. PCMag is obsessed with culture and tech, offering smart, spirited coverage of the products and innovations that shape our connected lives and the digital trends that keep us talking.
--------------------------------------------------

Title: Morgan Stanley Maintains Hold on Coherent (COHR), Lifts PT
URL: https://finance.yahoo.com/news/morgan-stanley-maintains-hold-coherent-181923155.html
Time Published: 2025-05-26T18:19:23Z
Description: On May 23, Morgan Stanley analyst Meta Marshall raised the price target on Coherent Corp. (NYSE:COHR) to $74 from $70, but reiterated her cautious stance...
--------------------------------------------------

Title: Nvidia to report quarterly results on Wednesday — here's what investors should watch for
URL: https://economictimes.indiatimes.com/news/international/us/nvidia-to-report-quarterly-results-on-wednesday-heres-what-investors-should-watch-for-nvidia-news/articleshow/121416588.cms
Time Published: 2025-05-26T14:27:11Z
Full Content:
(Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Jolt to Çelebi could turn a big gain for this Indian firm that once had deep Turkish ties What’s slowing Indian IT's AI deals? The answer is hidden in just two words. Why high gold prices are making both banks and their borrowers smile This is the reason why Infosys terminated hundreds of freshers in Mysuru Stock Radar: India Cements recovers nearly 40% from lows; moving in upwards channel pattern – time to buy? For risk-takers with medium-term perspective: 6 mid-cap stocks from different sectors with upside potential of over 35% PM Modi receives hero’s welcome in Gujarat On cam: Macron 'smacked by wife' Shashi Tharoor in Guyana: Strengthening ties, fostering trade Trump delays 50% EU tariff hike until July 9 talks Noem & Saar hold memorial for Embassy staff killed in US Trump hints at Iran nuclear deal breakthrough LIVE: PM Modi’s mega Gujarat visit IMD Predicts Record-Breaking Rainfall in Mumbai for 2025 Owaisi urges UN to ban Pakistan terror groups Kristi Noem visits Western Wall in Israel Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Slideshow Top Prime Articles Top Definitions Most Searched IFSC Codes Top Story Listing Private Companies Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Mitsotakis’ fury (with the Council of State), ND’s proposal regarding Karamanlis, OnlyFans girls beware of the AADE, National Bank’s deals, and Sorras’ coffee machine
URL: https://en.protothema.gr/2025/05/26/mitsotakis-fury-with-the-council-of-state-nds-proposal-regarding-karamanlis-onlyfans-girls-beware-of-the-aade-national-banks-deals-and-sorras-coffee-machine/
Time Published: 2025-05-26T13:38:22Z
Full Content:
Greetings, so the source is one of the best I have at M.M., so I don’t think they would mislead me when they told me the following yesterday: “The Prime Minister is furious about this case, where the Council of State suddenly turned people’s plots into fields (properties on the edges of settlements with less than 2,000 inhabitants), because as he himself says, it is not possible for us to have a huge demographic as well as housing problem – especially in the provinces but also generally throughout Greece – and for the state to…eat up people’s property overnight. Really, where did the judges remember this from, dating back to 1983 when the relevant legislation has been in effect until today?” the source concluded. They also told me that “Mitsotakis is not going to let this go under any circumstances and will exhaust all pressure and options in favor of the ownership of these plots.” The truth is we are talking about an absolute exaggeration and a major injustice. Mitsotakis’ Tours What with the generally good weather, and what with the improvement of the political climate in favor of ND, K.M. has decided to leave Maximos more and more often, touring Attica and beyond. On Saturday he was in Megara, where he spoke to citizens, while his schedule includes a tour in Evia this coming Thursday. According to the program, he will go to Aliveri and to the inauguration of the carbon capture and storage station at the AGET plant. The Proposal and Karamanlis Today or tomorrow, ND is expected to submit its proposal for the preliminary investigation concerning Kostas Karamanlis, on charges of breach of duty as a misdemeanor. I understand that on the same day there will also be some statement from the former minister. The legal document is being finalized, and there will be a reference to Christos Spirtzis, but no prosecution will be made due to the statute of limitations. OnlyFans – Girls and the AADE So, I hear from one of my sources in tax affairs that the OnlyFans business is booming in Greece too, with all social media – mainly TikTok but also Instagram – full of Greek or even foreign girls advertising the world’s oldest profession. And we’re talking about earnings in the range of 15–20 thousand euros per month, not bad at all especially when it’s tax-free. Now, for this to be circulating in the tax authorities, it doesn’t seem like good news for the girls. Audits are coming… The Banks Hit the Markets for Senior Bonds And now onto market news, starting with the banks which are ready this week and next to run senior bond issuances as part of the MREL framework. The time has come, and after all, all banks, systemic and non-systemic, have announced their intentions during the presentation of their quarterly results. The planning for the entire year is for issues totaling around 2.5 billion euros, which concern the replacement of older maturing bonds or refinancing of issues where such an option is foreseen. Banks are accelerating their moves because interest rates have dropped significantly, but mainly because markets are volatile and investors particularly cautious and selective. Moreover, in these issuances the competition is international, as according to the EBA, in 2025 across Europe the following issuances are expected: 262 billion in senior preferred, 107 billion in senior non-preferred, 18 billion in AT1, and 37 billion in Tier 2 bonds. Timing is always critical, so get ready for announcements from Greek banks entering the markets. The Swiss Franc Loan Arrangement is Delayed (but in the works) The Finance Ministry is not ready yet, but promoting a regulation for loans in Swiss francs is a matter of time. A political mandate has been given from M.M., which (rightly) believes that the Government must maintain continuity and that promises must be implemented, regardless of changes in personnel. The banks are under the (mistaken) impression that they roughly know what to expect, but since you can’t make an omelet without breaking eggs, in the end, they’ll be forced to shoulder part of the cost. More to follow soon. The Two Deals of National Bank and EFG Luxembourg The National Bank is going through a period of great activity and the signs indicate that there are already advanced talks for specific projects. However, that doesn’t mean moves will happen on a journalist’s timeline. My column’s information about EFG Luxembourg of Spyros Latsis, which has been circulating in the market lately, is that National did indeed consider the scenario of acquiring it, but decided not to proceed. It was judged that while it would be a major move with substance for National – as it significantly strengthens and shifts balances in the field of private wealth management – it is a deal worth 4.5 billion. Since markets are currently highly volatile and in general we don’t know what tomorrow will bring, a deal of such scale carries significant risks for National. I think it’s a safer bet to expect – soon – some kind of agreement in the insurance sector and another deal in a different direction – most likely abroad. Strong first quarter for OPAP Analysts expect OPAP to announce yet another strong quarter the day after tomorrow (28/5), supported by betting revenue and the steady growth of its online operations. At the same time, revenue from online casinos will continue the excellent performance of recent quarters, while Joker’s strong results—offering the highest jackpot in its history—will also play an important role. AXIA forecasts total gross gaming revenue (GGR) at €584.9 million, up 6.4% year-on-year, EBITDA at €201.1 million (+5.7%), while net profit after minority interests is estimated at €119.6 million (+5.6%). Attention, however, is turning to the upcoming quarters, and AXIA sees challenges on the horizon. Last year’s second half benefited from Euro 2024 and positive betting results, so comparisons in the upcoming quarters are expected to be more demanding. Furthermore, due to the significant rise in the stock (which analysts had not anticipated), surpassing price targets, AXIA states that OPAP’s recommendation and valuation are under review, with an update expected soon. Ungrateful people…Sorras’ coffee maker up for auction? Give us a break Ah, where are the days of Artemis Sorras, who promised to erase Greece’s debt with the… shares of the unforgettable Bank of the East and his $650 billion. Unfortunately, we let that… opportunity go to waste and lost both Sorras and his bizarre business dealings from the spotlight. Those days are so far gone that, as the column has learned, an auction is scheduled for May 30, with the debtor being the (also unforgettable) “non-profit civil company ‘E.SY. – Hellenic Assembly’,” legally represented, with last known headquarters in the municipality of Perama, Attica, at 1 Makedonias Street, now with unknown address. For those with poor memory, this was the party through which Sorras decided to save us—since we refused to save ourselves. Yet, this ungrateful people showed no gratitude at the ballot box, and “Hellenic Assembly” ended up in the dustbin of history. In the meantime, though, it seems they left behind some debts despite the… billions of their leader. So, a private individual is putting the following items up for auction on the 30th: 2 aluminum metal ladders, 8 boxes of brochures, 1 portable radiator, 3 halogen heaters, 1 INOX induction unit, dimensions approx. 1.5×1.0x1.0m, 1 professional INOX counter, approx. 1.5×1.0x1.0m with three drawers, 1 professional INOX counter with 3 sinks and 2 overlapping cupboard doors, approx. 2.0×1.0x0.8m, 1 EUROCAL wall shelf, 1 MORRIS mini fridge, 1 SILVERCREST coffee maker, 2 two-seater sofas, 1 wooden coffee table with wheels, 1 plastic table with metal base, 1 square melamine table with metal base, 1 white desk, 2 drawer units with 3 drawers each, 1 VERO desktop tower, 1 TURBOX monitor, 1 TURBOX keyboard, 1 REACT amplifier, 1 white melamine shelving unit approx. 3.0×1.5×0.5m, 12 chairs with wooden back and gray leather seat, 1 white melamine shelving unit approx. 1.6×1.0x0.5m, 1 shelving unit approx. 1.6×1.0x0.5m with a double-door cupboard, 1 BRΧΑΝΤ A/C unit, 1 whiteboard approx. 1.2×1.0m, 2 white desks with 2 drawers each, 120x80cm, 1 old HP photocopier out of service, 1 HP screen, 1 IBM keyboard, 1 water cooler “NERA KRITIS”, 1 TELEFUNKEN monitor, 1 LG monitor, 1 PACKARD BELL monitor, 17 metal chairs with wood and leather seats, 8 square tables approx. 80x80cm with metal legs, 1 HP computer out of service, 1 small SAMSUNG XPRESS M2675F multifunction printer, 1 bookshelf with 3 shelves and various binders approx. 200x80x50cm, 1 bookshelf with 2 shelves and various binders approx. 160x60x40cm, 1 air conditioner of unknown brand with mirrored grey surface, 1 LG TV remote control, 1 LG air conditioner remote control, 1 YOKOHAMA air conditioner. As noted in the seizure report, “the described movables are old, of average quality, and some of them in poor condition.” Thus, they were all valued together at €500 and will be offered with a starting price of €300. It is further clarified that all this… clutter has been placed “under the custody and safekeeping of the enforcing creditor I. P., due to the difficulty of transporting them,” and remains with him in limbo. What can I add? All… faithful (if any are left), come forth. The talented Mr. Milionis Vasilis Milionis, known from the Energa–Hellas Power case, had active involvement in the energy project of “Larisa Thermoelectric S.A.,” which includes the construction of a natural gas power plant in Larisa, announced with fanfare last week. The investment, worth around €600 million, is being carried out by a consortium including Israeli-owned Clavenia Ltd (38.5%), DEPA Commercial (35%), which joined six months ago, Sirec Energy—a private equity firm via its subsidiary EUSIF Larissa (16.5%)—and Volton (10%). The plant, with a net capacity of 792 MW, is expected, pending the final investment decision, to begin construction in early 2026 and be operational by 2028. In this project, the talented Mr. Milionis is said to have played a key role in its development, as chairman of the board until mid-2021 and later as advisor. I remind you that Vasilis Milionis, who was initially sentenced to 10 years in prison but was acquitted on appeal in 2019, shook the nation 15 years ago with the energy scandal in which he was a central figure. It was then revealed that two private electricity companies (Energa – Hellas Power) were collecting the “property tax” included in electricity bills (EETIDE – predecessor to ENFIA) from customers but were not passing it on to the State—a case for which they were accused of embezzling over €230 million. In May 2021, Milionis was replaced by an Israeli executive at “Larisa Thermoelectric S.A.,” and since then he had disappeared from the energy map. Only to reappear in the medical cannabis business again with the Israelis of Tikun Olam Hellas, reportedly also having a shareholder link to the new Larisa project. Some involved in the project said last week that the Israelis removed him from the ambitious Larisa plan when they realized he was an unacceptable figure in the market and that his name carried a negative connotation. Still, whether Milionis is or isn’t part of the company behind the grand plans for an energy hub in Thessaly (natural gas plant, data center, etc.), I hear that he now shows strong interest in the defense sector. From energy to medical cannabis and now to defense ventures—if nothing else, Milionis seems to have a natural talent for spotting where the money flows! Decisions for 26 projects worth €2.5 billion in Healthcare This afternoon, the Governmental Committee for Projects of National Strategic Importance will convene and—according to information—assign 26 significant projects with a total budget of €2.5 billion to the Hellenic Corporation of Assets and Participations (HCAP) and the Strategic Contracts Unit. The government is satisfied with the speed and quality of the projects implemented by the HCAP’s PPF unit in the health sector, which completed 92 renovated hospitals and 160 Health Centers within timelines and budgets. The stock exchange worries only because of its own success By just a tenth of a percentage point (0.10%), the General Index on the Athens Stock Exchange missed the chance to post its 6th consecutive positive week—thanks to Trump and his tariff announcements. Today, with both the London and New York markets closed, the Athens market will move guided solely by the short-term needs of institutional investors, who see the General Index up by +22.64% year-to-date, the Banking Index by +35.63%, and the FTSE25 by +25.57%. The major event of the week comes tomorrow with the annual MSCI review of stock market classifications, with the key question being whether the Greek market will be included in the watchlist for potential reclassification into the Developed Markets category. Inclusion of the Athens Stock Exchange in the watchlist is considered pivotal for fund managers and investment funds active in both Emerging and Developed Markets. News is also expected from Cyprus, not only regarding the large dividend (€0.48/share) from the Bank of Cyprus but also announcements about the future of its shareholder structure. The PotUS’s crafty moves The U.S. President carefully chose the day to announce his ideas about imposing a 50% tax on European products (starting June 1) and at least a 25% tariff on iPhones sold in the U.S. but manufactured abroad. As mentioned, Wall Street is closed today, Monday, for Memorial Day. This was the first time President Trump directly targeted specific companies with tariffs, later stating he would also impose 25% tariffs on Samsung products if not made in the U.S. Americans will have ample time to decide whether Trump is bluffing once again. Market speculators, however, had yet another opportunity to “play” with Trump’s statements. The Dow Jones dropped 256 points (-0.6%). The S&P 500 fell -0.7%. The Nasdaq -1%. All three indices recorded their worst week since April. It’s worth remembering that Apple CEO Tim Cook, like many others, made large donations to Trump’s Inaugural Fund and attended the Washington event, seated near Meta Platforms’ Mark Zuckerberg and Amazon’s Chairman Jeff Bezos when the colossal $500 billion tech investment plan was unveiled at the end of February. Markets have largely stopped debating the truth of the PotUS’s claims or the real impact of his threats on the growth prospects of the world’s largest economy. What they cannot ignore, however, is the impact they are likely to have on corporate earnings—whether that’s Apple or the countless U.S. companies that rely on overseas sales to generate profits. They’re building nuclear batteries for the Moon and Mars Zeno Power Systems has just raised $50 million to build nuclear batteries that work where regular batteries can’t: in the depths of the oceans, the Arctic, in space, and even on the Moon. Zeno Power’s technology uses the decay of radioactive material to generate energy—no sunlight or power outlets required. The company has already signed major contracts with NASA and the U.S. military and secured fuel from the Department of Energy. They’ve added a former Navy chief to their board of directors. The future of energy might not be solar—it might be nuclear, silent, and built for the coldest, darkest places, not just on planet Earth. Explore related questions
--------------------------------------------------

Title: 35+ Best Memorial Day Deals Under $50: The Best Savings on Budget-Friendly Tech, Home Goods and More
URL: https://www.cnet.com/deals/best-memorial-day-deals-under-50-2025-05-26/
Time Published: 2025-05-26T12:37:00Z
Full Content:
We're in the final stretch of sales -- snag these budget deals on earbuds, kitchen appliances and more before they're gone. Our expert deal-hunting staff showcases the best price drops and discounts from reputable sellers daily. If you make a purchase using our links, CNET may earn a commission. If you didn't get a chance to wrap up your shopping in the last couple of days, you're in luck -- Many of the best Memorial Day deals we've seen this year are still available for picking. There are plenty of lingering deals from retailers like Amazon, Best Buy and Home Depot, but many of these offers will end soon. If you've already finished shopping for big-ticket items like TVs, appliances or camping gear, tons of wallet-friendly deals can still be found on tech and daily essentials to stretch your remaining budget. Shop these deals now to score some serious savings before prices bounce back. Bookmark this page and check back often -- we’re refreshing it regularly as deals end, sell out or get last-minute markdowns. And if you’re hunting for even lower prices, be sure to explore our curated list of Memorial Day deals under $25 that are still live. Amazon Fire TV Stick 4K is a great way to convert any 4K TV with an HDMI port into a smart TV. Whether you have a TV that doesn't have smart features or if you just don't like the smart TV interface of your current TV, the Fire TV Stick 4K is the product for you. With this $20 discount off of the $50 list price, the Amazon Fire TV Stick 4K is an even better value. The JBL Go 4 is a tiny but mighty Bluetooth speaker. You can take it anywhere as it has a small loop that you can use to attach to your backpack, keychain or anywhere else. It's small enough to put in your pocket or purse, but it doesn't skimp on sound with 5 watts of power and ample volume. The battery lasts seven hours, so you don't have to worry about it running out of power quickly. This multi-device charger from ESR is perfect if you've been wanting to reduce the number of cables in your charging setup. It supports the Qi2 standard and will have your iPhone, Apple Watch and AirPods ready to tackle the day all at once. Ugreen's Nexode series has some of the best chargers we've seen. This particular adapter has a 4-port design (three USB-C and one USB-A) and can be used to charge more than just phones. It's also compatible with laptops and handheld consoles. If you're often jet-setting from one country to another or have friends and family that visit often, then this universal travel adapter is something you need. It has a portable design, safety shutters and international compatibility, making it a must-have. More Memorial Day deals under $50: Glad you asked. If you're looking for even more savings, here are the best still-available Memorial Day deals under $25. You can also take a gander at some of the best Memorial Day bargains being offered by Amazon and Walmart. For a grand overview, check out our roundup of the ultimate sales and lowest prices still available after Memorial Day weekend. Keep in mind that the holiday has passed, so the deals won't be around for much longer. Many of us at CNET have covered shopping events for over five years, including Black Friday, Prime Day, Memorial Day and countless others. We've gotten good at weeding out scams and superficial deals, so you see only the best offers from all over. When choosing deals to show you, we look for real discounts, quality reviews and remaining sale time. Our team of experts has tested countless products to ensure we're only sharing the best options under $50.
--------------------------------------------------

Title: We asked 10 everyday Americans for the best investment of their lives
URL: https://www.businessinsider.com/best-investments-ever-average-americans-stock-market-real-estate-wealth-2025-5
Time Published: 2025-05-26T09:55:01Z
Full Content:
You don't have to be a Wall Street strategist or top fund manager to make your money work. Business Insider asked 10 everyday Americans for the best investments they've made. Some stumbled into gains by serendipitously buying Magnificent Seven stocks before they took off. Others leaned into real estate or alternative investments. And yes, some struck it big on Reddit's r/WallStreetBets finds. All of this goes to show that there's no single path to investing success. With their investments, these Americans have achieved various wealth-building milestones, including financial independence, early retirement, and an increased sense of security with their money. Business Insider has verified all investments. Here's how they did it. Vanscoy, a full-time dad who manages his family's portfolios, isn't one to invest in flashy tech stocks. He describes his strategy as "conservative and focused on dividend growth." Vanscoy's investment in Nvidia (NVDA) wasn't part of his usual portfolio management plan. Right before the pandemic, Vanscoy came across news articles and posts on forums about new video game developments. "I asked my kids what was going on, and they said these Nvidia graphics cards are really popular and everyone's trying to get them. I did a little research, and I thought that might be a fun pick-and-shovel play," Vanscoy told Business Insider. Vanscoy bought 80 shares of Nvidia in March 2020 for a total of $21,641.60. He's since taken profits twice, once in 2021 and again in 2023, which paid for his initial investment. Now, he holds 1,292 shares with a cost basis of $6.76 per share — a $151,000 position with an over 1,500% gain. Fischer, a former data analyst turned finance content creator, bought a house with her now-husband before they got married in 2020. It seemed a little risky at the time but has since paid off majorly. "At the time, interest rates were around 3%, and buying made just as much financial sense as renting after the initial down payment," Fischer said. "It also gave us the chance to test living together before marriage and see what an integrated life looks like. If worst comes to worst and things didn't work out, we would just sell and split the equity in half," Fischer added. The couple purchased their house for a little under half a million after the down payment. They refinanced their 3.3% mortgage rate down to 2.99% in 2021, right before mortgage rates shot up to 7%. Now, their monthly mortgage is $2,487 — which is comparable to local rent for a similar unit in the Seattle area. Buying a house and locking down a low mortgage rate means Fischer doesn't have to worry about rising rents. "We're just incredibly grateful we were able to purchase our home when we did," Fischer said. Brinkman grew up helping his grandfather manage his personal accounts, which imbued him with a strong sense of financial responsibility. "When I was in my 20s, I moved to LA and was very tight on money," Brinkman said. "I had very little to live off of at the time, but I still prioritized investing." Between 2012 and 2013, Brinkman bought $4,000 of Apple (AAPL) and $5,000 of Tesla (TSLA), which he's held ever since. His investments have grown to roughly $43,000 and $167,000 since the initial buy-in, respectively. Brinkman, who works at a production company that creates images of cars, got the idea to invest in Tesla when he began seeing more Tesla vehicles in the showroom at his job. Similarly, Brinkman liked Apple computers in the early 2000s and decided to invest in the company when he had enough disposable income. Nowadays, Brinkman has taken a less risky approach to investing and regularly buys index funds every two weeks. Jiang, a former Google employee who now runs the personal finance platform Peek, enjoys investing in tech because it's an industry she's well-versed in. Jiang bought into Tesla in 2015 after hearing her boss and many others bragging about owning one. She had seen a rise in electric vehicle popularity, but consumers' brand association with Tesla made it stand out to her. "I was like, 'this is a category-defining company and I want to buy in,'" Jiang said. She grew her investment by 1,200%. She recently exited her position in the company, as she had achieved a substantial return and didn't like the volatility associated with CEO Elon Musk. More recently, Jiang has bet big on Palantir (PLTR). She's been following the company since before it became public and believes the company has an edge thanks to its sticky government customers and innovations in AI. Jiang jumped in last year and snatched up the stock at just $23 a share. Since then, the stock has taken off, and Jiang's investment is up roughly 420%. Jiang has no plans to sell her Palantir stock anytime soon: "My conviction is that it'll be as big as an Nvidia," she said. For 10 years before becoming a software engineer, Nassif was a full-time online poker player. It was common to be paid in bitcoin, which Nassif would then convert into cash to pay for living expenses. Over the years, Nassif has inadvertently collected 0.8 bitcoin through this process — a hefty position as bitcoin hits an all-time high. Earlier in his poker career, up to 50% of his net worth was tied up in the cryptocurrency, but Nassif felt that was too high of a risk to be sustainable, so he started selling down to his current position. Another one of his big wins was starting to purchase Nvidia stock 15 years ago. He now holds 342 shares with an over 3,000% gain. Nassif was influenced by his heavy bitcoin exposure, as he realized Nvidia's GPUs were in high demand for crypto mining. Similar to bitcoin, Nassif has also been gradually taking profits in the wake of the AI hype, as he doesn't want to have a large portion of his portfolio tied up in a single stock. Nassif no longer makes any big bets on individual stocks. "In the last few years, I've been incredibly boring," Nassif said. "It's just been index funds." For the past eight years, White has divided his money between 60% index funds and 40% individual stocks. When picking individual stocks, White considers himself a value investor and looks for companies at cheap entry points. White is a particularly big fan of "convenience-oriented" stocks that make life easier for consumers, and Netflix (NFLX) was a bet that paid off big for him. In early 2022, Netflix stock plunged by two-thirds after the company reported losing subscribers and threatened to crack down on password sharing. White bought 45 shares in the company for a cost basis of $244 per share. Today, the stock is nearing $1,200, and White's investment has yielded around a 380% return. "It was a very sound investment, very well-managed, very mature leadership team. So when public sentiment pushed the stock down, I believed that was an irrational action, and I decided to jump in and start a position," White said. Waresmith has been investing in real estate for over 10 years. She bought her first house, a single-family home in Tampa, in 2013 from savings she accumulated working as an aerial acrobat on a cruise ship. Looking back, Waresmith's favorite real estate investment has been multi-family homes. She refinanced her property in Tampa and pulled out cash to buy a duplex in Massachusetts in 2020, securing a mortgage rate of 3.2%. Waresmith then house-hacked by living in one unit on the property and renting out the other. The mortgage at the time was around $2,300, and she rented out the other unit for $1,700, almost covering the monthly payment. "I think house hacking is one of the single smartest financial decisions anyone can make, as long as you choose the right property," Waresmith said. "I would not have felt comfortable affording the mortgage on that house alone at the time." Because her living expenses were so low from house hacking. Waresmith was able to invest and purchase another multi-family home in 2022. After moving out of the duplex, Waresmith now rents out both sides of the house, and her rental income is enough to easily cover the mortgage, as the rent on one side is now $2,500. Brennan Schlagbaum typically likes to invest in ETFs, but the one time he bought an individual stock, he struck it big. "I focus on index funds primarily, but there are some obvious opportunities from time to time, and one of those was Meta," Schlagbaum said. He owns 36 shares of the company, which is his only single-stock investment. In 2022, Meta Platforms (META) dropped from above $300 to under $90 after a series of weak earnings and a broader tech sell-off. Despite the negative sentiment, Schlagbaum, a CPA who runs the financial literacy company Budgetdog, felt like the company was in a good position according to its financial statements. "They have great free cash flow, hardly any debt, tons of reinvestment," Schlagbaum said. He bought the stock in October 2022 at $97. The stock is now at $628 as of May 23, yielding a gain of almost 550%. Last year, Mark and his husband were looking to move to Chicago from Salt Lake City. It just so happened that another couple they knew was also looking to move there — and so the four of them decided to team up and buy a house together. Mark and his husband sold their Salt Lake City house, which they had bought right before the pandemic housing boom, in June 2024. The two couples ended up purchasing a multi-family home last July for around $800,000. They decided to put down 40% for their down payment instead of the traditional 20%, as they wanted to minimize their monthly mortgage payments amid the current interest rate environment. For Mark, the proceeds from selling his Salt Lake City house covered the down payment. The house has four floors and is set up as two apartments stacked on top of each other, and comes with six bedrooms and four kitchens. Mark and his co-owners are very pleased with the living arrangement, as it's helped them get much more bang for their buck. "With what we are each paying, we never could have found something similar separately, even if each couple had something half the size of this house," Mark said. Forsythe was scrolling through r/WallStreetBets in 2022 when a ticker caught his eye: AST SpaceMobile (ASTS), a satellite communications company pioneering a space-based cellular broadband network. The stock has a niche cult following on X dubbed the "SpaceMob", which Forsythe has been monitoring along with company news and earnings reports over the last few years. Forsythe started with an initial holding of 2,500 shares at $10. From 2022 to 2024, the stock price plunged to under $2, and Forsythe actually sold his shares at a loss when the stock hit $2.15. But shortly after, in a classic WSB YOLO fashion, Forsythe bought back in with 35,000 shares at $2.88 per share. "My thinking was, considering the due diligence and following the stock for two years, I would feel terrible giving up on it now, especially if it rebounded," Forsythe said of buying back in. Forsythe's bet paid off — after buying back in, the company announced partnerships with AT&T and Verizon, sending the share price soaring. His investment sits at around $900,000 today. Even factoring in his initial loss on the first 2,500 shares, that's still a hefty 650% gain on his total invested capital. "I don't plan on selling anything," Forsythe said. "I've diamond-handed it all the way from the ups and downs." Check out Business Insider's picks for best cryptocurrency exchanges Check out Business Insider's picks for best cryptocurrency exchanges Where Big Tech secrets go public — unfiltered in your inbox weekly. Sign up chevron down icon An icon in the shape of an angle pointing down. Jump to
--------------------------------------------------

Title: Trump's Middle East AI Deal: "Just Count the Server Racks"
URL: https://www.chinatalk.media/p/about-that-ai-middle-east-deal
Time Published: 2025-05-25T14:03:59Z
Full Content:
Zvi Mowshowitz of the Don't Worry About the Vase substack did an excellent job writing up the key dynamics of the deal. I’ll be running excerpts from his post with some comments of mine interspersed. Our government, having withdrawn the new diffusion rules, has now announced an agreement to sell massive numbers of highly advanced AI chips to UAE and Saudi Arabia (KSA). This post analyzes that deal and that decision. It is possible, given sufficiently strong agreement details (which are not yet public and may not be finalized) and private unvoiced considerations, that this deal contains sufficient safeguards and justifications that, absent ability to fix other American policy failures, this decision is superior to the available alternatives. Perhaps these are good deals, with sufficiently strong security arrangements that will actually stick. Perhaps UAE and KSA are more important markets and general partners than we realize, and the rest of the world really is unable to deploy capital and electrical power the way they can and there is nothing we can do to change this, and perhaps they have other points of strategic importance, so we have to deal with them. Perhaps they are reliable American allies going forward who wouldn’t use this as leverage, for reasons I do not understand. There are potential worlds where this makes sense. The fact remains that the case being made for this deal, in public, actively makes the situation seem worse. David Sacks in particular is doubling down and extending the rhetoric I pushed back against last week, when I targeted Obvious Nonsense in AI diffusion discourse. Even within the White House, the China hawks are questioning this deal, and Sacks responded by claiming to not even understand their objections and to all but accuse such people of being traitorous decels wearing trench coats. I stand by my statements last week that even if accept the premise that all we need care about are ‘America wins the AI race’ and how we must ‘beat China,’ our government’s policies, on diffusion and elsewhere, seem determined to lose an AI race against China. The point of the diffusion rules is to keep the AI chips secure and out of Chinese hands, both in terms of physical security and use of their compute via remote access. It is possible that the agreements we are making with UAE and KSA will replace and improve upon the functionality, in those countries in particular, of the diffusion rules. It’s not about a particular set of rules. It is about the effect of those rules. Give me a better way to get the same effect, and I’m happy to take it. When I say ‘something similar’ below, I mean in the sense of sufficient safeguards against the diversion of either the physical AI chips or the compute from the AI chips. Access to those chips is what matters most. Whereas market share in selling AI chips is not something I am inclined to worry about except in my role as Nvidia shareholder. I do not consider them reliable allies going forward, and there are various reasons that even the best version of these agreements would make me deeply uncomfortable, but it is possible to reach an agreement that physically locates many data centers in the Middle East and lets them reap the financial benefits of their investments and have compute available for local use, but does not in the most meaningful senses ‘hand them’ the compute in question. As in, no I do not trust them, but we could find a way that we do not have to, if they were fully open to whatever it took to make that happen. I also would highlight the implicit claim I made here, that the pool of American advanced AI chips is essentially fixed, and that we have sufficient funding available in Big Tech to buy all of them indefinitely. If that is not true, then the UAE/KSA money matters a lot more. Then there is the similar question of whether we were going to actually run out of available electrical power with no way to get around that. A lot of the question comes down to: What would have counterfactually happened to those chips? Would we have been unable to deploy them? [Jordan: See ChinaTalk’s recent coverage of an excellent IFP report that explored what a policy agenda to rapidly increase the energy available for AI deployment would look like] Despite leading the world in AI innovation, there’s no guarantee that America will rise to meet the challenge of AI infrastructure. Specifically, the key technological barrier for data center construction within the next 5 years is new power capacity With that in mind, here are the central points I highlighted last week: America is ahead of China in AI. Diffusion rules serve to protect America’s technological lead where it matters. UAE, Qatar and Saudi Arabia are not reliable American allies, nor are they important markets for our technology. We should not be handing them large shares of the world’s most valuable resource, compute. The exact diffusion rule is gone but something similar must take its place, to do otherwise would be how America ‘loses the AI race.’ Not having any meaningful regulations at all on AI, or ‘building machines that are smarter and more capable than humans,’ is not a good idea, nor would it mean America would ‘lose the AI race.’ AI is currently virtually unregulated as a distinct entity, so ‘repeal 10 regulations for every one you add’ is to not regulate at all building machines that are soon likely to be smarter and more capable than humans, or anything else either. ‘Winning the AI race’ is about racing to superintelligence. It is not about who gets to build the GPU. The reason to ‘win’ the ‘race’ is not market share in selling big tech solutions. It is especially not about who gets to sell others the AI chips. If we care about American dominance in global markets, including tech markets, stop talking about how what we need to do is not regulate AI, and start talking about the things that will actually help us, or at least stop doing the things that actively hurt us and could actually make us lose. Diffusion controls on AI chips we’ve enforced on China so far have had a huge impact. DeepSeek put out a highly impressive AI model, but by their own statements they were severely handicapped by lack of compute. Chinese adoption of AI is also greatly held back by lack of inference compute. China is competing in spite of this severe disadvantage. It is vital that we hold their feet to the fire on this. China has an acute chip shortage, because it physically cannot make more AI chips, so any chips it would ship to a place like UAE or KSA would each be one less chip available in China. Whenever you see arguments from David Sacks and others against AI diffusion rules, ask the question: Is an argument for a different set of export controls and a different chip regime that still protects against China getting large quantities of advanced AI chips? Or is it an argument, as it often is, that to preserve our edge in compute we should sell off our compute, that to preserve our edge in tech we should give away our edge in tech? As in, that what matters is our market share of AI chips, not who uses them? This is not a strawman, for example Ben Thompson argues exactly this very explicitly and repeatedly. Ben Thompson’s recent interview with Jensen Huang, CEO of Nvidia, made it clear both of them have this exact position. That to maintain America’s edge in AI, we need to sell our AI chips to whoever wants them, including China, because ‘China will not be held back’ as if having a lot more chips wouldn’t have helped them. And essentially saying that all Nvidia chips everywhere support the ‘American tech stack’ rather than China rather obviously turning around and using them for their own tech. He explicitly is yelling we need to ‘compete in China’ or else. Complete Obvious Nonsense talking of his own book, which one must remind oneself is indeed his job, what were you really expecting him to say? Well, what he is saying is that the way we ‘lose the AI race’ is someone builds a CUDA alternative or steals Nvidia market share. That his market is what matters. It’s full text. Not remotely a strawman. I would disagree with arguments of form #2 in the strongest possible terms. If it’s arguments of form #1, we can talk about it. We should keep these facts in mind as we analyze the fact that the United States has signed a preliminary chip deal with the UAE. There is a 5GW AUE-US AI campus planned, and is taking similar action in Saudi Arabia. The deals were negotiated by a team led by David Sacks and Sriram Krishnan. Lennart Heim: To put the new 5GW AI campus in Abu Dhabi (UAE) into perspective. It would support up to 2.5 million NVIDIA B200s. That's bigger than all other major AI infrastructure announcements we've seen so far. In exchange for access to our chips, we get what are claimed to be strong protections against chip diversion, and promises of what I understand to be a total of $200 billion in investments by the UAE. That dollar figure is counting things like aluminum, petroleum, airplanes, Qualcomm and so on. It is unclear how much of that is new. The part of the deal that matters is that a majority of the UAE investment in data centers has to happen here in America. I notice that I am skeptical that all the huge numbers cited in the various investment ‘deals’ we keep making will end up as actual on-the-ground investments. As in: Walter Bloomberg: UAE PRESIDENT SAYS UAE TO INVEST $1.4T IN U.S OVER NEXT 10 YEARS At best there presumably is some creative accounting and political symbolism involved in such statements. Current UAE foreign-direct-investment stock in the USA is only $38 billion, their combined wealth funds only have $1.9 trillion total. We can at best treat $1.4 trillion as an aspiration, an upper bound scenario. If we get the $200 billion we should consider that a win, although if the deal is effectively ‘all your investments broadly are in the West and not in China’ then that would indeed be a substantial amount of funds. Nor is this an isolated incident. The Administration is constantly harping huge numbers, claiming to have brought in $14 trillion in new investment, including $4 trillion from the recent trip to Arabia, or roughly half of America’s GDP. Jason Furman (top economic advisor, Obama White House): That’s nuts and baseless. I doubt the press releases even add up to that. But, regardless, press releases are a terrible way to determine the investment or the impact of his policies on it. Justin Wolfers: Trump has claimed a $1.2 trillion investment deal from Qatar. Qatar’s annual GDP is a bit less than $250 billion per year. So he’s claiming an investment that would require every dollar every Qatari earned over the next five years. UAE’s MGX will also be opening Europe’s largest data center in France, together with Nvidia, an 8.5 billion Euro investment, first phase to be operational in 2028. This has been in the works for a while. Not that the numbers ultimately matter all that much. What does matter is: How will we ensure the chips don’t fall literally or functionally into Chinese hands? It comes down to the security provisions and who is going to effectively have access to and run all this compute. I don’t see here any laying out of the supposed tough security provisions. Without going into details, if the agreements on both physical and digital security are indeed implemented in a way that is sufficiently tough and robust, if we are the ones who both physically and digitally control and monitor things on a level at least as high as domestically, and can actually have confidence none of this will get diverted, then that goes a long way. We don’t yet have enough of that information to say. The public explanations for the deal, and the public statements about what safety precautions are considered necessary, do not bring comfort. I very much do not like comments like this, made in response to the Bloomberg piece above. David Sacks (US AI Czar): If the concern [about the deal] is about diversion of advanced semiconductors to China, that’s an important policy objective but one that is easily addressed with a security agreement and a “trust but verify” approach. Modern AI supercomputers are 8 feet tall and weigh two tons; these are not like diamonds smuggled in a briefcase. To verify that chips are where they’re supposed to be, an auditor can just visit the data center and count server racks. Semianalysis: As such, physical inspections are key to ensuring the GPUs are where they are supposed to be. The White House is aware of the risks and is prepared to ensure the chips are where they are supposed to be. As Bloomberg reported (quotes David Sacks’s statement above). I’m sorry, what? You can ‘just visit the data center and count server racks’? It terrifies me to see so many people arguing for the deal explicitly saying that this is a good statement, rather than a terrifying statement that we are hoping no one involved in the actual work believes to be true. [Jordan: In 2022 I did a show with Kevin Wolf, the former head of BIS, who mentioned off-hand that there were literally only two people Commerce had in China to do site inspections. Since then BIS’ budget has barely budged and faced DOGE cuts in the early days of this administration. There is not the capacity to do datacenter inspections in a serious way today.] Jordan Schneider: Let's start with the last thing first. This 60-day unverified list: is there a potential that the impact of this could actually be a lot bigger than what we currently see, if some of these big names on the unverified entity list end up graduating to full entity-list status? Kevin Wolf: It's really up to the Chinese government. If they allow access, give the information that the Commerce Department is seeking, and allow the two people in China that do inspections and site visits to do their jobs… Jordan Schneider: Whoa, wait, two? There are, like, 30 firms on there! Kevin Wolf: Yeah, I know, but no other country has any, and the Commerce Department is the only export control organization on the planet with its own enforcement agents who are subject matter experts in export controls. I think diversion via remote access is far more likely than physical diversion, and this response does not address the remote access issue at all, but even simply treating this as a physical diversion issue, o3, is the quoted statement accurate? o3 (full one shot, full response quoted, in response to literally quoting Sacks without attribution and asking ‘is this accurate?’): Short version: Mostly wrong. Chips are tiny and high-value, so diversion is hard to stop with “count the racks.” Physical audits help but they are far from sufficient. High value-to-weight ratio.At ~$30 k per H100, a single 20 kg carry-on full of GPUs is > $1 m. That’s an attractive target. Racks are heavy; the payload isn’t. Modularity.GPUs slide out in minutes. An auditor sees the enclosure; they cannot tell if the trays were swapped yesterday or will be tomorrow. Dummy blanks cost <$10. Front companies & trans-shipping.Chips often leave the compliant data-center before any inspection window. Paperwork shows them installed; reality changes after the auditor flies home. Remote lease risk.Even if the hardware stays in the U.S., Chinese entities can simply rent time on it. Export-control rules treat “deemed exports” (remote access by restricted users) as violations. Limited audit frequency & competence.Meaningful verification requires random, intrusive, technically savvy inspections plus cryptographic attestation of firmware. Counting racks is neither. My best estimate: ≤ 30 % that “security agreement + occasional rack counting” alone keeps advanced GPUs out of China for > 3 years. ≥ 70 % that significant leakage continues absent tighter controls (HW tracking, cryptographic attestation, and supply-chain tagging). So the quoted claim is misleading: rack-level audits are helpful but nowhere near “easily addresses” the diversion problem. When I asked how many chips would likely be diverted from a G42 data center if this was the security regime, o3’s 90% confidence interval was 5%-50%. Note that the G42 data center is 20% of the total compute here, so if we generously assume no physical diversion risk in the other 80%, that’s 1%-10% of all compute we deploy in the UAE. Is that acceptable? The optimal amount of chip diversion is not zero. But I think this level of diversion would be a big deal, and the bigger concern is remote access. I want to presume, for overdetermined reasons, that Sacks’s statement was written without due consideration or it does not reflect his actual views, and we would not actually make this level of dumb mistake where they could literally just swap the chips out for dummy chips. I presume we are planning to use vastly superior and more effective precautions against chip diversion and also have a plan for robust monitoring of compute use to prevent remote access diversion. But how can we trust an administration to take such issues seriously, if their AI Czar is not taking this even a little bit seriously? This is not a one time incident. Similar statements keep coming. That’s why I spent a whole post responding to them. David Sacks is also quoted extensively directly in the Bloomberg piece, and is repeatedly very dismissive of worried about diversion of chips or of compute, saying it is a fake argument and an easy problem to solve, and he talks about these as if they were reliable American allies in ways I do not believe are accurate. Sacks also continues to appear to view winning AI to be largely about selling AI chips. As in, if G42, an Abu Dhabi-based AI firm, is using American AI chips, then it essentially ‘counts as American’ for purposes of ‘winning,’ or similar. I don’t think that is how this works, or that this is a good use of a million H100s. Bloomberg reports 80% of chips headed to the UAE would go to US companies, 20% to G42. I very much want us to think about the actual physical consequences of various actions, not what those actions symbolize or look like. I do think, despite everything else, it is a very good sign that David Sacks is ‘urging people to read the fine print.’ This is moderated by the fact that we do not have the fine print, so we can’t read it. The true good news there requires one to read all that fine print, and one also should not assume that the fine print will get implemented. Nor do we yet have access to what the actual fine print says, so we cannot read it. Dylan Patel and others at Semianalysis offer a robust defense of the deal, saying clearly that ‘America wins’ and that this benefits American AI infrastructure suppliers on all levels, including AI labs and cloud providers. They focus on three benefits: money, tying KSA/UAE to our tech stack, and electrical power, and warn of the need for proper security, including model weight security, a point I appreciated them highlighting. Those seem like the right places to focus, and the right questions to ask. How much of their money is really up for grabs and how much does it matter? To what extent does this meaningfully tie UAE/KSA to America and how much does that matter? How much do we need their ability to provide electrical power? How will the security arrangements work, will they be effective, and who will effectively be in charge and have what leverage? Specifically, on their three central points: They call this macro, but a better term would be money. UAE and KSA (Saudi Arabia) can make it rain, a ‘trillion-dollar floodgate.’ This raises two questions. Question one: Was American AI ‘funding constrained’? The big tech companies were already putting in a combined hundreds of billions a year. Companies like xAI can easily raise funds to build giant data centers. If Google, Amazon, Apple, Meta or Microsoft wanted to invest more, are they really about to run out of available funding? Are there enough more chips available to be bought to run us out of cash? Semianalysis seems to think we should be worried about willingness of American companies to invest here and thinks we will have trouble with the financing. I am not convinced of this. Have you seen what these companies (don’t have to) pay on corporate bonds? Did we need to bring in outside investors? Should we even want to, given these investments look likely to pay off? This is a major crux. If indeed American big tech companies are funding constrained in their AI investments, then the money matters a lot more. Whereas if we were already capable of buying up all the chips, that very much cuts the other way. Question two: As we discussed earlier, is the trillion-dollar number real? We keep seeing these eye-popping headline investment numbers, but they don’t seem that anchored to reality, and seem to include all forms of investment including not AI, although of course other foreign direct investment is welcome. Do their investments in US datacenters mean anything, and are they even something we want, given that the limiting factor driving all this is either constraints on chip availability or on electrical power? Will this be crowding out other providers? If these deals are so positive for American tech companies, why didn’t the stock market moves reflect this? No, I will not accept ‘priced in.’ They call this geopolitical, that UAE and KSA are now tied to American technology stacks. As they say, ‘if Washington enforces tight security protocols.’ We will see. David Sacks is explicitly dismissing the need for tight security protocols. Classically, as Trump knows well, when the bank loans you a large enough amount and you don’t pay it back, it is the bank that has the problem. Who is being tied to whose stack? They will be able to at least cut the power any time. It is not clear from public info what other security will be present and what happens if they decide to turn on us, or use that threat as leverage. Can they take our chips and their talents elsewhere? This can almost be looked at as a deal with one corporation. G42 seems like it’s going to effectively be on the UAE side of the deal, and it is going to have a lot of chips in a lot of places. A key question is, to what extent do we have the leverage on and control over G42, and to what extent does this mean they will act as a de facto American tech company and ally? How much can we trust that our interests will continue to align? Who will be dependent on who? Will our security protocols extend to their African and European outposts? Why does buying a bunch of our chips tie them into the rest of our stack? My technical understand is that it doesn’t. They’re only tied to the extent that they agreed to be tied as part of the deal (again, details unknown), and they could swap out that part at any time. In my experience you can change which AI your program uses by changing a few lines of code, and people often do. It is not obvious why KSA and UAE using our software or tech stack is important to us other than because they are about to have all these chips. These aren’t exactly huge markets. If the argument is they have oversized effect on lots of other markets, we need to hear this case made out loud. Seminanalysis points out China doesn’t even have the capacity to sell its own AI chips yet. And I am confused about the perspectives here on ‘market share’ and the implied expectations about customer lock-in. They call this infrastructure, I’d simply call it (electrical) power. This is the clearly valuable thing we are getting. It’s rather crazy that ‘put our most strategic asset except maybe nukes into the UAE and KSA’ was chosen over ‘overrule permitting rules and build some power plants or convince one of our closer allies to do it’ but here we are. So the question here is, what are the alternatives? How acute is the shortage going to be and was there no one else capable of addressing it? Also, even if we do have to make this deal now, this is screaming from the rooftops, we need to build up more electrical power everywhere else now, so we don’t have this constraint again in the future. Semianalysis also raises the concern about model weight security, but essentially think this is solvable via funding work to develop countermeasures and use of red teaming, plus defense in depth. It’s great to see this concern raised explicitly, as it is another real worry. Yes, we could do work to mitigate it and impose good security protocols, and keep the models from running in places and ways that create this danger, but will we? I don’t know. Failure here would be catastrophic. There are also other concerns even if we successfully retain physical and digital control over the chips. The more we place AI chips and other strategic AI assets there, the more we are turning UAE, Saudi Arabia and potentially Qatar into major AI players, granting them leverage I believe they can and will use for various purposes. David Sacks continues to claim to not understand that others think that ‘winning AI’ is mostly not about who gets to sell chips, who uses our models and picks up market share, or about superficially ‘winning’ ‘deals.’ He not only thinks it is about market penetration, he can’t imagine an alternative. He doesn’t understand that many, including myself, this is about who has compute and who gets superintelligence, and about the need for proper security. David Sacks: I’m genuinely perplexed how any self-proclaimed “China Hawk” can claim that President Trump’s AI deals with UAE and Saudi Arabia aren’t hugely beneficial for the United States. As leading semiconductor analyst Dylan Patel observed, these deals “will noticeably shift the balance of power” in America’s favor. The only question you need to ask is: does China wish it had made these deals? Yes of course it does. But President Trump got there first and beat them to the punch. Sam Altman: this was an extremely smart thing for you all to do and i’m sorry naive people are giving you grief. Tripp Mickle and Ana Swanson (NYT): One Trump administration official, who declined to be named because he was not authorized to speak publicly, said that with the G42 deal, American policymakers were making a choice that could mean the most powerful A.I. training facility in 2029 would be in the United Arab Emirates, rather than the United States. [Jordan: this is a wild line to be giving to the NY Times—I thought all the China hawks already got purged!] But Trump officials worried that if the United States continued to limit the Emirates’ access to American technology, the Persian Gulf nation would try Chinese alternatives. The hawks are concerned, because the hawks largely do not think that the key question is who will get to sell chips, but rather who gets to buy them and use them. This is especially true given that both America and China are producing as many top AI chips as they can, us far more successfully, and there is more than enough demand for both of them. One must think on the margin. Given that so many China hawks are indeed on record doubting this deal, if you are perplexed by this I suggest reading their explanations. Here is one example. Tripp Mickle and Ana Swanson (NYT): Mr. Goodrich said the United States still had the best A.I. engineers, companies and chips and should look for ways to speed up permitting and improve its energy grid to hold on to that expertise. Setting up some of the world’s largest data centers in the Middle East risks turning the Gulf States, or even China, into A.I. rivals, he said. “We’ve seen this movie before and we should not repeat it,” Mr. Goodrich said. Sam Winter-Levy, a fellow at the Carnegie Endowment for International Peace, said the huge chip sales did “not feel consistent with an America First approach to A.I. policy or industrial policy.” “Why would we want to offshore the infrastructure that will underpin the key industrial technology of the coming years?” he asked. This does not seem like a difficult position to understand? There are of course also other reasons to oppose such deals. Here is Jordan Schneider of China Talk’s response, in which he is having absolutely none of it, explicitly rejecting that either America or China has chips to spare for this. rejecting that UAE and KSA are actual allies, not expecting us to follow through with reasonable security precautions, and saying if we wanted to do this anyway we could have held out for a better deal with more control than this, I don’t know why you would be confused how someone could have this reaction based on the publicly available information: Jordan Schneider: It’s going to cannibalize US build-out and leave the world with three independent power-centers of AI hardware where we could’ve stuck to our guns, done more power generation at home, and only had China to deal with not these wild-card countries that are not actual allies. If this really is as important as we believe, why are we letting these countries and companies we deeply distrust get access to it? The Gulf’s BATNA wasn’t Huawei chips, it was no chips. Whatever we’re trying to negotiate for, we can play harder to get. BIS can just say they can’t buy Ascends and it’s not like there’s enough capacity domestically in China to service global demand absent the TSMC loophole they charged through. Plus, we’re offering to sell them 10× the chips that Huawei could conceivably sell them anytime soon even if they use the TSMC-fabbed wafers. Where’s the art-of-the-deal energy here? Right now I only see AMD and NVDA shareholders as well as Sama benefiting from all of this. I thought we wanted to raise revenue from tariffs? Why not charge 3× the market rate and put the premium into the US Treasury, some “Make America Great Again” industrial-development fund, use it to triple BIS’ budget so they can actually enforce the security side, put them on the hook for Gaza…I don't know literally anything you care about. How about a commitment not to invest in Chinese tech firms? Do we still care about advanced logic made in America? How about we only let them buy chips fabbed in the US, fixing the demand-side problem and forcing NVDA to teach Intel how to not suck. Speaking of charging through loopholes, all of the security issues Dylan raises in his article I have, generously, 15 % confidence in USG being able to resolve/resist industry and politicians when they push back. If it’s so simple to just count the servers, why hasn’t BIS already done it / been able to fight upstream industry lobbying to update the chips-and-SME regs to stop Chinese build-outs and chip acquisition? What happens when the Trump gets a call from the King when some bureaucrat is trying to stop shipments because they see diversion if they ever catch it in the first place? Why are we doing anything with G42 again? Fine, if you really decide you want to sell chips to the UAE, at the very least give American hyperscalers the off-switch. It’s not like they would’ve walked away from that offer! America has a ton to lose in the medium term from creating another cloud provider that can service at scale, saying nothing of one that has some deeply-discomforting China ties pretty obvious even to me sitting here having never gotten classified briefings on the topic. Do the deal’s details and various private or unvoiced considerations make this deal better than it looks and answer many of these concerns? Could this be sufficient that, if looked at purely through the lens of American strategic interests, this deal was a win versus the salient alternatives? Again: That is all certainly possible! Our negotiating position could have been worse than Jordan believes. We could have gotten important things for America we aren’t mentioning yet. The administration could have limited room to maneuver including by being divided against itself or against Congress on this. On the flip side, there are some potentially uncharitable explanations for all of this, that would be reasonable to consider. Instead of understanding and engaging with such concerns and working to allay them, Sacks has repeatedly decided to make this a mask off moment, and engage in a response that I would expect on something like the All-In Podcast or in a Twitter beef, but which is unbecoming of his office and responsibilities, with multiple baseless vibe and ad hominem attacks at once that reflect that he either is willfully ignorant of the views, goals and beliefs of those he is attacking and even who they actually are, or he is lying and does not care, or both, and a failure to take seriously the concerns and objections being raised. Here is another illustration of this: David Sacks (May 17): After the Sam Bankrun-Fraud fiasco, it was necessary for the Effective Altruists to rebrand. So they are trying to position themselves as “China Hawks.” But their tech deceleration agenda is the same, and it would cost America the AI race with China. There are multiple other people I often disagree with on important questions but whom I greatly respect who are working on in administration on AI policy. There are good arguments you can make in defense of this deal. Instead of making those arguments in public, we repeatedly get this. Everything Sacks says seems to be about vibes and implications first and actual factual claims a distant second at best. He doesn’t logically say ‘all so-called China hawks who don’t agree with me are secret effective altruists in trench coats and also decels who hate all technology and all of humanity and also America,’ but you better believe that’s the impression he’s going for here. Would China have preferred to ‘do this deal’ instead? That at best assumes facts, and arguments, not in evidence. It depends what they would get out of such a deal, and what we’re getting out of ours, and also the security arrangements and whether we’ve formed a long lasting relationship in which we hold the cards. I’m also not even sure what it would mean for China to have ‘done this deal,’ it does not have what we are offering. Semianalysis says they don’t have similar quantities of chips to sell, and might not have any, nor are their chips of similar quality. I do agree China would have liked to ‘do a deal’ in some general sense, where they bring UAE/KSA into their orbit, on AI and otherwise, although they don’t need access to electrical power. More capital and friends are always helpful. It’s not clear what that deal would have looked like. [Jordan: the fact that the administration is citing some old news of Huawei promising to gift just $75m in chips to the Malaysian government (and not even today! Over two years!) is policy malpractice. Plus, this administration just made clear that using Huawei Ascend chips violates US export controls, giving America an enormously powerful lever to dissuade fence-sitting countries not to buy into what is already an inferior tech offering.] Here’s Tyler Cowen being clear eyed about some of what we are selling so cheap. The most powerful AI training facility could be in the UAE, and you’re laughing? Tyler Cowen: Of course Saudi and the UAE have plenty of energy, including oil, solar, and the ability to put up nuclear quickly. We can all agree that it might be better to put these data centers on US territory, but of course the NIMBYs will not let us build at the required speeds. Not doing these deals could mean ceding superintelligence capabilities to China first. Or letting other parties move in and take advantage of the abilities of the Gulf states to build out energy supplies quickly. Energy and ability to overcome NIMBYs is only that which is scarce because America is refusing to rise to this challenge and actually enable more power generation. Seriously, is there nowhere in America we can make this happen at scale? If we wanted to, we could do this ourselves easily. We have the natural gas, even if nuclear would be too slow to come online. It is a policy choice not to clear the way. And no, I see zero evidence that we are pulling out the stops here and coming up short. I think this frame is exactly correct - that this deal makes sense if and only if all of: The security deal is robust and we retain functional control over where the compute goes. We trust our friends here to remain our friends at a reasonable price. We counterfactually would not have been able to buy these chips and build data centers to power these chips. As far as I can tell China already has all the power it needs to power any AI chips it can produce, it is using them all, and its chip efforts are not funding constrained. So for want of electrical power, and for a few dollars, we are handing over a large amount of influence over the future to authoritarian powers with very different priorities and values? Tyler Cowen: In any case, imagine that soon the world’s smartest and wisest philosopher will soon again be in Arabic lands. We seem to be moving to a world where there will be four major AI powers — adding Saudi and UAE — rather than just two, namely the US and China. But if energy is what is scarce here, perhaps we were headed for additional AI powers anyway, and best for the US to be in on the deal? Who really will have de facto final rights of control in these deals? Plug pulling abilities? What will the actual balance of power and influence look like? Exactly what role will the US private sector play? Will Saudi and the UAE then have to procure nuclear weapons to guard the highly valuable data centers? Will Saudi and the UAE simply become the most powerful and influential nations in the Middle East and perhaps somewhat beyond? Yes. Those are indeed many of the right questions, once you think security is solid. Who is in charge of these data centers in the ways that matter? Won’t they at minimum have the ability to cut the power at any time? Who gets to decide where the compute goes? What are they going to do with all this leverage we are handing them? Is this what it means to have the future be based on American or Democratic values? Do you like ‘the values’ of the UAE and Saudi Arabian authorities? Tyler Cowen: I don’t have the answers to those questions. If I were president I suppose I would be doing these deals, but it is very difficult to analyze all of the relevant factors. The variance of outcomes is large, and I have very little confidence in anyone’s judgments here, my own included. Few people are shrieking about this, either positively or negatively, but it could be the series of decisions that settles our final opinion of the second Trump presidency. The administration thinks that the compute in question will remain under the indefinitely control of American tech companies, to be directed as we wish. Sriram Krishnan: Reflecting on what has been an amazing week and a key step in global American AI dominance under President Trump. These Middle East AI partnerships are historic and this “AI diplomacy” will help lock in the American tech stack in the region, help American companies expand there while also building infrastructure back in the U.S to continue expanding our compute capacity. This happens on top of rigorous security guarantees to stop diversion or unauthorized access of our technology. More broadly this helps pull the region closer to the U.S and aligns our technological interests in a very key moment for AI. It’s a very exciting moment and a key milestone. I hope that they are right about this, but I notice that I share Tyler’s worry that they are wrong. Similarly, Saudi Arabia’s Humain is going to get ‘several hundred thousand’ of Nvidia’s most advanced processors, starting with 18k GB300 Grace Blackwells. The justification given for rescinding the Biden diffusion rules is primarily that failure to do this would have ‘weakened diplomatic relations with dozens of countries by downgrading them to second-tier status.’ But, well, not to reiterate everything I said last week, but on that note I have news. One, we’re weakening diplomatic relations with essentially all countries in a series of unforced errors elsewhere, and we could stop. [Jordan: that feeling when you treat Dubai better than Canada] Two, most of the listed tier two countries have always had second-tier status. There’s a reason Saudi Arabia isn’t in Five Eyes or NATO. We can talk price about which countries should have which status, but no our relations are not all created equal, not when it comes to strategically vital national interests and to deep trust. I don’t share Sacks’s stated view that these are some of our closest and most trustworthy allies. Why does this administration seem to always want to make its deals mostly with authoritarian regimes, usually in places where Trump has financial ties? Tripp Mickle and Ana Swanson (NY Times): The announcements of the two deals follow reports that $2 billion has flowed to Trump companies over the last month from the Middle East, including a Saudi-backed investment in Trump’s cryptocurrency and plans for a new presidential airplane from Qatar. There’s always Trust But Verify. The best solution, if you can’t trust, is often to set up things so that you don’t have to. This can largely be done. Will we do it? And what will we get in return? What is announced mostly seems to be investments and purchases, that what we are getting are dollars, and Bloomberg is skeptical of the stated dollar amounts. This deal is very much not a first best solution. It is, at best, a move that we are forced into on the margin due to our massive unforced errors in a variety of other realms. Even if it makes sense to do this, it makes even more sense to be addressing and fixing those other critical mistakes. Electrical power is the most glaring in the context of this particular. There needs to be national emergency level focus on America’s inability to build electrical power capacity. Where are the special compute zones? Where are the categorical exemptions? Where is DOGE with regard to the NRC? Where is the push for real reform on any of these fronts? Instead, we see story after story of Congress actively moving to withdraw even the supports that are already there, including plans to outright abrogate contracts on existing projects. The other very glaring issue is trade policy. If we think it is this vital to maintain trade alliances and open up markets, and maintaining market share, why are we otherwise going in the opposite direction? Why are we alienating most of our allies? And so on. The argument for this deal is, essentially, that it must be considered in isolation. That other stuff is someone else’s department, and we can only work with what we have. But this is a very bitter pill to be asked to swallow, especially as Sacks himself has spoken out quite loudly in favor of many of those same anti-helpful policies, and the others he seems to be sitting out. You can argue that he needs to maintain his political position, but if that also rules out advocating for electrical power generation and permitting reform, what are we even doing? If we swallow the entire pill, and consider these deals only on the margin, without any ability to impact any of our other decisions, and only with respect to ‘beating China’ and ability to ‘win the AI race,’ and assume fully good faith and set aside all the poor arguments and consider only the steelman case, we can ask: Do these deals help us? I believe that such a deal is justifiable, again on the margin and regarding our position with respect to China, if and only if ALL of the following are true: Security arrangements are robust, the chips actually do remain under our physical control and we actually do determine what happens with the compute. And things are set up such that America retains the leverage, and we can count on UAE/KSA to remain our friends going forward. This was essentially the best deal we could have gotten. This represents a major shift in our or China’s ability to stand up advanced AI chips, because for the bulk of these chips either Big Tech would have run out of money, or we would have been unable to source the necessary electrical power, or China has surplus advanced AI chips I was not previously aware of and no way to deploy them. Entering into these partnerships is more diplomatically impactful, and these friendships are more valuable, than they appear to me based on public info. ChinaTalk is a reader-supported publication. To receive new posts and support our work, consider becoming a free or paid subscriber. Check out some ChinaTalk coverage from least year about China’s Middle East AI ambitions. I spent two years in a China Studies master’s program. It was fun reading Dream of the Red Chamber and Xi speeches — but I really wish I had come out of it with some hard skills, too. Had I only known about the University of San Francisco’s new Master of Science in Applied Economics Excellent commentary. I'm increasingly baffled by the inconsistencies in how the administration views AI. On one hand, it's the most important technology of the century, and will definitively decide the outcome of the U.S.-China competition. Yet we're willing to outsource and offshore data center development. I realize the analogy is flawed, but it has the same feeling as saying we want to outsource production of the B-21 because we have too many regulations, it will take too long to get permits and build, we can't afford it, and BTW we'll be able to provide oversight and monitoring of the manufacturing plant. Not something we would do in a million years. So if AI is that critical, why would we ever agree to this chip sales/data center deal--purported to result in what will become the largest cluster in the world? (While cutting BIS resources.) Rather than being about national self-interests, this has all the earmarks of a pure transactional agreement, centered on money and power. And while it's easy to be skeptical of the claims about AGI and ASI, if the assertion is that AI is going to be the most powerful, game-changing tech of the century, why should we ever stop states from trying to implement reasonable oversight and regulations? The states are doing it because there's a major gap at the federal level. The hyperbolic claims about over-regulation of AI during the previous administration are belied by the facts on the ground. In reality, the previous EOs directed very little regulation, outside mandating a number of actions within the government itself. I'm all for smart, tailored, less intrusive regulations. But it's really hard to justify scant or no regulatory actions, at the same time the administration is arguing that this technology will forever change the world. We're trying to create another Taiwan we have to protect lol No posts Ready for more?
--------------------------------------------------

Title: Bond Yields & Stocks: Something’s Happening Here
URL: https://www.forbes.com/sites/bill_stone/2025/05/25/bond-yields--stocks-somethings-happening-here/
Time Published: 2025-05-25T12:24:45Z
Full Content:
ByBill Stone ByBill Stone, Contributor. The recent sharp rise in US Treasury bond yields has raised questions about whether the move is ... More connected to US fiscal policy or the end of American exceptionalism. The evidence for alternate explanations is examined. Despite some pundits being quite convinced that U.S. fiscal policy or the end of American exceptionalism is the reason for the recent rise in U.S. Treasury yields, Buffalo Springfield was closer to reality when they sang, “Something’s happening here, What it is ain’t exactly clear.” It is not disputed that 10-year U.S. Treasury yields are 52 basis points (0.52%) higher since the 2025 low of 3.99% on April 4. Using U.S. Treasury Inflation-Protected Securities data, expected higher inflation accounted for 15 basis points, while the remaining 37 are from a higher real (after-inflation) return. 10-Year US Treasury Note: Yield Decomposition Notably, yields were at their nadir when the betting odds of recession were at their highest. As one should expect, yields have risen as the odds of a recession have declined. Directionally, the move in yields is logical, though some might still argue that yields have increased more than is warranted. Yields & Recession Odds Government bond yields in the U.S. have risen more than many other developed countries since April 4. Interestingly, our neighbor to the north has experienced a similar increase. 10-Year Government Bond Yield Change Since US Low Notably, global government bond yields hit their low during the pandemic and have trended higher since then. Global Yields If one looks at government yields since the end of last year, a different picture emerges. While higher government bond yields have been a global phenomenon, the U.S. has seen slightly lower yields. 10-Year Government Bond Year-To-Date Yield Change The other proof point for those arguing for the danger of U.S. fiscal policy or the end of American exceptionalism is the recent weakness in the U.S. currency relative to other currencies. While there is no doubt that the U.S. dollar has been weaker year-to-date, it follows a period of exceptional relative strength. The U.S. dollar remains stronger than it has been most of the time since 1999. Further, as shown in the historical data, periods of a weaker U.S. dollar do not uniformly lead to higher U.S. Treasury bond yields. US Dollar Without exception, the fiscal position of large countries, as measured by government debt relative to GDP, deteriorated with the impact of spending during the pandemic. Most countries were already piling on more debt relative to economic activity, but the pandemic accelerated the trend. Government Debt-To-GDP There is no magic level of debt-to-GDP that signals disaster since countries with a more resilient economy can service more debt. Furthermore, countries like the US, which has a high per capita GDP, control of the global reserve currency, and only issues debt in that currency, can handle significantly more debt levels than most other countries. German government debt levels are understated relative to reality. While Germany isn’t legally liable for the debts of its profligately spending neighbors, it shares a common currency and its share of European Union issued debt. Historically, Germany has been forced to contribute the most to bailout funds when other EU countries have encountered problems. Recent Government Debt-To-GDP Ratios The U.S. House of Representatives passed its tax legislation, which, despite reports to the contrary, does not make the U.S. fiscal situation any worse, according to Strategas. Tariffs should produce about $200 billion in revenue annually and are not included in the legislation’s official scoring, leading to much confusion. The U.S. Senate will almost certainly make some changes. So, the House bill is unlikely to be the final version implemented. The S&P 500 sits only 5.6% below its mid-February high in a robust rebound from the 19% decline from the peak reached on April 8. The Magnificent 7, consisting of Microsoft (MSFT), Meta Platforms (META), Amazon.com (AMZN), Apple (AAPL), NVIDIA (NVDA), Alphabet (GOOGL), and Tesla (TSLA), has fared worse, and the group is 12% below its mid-December summit. Market Returns The proximate cause of the decline in stocks last week was the reheating of the tariff war, which increased the headwinds for the U.S. and global economy. President Donald Trump threatened the European Union with a 50% tariff and smartphone makers, notably Apple (AAPL), with a 25% tariff rate. The betting odds of recession rose to 41%, sending stocks 2.6% lower last week. Betting Odds of 2025 US Recession Despite the likely economic drag from the tariffs, only two expected rate cuts for 2025 are expected. There is little chance of a rate cut at the mid-June Federal Reserve meeting. Number Of Fed Rate Cuts Expected The primary focus will likely remain on the fallout from the tariffs, with markets watching for any changes in U.S. policy and retaliation or concessions from other countries. Friday’s April inflation reading will likely be friendly. The Core PCE Price Index is the Federal Reserve’s favorite measure of inflation and should moderate to 2.5% year-over-year from the 2.6% pace in March. The last Magnificent 7 stock to report earnings is Nvidia (NVDA) on Wednesday after the close. As the leader in artificial intelligence chips, its results and forecasts will be closely watched to judge the health of technology spending. The rise in U.S. Treasury yields does not point to a U.S.-specific problem; instead, it is a function of the lower probability of an economic downturn and perhaps a shift in the global appetite for government bonds. Yields are not high enough yet to significantly negatively impact stock valuation since the higher yields are accompanied by less risk of an earnings decline from a recession. Like many other countries, the current fiscal trajectory in the U.S. is unsustainable, but the recent tax bill wouldn’t worsen things. Investors can be forgiven for wishing it improved the path, but the Senate will have their say next, and bond market participants will be watching closely. Government bond investors are demanding higher yields from most countries, so there might be a shift in the willingness of markets to fund large deficits, but it is too early to know for sure. Disclosure: Glenview Trust may hold the stocks mentioned in this article within its recommended investment strategies.
--------------------------------------------------

Title: Best Memorial Day Deals Under $50: We've Found 35+ Deals Offering Great Value This Weekend
URL: https://www.cnet.com/deals/best-memorial-day-deals-under-50-2025-05-25/
Time Published: 2025-05-25T10:24:02Z
Full Content:
We're in the final stretch of sales -- snag these budget deals on earbuds, kitchen appliances and more before they're gone. Our expert deal-hunting staff showcases the best price drops and discounts from reputable sellers daily. If you make a purchase using our links, CNET may earn a commission. If you didn't get a chance to wrap up your shopping in the last couple of days, you're in luck -- Many of the best Memorial Day deals we've seen this year are still available for picking. There are plenty of lingering deals from retailers like Amazon, Best Buy and Home Depot, but many of these offers will end soon. If you've already finished shopping for big-ticket items like TVs, appliances or camping gear, tons of wallet-friendly deals can still be found on tech and daily essentials to stretch your remaining budget. Shop these deals now to score some serious savings before prices bounce back. Bookmark this page and check back often -- we’re refreshing it regularly as deals end, sell out or get last-minute markdowns. And if you’re hunting for even lower prices, be sure to explore our curated list of Memorial Day deals under $25 that are still live. Amazon Fire TV Stick 4K is a great way to convert any 4K TV with an HDMI port into a smart TV. Whether you have a TV that doesn't have smart features or if you just don't like the smart TV interface of your current TV, the Fire TV Stick 4K is the product for you. With this $20 discount off of the $50 list price, the Amazon Fire TV Stick 4K is an even better value. The JBL Go 4 is a tiny but mighty Bluetooth speaker. You can take it anywhere as it has a small loop that you can use to attach to your backpack, keychain or anywhere else. It's small enough to put in your pocket or purse, but it doesn't skimp on sound with 5 watts of power and ample volume. The battery lasts seven hours, so you don't have to worry about it running out of power quickly. This multi-device charger from ESR is perfect if you've been wanting to reduce the number of cables in your charging setup. It supports the Qi2 standard and will have your iPhone, Apple Watch and AirPods ready to tackle the day all at once. Ugreen's Nexode series has some of the best chargers we've seen. This particular adapter has a 4-port design (three USB-C and one USB-A) and can be used to charge more than just phones. It's also compatible with laptops and handheld consoles. If you're often jet-setting from one country to another or have friends and family that visit often, then this universal travel adapter is something you need. It has a portable design, safety shutters and international compatibility, making it a must-have. More Memorial Day deals under $50: Glad you asked. If you're looking for even more savings, here are the best still-available Memorial Day deals under $25. You can also take a gander at some of the best Memorial Day bargains being offered by Amazon and Walmart. For a grand overview, check out our roundup of the ultimate sales and lowest prices still available after Memorial Day weekend. Keep in mind that the holiday has passed, so the deals won't be around for much longer. Many of us at CNET have covered shopping events for over five years, including Black Friday, Prime Day, Memorial Day and countless others. We've gotten good at weeding out scams and superficial deals, so you see only the best offers from all over. When choosing deals to show you, we look for real discounts, quality reviews and remaining sale time. Our team of experts has tested countless products to ensure we're only sharing the best options under $50.
--------------------------------------------------

Title: ValueAct takes a stake in Rocket Cos. Here’s how the activist may help lift the share price
URL: https://www.cnbc.com/2025/05/24/valueact-takes-a-stake-in-rocket-cos-how-the-activist-may-help-lift-shares.html
Time Published: 2025-05-24T12:52:55Z
Description: Activist ValueAct has a keen appreciation for artificial intelligence, and Rocket is poised to supercharge its business.
--------------------------------------------------

Title: How to Liberate Yourself
URL: https://erickimphotography.com/blog/2025/05/23/how-to-liberate-yourself/
Time Published: 2025-05-23T23:33:56Z
Full Content:
It is all here: Financial, economic independence, liberation: MSTU—> 2x levered long MSTR, REX Shares, trustworthy as the CEO did a live interview with Saylor on BMAX, … which holds corporate Bitcoin backed bonds ,,, mostly Strategy. So,,, if bitcoin goes up 5% in a day, Strategy will go up 10%, then MSTU (2x MSTR) will go up 20%—> simple math! Buy bitcoin with Coinbase, mortgage as much of it as you can, use the cash, to buy MSTR and or MSTU (2x levered long MSTR, which is essentially 4x bitcoin). You can then: Once JPMORGAN Chase starts offering you the chance to buy bitcoin with them and or to custody it with them, then you know you’ve arrived! Introduction to Bitcoin Lecture Video Super pumped to share with you, my first full length lecture on an introduction to bitcoin, the bitcoin Revolution, and also this edited transcript that I provided for you! STOIC VLOG Something I have been meaning to write or create or do is like some sort of book, ebook, pamphlet, or introductory primer to stoicism. I really think that stoicism is probably one of the most useful and philosophical models to live normal every day real life. Yet, I haven’t really found a good instructional guide on it, especially when I was self teaching it to myself. Consider this a practical primer, cutting through the BS: Stoicism, stoic, the stoa in ancient Greece– essentially the stoa was like some sort of portico, patio, pillar, outside, essentially a spot where guys would just hang out, talk shop, talk philosophy, etc. ￼ ￼ I think about the show “Hey Arnold” in which I was raised with… the notion of “stoop kid“, the notion of a stoop is that in a lot of cities, especially the east coast in New York, you have this little stoop or porch, stairs that go outside your front door… and you could just hang out there, engage in social and neighborhood life etc. One of my happiest moments was when I was living in Providence Rhode Island, and then COVID-19 hit. Everything was closed, besides the park. I can still go to the park, hang out, workout, do chin ups– I learned how to do muscle ups, more bodyweight calisthenics stuff, and also… I had a lot of fun with this “rock toss“ challenge and workout… in the middle of the park was a huge ass rock and huge ass stone, and every single day I would go there pick it up, and then eventually work out with it; throwing it around for fun, doing overhead presses with it, clean and jerks, squats, and eventually I would just throw it around for fun. Funny enough it might have been the most fit I was in my life… this was the true “functional” fitness. The inspiration — Hector lifting an insanely massive stone (barely 2 strong men could lift it)… using it to break down the door of the ships of the other side. Anyways, the reason why that period of covid was so good is that it was in the middle of beautiful Providence Rhode Island summer, so nice and bright and warm and lovely… and one of the good things was going to the park was like an open forum, a new anatheum for a lot of really cool guys to come, hang out, talk shop, go topless and shirtless, workout and hang out. I met some really interesting people during that period of time. I met some guys who were really cool. For example, one guy I met was in the US military Navy, I think he was training to be a Navy seal or Delta force or something. Another guy in some sort of ROTC training, another cool guy from the hood, and also I would say I probably met half a dozen friendly drug dealers there. And of course a lot of people who believed in conspiracy theories; really friendly, a little weird, but overall good guys. Anyways, one of the biggest benefits of hanging out at that outdoor park, open air, nothing but green grass, the beautiful sun and the fitness equipment was that I think having this sort of open air environment is actually very conducive to socializing, thinking and thought, and pro social behavior. My theory about a lot of modern day antisocial behavior has to do with the structures which enclose us. For example, almost universally most guys at the gym are extremely antisocial. Why? My theory is that because most gyms have closed, cramped narrow ceilings, and do not have access to natural light, or outside space. Cramped indoor spaces promote antisocial behavior. The only good gym I went to which was interesting was the golds gym in Venice, which has this really big outdoor workout area. I think this is much more natural and more fun and better; to be able to work out directly outside outdoors, with your shirt off. Stoics, stoicism — it was originally I think codified by this guy named Zeno, and over time he picked up some followers. Essentially the whole thing happened organically; Zeno would first share his thinking on philosophy ethics and pragmatic ways to deal with other people and the downsides of life, he built a following, and then his followers would propagate the thoughts and start their own little schools of thoughts, their own little stoic clubs. Would I like about stoicism is how loosey goosey it is. It is kind of like zen, or taoism… it is not really quantified as a religion, or a strict moral order. In fact, a lot of the ancients stoics would meditate on random stuff like cosmology, natural sciences like Seneca, how volcanoes worked or whatever. I think nowadays in today’s world, we focus primarily on the pragmatic side; how to deal with fear, uncertainty, downsides etc. ￼I think I might’ve first learned about stoicism from Nassim Taleb and his ANTIFRAGILE book. I was curious, and my curiosity went to deep. To quote NASSIM TALEB and the Venetian saying “The ocean goes deeper, the deeper you wade into it.” I literally consumed every single book I could find on stoicism, even the obscure ones. Funny enough, a lot of the stoic thinkers tried to claim other philosophers as being stoic, like Seneca did with Diogenes the cynic. ￼ Cynic, cynicism, actually comes from the word canine, the dog. Diogenes was considered the “dog” philosopher, first used as a pejorative, but ultimately Diogenes reappropriated that title for fun! He saw dogs as tough, almost like wild wolves, rather than seeing them as a negative thing. Even Achilles when he was raging against king Agamemnon, he called him “dog faced“ as a heaping insult. First, Seneca. Seneca the younger, his dad was called Seneca the elder. In fact, this is such a big deal because Cindy and I named our first son, Seneca, directly after the stoic philosopher. This is true soul in the game; if you name your kid after your favorite philosopher, certainly it is a sign that you really liked that philosopher, or found them impactful. ￼￼￼The reason why I really like Seneca the stoic philosopher is because he had real connections to real reality. What that means is he wasn’t just on the sidelines; he actually existed in the real world, engaged in real politics, was even advisor to the emperor Nero, the bad one, who eventually low-key coerced Seneca to commit suicide, in a manly, dignified manner. I think this was because maybe… there was actually a plan to overthrow Nero, and essentially Nero found out. ￼ I really like Seneca because his writing is accessible, practical and pragmatic, and interesting. A lot of thinkers tend to lack connections to real reality, I have no tolerance for boring philosophers to talk about metaphysics, which is things which are not physical. Like thoughts ideas, the universe, electricity and energy, strange phenomenon and conspiracy theories on ghosts, “energy” whatever. ￼￼For a long time, I would hear the term “metaphysics” being thrown around, and I had zero idea what it actually meant. ￼ Meta– on top of. Or nestled within. Physics — the physical, physical phenomenon like gravity, first principles. The reason why metaphysics philosophers tend to be a bunch of losers is that they are all weak and anemic, nerds or geeks or weaklings who seem to have some sort of physiological degeneracy, which encourages them to opine or talk or think about impractical things, superficial things. Personally speaking, I think philosophy must be practical. ￼￼￼￼ Practical, praxis, ￼practice — to do! I have a very funny ideal; the general idea is that your body looks like a demigod, and your physiology is out of control. The general idea is one must be tall, strong, highly muscular, low body fat percentage, I’m not exactly sure what my body fat percentage is, but maybe it’s around 5%. ￼￼Also, physical fitness is critical to any stoic. My ideal is to walk 50 miles a day, eat 20 pounds of meat like Milo of Croton a day. And also, abstinence from silly things like media, alcohol, drugs, marijuana etc. Trust no thinker who does drugs! Even our best friend Nietzsche said that coffee was bad, because it would make people dark and gloomy. He encouraged 100% cocoa powder instead. ￼￼￼ Simple technique: First, look at a picture or a portrait or a full body shot, ideally topless of the artist, philosopher or thinker or individual… then judge their thoughts later. Why? My theory is this: the thoughts of an individual is hugely affected by their bodily physiology. For example, an extreme example: if somebody is locked inside a solitary confinement cell, and not permitted to go outside for years, but, he had a pen and pad and would jot down some thoughts… Would be the quality of these thoughts? Certainly dark and morose. In today’s world, why does it matter, what is the significance of stoicism, etc.? First and foremost, I think we are living in a troubling time, especially with the advent of modern day internet based media and advertising. I think 99% of what is propagated on the internet is fear mongering, and what is hate? Hate is just fear. The first thought on stoicism is that it is just fear conquering. What I discovered about street photography, is that 99% of it is conquering your fears. Conquering your fears of upsetting other people, getting in some sort of verbal or physical altercation etc. In fact my bread and butter workshop is my conquering your fears and street photography workshop, the workshop which is still interesting to me even after a decade. Why is this so important? I think it is rooted in almost everything; conquering your fears is rooted in entrepreneurship, innovation, risktaking and real life. Even my speculation in crypto. 99.9% of crypto speculation is just balls. Having the balls to make big bets, and when things go south, knowing how to master your emotions. A simple extra I have is this: just imagine it will all go down to zero. It was useful because when I was in college, my sophomore year I got really into trading stocks, and I eventually lost my whole life savings, maybe around $3500 USD, and some bad penny stock which I actually misread the financials… the whole time I thought the company was making a profit, but actually it was taking a loss. I actually didn’t know that if profits are written in parentheses, it means a loss. It was funny because my initial start as an investor was back in high school, I bought some Adobe stock when I was a high school junior, and also some mutual funds, which both went up after about 4-5 years. ￼Also I remember in elementary school computer class, when I was in the sixth grade in Bayside Queens, there was some sort of stock stimulation trading game, and actually it was funny… the kids who made the most money and were the most successful just put 100% of everything into Apple, note this is when we were only 12 years old, and I was born in 1988. ￼Funny enough, it seems that stoicism actually plays well with capitalism. Why? According to modern day capitalist thinking, the best way to approach life is to be objective, strong, stoic, unemotional, logical and rational. Also, ￼with modern day media there is so much fear mongering in the news, about some sort of global armageddon, global financial ruin, etc. I call it “fear porn”. Therefore stoicism as a mindset is useful to think and position your mind in such a way that you could consider that life is all upside, no downside. In fact, if I could summarize stoicism in one sentence, it is that life is all upside, no downside. Inspired by NASSIM TALEB. So, is stoicism useful to you if you’re a man or a woman? Does it matter? The good thing is I think it could apply to both sexes. Conquering sexism and social pressures is useful if you’re woman, and also if you’re a man. Also, gender is social. Lot of the expectations set on us by society is socialized and gamed to a certain degree. First, we got to unchain ourselves from modern day ethics and morality. I believe that all modern day philosophy and thinking and ethics and religion is bad. For example, the notion of turning the other cheek is a patently bad one. Why did Jesus turn his cheek? It is because he lacked on army. Also, philosophically I think we should put no trust in Socrates. ￼￼￼I thought which has puzzled me for a long time was this “Why was Socrates so ugly? Monster in face, monster in soul. I think Socrates was a degenerate, and he lacked any sort of real power. Therefore he turned logic and rationality into his terrorizing weapon (via Nietzsche). Back in the day, you didn’t need logic or rationality to have things your way, you simply was able to dictate that which you wanted to pause it, because you had a military force behind you. Just think about Machiavelli and IL PRINCIPE– the reality of being a mercurial prince, king, and military leader is hard, stoic, “immoral”. But ultimately it all comes down to war, conquest, the military. Trust nobody who uses rationality or logic as their tyrannizing weapon. ￼ In fact, I believe that all should have the body and strength of some sort of super soldier. Essentially look like all the guys from the movie 300, this is our ideal. Demigod physique. ￼ And this is the true courage of Kanye West; he literally put everything on the line, and even lost his spouse and I think maybe his kids? All for the sake of revealing inequities. ￼ “I throw these Maybach keys fucking c’est la ￼vie! I know that we the new slaves.”￼- Ye The fun thing about stoicism is that you could just make it up as you go, devise your own strategies and whatever. “Fucking c’est la vie!” My favorite Kanye West line. Essentially the general idea is that in life, one should not take things too seriously. Laughter is golden, I forget the philosopher who was called the laughing philosopher… Democritus?; better to laugh about the follies of human beings rather than to be dark and morose about it. Also, thoughts from the Odyssey; if you look far enough ￼ into the future, everything becomes comedic and hilarious. So when you’re in some sort of bad situation, just think to yourself “Perhaps one day, 20 or 30 years from now… I will look back at this and just laugh!” It will just be humorous. Honestly, laughter, and kind of being able to joke about things might be the best way to live life and deal with setbacks. ￼ Problems in modern day life: A funny thing I have learned is that when you call something something, it isn’t that. For example, if someone calls something a “luxury car”, it ain’t. For example, a true modern day luxury car is maybe a Tesla, but Tesla never calls itself a luxury car. Also the ultimate luxury technology company is probably Apple… but Apple is very intelligent and not calling themselves a luxury brand. A pro tip is when it comes to websites, read the alternative text, the header text, the stuff that shows up in the tab of your browser window. If the website, the automotive retailer tries to market themselves as a “luxury” brand, typically it is actually a sign that it isn’t a luxury brand it isn’t luxury brand. Thought: what are some good examples of true luxury brands which don’t overly calls itself luxury? ￼￼ In someways, we can think and consider stoicism as our new luxury. In fact, having luxury, luxury of mind and soul… and luxury of freedom of speech, isn’t this the ultimate luxury? When somebody asked Diogenes the cynic; “What is the best human good”? He said “Freedom of speech, speaking your mind, having the power to see whatever is on your mind.” In fact, my current joy is becoming more and more free talking, and free riding. What that means is this; I’m ain’t going to censor myself no more, even if I might be politically incorrect insensitive or whatever. ￼ Also, I would prefer to speak my mind and seriously hurt the feelings of others, rather than soften it for the sake of the other person. ￼￼￼Similarly speaking, when people call themselves “influencers”, they are not influencers. Ultimately I think we should think of stoicism just like having another tool inside our tool kit. For example, if you’re a chef, you’re going to have different knives for different purposes. If you’re going to cut a big piece of meat, you probably want a big ass meat cutting knife, not something you would use to slice an apple with. Similarly speaking, if you’re going to scoop out the insides of an avocado, better to use a spoon rather than using a fork, or a knife. ￼ I think the problem is when some people get too into stoicism (I prefer writing stoicism with a lowercase￼), they think that everything needs to be consistent, and must fit into this nice little neat box of what is considered “stoicism“. This is a bad line of thinking… let us consider that Marcus Aurelius never even mentioned stoicism in his writings, his collections of thoughts, which we moderns call THE MEDITATIONS… it was just essentially his personal diary, to help him conquer his own personal fears and thoughts, I don’t think he ever intended it to be published publicly. I think he just wrote it to himself as self therapy. And I think the only stoic philosopher he even mentions is maybe Epictetus. For myself, I just come out with certain to work out thoughts and techniques because it helps me, and when I find these tricks or techniques or secret hacks or cheat codes… My passion is to simply share it with others. ￼￼￼￼ And ultimately, things are ever in flux and evolving and changing and adapting. For example, I’ve discovered the quality of my thinking is different when I am in Culver City Los Angeles, compared to being in the boring suburbs of Orange County. Also depending on my social environments… my stoic thoughts are different when I am in a gym, vs just working out by myself in my parking spot in the back of my apartment. Also, the quality of my thoughts is different when living with family members or other people versus just living with myself Cindy and Seneca. Assuming you’re not growing your own vegetables and living in the middle of nowhere… you probably have some interaction with other human beings. As long as you have an iPhone, an Android phone, a smartphone, a 4G or 5G internet connection, wifi, a laptop, have to buy groceries somewhere… you’re still going to have to interact with other human beings. And this is good. There is no other greater joy than other human beings. In fact, modern-day society is strange because in someways, the ethos is to be antisocial and to be cowardly. But in fact, the best way to think about things is that real life is interaction with other human beings, and social conquest. One can imagine a lot of modern day entrepreneurship as simply a big dick swinging contest. He who is the most masculine confident tall and strong and stoic shall win. Assume that everyone is mentally insane: Have you ever been out in public, and you see some sort of crackhead or strange homeless person who acts radically, smells terrible, and is obviously mentally ill? Do you hate them for it? When they say something weird to you… do you take it personally? No. Why? They are crazy. Perhaps we should just adopt this stoic mindset towards other people; some people are actually physiologically ill, mentally unwell… don’t trust the opinion of nobody. A lot of people are trying to actually deal with their own inner demons: For example, becoming the successful photographer and street photographer I am today… I’ve dealt with some individuals who would say anonymous bad things about me, and later I found out that their mom just died or something. I cannot imagine what it feels like losing a mother… therefore if somebody spew some hate on me because something bad happened to them, I’m not gonna take it personally. ￼￼￼￼￼Self-flagellation: I think a lot of people who are sick, mentally or physiologically self flagellate themselves. Essentially the way that they deal with other people or themselves is some sort of metaphorical self-flagellation. For example… you know those strange individuals who have the whip and whip themselves, and inflict pain on themselves? I think some people do this metaphorically to themselves and others. You just want to stay away from them. My personal theory on fear is that a lot of it is tied to morality and ethics. I think the general idea is not necessarily that we are afraid of anything… I think the true fear is that we’re afraid that we are some sort of bad evil unethical immoral person. For example in street photography, the general ethical thought is that it is immoral to take a photo of somebody without their permission, because there is some sort of it inherent evil behind it. Is this true? No. Taking photos and not really a big deal. I think it is because some people are just overly sensitive, which once again comes from some sort of physiological weakness. For example, if you’re a weightlifter who could lift 1000 pounds, assuming you’re not taking any steroids or anything… are small things going to bother you? No. But let us assume that you are a skinny fat man, all you do is drink alcohol and smoke marijuana and watch Netflix, and you spent too much time on Reddit… you are 40% body fat, and have never lifted in your life. And also your testosterone is low and you never go outside. Certainly the quality of your thoughts is going to be different than if you’re a happy gay monster, lifting weights outside in the direct sun, laughing and having fun. ￼￼￼ In fact, I’ve actually personally discovered that the reason why a lot of people hate me is because I am so happy jovial and gay. They are secretly suspicious or envious of me? ￼ Probably one of my worst experiences was this jarring transition; I was super happy insanely happy being in Vietnam in 2017; with a beautiful weather, the beautiful light, the happy people the great amenities etc.… and then that winter Cindy and I went to Europe, in Marseille Berlin and Prague, and maybe London… seriously the worst winter of my life. Why? I wonder if so much miserable feelings and thoughts simply comes from the darkness and lack of light. a lot of Europe is actually quite miserable; dark, unhygienic, morose. Even Nietzsche had a thought about Schopenhauer; How much of these emo European philosophers came from the fact that it was just complaining about the cold weather in Germany etc.?￼￼For myself, my ideal weather is Southeast Asia; I love being in Phnom Penh Cambodia, Vietnam etc. In the states, am I the only one who loves living in Los Angeles? Dr. Dre and Kendrick Lamar said that LA was the best for women weed and weather… I would definitely say the biggest upside of living in Los Angeles is the light, the sunlight. It actually does get quite cold here, but usually most reliably even in December during the winter time, the sun will always come up. As long as there is bright sunny light, I will be happy. And I think maybe for myself, considering that I am a photographer, and photography means painting with light… light for me is critical. I also wonder how much of it is a physiological thing and a genetic trait; for example I could even recall being a young child, and my mom telling me that the most critical thing in finding a home or an apartment was light and natural light. Even now… 90% of my happiness comes from being able to ￼have access to natural light, ideally floor to ceiling windows facing directly the sun, having some sort of modern temperature regulated apartment and home. Even living in our tiny studio minimalistic luxury apartment in Providence Rhode Island, where it was always 75° warm and cozy, and not frigid and damp and cold and dark and humid… I was always good. But moving to an older house, where it always felt damp and cold… this literally lowered my happiness by 1000%. Therefore, if you’re feeling miserable sad or whatever… I say spend three months living in Hanoi or Saigon in Vietnam, or go to Phnom Penh Cambodia. I wonder if 90% of peoples misery is simply due to the weather. ￼￼”Better to be a gay monster than a sentimental bore!” – Fernandino Galliani, via Nietzsche My stoic ideal is somebody who is happy, gay, smiling, no headphones or AirPods on, no sunglasses on, no hat, no facial hair, no baggy oversized clothing, no tint in their car. Somebody who makes great eye contact, laughs, stands up upright, jokes, and fools around. Like an overgrown child. ￼Also, lift weights at least once every day, ideally in the direct sun. Just buy some weightlifting equipment on Titan.fitness, I like the farmers carry handles, the Olympic loadable dumbbell, and also the Texas power squat bar. Just buy some cheap weights, and or buy a heavy 400 pound sandbag, and just have fun throwing it around. A true stoic should look something like Hercules or Achilles. Or like ERIC KIM; I have the aesthetic and the physique of Brad Pitt in FIGHT CLUB except with a lot more muscle. Like my friend Soren says, the Adonis physique and proportions. I think a real stoic is sexy, happy and fun. Who doesn’t take life too seriously; and think of everything like a fun game. A real stoic would be joyful and cheery like three-year-old child without any adulteration from the outside world. I don’t like talking with or hanging out with adults, uninteresting. At what point or age do people become so emo? Typically, highschoolers are very optimistic. Even college students. But I think at least in maybe college in high school nowadays… the bad trend is towards “over concern”, about the world the planet ethics animals etc. I find a lot of this thinking superficial, performative, and uncritical. I think “animal rights“, “saving the planet” is this new pseudo world religion; which is just capitalism 3.0. I find the whole pet industry the whole dog industry to be insanely bizarre, and I trust nobody who talks about “saving the planet“ who owns an iPhone, owns any sort of car, or has an Amazon prime subscription. Certainly not any vegans. Animals are animals. They are lower on the hierarchy and totem pole on earth. Man is the apex predator, the apex bully and the apex tyrant. Should we care for animals or “animal rights”? No. Animals are our slaves. If you consider even dogs and pets… they are essentially our emotional slaves. People talk a lot about the virtuosity of dogs being loyal or whatever… and giving you unconditional love. This seems like some sort of emotional slavery. The only dogs I respect are some sort of canine dogs, some sort of attack or defense dogs, or hunting dogs. For example, John Wick 3; Halle Barry and her dogs. An animal should either be a weapon, or nothing. Essentially it looks like men no longer have a backbone. No more spine. I trust nobody who owns a dog. Let us not forget; they call it dog ownership, or “owning a pet”. There is no more concept of “human ownership, or “owning a human.”￼￼￼ What is the end goal of humanity? To me it is towards entrepreneurship, innovation, art and aesthetics, philosophy etc. Design. Stoicism should be considered a tool which could aid you in these things. For example, I think 99% of entrepreneurship is courage. Stoicism could help you with that. I also think with design, great design is also 99% courage, having the courage to attempt something that won’t sell or be received well… stoicism is all about practical courage. The only designers with courage include Steve Jobs, Jony Ive, Elon Musk, Kanye West. ￼ Also, weightlifting. To attempt to lift a certain weight you have never attempted before takes great courage. For example, me atlas lifting 1000 pounds; that is 10 plates and a 25 on each side, ￼￼￼this is true stoic training. Why? The fear of injury is what holds most people back; if you had successfully conquered this fear and not injured yourself, this is pure stoic bliss. I think the only and the only proper way to lift weights is one repetition maximum training. That is; what is the maximum amount of weight you’re able to successfully lift or move, even half an inch? To me, the courage is the success. Even if you had the courage to attempt it… that is what is considered success. ￼￼ Simple exercises to do include the atlas lift, innovated by ERIC KIM, or a one repetition max rack pull. Or, a high trap bar deadlift, heavy Farmer’s walks, or heavy sandbag carries. Or even a simple thing you could do is go to the park or to the local nature center, find the biggest rock there and just see if you could pick it up. If you’re interested in stoicism, and have had some interesting thoughts on stoicism, one of the most noble things you could do is start your own blog. I think blogs are 1000 times more effective than publishing some sort of static printed book; I think the problem in today’s world is that everyone is seeking some sort of legitimacy by being picked up by some sort of legitimate publisher and getting “published“, and seeing your printed book at Barnes & Noble whatever. I say it is better to be open source, free and permissionless, ￼￼￼decentralized. Just publish your thoughts and book as a free PDF, and just host it on dropbox, Google Drive, or your own web server. Share the link freely, and also just publish the raw text as a big blog post. ￼ Even Sam Bankman-Fried wisely thought; 99.9% of books could just be summarized as big blog posts. Don’t trust any modern day published book which isn’t free, because… there is some sort of hidden clout chasing somewhere. ￼ Even one of the worst compromises that led to the demise of Ray Dalio was the fact that he took his Principles book, which was essentially a free ebook PDF on his website, and then took it off, because I think he got a book deal with Simon and Schuster. After he did that, he lost my respect. If you’re already independently wealthy, and you don’t crowd source your self-esteem… why would you need to externally validate yourself by getting some sort of constipated publisher and annoying editor? Editors are bad. Start your own blog and start blogging your own thoughts on stoic philosophy, and even start a YouTube channel and start vlogging on it. ￼￼My generalized thought is simple: if your thought your idea your blog post your video or whatever could even impact the life of one other human being on planet earth… it is worth it. ERIC What is the secret to the maximum amount of happiness in life? The maximum amount of danger. (Nietzsche). ERIC FIN Become invincible: Learn from the master stoics: See all philosophy > ALL OPEN SOURCE EVERYTHING! EK IDEAS
--------------------------------------------------

Title: DoubleVerify Holdings, Inc. Investors: Robbins LLP Reminds DV Shareholders of the Pending Class Action Lawsuit
URL: https://www.globenewswire.com/news-release/2025/05/23/3087752/32719/en/DoubleVerify-Holdings-Inc-Investors-Robbins-LLP-Reminds-DV-Shareholders-of-the-Pending-Class-Action-Lawsuit.html
Time Published: 2025-05-23T21:26:00Z
Full Content:
May 23, 2025 17:26 ET | Source: Robbins LLP Robbins LLP SAN DIEGO, May 23, 2025 (GLOBE NEWSWIRE) -- Robbins LLP reminds stockholders that a class action was filed on behalf of investors who purchased or otherwise acquired DoubleVerify Holdings, Inc. (NYSE: DV) common stock between November 10, 2023 and February 27, 2025. DoubleVerify operates a software platform for digital media measurement and advertising optimization services. For more information, submit a form, email attorney Aaron Dumas, Jr., or give us a call at (800) 350-6003. The Allegations: Robbins LLP is Investigating Allegations that DoubleVerify Holdings, Inc. (DV) Misled Investors Regarding its Business Prospects According to the complaint, during the class period, defendants failed to disclose that: (a) DoubleVerify’s customers were shifting their ad spending from open exchanges to closed platforms, where the Company’s technological capabilities were limited and competed directly with native tools provided by platforms like Meta Platforms and Amazon; (b) DoubleVerify’s ability to monetize on its Activation Services was limited because the development of its technology for closed platforms was significantly more expensive and time-consuming than disclosed to investors; (c) DoubleVerify’s Activation Services in connection with certain closed platforms would take several years to monetize; (d) DoubleVerify’s competitors were better positioned to incorporate AI into their offerings on closed platforms, which impaired DoubleVerify’s ability to compete effectively and adversely impacted the Company’s profits; (e) DoubleVerify systematically overbilled its customers for ad impressions served to declared bots operating out of known data center server farms; and (f) DoubleVerify’s risk disclosures were materially false and misleading because they characterized adverse facts that had already materialized as mere possibilities. The complaint alleges that the truth was revealed on February 27, 2025, when DoubleVerify reported lower-than-expected fourth quarter 2024 sales and earnings due in part to reduced customer spending and the suspension of DoubleVerify services by a large customer. Defendants also disclosed that the shift of ad dollars from open exchanges to closed platforms was negatively impacting the Company. On this news, DoubleVerify’s stock price dropped $7.83 per share, or 36%, from a closing price of $21.73 on February 27, 2025, to a closing price of $13.90 on February 28, 2025. What Now: You may be eligible to participate in the class action against DoubleVerify Holdings, Inc. Shareholders who want to serve as lead plaintiff for the class are required to file their papers with the court by July 15, 2025. The lead plaintiff is a representative party who acts on behalf of other class members in directing the litigation. You do not have to participate in the case to be eligible for a recovery. If you choose to take no action, you can remain an absent class member. For more information, click here. All representation is on a contingency fee basis. Shareholders pay no fees or expenses. About Robbins LLP: A recognized leader in shareholder rights litigation, the attorneys and staff of Robbins LLP have been dedicated to helping shareholders recover losses, improve corporate governance structures, and hold company executives accountable for their wrongdoing since 2002. To be notified if a class action against DoubleVerify Holdings, Inc. settles or to receive free alerts when corporate executives engage in wrongdoing, sign up for Stock Watch today. Attorney Advertising. Past results do not guarantee a similar outcome. A photo accompanying this announcement is available at https://www.globenewswire.com/NewsRoom/AttachmentNg/39664050-92f3-4fd9-ac56-162b985c1fde Robbins LLP is Investigating Allegations that Red Cat Holdings (RCAT) Misled Investors Regarding its Production Capacity Robbins LLP is Investigating Allegations that Elevance Health, Inc. (ELV) Failed to Disclose the Impact Medicaid Redetermination Would Have on its Business
--------------------------------------------------

Title: Wall Street cheers as Tesla gets massive price target boost, Dan Ives predicts a golden age of autonomous growth
URL: https://economictimes.indiatimes.com/news/international/us/wall-street-cheers-as-tesla-gets-massive-price-target-boost-dan-ives-predicts-a-golden-age-of-autonomous-growth/articleshow/121369176.cms
Time Published: 2025-05-23T19:07:05Z
Full Content:
(Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. What pizzas are Indians eating? The clue lies with India’s largest QSR. Why high gold prices are making both banks and their borrowers smile How does IndusInd’s ‘fraud’ tag affect its future? RBI has cut repo rates twice in 3 months. But that isn’t enough to boost the economy! As India steps up commercial shipbuilding, it needs a supply chain boost Stock Radar: RITES stock breaks out from Ascending Triangle pattern; time to buy? Trump calls for 50% tariffs on EU goods Bondi condemns Israeli Embassy shooting 'We didn’t hit their army, only terror camps' Vance's big remark on Houthis weeks after ceasefire Trump threatens Apple with 25% tariff $50 Billion Door Opens for US Firms Iran’s fresh warning to US amid nuke tensions Mukesh Ambani's 6 commitments for Northeast states Live: PM Modi's address at Rising Northeast Investors Summit Iranian FM warns US against deal terms on enrichment Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Story Listing Top Prime Articles Top Definitions Top Slideshow Private Companies Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Tesla stock gets a huge price target hike as Wedbush hails 'golden age of autonomous growth'
URL: https://finance.yahoo.com/news/tesla-stock-gets-huge-price-144500442.html
Time Published: 2025-05-23T14:45:00Z
Description: Tesla (TSLA) might have been in the news recently for all the wrong reasons, but that’s not stopping Wedbush from issuing one of Wall Street’s most bullish...
--------------------------------------------------

Title: Tesla stock gets a huge price-target hike as Wedbush hails 'golden age of autonomous growth'
URL: https://qz.com/tesla-robotaxis-austin-fsd-autonomous-ai-1851782128
Time Published: 2025-05-23T14:31:08Z
Full Content:
Tesla (TSLA) might have been in the news recently for all the wrong reasons, but that’s not stopping Wedbush from issuing one of Wall Street’s most bullish calls on the company in months. In a note Friday morning, analyst Dan Ives raised Tesla’s price target from $350 to $500, saying “the golden age of autonomous is now on the doorstep for Tesla.” The price hike is part of Ives’ larger vision for the company: a possible $2 trillion market cap by the end of 2026 in a “bull case scenario.” The potential catalyst? The long-promised launch of Tesla’s robotaxi service, which CEO Elon Musk claims will debut in Austin by the end of June — a “key next chapter of growth,” according to Ives. Wedbush’s price hike reflects this “massive stage of valuation creation ahead.” But Wedbush’s vision of Tesla is less about cars and more about code. “We have never viewed Tesla simply as a car company,” Ives wrote. “Instead we have always viewed Musk and Tesla as a leading disruptive technology global player and the first part of this grand strategic vision has taken shape over the past 5 years.” Ives sees Tesla as a platform positioned to capitalize on the convergence of autonomous driving and AI — an “AI revolution” that could eventually put Tesla in the same league as Nvidia (NVDA), Microsoft (MSFT), Palantir (PLTR), Amazon (AMZN), Meta (META), OpenAI, and Alphabet (GOOGL). “We believe Tesla remains the most undervalued AI play in the market today,” Ives said. “The core focus for investors is the AI Revolution is now coming to Tesla...which will make Tesla one of the best pure plays on AI over the next decade.” Ives said AI and autonomous are worth at least $1 trillion for Tesla. And the “golden goose”? The EV-maker’s Full Self-Driving (FSD) software, which has recently been more broadly rolled out. Wedbush said the automaker could see FSD adoption rates rise past 50% “and change the financial model/margins for Tesla looking ahead.” Still, the road to autonomy is littered with setbacks. Tesla’s FSD system has faced regulatory scrutiny and safety concerns for years, although Ives speculated that the close ties between Musk and President Donald Trump could help pave the way for fewer regulatory hurdles. And Musk’s timelines for autonomous Teslas have come and gone without delivery; FSD remains driver-assist tech, requiring a human at the wheel. Tesla also isn’t alone in the robotaxi push — Alphabet’s Waymo and Uber (UBER) are still major contenders in the U.S., and China’s Baidu (BIDU) is pushing forward on robotaxis abroad. Still, Ives believes Tesla has a leg up on the competition. “Given its unmatched scale and scope globally,” he wrote, “we believe Tesla has the opportunity to own the autonomous market and down the road license its technology to other auto players both in the US and around the globe.” Wedbush maintains its “Outperform” rating on Tesla, even as Ives warned that the stock’s trajectory will be volatile. “Rome was not built in a day,” he wrote, “and neither will Tesla’s autonomous and robotics strategic vision.” Our free, fast, and fun briefing on the global economy, delivered every weekday morning.
--------------------------------------------------

Title: The best IEMs for gaming in 2025, tested and reviewed
URL: https://www.popsci.com/gear/best-iems-for-gaming/
Time Published: 2025-05-23T14:00:00Z
Full Content:
By Chris Coke Published May 23, 2025 10:00 AM EDT We may earn revenue from the products available on this page and participate in affiliate programs. Learn more › I’ll tell you a secret: If you’re a gamer searching for the best sound quality for your games, you shouldn’t be looking at a gaming headset. Audio enthusiasts know that the best sound quality—for movies, games, and everything in between—comes from great headphones and in-ear monitors (IEMs). But when it comes to getting the the best isolation, audio, and comfort in a compact form factor, IEMs are the only way to go especially if you game on the go. If you’re on the hunt for an upgrade to your current headset and don’t mind using a separate microphone (at least, most of the time), there are lots of incredible options out there. I’ve been part of the IEM hobby for years and have done the hard work of testing dozens upon dozens of sets, on the hunt for the best of the best—like our top overall, the FiiO FA19. If you’re a gamer, this list is for you. I’ve been a gamer for as long as I can remember, cutting my teeth on the Atari 2600. In fact, I got my start in journalism writing about games for a number of small websites. In my adult life, I’ve fallen in love with the world of audio and IEMs in particular. I currently maintain an archive of over 100. Between my closet and storage spaces, I have around 30 pairs of headphones and have given away more gaming headsets than most people will in a lifetime. TL;DR: I love tech, I love gaming, and I love writing about the best picks for different types of people. This list represents my biggest hobbies, rolled into one. Chris Coke Why it made the cut: A customizable sound signature, great details, and a comfortable fit, the FA19 is tops for gaming. If you’re looking for the best of the best when it comes to gaming IEMs, the FiiO FA19 has you covered. This set doesn’t come cheap, but it’s an absolute banger for gaming and music alike. With its customizable sound signature, comfortable fit, and fantastic sound quality, it’s a stellar pick no matter what genre you’re gaming in. FiiO has been one of the biggest companies to watch for a number of years. Getting its start making portable digital audio players (DAPs) and amps, it has since expanded into IEMs, over-ear headphones, desktop amps and streaming media players, speakers, CD and cassette players, and that’s not even the full list. What’s even more impressive is that in most areas where it’s attempted to compete, it’s been able to offer high-quality options at prices that are comparatively quite reasonable. The FA19 is exactly such an option. It uses 10 balanced armatures per side and a shell 3D printed in medical-grade resin. It’s a comfortable fit, even as it offers 20 total speakers between its two earpieces, which usually necessitates a monster shell. The FA19 isn’t small, but it’s not uncomfortably large like many of its competitors are. That many drivers might seem excessive, but there’s a good reason for that excess. Each driver can concentrate on a particular frequency band, ensuring that none are pushed beyond their limits and that distortion is never a concern. Just as importantly, for gaming, all of those drivers make it easier to hear each distinct detail in the environment around you. You’ll never have to worry about footsteps or enemy vehicles sneaking up on you because they got lost in the mix. The FA19 provides exceptionally good layering, so you’ll always be able to make out every unique element that makes up your soundscape. It has also allowed the acoustic engineers at FiiO to fine-tune both of its distinct sound signatures. While the FA19 is hardly the only pair of in-ear monitors to offer a tuning switch, it is one of the few where its addition makes a meaningful difference to the sound. Here, you’re given the option between a reference tuning that’s perfect for discerning small details and a hi-fi tuning that provides a more exciting sound signature with added bass and treble. When I tested these, I found that I preferred the hi-fi tuning for its added energy and that it still performed well even in competitive shooters. For the utmost clarity, its reference option makes for a great alternative. And, surprise, surprise (not really), they are fantastic for music. It comes with a modular cable that supports a standard 3.5mm single-ended connection, as well as a 4.4mm Pentaconn connection for balanced sources, so you should be able to connect to everything from your motherboard to prestigious audiophile hardware (I recommend using a dedicated DAC for the best listening experience). When I’m not gaming with them, I’m using them to play guitar on stage with my church for a clean monitor mix. The only thing they lack is a microphone and a more accessible price point. But with the exception of only a few options in the IEM world, you’ll need to provide your own microphone anyway, so it’s hard to be too critical of this point. Overall, the FiiO FA19 is a pricey but outstanding pair of IEMs. They’re exceptional for gaming. And just about everything else, if you can afford them, they’re an easy recommendation. Tony Ware Why it made the cut: An expansive, holographic soundstage with impressively precise imaging offers strong spatial awareness in competitive gaming. The Kiwi Ears Aether carves a niche with a deft touch, walking the sonic tightrope between analytical precision and atmospheric immersion. These IEMs offer a soundstage that punches above their price point—an expansive, layered field that critics and users alike describe as impressively holographic. It’s not just wide; it’s well-structured, delivering the kind of positional accuracy that audiophiles crave and gamers covet. For music, the Aether’s oversized 15.3mm planar-magnetic diaphragms impress with articulate mids and a tight low end—clean enough for complex jazz arrangements, yet warm enough to give synth-heavy ambient tracks a sense of emotional heft. Highs shimmer without searing, providing detail without fatigue, a characteristic that makes long listening sessions a joy rather than a chore. The soundstage—wide, tall, and deep—creates breathing room for instruments, giving the illusion of space that’s more reminiscent of open-back headphones than a pair of IEMs. But where the Aether really surprises is in gaming mayhem. Footstep tracking in FPS titles is intuitive, thanks to that same dimensional clarity. Environmental cues and directional insights feel more vivid, not just heard but placed—a rare trait in IEMs under $500. Some might perceive a dip in sub-bass impact compared to bass-heavy sets (like the Kiwi Ears x HBB Punch), but it’s a tradeoff that benefits overall transitions and transparency. There’s a cohesion, a refinement to the sound throughout the frequency range, thanks to the single-driver, crossover-free setup (the same reason dynamic driver fans love the low-profile Sennheiser IE 600). The Kiwi Ears Aether isn’t trying to win you over with bombast. It’s a refined, thoughtful performer—an IEM that feels just as at home in a playlist deep-dive as it does in a multiplayer skirmish. Turtle Beach Why it made the cut: This affordable pair of IEMs adds a microphone to the mix and a surprisingly good tuning. Look no further than the Turtle Beach Battle Buds for gaming performance on a budget. At only $30 MSRP and frequently available for less, these earbuds represent one of the best values in IEMs dedicated to gaming. Between its price, performance, and versatility, they’re one of the only pairs of affordable “gaming earbuds” I recommend comfortably. They’re a great value for gamers on a budget. While there are plenty of IEMs that offer in-line microphones, there are very few that feature dedicated boom mics, but that’s exactly what this has. It’s clearer and louder than most in-line mics due to its proximity to your mouth, and that comes in clutch when you need to strategize with your team. When it’s not needed, it simply unplugs from the left earpiece, allowing you to have less weight and improved comfort as needed. Turtle Beach wisely outfitted each earpiece with a silicone retention wing, similar to exercise earbuds, so even with the mic installed, its weight doesn’t get in the way of comfort and stability. The quality of that mic is also surprisingly good. Its closest competitor is the SteelSeries Tusq, which is significantly more expensive at around $50. To my ear, the Battle Buds offer clearer pick-up, though neither offers software enhancements on their own. Since they connect with a standard 3.5mm jack, however, they’re compatible with third-party software. My ASUS ROG Maximus Hero gaming motherboard was able to apply its enhancements without difficulty. The earbuds themselves sound fine, but their quality won’t blow your socks off. I was able to pick out footsteps easily enough, but the Battle Buds lack the impressive layering and soundstage of other options on this list. As the saying goes, you get what you pay for. But for $30 or less, they’re not bad and will let you hear everything you need to without distorting or making you cringe from sibilance. The simplicity of their design, being traditional wired earbuds, also makes them versatile to use with different gaming platforms. Essentially, if your gaming system has a 3.5mm jack, they’re going to work, so PC, Switch, PS5, and Xbox are all perfectly compatible. At the same time, there’s no built-in DAC to add extra features like surround sound that would usually come with gaming headsets. This is something to keep in mind for most IEMs, however, and even those that do include DACs rarely offer surround sound or gaming features. As with most budget audio products, there are some concessions you’ll need to make with the Turtle Beach Battle Buds, but for their price, they are a fantastic value for anyone who wants the fit and function of in-ear monitors with a dedicated boom mic. There are plenty of other options that are worth considering, especially if you have a particular feature or purpose in mind. Even if you look for the “best earbuds for esports,” you’ll still find dozens of options to sift through, so I recommend a few other, more specialized sets to anyone scouring the market. For esports, look no further than the ZiiGaat x Fresh Reviews Arete. This set was tuned by Fresh Reviews, a popular YouTuber specializing in finding the best IEMs, headphones, and mice for competitive gaming. Though the brand is still relatively new, it has made a good name for itself with its handful of releases so far. This particular set features a single dynamic driver and four balanced armatures and can deliver a full and detailed sound signature that is also enjoyable for music. Still, it’s clear that competitive gaming was at the forefront of the YouTuber’s mind throughout the design process. It can pull important details like footsteps out of the mix in a way that enhances your situational awareness without sounding tinny like some full-sized gaming headsets. Like most picks, you will need a separate microphone with the ZiiGatt x Fresh Reviews Arete, which is something to keep in mind before pulling the trigger. Because they support a standard detachable cable, however, it’s easy to replace with a different option that includes one, like the Antlion Kimura. If you’re interested in cutting the cord, look no further than the Arc 3 Gaming Earbuds from Cleer Audio. While this is technically a cheat—as earbuds and in-ear monitors aren’t the same thing, and open-ear true wireless earbuds are even more removed—but it’s hard to deny the multiplatform, multipoint tech on display here. Featuring 16.2mm graphene drivers suspended at the ear canal opening to beam expansive, optimized spatial audio into your head while you maintain situational awareness. You don’t want someone sneaking up on you in real life anymore than you do in a game. Cleer Audio While there are plenty of true wireless earbuds to choose from, most pair over standard Bluetooth, which isn’t suited for gaming thanks to its higher latency. The Arc 3 buds sidestep that entirely with an included USB Type-C 2.4GHz dongle for a dedicated, ultra-low 30ms latency connection. You can plug it directly into your PlayStation 4 or 5, Nintendo Switch, Meta Quest, or your PC and enjoy wired-like gaming with the ease and freedom of true wireless. The Arc 3 simultaneously features Bluetooth 5.4 with Snapdragon Sound (supporting SBC and AAC but also aptX Lossless and LDAC) for hi-res music listening or conversations on the go. Plus there’s a dedicated app to fine-tune its performance and sound, which includes an adaptive mode that adjusts the volume to your surroundings. Their case is a little bulkier than the Galaxy Buds Pro 3 or Apple AirPods Pro, but along with the built-in batteries in each earbud, they offer an impressive 50 hours of battery life. Plus, they’re tuned for gaming audio and Dolby Atmos surround sound, so whether you’re playing a competitive shooter or getting lost in your favorite RPG, these earbuds have something for everyone. Finally, we have an additional suggestion for getting lost in your game with a deep, realistic soundstage and distinct details that enhance your immersion. For strong intelligibility and accurate placement of elements, the 7Hz Timeless 2 is another killer pick. The original Timeless has become legendary in the IEM world as one of the first sets to really kick off the affordable planar-magnetic revolution. This successor was a long time coming, but the Timeless 2 has been well worth the wait. It smooths out the frequency response of the original while still maintaining its excellent detail retrieval and crisp, wide sound signature, though its circular ear shells may appear uncomfortable at first, rest assured that the inner faces are shaped normally to fit comfortably in most ears. This is a set that you can listen to for hours and forget the world with. The stock sound signature is vocal forward with a cooler midrange, but it also comes with a set of interchangeable nozzles to customize the sound signature; however, I would recommend avoiding these as the stock nozzle provides the most balanced sound overall. Choosing a great pair of in-ear monitors is never as straightforward as you’d like it to be, and when you add gaming performance to the mix, it can be even more confusing. But, with the right pair of IEMs, you’ll find a new world of sound quality opens up for you, and you’ll hear your games in a new way—a way that gaming headsets usually struggle to come close to. So, grab an external mic and get to business. Great gaming audio is a quality pair of IEMs away. More deals, reviews, and buying guides Chris Coke specializes in the nuanced world of personal computing, whether it’s reviewing a laptop or sharing the best mechanical keyboard for your gaming PC. He began writing about consumer technology and video games professionally in 2013 for sites such as Tom’s Hardware, IGN, Reviewed, PC Perspective, and MORPG.com. At Popular Science, Chris covers a wide range of tech and buying guides, helping readers improve their lives through technology. Popular Science started writing about technology more than 150 years ago. There was no such thing as “gadget writing” when we published our first issue in 1872, but if there was, our mission to demystify the world of innovation for everyday readers means we would have been all over it. Here in the present, PopSci is fully committed to helping readers navigate the increasingly intimidating array of devices on the market right now. Our writers and editors have combined decades of experience covering and reviewing consumer electronics. We each have our own obsessive specialties—from high-end audio to video games to cameras and beyond—but when we’re reviewing devices outside of our immediate wheelhouses, we do our best to seek out trustworthy voices and opinions to help guide people to the very best recommendations. We know we don’t know everything, but we’re excited to live through the analysis paralysis that internet shopping can spur so readers don’t have to. Find out more about our product evaluation process. By Chris Coke, Tony Ware By Tony Ware By Chris Coke, Tony Ware By Stan Horaczek By Tony Ware By Tony Ware By David Nield By Tony Ware By Chris Coke, Tony Ware By Brandt Ranj, Chris Coke, Tony Ware By Sarah Jones, Tony Ware By Sarah Jones By Stan Horaczek By Sarah Jones, Tony Ware By Stan Horaczek, Tony Ware By Tony Ware By Brandt Ranj By Brandt Ranj By Brandt Ranj By Chris Coke By Brandt Ranj By Brandt Ranj By Chelsea Frank By John Alexander Breakthroughs, discoveries, and DIY tips sent every weekday. By signing up you agree to our Terms of Service and Privacy Policy. Articles may contain affiliate links which enable us to share in the revenue of any purchases made. Registration on or use of this site constitutes acceptance of our Terms of Service. © 2025 Recurrent. All rights reserved.
--------------------------------------------------

Title: Why Buy Microsoft Stock?
URL: https://www.forbes.com/sites/greatspeculations/2025/05/23/why-buy-microsoft-stock/
Time Published: 2025-05-23T13:45:55Z
Full Content:
ByTrefis Team ByTrefis Team, Contributor. Microsoft store in Manhattan, New York City, United States of America on July 16th, 2024. (Photo by ... More Beata Zawrzel/NurPhoto via Getty Images) Why would you want to pay 50x cash flow for Microsoft’s (NASDAQ:MSFT) stock, when you can buy Nvidia at about the same price? Nvidia’s revenues are growing close to 100%, Microsoft’s at 15%. Yes, not 50%, but 15%. What’s worse - Nvidia’s cash flow margins are about 47%, almost 1.7x Microsoft’s margins. In other words, more of that top line growth that Nvidia gets - makes its way to free cash flow! Cash that can be reinvested or distributed to investors as return. Good thing, who can argue? But Nvidia comes with large risk. Our High Quality (HQ) portfolio diversifies away stock-specific risks while giving exposure to upside, and has outperformed the S&P 500, the Nasdaq, and Russell 2000 - and has clocked > 91% returns since inception. Well, you could argue - Microsoft is a known, stable game. For more than 40 years, this thing has been running like a machine. Nvidia’s phenomenal growth is recent - it might fizzle. AI might fizzle. Large AI models might fizzle - or worse, deemed unnecessary. Deepseek showed small models work just as well, right? They didn’t even use Nvidia’s latest chips. However, what are the odds that Nvidia’s chips will cease to be the hottest date in town? There lies the guesswork. Nvidia revenues grew more than 80% annually, on average, in the last 3 years - and last year grew more than 100%. Could this drop to 60% or 40% growth? It has to - soon! See: Buy or Sell Nvidia? Nvidia’s biggest customers are the likes of Microsoft, Google, Meta, and Amazon. These companies, each of them, spent more than tens of billions last year on Nvidia chips. Can Microsoft, Google, and others continue to spend 100% MORE on Nvidia chips next year? And the year after? Can they do it while their own revenue grows only 15% annually? The answer is absolutely not. The Nvidia 80-100% growth music has to stop at some point. When this high pitched growth music stops, Nvidia’s valuation will drop. A lot. But then, it might continue at a slower pace, at 20-30%, not bad, but not ridiculously high either. Point is, that’s why buying a little bit of Microsoft is OK. It’s also why you build a portfolio. A resilient one. To balance risk-reward. We did it in spades with the Trefis High Quality (HQ) portfolio. Balancing risk-reward is how HQ outperformed the S&P 500, the Nasdaq, and Russell 2000. HQ outperformed all of them, and has clocked >91% returns since inception. That’s why you buy a little bit of Microsoft, or Google, META, Amazon, and perhaps even Edwards Lifesciences. Comparison as a tool: The purpose of comparing Microsoft with Nvidia is all about understanding the risk-reward tradeoff of an investment of interest, in this case Microsoft. In practice, investment decisions are about understanding relative attractiveness. Buy Microsoft stock or keep your money as interest earning cash? Or buy an ETF on the S&P 500? Is expected return on Microsoft stock more than that on cash - by how much? What’s the downside risk you bear to earn that extra return on Microsoft? Drawing contrast with a specific “anchor” asset, in this case Nvidia and Microsoft, serves as a powerful tool to assess the risk-reward tradeoff. Note: Always use an appropriate comparison for a ticker. Microsoft example was about “growth” and reasonably high valuation. A comparison with Nvidia provides interesting perspective as the stock offers much more at a similar valuation (FCF based) - but also carries with it, a huge downside risk. It is exactly this downside risk, versus relative upside tradeoffs we made - at scale, in constructing the Trefis High Quality (HQ) Portfolio. With a selection of 30 stocks, it has demonstrated a history of comfortably outperforming the S&P 500 over the past 4-year span. What accounts for this? As a collective, HQ Portfolio stocks achieved superior returns with reduced risk compared to the standard index, with a smoother performance evident in HQ Portfolio performance metrics.
--------------------------------------------------

Title: Dave Vellante’s Breaking Analysis: The complete collection
URL: https://siliconangle.com/2025/05/23/dave-vellantes-breaking-analysis-complete-collection/
Time Published: 2025-05-23T10:33:12Z
Full Content:
UPDATED 06:33 EDT / MAY 23 2025 BREAKING ANALYSIS by Dave Vellante Breaking Analysis is a weekly editorial program combining knowledge from SiliconANGLE’s theCUBE with spending data from Enterprise Technology Research. Branded as theCUBE Insights, Powered by ETR, the program is our opportunity to share independent, unfiltered editorial with SiliconANGLE, theCUBE and Wikibon communities. The program and conclusions we produce are data-driven, tapping ETR’s proprietary spending data set. Episode 221 – Nvidia, Broadcom and the expanding breadth of AI – We attended both Nvidia Corp.’s GTC conference and Broadcom Inc.’s investor day this week where the artificial intelligence platform shift was on full display. In our view, GTC24 was the most important event in the history of the technology industry, surpassing Steve Jobs’ iPod and iPhone launches. The event was not the largest but, in our opinion, it was the most significant in terms of its reach, vision, ecosystem impact and broad-based recognition that the AI era will permanently change the world. Meanwhile, Broadcom’s first investor day underscored both the importance of the AI era and the highly differentiated strategies and paths that Nvidia and Broadcom are each taking. We believe Nvidia and Broadcom are currently the two best-positioned companies to capitalize on the AI wave and will each dominate their respective markets for the better part of a decade. But importantly, we see them each as enablers of a broader ecosystem that collectively will create more value than either of these firms will in and of themselves. In this Breaking Analysis, we will share our perspectives on the state of AI and how Nvidia and Broadcom are each leading the way with dramatically different but overlapping strategies that may be headed for an eventual collision course. Watch the full video analysis. Episode 220 – Navigating NVIDIA & the AI Trade – Sell, hold or double down? – Heading into the second half of 2023, some investors felt that the semiconductor run up last summer was a harbinger for a broader tech rally. That thesis proved prescient and rewarded managers who took on risk at the time with leading firms in semiconductors, security and enterprise software. The question is, where do we go from here? In this Breaking Analysis we welcome back Ivana Delevska, the founder and chief investment officer at Spear Invest, Nasdaq SPRX. Some have compared SPRX to a miniature version of Cathie Wood’s ARKK fund. However SPRX is more sector agnostic where Delevska focuses more broadly on growth themes such as her current emphasis on cybersecurity, semiconductors, and enterprise software. Watch the full video analysis. Episode 219 – Why CrowdStrike is separating from the cybersecurity pack – It’s been an interesting month in the cybersecurity space. The sector has been somewhat less affected by budget tightening these past twenty-four months and at the same time has benefitted from AI tailwinds. But in the past several weeks we’ve seen some separation in key highflying cybersecurity names. Specifically, Palo Alto shocked the street last month with a $600M billings forecast surprise and sounded the alarm that there were cracks in its consolidation execution. This dragged down other consolidation players in sympathy, namely CrowdStrike and Zscaler. But our research shows that the dynamics facing these three companies are quite different. Of particular note, CrowdStrike’s earnings print highlights the company’s impressive momentum while recent negativity around Zscaler is a bit of a head scratcher for us, which we’ll try to explain. In this Breaking Analysis we take a more narrow look at the information security space and dig deeper into the continued success of CrowdStrike. With recent survey data from ETR, we continue to advance our premise that platforms beat products and we identify several levers that are powering CrowdStrike’s path to $5B by FY 2026 and to $10B by the end of the decade. Watch the full video analysis. Episode 218 – The unplanned genius of Broadcom’s route to AI dominance – Broadcom is perhaps the most unique company in the technology business. It doesn’t simply chase markets that are on steep growth curves and can deliver short term ROI. Rather it goes after established markets with durable franchises. Broadcom focuses its R&D on serving customers in these markets with major engineering investments to achieve a dominant position in each of its target sectors. And sometimes, the company lucks out with this strategy and catches a wave accidentally by design. In this Breaking Analysis we extract key nuggets from our sit down at MWC this week with Charlie Kawwas, president of Broadcom’s Semiconductor Solutions Group, and we unpack the contrarian business technology model of Broadcom. Watch the full video analysis. Episode 217 – Cloud optimization wanes as AI slowly lifts off – The past twenty-four months have seen cloud spending face dual headwinds of macroeconomics and the ability to dial down resources as needed – i.e. cloud optimization. Nonetheless, the big four hyperscalers clocked in between $170 – $190B in IaaS and PaaS revenue last year depending on how you factor the leaked court documents suggesting Azure is much smaller than previously believed. Regardless, hyperscaler growth continued to outpace almost all markets, accelerating between 18-19% in revenue terms last year, despite their enormous size. As we progress into 2024, IT decision makers are cautiously optimistic about spending levels, especially for the second half. All hyperscalers report that cloud optimization is slowing although pockets of cloud cost cutting remain. While AI gets all the headlines, its contribution to revenue is still a small fraction of the overall spending pie. For example, we estimate that Microsoft’s AI services accounted for around $800M this past quarter. But the trajectory for AI services and the potential uplift looks promising for all four hyperscalers. We think collectively the generative AI uplift in cloud will surpass $10B this year. In this Breaking Analysis we update you on our latest hyperscale cloud spending and market share data. We’ll analyze the ETR survey data on cloud optimization, assess the Gen AI updraft for the big 3 US cloud players and look at some of the industry trend data on cloud spend by platform. Watch the full video analysis. Episode 216 – Intel Foundry is a bold bet filled with uncertainty – As an American, you can’t help but root for Intel CEO Pat Gelsinger to succeed. His vision to bring semiconductor manufacturing leadership back to the United States is more than just a quaint nationalistic sentiment. Rather it’s a strategic imperative for the country, its military, global competitiveness and access to future technological innovations in the AI era. But his strategy is dependent upon the success of Intel both as a designer and a leading manufacturer of advanced chips. As such this choice puts Intel in a multi-front war with highly capable leaders in several markets, including names like AMD, NVIDIA, AWS, Google, Microsoft, Apple, Tesla and other chip designers…even perhaps OpenAI. As well Intel competes with with established manufacturers like Taiwan Semiconductor and Samsung. Moreover, Intel’s business model has been disrupted by Arm which has created a volume standard powered by the iPhone and mobile technologies. Finally, China, Inc. looms as a long-term competitor further underscoring the imperative. But the trillion dollar questions are: 1) What are the odds that Intel’s strategy succeeds; and 2) Are there more viable alternative strategies for both Intel and the United States? Watch the full video analysis. Episode 215 – Slicing the Gordian Knot – A leap to real time systems of truth – In order to support the vision of the sixth data platform, that is, a capability that allows a globally consistent, real-time, intelligent digital representation of a business, we believe the industry must rethink the single system of truth. Specifically, we envision a new data platform that marries the best of relational and nonrelational capabilities and breaks the multi-decades tradeoffs between data consistency, availability and global scale. Further, we see the emergence of a modular data platform that automates decision-making by combining historical analytic systems with transactions to enable artificial intelligence to take action. In this Breaking Analysis, we welcome two innovators, Eric Berg, chief executive of Fauna Inc., and S. Somasegar, managing director at Madrona Ventures. Watch the full video analysis. Episode 214 – Enterprise Technology Predictions 2024 – Predictions about enterprise tech have never been more uncertain. They become even more challenging when you try to make forecasts that are measurable. Generally, our belief is we should be able to look back a year later and say with some degree of certainty whether the prediction came true – ideally with some quantifiable evidence to back that up. In this Breaking Analysis and for the third year in a row, we collaborate with Erik Bradley of Enterprise Technology Research and to share our annual enterprise technology predictions. Watch the full video analysis. Episode 213 – 2024 IT spending outlook shows cautious start with optimistic finish – According to recent spending intentions data from over 1,700 information technology decision-makers, executives anticipate a 4.3% growth in technology budgets for the year, which is an improvement from the 3.5% growth seen in 2023 and higher than the 3.8% expectation from October. However, the forecasts for 2024 are back-loaded, with Q1 2024 forecasts at 2.4%, indicating that the optimism is concentrated in the second half of the year. Watch the full video analysis. Episode 212 – Unifying intelligence in the age of data apps – We believe the future of intelligent data apps will enable virtually all organizations to operate a platform that orchestrates an ecosystem similar to that of Amazon.com. By this we mean dynamically connecting and digitally representing an enterprise’s operations including its customers, partners, suppliers and even competitors. This vision includes the ability to rationalize top down plans with bottom up activities across the many dimensions of a business – e.g. demand, product availability, production capacity, geographies, etc. Unlike today’s data platforms, which generally are based on historical systems of truth, we envision a prescriptive model of a business’ operations enabled by an emerging layer that unifies the intelligence trapped within today’s application silos. In this Breaking Analysis, we explore in depth, the semantic layer we’ve been discussing since early last year. To do so we welcome Molham Aref, the CEO of RelationalAI. Watch the full video analysis. Episode 211 – Grading Our 2023 Enterprise Technology Predictions – Predictions about the future of enterprise tech are streaming to our inboxes, literally by the thousands. Most are thoughtful and we will review those prior to publishing our 2024 predictions later in January. As is our tradition, we try to make our own predictions more challenging by citing forecasts that are measurable and have either a numeric tied to them or a binary outcome. Our belief is if we make a prediction, you should be able to look back a year later and say with some degree of certainty whether the prediction came true or not. With some empirical evidence to back that up. In this Breaking Analysis we grade the 2023 predictions we made with ETR’s Erik Bradley. We look back at what we said in January about the macro IT spending environment, cost optimization, security, generative AI, cloud, blockchain, data platforms, automation and tech events. Watch the full video analysis. Episode 210 – David vs Goliath reimagined – OpenAI’s approach to AI supervision – Artificial general intelligence, or AGI, has people both intrigued and fearful. As a leading researcher in the field, last July, OpenAI introduced the concept of superalignment via a team created to study scientific and technical breakthroughs to guide and ultimately control AI systems that are much more capable than humans. OpenAI refers to this level of AI as superintelligence. Last week, this team unveiled the first results of an effort to supervise more powerful AI with less powerful models. While promising, the effort showed mixed results and brings to light several more questions about the future of AI and the ability of humans to actually control such advanced machine intelligence. In this Breaking Analysis we share the results of OpenAI’s superalignment research and what it means for the future of AI. We further probe ongoing questions about OpenAI’s unconventional structure which we continue to believe is misaligned with its conflicting objectives of both protecting humanity and making money. We’ll also poke at a nuanced change in OpenAI’s characterization of its relationship with Microsoft. Finally we’ll share some data that shows the magnitude of OpenAI’s lead in the market and propose some possible solutions to the structural problem faced by the industry. Watch the full video analysis. Episode 209 – Moving beyond separation of compute & storage…Journey to the 6th data platform – We believe today’s so-called modern data stack, as currently envisioned, will be challenged by emerging use cases and AI-infused apps that begin to represent the real world, in real time, at massive data scale. To support these new applications, a change in underlying data and data center architectures will be necessary, particularly for exabyte scale workloads. Today’s generally accepted state of the art of separating compute from storage, must evolve in our view to separate compute from data and further enable compute to operate on a unified view of coherent and composable data elements. Moreover, our opinion is that AI will be used to enrich metadata to turn strings (i.e. ASCII code, files, objects, etc.) into things that represent real world capabilities of a business. In this Breaking Analysis we continue our quest to more deeply understand the emergence of a sixth data platform that can support intelligent applications in real time. To do so we are pleased to welcome two founders of VAST Data, CEO Renen Hallak and Jeff Denworth. VAST just closed a modest $118M financing round that included Fidelity at a valuation of ~$9B, which implies a quite minor change to the cap table. Watch the full video analysis. Episode 208 – re:Invent ’23 underscores a new simplicity mandate for AWS – Generative AI has created a new mandate in enterprise tech with significant implications for all companies generally and AWS specifically. Amazon’s powerful playbook based on agility, developer choice, power, scale, reliability and security must now evolve to accommodate simplicity and coherence for mainstream customers. This imperative came into clear focus at AWS re:Invent ’23. AWS continues to innovate at a fast pace, but must now do so in a changing customer environment that increasingly values direct user productivity gains through software. In this Breaking Analysis we share our take on how AWS is navigating this challenge. We’ll review Amazon’s strategy to compete in the nascent Gen AI era and we’ll provide commentary on the chess moves it’s making with Anthropic, Nvidia and other partners to maintain its leadership position. We’ll also discuss the challenges of doing so as a $90+B giant in a fast-moving market. Watch the full video analysis. Episode 207 – The OpenAI meltdown…Winners and losers in the battle for AI supremacy – Conventional wisdom says Microsoft is the big winner in the recent OpenAI saga. We don’t quite see it that way. Both Microsoft and OpenAI are in a worse position today than they were last Thursday, prior to the firing of OpenAI CEO Sam Altman and the ongoing public drama that ensues. Microsoft and OpenAI had a huge lead in market momentum, AI adoption and feature acceleration and were setting the narrative in AI. Our discussions with customers and industry insiders leads us to conclude that the duo has put its substantial lead at risk. While Satya Nadella is making lemonade from lemons, the window was just cracked open for the competition and it’s more clear than ever that one large language model will not rule them all. In this Breaking Analysis we weigh in on the impacts of the OpenAI meltdown with a deeper look at the customer perspective and how it alters the competitive landscape in the battle for AI supremacy. As well, the amazing data team at ETR has run a quick survey of OpenAI Microsoft customers to gauge reactions and we’ll share that fresh data. Watch the full video analysis. Episode 206 – The copilot era takes flight at Microsoft Ignite 2023 – Microsoft Ignite 2023 was one part a celebration of yearlong technological innovations, one part announcing the general availability of previously announced products, one part vision, one part ecosystem and four parts copilots everywhere. Copilots promise an historic software-led productivity increase. Perhaps for the first time in industry history we’re seeing huge demand for software coincide with the ability to make it easier to write software. Just as AWS turned the data center into an API, copilots are turning software development into natural language, enabling many more people to create. The implications on productivity are massive and we believe will kick off a new wave of growth that will become increasingly noticeable throughout 2024. In this Breaking Analysis we give you our impressions of Microsoft Ignite 2023. theCUBE Research Analyst George Gilbert and CUBE Collective contributor Sarbjeet Johal both weighed in for this episode and we’ll also share some recent ETR data that shows the progression of some of the major AI players in the past twelve months and the the relative impact Gen AI has had on each of their businesses. Watch the full video analysis. Episode 205 – IBM turns the corner with Watson, why 2.0 is a breakthrough opportunity – With Watson 1.0, IBM deviated from the silicon valley mantra, fail fast, as it took nearly a decade for the company to pivot off of its original vision. In our view, a different dynamic is in play today with Watson 2.0 – i.e. watsonx. IBM’s deep research in AI and learnings from its previous mistakes, have positioned the company to be a major player in the Generative AI era. Specifically, in our opinion, IBM has a leading technological foundation, a robust and rapidly advancing AI stack, a strong hybrid cloud position (thanks to Red Hat), an expanding ecosystem and a consulting organization with deep domain expertise to apply AI in industry-specific use cases. In this Breaking Analysis we share our takeaways and perspectives from a recent trip to IBM’s research headquarters. To do so we collaborate with analyst friends in theCUBE Collective, Sanjeev Mohan, Tony Baer and Merv Adrian. We’ll also share some relevant ETR spending data to frame the conversation. Watch the full video analysis. Episode 204 – The Gen AI power law – How data diversity influences adoption – Our research indicates that the adoption of generative AI is occurring across a variety of frameworks with diverse deployment models, data sets and domain specificities. The transformative power of generative AI for industries is colossal, and we are convinced that AI will gravitate to where data lives. The power law of Gen AI, developed by theCUBE Research team, describes the adoption patterns we see emerging in the cloud, on-premises and at the edge with a long tail of highly specific domain models across every industry. In this Breaking Analysis we revisit the Gen AI power law which informs how we see adoption happening within industries and organizations globally. We introduce new survey data from ETR that looks at some of the vendors being adopted or considered with a special peek into the emerging technology sector with data on Hugging Face, Anthropic, Cohere and Jasper. We also comment on the adoption of Meta’s Llama 2 and the potential impact of open source and other third parties on the shape of the curve. We share our estimates of Microsoft Azure’s AI revenue impact, which we see approaching a $2B run rate exiting 2023. Finally, we dig into the impact of retrieval augmented generation (RAG) using real world examples with some caveats of RAG implementations that practitioners should consider. Watch the full video analysis. Episode 203 – AI revenue riddle – Azure sees gains, when will other cloud titans see the surge? – In 1987, Nobel Prize-winning economist Bob Solow famously observed, “You can see the computer age everywhere but in the productivity statistics.” This proclamation became known as the productivity paradox. Ironically, Solow’s statement preceded the greatest productivity boom since the dawn of the computer age which subsequently came to fruition in the 1990’s. It can be argued that a similar pattern is being seen today where AI is everywhere but generally not showing up in earnings numbers or productivity statistics…yet. In this Breaking Analysis we squint through the latest earnings reports from Microsoft, Alphabet and Amazon to understand what’s happening in cloud, evaluate the impact or lack thereof of AI on cloud earnings momentum and explain how we think about the future impact of generative AI and cloud. Watch the full video analysis. Episode 202 – From hype to reality, the true state of AI adoption – MIT professor and economist Erik Brynjolfsson said recently that he’d be disappointed if AI didn’t lift the current anemic 1.2% productivity growth rate to 3% or even 4%. This would be a good thing for business and government as it could potentially help with the labor shortage, drive earnings growth and increase tax revenues, which would ostensibly help address current debt levels. This is one of the promised impacts of AI. While the hype surrounding Gen AI has narrowly propped up certain sectors of the market, like AI startups and the magnificent seven, the macro effects have not been felt thus far as adoption remains largely experimental. In this Breaking Analysis and ahead of Supercloud 4, ETR’s Erik Bradley and Daren Brabham join the program to share the latest trends on AI adoption, how Gen AI is being used, some of the deployment models and the AI leaderboard based on spending momentum and presence in the market. Watch the full video analysis. Episode 201 – Get Ready for the Sixth Data Platform – In the next 3-5 years, we believe a set of intelligent data apps will emerge requiring a new type of modern data platform to support them. We refer to this as a sixth data platform. In our previous research we’ve used the metaphor “Uber for everyone” to describe this vision; meaning software-based systems that enable a digital representation of a business. Watch the full video analysis. Episode 200 – Lower for longer…Tech spending remains tepid – We’re getting used to the phrase, “higher for longer,” referring to the realization that interest rates are expected to remain elevated for a period of time. This trend is having an inverse effect on enterprise tech spending growth rates. Prior to the Fed’s tightening binge for example, IT decision makers (ITDMs) in aggregate expected annual technology spending to increase by 7.5%. Eleven fed interest rate hikes later, ITDMs estimate that their 2023 budgets will be up only 2.9%, with an expectation, or perhaps it’s a wishful hope, that their budgets will increase 3.8% in 2024. In this our 200th Breaking Analysis, we preview the current spending climate and where AI fits in relation to other sectors. We’ll also share with you a snapshot of the leaders in terms of spending velocity for their platforms; and how their performance compares to peers relative to earlier survey periods. Watch the full video analysis. Episode 199 – Cisco Splunk under the microscope, joint customers weigh in – In this special Breaking Analysis, theCUBE’s Dave Vellante talks with Enterprise Technology Research’s Erik Bradley as he shares the results of ETR’s recent flash survey assessing joint customer perceptions and likely spending actions as a result of the Cisco acquisition of Splunk. Watch the full video analysis. Episode 198 – Bob Muglia on Uber for everyone…how the future of data apps will evolve – The future of intelligent data applications: Uber for everyone. In June, we put forth our scenario about “Uber for all.” We believe a new breed of data apps is emerging and we used Uber as an example where data about people, places and things is brought together in a data-coherent, real-time system. In this special Breaking Analysis, analysts Dave Vellante and George Gilbert sat down with friend of theCUBE, entrepreneur, investor and software executive, Bob Muglia. Bob formerly worked at Microsoft, was the CEO of Snowflake and is a visionary trend spotter. Watch the full video analysis. Episode 197 – Cloud security powers CrowdStrike momentum, Gen AI is next – George Kurtz is pumped up…and why not? CrowdStrike’s business appears to be on a fast track and entering a new phase of growth, despite the difficult macro and elongated sales cycles. The company’s products are considered best in class, its business is growing steadily and an improved profitability and cash flow outlook had investors excited, at least up until this week. A still challenging environment and a rich 13X revenue multiple perhaps led to some profit taking, but Gen AI could be the next catalyst for the company. In the race to close the SecOps staffing gap, CrowdStrike has what appears to be a strong play with a natural language-based intelligent assistant known as Charlotte AI. In this Breaking Analysis we update our scenario on security leader CrowdStrike. We’ll review the company’s recent progress, share survey data that shows where it is strong and where there may be icebergs ahead. And we’ll preview Fal.Con 2023 which takes place next week in Las Vegas. Watch the full video analysis. Episode 196 – Copilot or competitor – How gen AI bolsters and buffets UiPath’s Northstar – UiPath’s recent earnings beat and raise provides some evidence that thus far, Gen AI has not been dilutive for the company. As an early leader that is transforming beyond RPA toward end-to-end enterprise automation, UiPath, like all automation providers, has always faced adoption headwinds beyond isolated deployments. In this sense, Gen AI should bolster adoption and be a positive force. The flip side is that widely available tools like chatbots and generalized foundation models could eat away at the low end of the automation TAM, highlighting the urgency for companies like UiPath to move up market and accelerate innovation that brings differentiation from commoditized tools; and, importantly, create distance from embedded AI within mainstream enterprise SaaS platforms like Slack GPT and Salesforce Einstein. In this Breaking Analysis we briefly review the recent earnings print from UiPath. We’ll look at ETR survey data that shows Microsoft Power Automate’s impact on the automation market and how it is forcing UiPath to target larger accounts with a more functional product set. As well we’ll look at the impact that AI is having in these larger accounts and test UiPath management assertions that Gen AI will be a tailwind for the company. Watch the full video analysis. Episode 195 – Google goes all in on the AI cloud – At Cloud Next, Google showcased its strong leadership position in data and AI. In our view, Google’s messaging, demos and tech-centric narrative have broad appeal for developers and next generation startups. As well, the company’s focus on solutions, contrasts its strategy to the typically disjointed services we’ve seen from AWS over the past decade. Google also showed off an expanded ecosystem of GSIs and smaller CSPs, encouraging the broad use of Google’s kit globally. While Google remains a distant third in the Iaas/PaaS race, with revenue one-fifth the size of AWS, it is playing the long game and betting the house on AI as a catalyst to its cloud future. In this Breaking Analysis we unpack the key takeaways from Google Cloud Next with Rob Strechay and George Gilbert. We’ll share ETR data that positions Google’s AI relative to other leaders and we’ll contrast Google’s data-centric strategy with traditional architectural models. Watch the full video analysis. Episode 194 – Snowflake has Momentum with AWS & Microsoft…Why Google may not be Next – Recent earnings prints from Amazon and Snowflake, along with new survey data, have provided additional context on top of the two events that Snowflake and Databricks each hosted last June. Specifically, we believe that the effects of cloud optimization are still being felt but are nearing the end of peak negative impact on cloud companies. Snowflake’s recent renewal with Microsoft better aligns sales incentives and should improve the company’s traction with Microsoft Azure, a platform that has long favored Databricks. Google however remains a different story as its agenda is to build out its own data cloud stack, rather than supporting Snowflake’s aspirations. Watch the full video analysis. Episode 193 – VMware’s Future – Navigating Multi cloud Complexity & GenAI Under Broadcom’s Wing – The FTC continues to drag its feet on approving Broadcom’s acquisition of VMware. Ironically, in our view, these delays only hurt the very competitive environment the FTC claims to be protecting. The AI era is accelerating at a breakneck pace and the big 3 hyperscale cloud vendors already have a sizable lead on legacy incumbents. If preserving competition is truly the agenda of the U.S. government, it should recognize that VMware, its enterprise ecosystem and market forces have the potential to neutralize cross cloud complexity and give customers a viable alternative to increasingly powerful public cloud players. In this Breaking Analysis and ahead of VMWare Explore 2023, we revisit our views on Broadcom’s rationale and likely actions post acquisition. We’ll share current ETR survey data to place VMware’s position in context to the major cloud players, speculate on its AI agenda and give a preview of next week’s VMware Explore. To do so we welcome CUBE analyst Rob Strechay and friend of theCUBE, Zeus Kerravala, principal of ZK Research. Watch the full video analysis. Episode 192 – Cloud vs. On-Prem Showdown: The Future Battlefield for Generative AI Dominance – The data from enterprise customers is clear but conflicted. While 94% of customers say they’re spending more on AI this year, they’re doing so with budget constraints that will steal from other initiatives. As well, the choice of where customers plan to run generative AI is split almost exactly down the middle in terms of public cloud vs. on-premises/edge. Further complicating matters, developers report the experiences in the public cloud with respect to feature richness and velocity of innovation has been outstanding. At the same time, organizations express valid concerns about IP leakage, compliance, legal risks and cost that will limit their use of the public cloud. In this Breaking Analysis we’ll share the most recent data and thinking around the adoption of large language models and address the factors to consider when thinking about how the market will evolve. As always, we’ll share the latest ETR data to shed new light on key issues customers face balancing risk with time to value. Watch the full video analysis. Episode 191 – Tech Stocks Beyond the Magnificent Seven – After a tough 2022, the first half of 2023 has shown impressive strength paying off earlier technology bets. For sure investors in the so-called Magnificent Seven, i.e. Apple, Alphabet, Amazon, Microsoft, Meta, Nvidia and Tesla have been rewarded. But sharp investors have sought alpha beyond these names, riding the wave of secular trends in AI, cybersecurity, cloud infrastructure and software as well as other emerging spaces like cleantech and robotics. As we enter the second half of 2023, the run up in tech combined with macro uncertainty has many investors taking a cautious posture. But prudent earnings guidance sets up for a positive outlook in the mid-term, especially for those companies that can capitalize on the AI wave. In this Breaking Analysis we’re pleased to have back, founder and Chief Investment Officer of Spear Invest, Ivana Delevska to assess the current state of the market and explore how this investor is playing AI’s rising tide. We’ll also analyze ETR data to drill deeper into semis, cloud infrastructure, generative AI, cybersecurity and Snowflake. Watch the full video analysis. Episode 190 – What Leaked Court Docs Tell us About AWS, Azure & Google Cloud Market Shares – Recently leaked court documents during the Microsoft Activision hearing require us to revisit our cloud forecasts and market share data. The poorly redacted docs, which have since been removed from public viewing, suggest that Microsoft’s Azure revenue is at least 25% lower than our previous estimates. As a result, we’ve cut and revised our Azure revenue figures which in turn increases AWS’ big 4 hyperscale cloud market share. Our new estimates show that AWS maintains a greater than 50% share of revenue through 2023. While the change also helps Google Cloud, its market share is only modestly affected. In this Breaking Analysis we update our hyperscaler cloud revenue estimates and market share data. We’ll also explain how the ETR data on cloud should be interpreted in this context and look forward to potential catalysts for cloud growth, including acceleration in Q4 attributable to generative AI. Watch the full video analysis. Episode 189 – AI gives cyber attackers the advantage…for now – Cloud complexity, tools sprawl and the AI awakening further tip the balance in favor of cyber attackers. Combined with corporate inertia, AI-washing, LLM inconsistency and the pace of change, we believe for now anyway, adversaries have the advantage over defenders. Moreover, macro spending headwinds continue to force organizations to make budget tradeoffs, not the least of which is how to fund AI experiments and deployments. Notably, however, 45% of organizations are using LLMs in production for use cases that may very well improve the productivity of SecOps teams in the long run and accelerate the cat and mouse game back to a state of quasi-equilibrium. In this Breaking Analysis we share key takeaways from Supercloud 3 – AI meets cloud security – and put forth new spending data from the latest ETR survey that shows which security firms are best positioned in the AI race to capitalize on the wave. Watch the full video analysis. Episode 188 – AI won’t be a winner takes all market – The AI heard ’round the world has put the machine intelligence sector back in the spotlight. But when you squint beyond the press hype, the data shows that artificial intelligence is now the number one sector in terms of relative spending velocity in the ETR taxonomy. Normally market hype leads deployments, but the data suggests that spending activity and market penetration for AI are coinciding with the hype. While hyperscale cloud players are reaping the rewards, we think this is a rising tide that’s going to lift all AI ships, those both plainly in sight and others that may not be so visible. In this Breaking Analysis we dig deeper into the AI space with spending data from ETR and one of the best minds in tech generally, and AI specifically, Jeff Jonas, CEO, founder, and chief scientist at Senzing. Watch the full video analysis. Episode 187 – Connecting the dots on the emerging Databricks tech stack – The recent Databricks Data+AI Summit attracted a large audience and, like Snowflake Summit, featured a strong focus on large language models, unification and bringing AI to the data. While customers demand a unified platform to access all their data, Databricks and Snowflake are attacking the problem from different perspectives. In our view, the market size justifies the current enthusiasm seen around both platforms but it’s unlikely that either company has a knockout blow for the other. This is not a head on collision. Rather Snowflake is likely years ahead in terms of operationalizing data. Developers can build applications on one platform, like Oracle when it won the market, that perform analysis and take action. Databricks likely has a similar lead in terms of unifying all types of analytic data – e.g. BI, predictive analytics & generative AI. Developers can build analytic applications across heterogeneous data, like Palantir today. But they have to access external operational applications to take action. In this Breaking Analysis we follow up last week’s research by connecting the dots on the emerging tech stack we see forming from Databricks. With an emphasis on how the company is approaching generative AI, unification and governance…and what it means for customers. To do so we tap the knowledge of three experts who attended the event, CUBE analysts Rob Strechay and George Gilbert and AI market maven Andy Thurai of Constellation Research. Watch the full video analysis. Episode 186 – Connecting the Dots on Snowflake’s Data Cloud Ambitions – Over the past several months we’ve produced a number of in-depth analyses laying out our mental model for the future of data platforms. There are two core themes: 1) Data from people, places, things, and activities in the real world drives applications, not people typing into a UI; and 2) Informing and automating decisions means all data must be accessible. That drives a change from data locked in application silos to application logic being embedded in a platform that manages an end-to-end representation of an enterprise in its data. This week’s Snowflake Summit further confirmed our expectations with a strong top line message of “All Data / All Workloads” and a technical foundation that supports an expanded number of ways to access data. Squinting through the messaging and firehose of product announcements, we believe Snowflake’s core differentiation is its emerging ability to be a complete platform for data applications. Just about all competitors either analyze data or manage data. But no one vendor truly does both. To be precise, managing data doesn’t mean running pipelines or serving analytic queries or AI/ML models. It means managing operational data so that analytics can inform or automate operational activities captured in transactions. With data as the application foundation, the platform needs robust governance. In this week’s Breaking Analysis, we try to connect the dots between Snowflake’s high level messaging and its technical foundation to better understand the core value it brings to customers and partners. As well, we’ll explore the ETR data with some initial input from the Databricks Data + AI Summit to assess the position and prospects of these two leaders along with the key public cloud players. Watch the full video analysis. Episode 185 – HPE wants to turn supercomputing leadership into gen AI profits – HPE’s announcement of an AI cloud for large language models highlights a differentiated strategy that the company hopes will lead to sustained momentum in its high performance computing business. While we think HPE has some distinct advantages with respect to its supercomputing IP, the public cloud players have a substantial lead in AI with a point of view that generative AI is fully dependent on the cloud and its massive compute capabilities. The question is can HPE bring unique capabilities and a focus to the table that will yield competitive advantage and ultimately, profits in the space? In this Breaking Analysis we unpack HPE’s LLM-as-a-service announcements from the company’s recent Discover conference and we’ll try to answer the question: Is HPEs strategy a viable alternative to today’s public and private cloud gen AI deployment models, or is it ultimately destined to be a niche player in the market? To do so we welcome to the program CUBE analyst Rob Strechay and Vice President / principal analyst from Constellation Research, Andy Thurai. Watch the full video analysis. Episode 184 – Uber’s architecture represents the future of data apps…meet its architects – Uber has one of the most amazing business models ever created. The company’s mission is underpinned by technology that helps people go anywhere and get anything. The results have been stunning. In just over a decade, Uber has become a firm with more than $30 billion in annual revenue, an annual bookings run rate of $126B and a market capitalization near $90 billion today. Moreover, the company’s productivity metrics are 3-5 times greater than what you’d find at a typical technology company when, for example, measured by revenue per employee. In our view, Uber’s technology stack represents the future of enterprise data apps where organizations will essentially create real time digital twins of their businesses and in doing so, deliver enormous customer value. In this Breaking Analysis, we introduce you to one of the architects behind Uber’s groundbreaking fulfillment platform. We’ll explore their objectives, the challenges they had to overcome, how Uber has done it and why we believe their platform is a harbinger for the future. Watch the full video analysis. Episode 183 – Snowflake Summit will reveal the future of data apps…here’s our take – Our research and analysis points to a new modern data stack that is emerging where apps will be built from a coherent set of data elements that can be composed at scale. Demand for these apps will come from organizations that wish to create a digital twin of their business to represent people, places, things, and the activities that connect them, to drive new levels of productivity and monetization. Further, we expect Snowflake, at its upcoming conference, will expose its vision to be the best platform on which to develop this new breed of data apps. In our view, Snowflake is significantly ahead of the pack but faces key decision points along the way to its future to protect this lead. In this Breaking Analysis and ahead of Snowflake Summit later this month, we lay out a likely path for Snowflake to execute on this vision; and we address the milestones and challenges of getting there. As always, we’ll look at what the ETR data tells us about the current state of the market. To do all this we welcome back CUBE contributor, George Gilbert. Watch the full video analysis. Episode 182 – Cisco needs to simplify…Here’s how – With a nearly $60B revenue run rate, growing at 14% and throwing off over $5B in operating cash last quarter, Cisco has an awesome business. But customers are vocal about the complexity of Cisco’s portfolio and if not addressed head on, the company risks encountering friction beyond just economic headwinds. We believe Cisco’s challenges are most decidedly not product breadth and depth, rather the company’s mandate is to integrate the piece parts of its intricate offerings to create more facile and seamless experiences for customers. In this Breaking Analysis and ahead of Cisco Live US, we dig deeper into Cisco’s business and double click on three key areas of its portfolio including: 1) Security; 2) Networking; and 3) Observability. With spending data from ETR and a guest appearance from SiliconANGLE contributor and market watcher Zeus Kerravala, principal at ZK Research. Watch the full video analysis. Episode 181 – The future of AI is real time data…Meantime GPUs are making all the headlines – The era of AI everything continues to excite. But unlike the Internet era, where any company announcing a dotcom anything immediately rose in value, the AI gods appear to be more selective. Nvidia beat its top line whisper number by more than $300M and the company’s value is rapidly approaching one trillion dollars. Marvell narrowly beat expectations this week but cited future bandwidth demand driven by AI and the stock was up more than 20% on Friday. Broadcom was up nearly 10% on sympathy with the realization that connect-centricity beyond the CPU is what the company does really well. Meanwhile, other players like Snowflake, which also narrowly beat earnings Wednesday and touted AI as a future tailwind, got hammered as customers dial down cloud consumption. In this Breaking Analysis we look at the infrastructure of AI examining the action at the silicon layer specifically around Nvidia’s momentum. Since much of AI is about data, we’ll also look at the spending data on two top data platforms, Snowflake and Databricks to see what the survey data says and examine the future of real time data and automation as a catalyst for massive productivity growth in the economy. To do so we have a special Breaking Analysis panel with John Furrier and David Floyer. Watch the full video analysis. Episode 180 – The AI powered hybrid multi super cloud – AI will now add superpowers to every triggering buzzword, hence the title of this week’s post. Look past the buzz and you’ll find substance somewhere. The spring conference season is kicking into high gear, so it’s a time to get serious and extract the signal from the event noise. This week we’ll see Microsoft Build, which will no doubt volley shots back from the messaging at Google I/O. Two other big events will take place this week, Red Hat Summit / Ansible Fest in Boston and the annual Dell Technologies World in Las Vegas. theCUBE will be covering both of these shows and we want to take this opportunity to update you on the state of hybrid multi-cloud…what we call supercloud. In this Breaking Analysis we examine some of the key infrastructure players in hybrid multi-cloud with a focus on Red Hat and Dell Technologies, two firms that increasingly are partnering with each other as VMware’s future evolves. We’ll share recent ETR survey data on the position of several other hybrid/cross-cloud players including Cloudflare, Equinix, HPE, IBM, Oracle, VMware and others. We’ll also share what we expect to hear at Red Hat Summit and Dell Technologies World this year. Watch the full video analysis. Episode 179 – Searching for gold in enterprise AI – The AI gold rush is on. The paths to monetization are seemingly endless but the most obvious converge on making humans more productive or supercharging existing business models like search advertising or subscription licenses. Much of AI adoption in enterprise IT is hidden. Our research shows a very high overlap (around 40-60%) between AI adoption in enterprise tech and embedded AI inside software from the likes of Salesforce, ServiceNow, Workday, SAP, Oracle and other major players. But the rapid advancements of tools from AI leaders and an emerging group of independent firms is causing customers to think differently. Catalyzed by the OpenAI Microsoft partnership, organizations are rapidly trying to figure out how to apply these tools to create competitive advantage. Every firm on the planet wants to ride the AI wave. Virtually overnight, investment capital has shifted to fund early stage AI startups with much less funding required relative to previous boom cycles. In this Breaking Analysis we review ETR data to quantify the state of AI spending in the enterprise and look at the positions of several key players in the space that offer AI tools and platforms. To do so we invite Andy Thurai, CUBE contributor, VP and principal analyst at Constellation Research. Andy will help us unpack the hits and misses from this past week’s Google IO conference and give us his perspectives on what it takes to catch the AI wave and avoid becoming driftwood. Watch the full video analysis. Episode 178 – Desperately Seeking Cloud Repatriation – While we’ve been skeptical about repatriation as a notable movement, anecdotal evidence suggests that it is happening in certain pockets. Even though we still don’t see cloud repatriation broadly showing in the numbers, certain examples have caught our attention. In addition, the potential impact of artificial intelligence raises some interesting questions about where infrastructure should be physically located and causes us to revisit our premise that repatriation is an isolated and negligible trend. In this Breaking Analysis we look at a number of sources, including the experiences of 37signals, which has documented its departure from public clouds. We’ll also examine the relationship between repatriation and SRE Ops skill sets. As always we’ll look at survey data from our partners at ETR, a recent FinOps study published by Vega Cloud and revisit the Cloud Repatriation Index, which we believe is breaking a three-year trend. Watch the full video analysis. Episode 177 – Don’t be fooled by slowing cloud growth…cost optimization is a feature not a bug – The big three US cloud players all announced earnings this past week and, as expected, cloud growth is slowing. But don’t kid yourselves. Hyperscale clouds remain the epicenter of innovation in tech and foundation models like GPT will only serve to harden this fundamental fact. Our data suggests the deceleration in cloud spend is a function of two related factors: 1) Cautious consumption patterns; and 2) Aggressive cloud optimization, which is being promoted by the big three cloud vendors in an attempt to lock in customers to longer term commitments. There is still no clear evidence in the numbers that repatriation is a factor. Rather, the ability to quickly dial down spending and pause projects is an attractive feature of cloud computing and one that, until now, has never really been seen on a broad market basis. In this Breaking Analysis we try to explain the implications of this seemingly simple but nuanced dynamic. We’ll review the latest hyperscale cloud data for the big three players, share our analysis of certain comments made by cloud executives and show you the latest ETR data on spending and market presence in the cloud, Watch the full video analysis. Episode 176 – RSA 2023 Security Identity Crisis Part 2 – The narrative from security vendors is organizations don’t spend enough money on cyber defense. Maybe…but will spending more actually address the problems organizations face? The conventional wisdom is it will help; or at least it can’t hurt, but as we and others have pointed out over the years, a crowded market and mega VC funding have created more tools, more complexity and more billionaires…but are we safer? In this Breaking analysis we follow up last week’s episode and continue with Part 2. In an homage to the keynote from RSA CEO Rohit Ghai, we ask, is there a looming identity crisis in the security industry? This week we’re excited to introduce the newest member of the SiliconANGLE editorial team, long time journalist, David Strom. With David, we’ll unpack the data and bring additional context to the ETR body of work. We’ll also look at some recent data from Unit 42, Palo Alto’s threat intelligence and response division. As well, we’ll dig into the anatomy of a recent double supply chain hack. Watch the full video analysis. Episode 175 – RSA 2023 highlights an identity crisis in the age of AI – In this Breaking Analysis, theCUBE host Dave Vellante updates on the latest trends in the cybersecurity market and what to expect at the RSA Conference 2023. Join our real-time analysis coverage from #RSAC here: https://www.thecube.net/rsa-conference-2023 We’ll also share the latest Enterprise Technology Research spending data and drill into the areas of cybersecurity that are seeing the most action. As always, we’ll highlight those companies with the strongest (and weakest) momentum and close with a look at some of the emerging technology players in security that might be ripe for acquisition. To do all this, we once again welcome in our colleague Erik Bradley from ETR. Watch the full video analysis. Episode 174 – Hidden Gems from HPE GreenLake Storage Day – On Tuesday, April 4, HPE invited a number of industry analysts to participate in HPE GreenLake Storage Day. Notably, HPE declared 2023 the year of storage. While the company made several storage-related announcements, perhaps even more interesting was what the event tells us about HPE’s culture, its strategy and the future direction of the company. In this Breaking Analysis we’ll share our takeaways from HPE’s event, held in Houston, Texas, which included attendance at Antonio Neri’s quarterly all-hands meeting. We’ll try to emphasize areas that have not necessarily been the focus of most press and industry analyst write ups to date. We’ll also take a look at the latest ETR survey data to put HPE’s market position in context across several of its major segments. Watch the full video analysis. Episode 173 – Semis rebound but enterprise tech spending remains soft – A rebound in semiconductor stocks has many investors asking if this is a harbinger of good news for the broader enterprise tech sector. Indeed the SOXX semiconductor ETF is up nearly 30% year to date as of this posting, as are bellwether fab suppliers like Applied Materials and Lam Research. Nvidia is up over 90% YTD and AMD over 50%. Even the beleaguered Intel is up 22%. But key enterprise software names have not yet rebounded and according to this week’s guest, the divergence between semis and B2B software is getting hard to ignore. In this Breaking analysis we examine the the bifurcation between the performance of semis and broader enterprise tech. And we’ll try to answer the question: Is the uptick in semiconductors an early indicator of a broader enterprise tech recovery, or is this a false signal that warrants continued caution? To examine these issues we welcome back Ivana Delevska, the founder and chief investment officer of SPEAR Invest. All statements made regarding companies or securities are strictly beliefs, points of view and opinions held by SiliconANGLE Media, Enterprise Technology Research, other guests on theCUBE and guest writers. Such statements are not recommendations by these individuals to buy, sell or hold any security. The content presented does not constitute investment advice and should not be used as the basis for any investment decision. You and only you are responsible for your investment decisions. Disclosure: Many of the companies cited in Breaking Analysis are sponsors of theCUBE and/or clients of Wikibon. None of these firms or other companies have any editorial control over or advanced viewing of what’s published in Breaking Analysis. Watch the full video analysis. Episode 172 – Which tech firms are most exposed to the banking crisis? – The viral awareness and adoption of foundation models like ChatGPT have created both an opportunity and threat to automation platforms generally and RPA point tools specifically. On the one hand, large language models can reduce complexity and accelerate the adoption of enterprise automation platforms. The flip side is software robots are designed to improve human productivity through intelligent automation and GPT models could cannibalize some, if not many use cases initially targeted by RPA vendors. This reality is causing customers to rethink their automation strategies and vendors to rapidly evolve their messaging to position foundation models as an accelerant to their platforms. In this Breaking Analysis we provide you with a perspective on how foundation models could impact automation platforms. We review ETR data that quantifies the ascendency of OpenAI. We also show survey data that measures the overlap between ML/AI systems and automation platforms. Then we review the recent quarterly performance of UiPath and share how we think the company must position itself with respect to the onslaught of noise and potential disruption from GPT models. Watch the full video analysis. Episode 171 – GPT models are a two edged sword for automation platforms – The viral awareness and adoption of foundation models like ChatGPT have created both an opportunity and threat to automation platforms generally and RPA point tools specifically. On the one hand, large language models can reduce complexity and accelerate the adoption of enterprise automation platforms. The flip side is software robots are designed to improve human productivity through intelligent automation and GPT models could cannibalize some, if not many use cases initially targeted by RPA vendors. This reality is causing customers to rethink their automation strategies and vendors to rapidly evolve their messaging to position foundation models as an accelerant to their platforms. In this Breaking Analysis we provide you with a perspective on how foundation models could impact automation platforms. We review ETR data that quantifies the ascendency of OpenAI. We also show survey data that measures the overlap between ML/AI systems and automation platforms. Then we review the recent quarterly performance of UiPath and share how we think the company must position itself with respect to the onslaught of noise and potential disruption from GPT models. Watch the full video analysis. Episode 170 – Databricks faces critical strategic decisions…here’s why – When Apache Spark became a top level project in 2014, and shortly thereafter burst onto the big data scene, it along with the public cloud disrupted the big data market. Databricks cleverly optimized its tech stack for Spark and took advantage of the cloud to deliver a managed service that has become a leading AI and data platform among data scientists and data engineers. However, emerging customer data requirements and market forces are conspiring in a way that we believe will cause modern data platform players generally and Databricks specifically to make some key directional decisions and perhaps even reinvent themselves. In this Breaking Analysis we do a deeper dive into Databricks. We explore its current impressive market momentum using ETR survey data. We’ll also lay out how customer data requirements are changing and what we think the ideal data platform will look like in the mid-term. We’ll then evaluate core elements of the Databricks portfolio against that future vision and close with some strategic decisions we believe the company and its customers face. To do so we welcome in our good friend George Gilbert, former equities analyst, market analyst and principal at Tech Alpha Partners. Watch the full video analysis. Episode 169 – MWC 2023 goes beyond consumer & deep into enterprise tech – While never really meant to be a consumer tech event, over time, the rapid ascendancy of smartphones captured much of the agenda at Mobile World Congress, now MWC. And while device manufacturers continue to have a major presence at the show, the maturity of intelligent devices, longer lifecycles and the disaggregation of the network stack have created more interest in enterprise-class technologies than ever before. Semiconductor manufacturers, network equipment players, infrastructure companies, cloud vendors, software providers and a spate of startups are eyeing the trillion dollar plus telecommunications industry as one of the next big things to watch this decade. In this Breaking Analysis we bring you part 2 of our ongoing coverage of MWC 2023. With some new data on enterprise players specifically within large telco environments. We’ll also take a brief glimpse at some of the pre-announcement news from the show and corresponding themes ahead of MWC. We’ll close with the key innovation areas we’ll be covering at the show on theCUBE. Watch the full video analysis. Episode 168 – MWC 2023 highlights telco transformation & the future of business – The world’s leading telcos are often branded as monopolies that lack innovation. Telcos have been great at operational efficiency, connectivity and living off of transmission services. But in a world beyond telephone poles and basic wireless services, how will telcos modernize, become more agile and monetize new opportunities brought about by 5G, private wireless and a spate of new innovations in infrastructure, cloud, data, AI and apps? It’s become table stakes for carriers to evolve their hardened, proprietary infrastructure stacks to more open, flexible, cloud-like models. But doing so brings risks that telcos must carefully balance as they strive to deliver consistent quality of service while at the same time moving faster and avoiding disruption. In this Breaking Analysis and ahead of MWC23, we explore the evolution of the telco business and how the industry is in many ways, mimicking a transformation that took place decades ago in enterprise IT. We’ll model some of the traditional enterprise vendors using ETR data and investigate how they’re faring in the telecomms vertical. And we’ll pose some of the key issues facing the industry this decade. Watch the full video analysis. Episode 167 – Google’s Point of View on Confidential Computing – Confidential computing is a technology that aims to enhance data privacy and security by providing encrypted computation on sensitive data in use and isolating data from apps and other host resources in fenced off enclaves. The concept of confidential computing is gaining popularity, especially in the cloud computing space where sensitive data is commonly stored and processed. However, there are some who view confidential computing as an unnecessary technology and a marketing ploy by cloud providers, aimed at calming customers who are cloud-phobic. In this Breaking Analysis we revisit the notion of confidential computing and explore whether it’s just marketing or a key part of a trusted security strategy. To do so we’ll invite two Google experts to the show. But before we get there let’s summarize the overall market climate briefly with some ETR data. Watch the full video analysis. Episode 166 – Cloud players sound a cautious tone for 2023 – The unraveling of market enthusiasm continued in Q4 of 2022 with the earnings reports from the U.S. hyperscalers now all in. As we said earlier this year, even the cloud is not immune from the macro headwinds and the cracks in the armor we saw from the data we shared last summer are playing out into 2023. For the most part, actuals are disappointing beyond expectations, including our own. It turns out that our estimates for the big 3 hyperscale revenue missed by $1.2 billion or 2.7% lower than we had forecast from our most recent November estimates. We expect decelerating growth rates for the hyperscalers will continue through the summer of 2023 and won’t abate until comparisons get easier. In this Breaking Analysis we share our view of what’s happening in cloud markets – not just for the hyperscalers but other firms that have hitched a ride on the cloud. And we’ll share new ETR data that shows why these trends are playing out, tactics customers are employing to deal with their cost challenges and how long the pain is likely to last. Watch the full video analysis. Episode 165 – Enterprise Technology Predictions 2023 – Making predictions about the future of enterprise tech is more challenging if you strive to lay down forecasts that are measurable. In other words if you make a prediction, you should be able to look back a year later and say with some degree of certainty whether the prediction came true or not. With evidence to back that up. In this Breaking Analysis we aim to do just that with predictions about the macro IT spending environment, cost optimization, security – lots to talk about there – generative AI, cloud and supercloud, blockchain adoption, data platforms, including commentary on Databricks, Snowflake and other key players, automation, events and we may even have some bonus predictions. To make all this happen we welcome back for the third year in a row, Erik Bradley our colleague from ETR. As well, you can check out how we did with our 2022 predictions. Watch the full video analysis. Episode 164 – ChatGPT Won’t Give OpenAI First Mover Advantage – OpenAI, the company, and ChatGPT have taken the world by storm. Microsoft reportedly is investing an additional $10B in the startup. But in our view, while the hype around ChatGPT is justified, we don’t believe OpenAI will lock up the market with its first mover advantage. Rather we believe that success in this market will be directly proportional to the quality and quantity of data that a technology company has at its disposal, and the compute power it has to run the system. This market is unlikely to display winner take all dynamics and will probably evolve in a more fragmented fashion than cloud. In this Breaking Analysis we unpack the excitement around ChatGPT and debate the premise that the company’s early entry into the space may not confer “game over” advantage to OpenAI. To do so we welcome CUBE collaborator Sarbjeet Johal & John Furrier, cohost of theCUBE. Watch the full video analysis. Episode 163 – Supercloud2 Explores Cloud Practitioner Realities & the Future of Data Apps – Enterprise tech practitioners, like most of us, want to make their lives easier so they can focus on delivering more value to their business. They want to tap best of breed services in the public cloud and at the same time connect their on-prem intellectual property to emerging applications which drive top line revenue and bottom line profits. But creating a consistent experience across clouds and on-prem estates has been an elusive capability for most organizations, forcing tradeoffs and injecting friction into the system. The need to create seamless experiences is clear and the technology industry is starting to respond with platforms, architectures and visions of what we call Supercloud. In this Breaking Analysis we give you a preview of Supercloud2, share key findings leading up to the event and highlight some of the areas we’ll be probing in the live program. Watch the full video analysis. Episode 162 – CIOs in a holding pattern but ready to strike at monetization – Recent conversations with IT decision makers show a stark contrast between the period exiting 2023 versus the mindset leaving 2022. CIOs are generally funding new initiatives by pushing off or cutting lower-priority items. While security efforts are still being funded, those that enable business initiatives that generate revenue take priority over cleaning up legacy technical debt. The bottom line is, for the moment at least, the mindset is not to cut everything, rather it’s to put a pause on cleaning up legacy hairballs and continue to fund initiatives to drive monetization. Cloud has become fundamental and getting data “right” is a consistent theme that appears to be an underpinning of initiatives getting funded today. In this Breaking Analysis we tap recent discussions from two primary sources: year-end ETR roundtables with IT decision makers and conversations on theCUBE with data, cloud and IT architecture practitioners. Watch the full video analysis. Episode 161 – AI goes mainstream but ROI remains elusive – A decade of big data investments combined with cloud scalability, the rise of more cost effective processing and the introduction of advanced tooling has catapulted machine intelligence to the forefront of technology investments. No matter what job you have, your operation will be AI powered within five years and machines may be doing your job in the future. Artificial intelligence is being infused into applications, infrastructure, equipment and virtually every aspect of our lives. AI is proving to be extremely helpful at controlling vehicles, speeding medical diagnoses, processing language, advancing science and generally raising the stakes on what it means to apply technology for business advantage. But business value realization has been a challenge for most organizations due to lack of skills, complexity of programming models, immature technology integration, sizable up front investments, ethical concerns and lack of business alignment. Mastering AI technology and a focus on features will not be a requirement for success in our view. Rather figuring out how and where to apply AI to your business will be the crucial gate. That means understanding the business case, picking the right technology partner, experimenting in bite sized chunks and quickly identifying winners to double down on from an investment standpoint. In this Breaking Analysis we update you on the state of AI with a focus on interpreting the latest ETR survey data around ML/AI and data. We’ll explore what it means for the competitive environment and what to look for in the future. To do so we invite into our studios Andy Thurai of Constellation Research. Andy covers AI deeply, he knows the players and the pitfalls of AI investment. Watch the full video analysis. Episode 160 – Grading our 2022 enterprise technology predictions – Nailing technology predictions in 2022 was tricky business. Projections on the performance of markets, identifying IPO prospects and making binary forecasts on data, AI, the macro spending climate, along with other related topics in enterprise tech, carried much uncertainty. 2022 was characterized by a seesaw economy where central banks were restructuring their balance sheets, the war in Ukraine fueled inflation, supply chains were a mess and the unintended consequences of digital acceleration are still being sorted. In this Breaking Analysis we continue our annual tradition of openly grading our previous year’s enterprise tech predictions. You may or may not agree with our self-grading system but we give you the data to draw your own conclusions. Watch the full video analysis. Episode 159 – How Palo Alto Networks Became the Gold Standard of Cybersecurity – Palo Alto Networks has earned a reputation as the leader in security. You can measure this in revenue, market cap, execution and, most importantly, conversations with CISOs. The company is on track to double its revenues to nearly $7B in FY23 from FY20. This despite macro headwinds which will likely continue through next year. Palo Alto owes its position to a clarity of vision and strong execution of a TAM expansion strategy bolstered by key acquisitions and integrations into its cloud & SaaS offerings. In this Breaking Analysis, and ahead of Palo Alto Ignite, we bring you the next chapter on top of last week’s cybersecurity update. We’ll dig into the ETR spending data on Palo Alto Networks, provide a glimpse of what to look for at Ignite and posit what Palo Alto needs to do to stay on top of the hill. Watch the full video analysis. Episode 158 – Cyber Firms Revert to the Mean – While by no means a safe haven, the cybersecurity sector has outpaced the broader tech market by a meaningful margin. That is up until very recently. Cyber security remains the number one technology priority for the c-suite but as we’ve previously reported, the CISO’s budget has constraints; just like other technology investments. Recent trends show that economic headwinds have elongated sales cycles, pushed deals into future quarters and, just like other tech initiatives, are pacing cybersecurity investments and breaking them into smaller chunks. In this Breaking Analysis we explain how cybersecurity trends are reverting to the mean and tracking more closely with other technology investments. We’ll make a couple of valuation comparisons to show the magnitude of the challenge and which cyber firms are feeling the heat, and which aren’t as much. We’ll then show the latest survey data from ETR to quantify the contraction in spending momentum and close with a glimpse at the landscape of emerging cybersecurity companies that could be ripe for acquisition, consolidation or disruptive to the broader market. Watch the full video analysis. Episode 157 – re:Invent 2022 marks the next chapter in data & cloud – The ascendency of AWS under the leadership of Andy Jassy was marked by a tsunami of data and corresponding cloud services to leverage data. Those services mainly came in the form of primitives – i.e. basic building blocks that were used by developers to create more sophisticated capabilities. AWS in the 2020s, led by CEO Adam Selipsky, will be marked by four high level trends in our view: 1) A rush of data that will dwarf anything previously seen; 2) Doubling down on investments in the basic elements of cloud – compute, storage, database, security, etc; 3) Greater emphasis on end-to-end integration of AWS services to make data accessible to more professionals and further accelerate cloud adoption; and 4) Significantly deeper business integration of cloud, beyond IT, as an underlying element of organizational transformation. In this Breaking Analysis we extract and analyze nuggets from John Furrier’s annual sit down with the CEO of AWS. We’ll share data from ETR and other sources to set the context for the market and competition in cloud and we’ll give you our glimpse of what to expect at re:Invent 2022. Watch the full video analysis. Episode 156 – Snowflake caught in the storm clouds – A better than expected earnings report in late August got people excited about Snowflake again but the negative sentiment in the market has weighed heavily on virtually all growth tech stocks. Snowflake is no exception. As we’ve stressed many times, the company’s management is on a long term mission to simplify the way organizations use data. Snowflake is tapping into a multi-hundred billion dollar total available market and continues to grow at a rapid pace. In our view the company is embarking on its third major wave of innovation, data apps, while its first and second waves are still bearing significant fruit. For short term traders focused on the next 90 or 180 days, that probably doesn’t matter much. But those taking a longer view are asking, should we still be optimistic about the future of this high flier or is it just another over-hyped tech play? In this Breaking Analysis we take a look at the most recent survey data from ETR to see what clues and nuggets we can extract to predict the near future and the long term outlook for Snowflake. Watch the full video analysis. Episode 155 – Cloudflare’s Supercloud…What Multi Cloud Could Have Been – Over the past decade, Cloudflare has built a global network that has the potential to become the fourth U.S.-based hyperscale-class cloud. In our view, the company is building a durable revenue model with hooks into many important markets. These include the more mature DDoS protection space, but also extend to growth sectors such as zero trust, a serverless platform for application development and an increasing number of services such as database and object storage. In essence, Cloudflare can be thought of as a giant, distributed supercomputer that can connect multiple clouds and act as a highly efficient scheduling engine– allocating and optimizing resources at scale. Its disruptive DNA is increasingly attracting novel startups and established global firms looking for a reliable, secure, high performance, low latency and more cost effective alternative to AWS and legacy infrastructure solutions. In this Breaking Analysis we initiate deeper coverage of Cloudflare. While the stock got hammered this past week on tepid guidance, we are optimistic about the company’s future. In this post, we’ll briefly explain our take on the company and its unique business model. We’ll then share some peer comparisons with both a financial snapshot and some fresh ETR survey data. Finally we’ll show some examples of how we think Cloudflare could be a disruptive force with a supercloud-like offering that, in many respects, is what multi-cloud should have been. Watch the full video analysis. Episode 154 – Even the Cloud Is Not Immune to the Seesaw Economy – Have you ever driven on the highway and traffic suddenly slows way down? And then after a little while it picks up again and you’re cruising along thinking…ok that was weird but it’s clear sailing now…only to find out in a bit that that traffic is building up again, forcing you to pump the brakes as the traffic patterns ebb and flow? Well welcome to the seesaw economy. The Fed induced fire that prompted a rally in tech is being purposely extinguished by that same Fed and virtually every sector of the tech industry is having to reset its expectations – including the cloud. In this Breaking Analysis we’ll review the implications of this week’s earnings announcements from the big 3 cloud players. The growth of AWS and Azure slowed while Google Cloud Platform’s growth accelerated. We’ll explain why GCP’s growth is still not fast enough. We’ll update you on our quarterly IaaS forecasts and share the latest cloud survey data from ETR. Watch the full video analysis. Episode 153 – Survey Says! Takeaways from the latest CIO spending data – The overall technology spending outlook is deteriorating. And yet there are positive signs making things unpredictable. The negative sentiment is of course being driven by macroeconomic factors and earnings forecasts that have been coming down all year while interest rates keep rising. Making matters worse is many people think earnings estimates are still too high. It’s understandable why there’s so much uncertainty. Technology continues to boom. Digital transformations are happening in earnest. Leading companies have momentum and long cash runways. Moreover, the CEOs of these leading companies are still really optimistic. But strong guidance in an environment of uncertainty is risky and makes navigation more challenging. In this Breaking Analysis we try to put some guardrails on the market by sharing takeaways from from ETRs latest spending survey, which was released to their private clients on the 21st of October. Today we’re going to review the macro spending data, convey where CIOs think their cloud spend is headed, look at the actions organizations are taking to manage uncertainty and then review some of the technology companies that have the most positive and negative outlooks in the ETR data set. Watch the full video analysis. Episode 152 – CEO Nuggets from Microsoft Ignite & Google Cloud Next – This past week, we saw two of the “Big 3” cloud providers present an update of their respective cloud visions, business progress, announcements and innovations. The content at these events had many overlapping themes including modern cloud infrastructure at global scale, applying advanced machine intelligence, end-to-end data platforms, the future of work, automation and a taste of the metaverse/Web 3.0. And more. Despite the striking similarities, the differences between these two cloud platforms, and that of AWS, remain significant with Microsoft leveraging its massive application software footprint to dominate virtually all markets; AndGoogle doing everything in its power to keep up with the frenetic pace of today’s cloud innovation which was set into motion a decade and a half ago by AWS. In this Breaking Analysis, we unpack the immense amount of content presented by the CEOs of Microsoft and Google Cloud at Microsoft Ignite and Google Cloud Next. We’ll also quantify with ETR survey data, the relative position of these two cloud giants in four key sectors – Cloud IaaS, BI analytics, data platforms and collaboration software. Watch the full video analysis. Episode 151 – Analyst Take on Dell – Dave Vellante provides his take on the transformation of Dell to Dell EMC to Dell Technologies, the impact of the VMware spin out and what the future holds for this bellwether infrastructure player. Watch the full video analysis. Episode 150 – Latest CIO Survey Shows Steady Deceleration in IT Spend – Is the glass half full or half empty? Well, it depends on how you want to look at it. CIOs are tapping the brakes on spending that’s clear. The latest macro survey from ETR quantifies what we already know to be true, that IT spend is decelerating. CIOs and IT buyers forecast that their tech spend will grow by 5.5% this year, a meaningful deceleration from their year end 2021 expectations…but these levels are still well above historical norms – so while the feel good factor may be in some jeopardy, overall things are pretty good – at least for now. In this Breaking Analysis, we update you on the latest macro tech spending data from Enterprise Technology Research, including strategies organizations are employing to cut costs…and which project categories continue to see the most traction. Watch the full video analysis. Episode 149 – As the tech tide recedes, all sectors feel the pinch – Virtually all tech companies have expressed caution on their respective earnings calls. And why not… the macroeconomic environment is full of uncertainties and there’s no upside to providing aggressive guidance when sellers punish even the slightest miss. Moreover, the spending data confirms the market is softening across the board, so it’s becoming expected that CFOs will guide cautiously. But companies facing execution challenges can’t hide behind the macro. Which is why it’s important to understand which firms are best positioned to maintain momentum through the headwinds and come out the other side stronger. In this Breaking Analysis we’ll do three things: 1) Share a high-level view of the spending squeeze almost all sectors are experiencing; 2) Highlight some of those companies that continue to show notably strong momentum – and relative high spending velocity on their platforms – albeit less robust than last year; and 3) give you a peek at how one senior technology leader in the financial sector sees the competitive dynamic between AWS, Snowflake and Databricks. Watch the full video analysis. Episode 148 – UiPath is a Rocket Ship Resetting its Course – Like a marathon runner pumped up on adrenaline, UiPath sprinted to the lead in what is surely going to be a long journey toward enabling the modern automated enterprise. In doing so, the company has established itself as a leader in enterprise automation, while at the same time getting out over its skis on critical execution items and disappointing investors along the way. In our view, the company has plenty of upside potential but will have to slog through its current challenges including restructuring its go to market, prioritizing investments, balancing growth with profitability and dealing with a difficult macro environment. In this Breaking Analysis and ahead of Forward5, UiPath’s customer conference, we once again dig into RPA and automation leader UiPath, to share our most current data and view of the company’s prospects, its performance relative to the competition and market overall. Watch the full video analysis. Episode 147 – How CrowdStrike Plans to Become a Generational Platform – In just over ten years, CrowdStrike has become a leading independent security firm. It has more than $2B in annual recurring revenue, nearly 60% ARR growth, a roughly $40B market capitalization, very high retention and a path to $5B in revenue by mid-decade. The company has joined Palo Alto Networks as a gold standard pure play cyber firm. It has achieved this lofty status with an architecture that enables it to go beyond point product. Combine this with outstanding go to market, solid financial execution, some sharp acquisitions and an ever-increasing total available market and you have the formula for a great company. In this Breaking Analysis and ahead of Fal.Con, CrowdStrike’s user conference, we take a deeper look into the company, its performance, its platform and customer survey data from our partner ETR. Watch the full video analysis. Episode 146 – We Have the Data…What Private Tech Companies Don’t Tell you About Their Business – The negative sentiment in tech stocks, caused by rising interest rates, less attractive discounted cash flow models and more tepid forward guidance, is easily measured by public market valuations. And while there’s lots of talk about the impact on private companies, their cash runways and 409A valuations, measuring the performance of non-public companies isn’t as easy. IPOs have dried up and public statements by private companies accentuate the good and hide the bad. Real data, unless you’re an insider, is hard to find. In this Breaking Analysis we unlock some of the secrets that non-public, emerging tech companies may or may not be sharing. We do this by introducing you to a capability from ETR that we’ve not previously exposed in Breaking Analysis. It’s called the ETR Emerging Technology Survey and is packed with sentiment and performance data based on surveys of more than 1,000 CIOs & IT buyers covering more than 400 private companies. The survey will highlight metrics on the evaluation, adoption and churn rates for private companies and the mindshare they’re able to capture. We’ve invited back our colleague Erik Bradley of ETR to help explain the survey and the data we’re going to cover in this post. Watch the full video analysis. Episode 145 – VMware Explore 2022 will mark the start of a Supercloud journey – While the precise direction of VMware’s future is unknown, given the planned Broadcom acquisition, one thing is clear; the subject of what Hock E. Tan plans will not be the main focus of the agenda at the upcoming VMware Explore event next week in San Francisco. We believe that despite any uncertainty, VMware will lay out for its customers what it sees as its future. And that future is multi-cloud or cross cloud services; what we would call supercloud. In this Breaking Analysis we drill into the latest ETR survey data on VMware. We’ll share with you the next iteration of the supercloud definition based on feedback from dozens of contributors. And we’ll give you our take on what to expect at VMware Explore next week. Watch the full video analysis. Episode 144 – What Black Hat ’22 tells us about securing the Supercloud – Black Hat 2022 was held in Las Vegas last week, at the same time as theCUBE’s supercloud event. Unlike AWS re:Inforce, where words are carefully chosen to put a positive spin on security, Black Hat exposes all the warts of cybersecurity and openly discusses its hard truths. It’s a conference attended by technical experts who proudly share some of the vulnerabilities they’ve discovered and of course by numerous vendors marketing their products and services. In this Breaking Analysis we summarize what we learned from discussions with several people who attended Black Hat and our analysis from reviewing dozens of keynotes, articles, videos, session talks, Dark Reading interviews and data from a recent Black Hat attendees survey conducted by Black Hat and Informa. We’ll also share data from ETR in a recent post discussing how Zscaler became the last line of defense for a manufacturing firm. We’ll end with a discussion of what it all means for the challenges around securing the supercloud. Watch the full video analysis. Episode 143 – Further defining Supercloud With tech leaders VMware, Snowflake, Databricks & others – At our inaugural Supercloud22 event we sought community input to evolve the concept of a supercloud by iterating on the definition, the salient attributes and examples of what is and is not a supercloud. We asked several technologists including experts from VMware, Snowflake, Databricks, HashiCorp, Confluent, Intuit, Cohesity and others to help test the supercloud definition, operational models, service models and principles. In this Breaking Analysis we unpack our discussions with these technology leaders and apply their input to iterate the definition of supercloud. We then go in-depth to examine Snowflake’s Data Cloud architecture as a leading example of supercloud. Watch the full video analysis. Episode 142 – What we hope to learn at Supercloud22 – The term supercloud is relatively new, but the concepts behind it have been bubbling for years. Early last decade when NIST put forth its original definition of cloud computing, it said services had to be accessible over a public network…essentially cutting the on-prem crowd out of the conversation. A guy named Chuck Hollis, a CTO at EMC and prolific blogger objected to that criterion and laid out his vision for what he termed a private cloud. In that post he showed a workload running both on premises and in a public cloud, sharing the underlying resources in an automated and seamless manner – what later became more broadly known as hybrid cloud. That vision, as we now know, really never materialized and we were left with multi-cloud…sets of largely incompatible and disconnected cloud services running in separate silos. The point is, what Hollis put forth – i.e. the ability to abstract underlying infrastructure complexity and run workloads across multiple heterogeneous estates with an identical experience – is what supercloud is all about. Watch the full video analysis. Episode 141 – How the cloud is changing security defenses in the 2020s – AThe rapid pace of cloud adoption has changed the way organizations approach cybersecurity. Specifically, the cloud is increasingly becoming the first line of cyber defense. As such, along with communicating to the board and creating a security-aware culture, the CISO must ensure that the shared responsibility model is being applied properly. The DevSecOps team has emerged as the critical link between strategy and execution, while audit becomes the “free safety” in the equation – i.e. the last line of defense. In this Breaking Analysis we share our puts and takes from AWS re:Inforce with an update on the latest hyperscale IaaS market shares; and insights from ETR survey data. We’ll also dig deeper into some technical aspects of AWS Nitro, a system we believe is one of AWS’ secret weapons, with a focus on confidential computing and what it means for the future of systems architecture. Watch the full video analysis. Episode 140 – AWS re:Inforce marks a summer checkpoint on cybersecurity – After a two year hiatus, AWS re:Inforce is back on as an in-person event in Boston next week. Like the all-star break in baseball, re:Inforce gives us an opportunity to evaluate the cybersecurity market overall, the state of cloud security and what AWS is up to in the sector. In this Breaking Analysis, we’ll share our view of what’s changed since our last cyber update in May, we’ll look at the macro environment, how it’s impacting cybersecurity plays in the market, what the ETR data tells us and what to expect at next week’s AWS re:Inforce. Watch the full video analysis. Episode 139 – Amping it up with Frank Slootman – Organizations have considerable room to improve their performance without making expensive changes to their talent, structure or fundamental business model. You don’t need a slew of consultants to tell you what to do. You already know. What you need is to immediately ratchet up expectations, energy, urgency and intensity. Fight mediocrity every step of the way. Amp it up and the results will follow. This is the fundamental premise of a hard hitting new book written by Frank Slootman, CEO of Snowflake, and published earlier this year. It’s called, Amp it Up, Leading for Hypergrowth by Raising Expectations, Increasing Urgency and Elevating Intensity. At Snowflake Summit last month, I was invited to interview Frank on stage about his book. I’ve read it several times and if you haven’t picked it up, you should. Even if you have read it, in this Breaking Analysis we’ll dig deeper into the book and share some clarifying insights and unpublished nuances of Frank’s philosophy. You’ll hear directly from Slootman himself with excerpts from my one on one conversation with him. Watch the full video analysis. Episode 138 – Answering the top 10 questions about SuperCloud – As we exited the isolation economy last year, supercloud is a term we introduced to describe something new that was happening in the world of cloud. In this Breaking Analysis we address the ten most frequently asked questions we get on supercloud. Watch the full video analysis. Episode 137 – H1 of ‘22 was ugly…H2 could be worse…Here’s why we’re still optimistic – After a two year epic run in tech, 2022 has been an epically bad year in the market. Through yesterday, the Nasdaq composite is down 30%, the S&P 500 is off 21%, the DJIA down 16% and the poor HODLers of BTC have had to endure a nearly 60% decline year to date. Watch the full video analysis. Episode 136 – Tech Spending Intentions are Holding Despite Macro Concerns – Much of the energy around data innovation that dispersed with the decline of Hadoop’s relevance is coalescing in a new ecosystem spawned by the ascendency of Snowflake’s Data Cloud. What was once seen as a simpler cloud data warehouse and good marketing with Data Cloud, is evolving rapidly with new workloads, a vertical industry focus, data applications, monetization and more. The question is will the promises of data be fulfilled this time around or is it same wine, new bottle? Watch the full video analysis. Episode 135 – Snowflake Summit 2022…All About Apps & Monetization – Much of the energy around data innovation that dispersed with the decline of Hadoop’s relevance is coalescing in a new ecosystem spawned by the ascendency of Snowflake’s Data Cloud. What was once seen as a simpler cloud data warehouse and good marketing with Data Cloud, is evolving rapidly with new workloads, a vertical industry focus, data applications, monetization and more. The question is will the promises of data be fulfilled this time around or is it same wine, new bottle? Watch the full video analysis. Episode 134 – How Snowflake Plans to Make Data Cloud a De Facto Standard – When Frank Slootman took ServiceNow public, many people undervalued the company, positioning it as just a better help desk tool. It turns out the firm actually had a massive TAM expansion opportunity in ITSM, HR, logistics, security, marketing and customer service management. NOW’s stock price followed the stellar execution under Slootman and CFO Mike Scarpelli’s leadership. When they took the reins at Snowflake, expectations were already set that they’d repeat the feat but this time, if anything, the company was overvalued out of the gate. It can be argued that most people didn’t really understand the market opportunity any better this time around. Other than that it was a bet on Slootman’s track record of execution…and data. Good bets; but folks really didn’t appreciate that Snowflake wasn’t just a better data warehouse. That it was building what the company calls a data cloud…and we’ve termed a data supercloud. In this Breaking Analysis and ahead of Snowflake Summit, we’ll do four things: 1) Review the recent narrative and concerns about Snowflake and its value; 2) Share survey data from ETR that will confirm almost precisely what the company’s CFO has been telling anyone who will listen; 3) Share our view of what Snowflake is building – i.e. trying to become the de facto standard data platform; and 4) Convey our expectations for the upcoming Snowflake Summit next week at Caesar’s Palace in Las Vegas. Watch the full video analysis. Episode 133 – MongoDB Sends Strong Signals Despite Cautious Macro Tones – Earnings season has shown a conflicting mix of signals for software companies. Most firms are expressing caution over macro headwinds citing a combination of Ukraine, inflation, interest rates, EMEA softness, currency, supply chain and general demand for technology. But MongoDB, along with a few other names appeared more sanguine, thanks to a beat in the recent quarter and a cautious but upbeat outlook for the near term. In this Breaking Analysis, ahead of MongoDB World 2022, we drill into the company’s business and what ETR survey data tells us in the context of overall demand and the patterns from other software companies. Watch the full video analysis. Episode 132 – Broadcom, Taming the VMware Beast – In the words of CUBE analyst and former CTO David Nicholson, Broadcom buys old cars. Not to restore them to their original beauty…nope…they buy classic cars to extract the platinum that’s inside the catalytic converter. Broadcom’s planned $61B acquisition of VMware will mark yet another new era for the virtualization leader, a mere seven months after finally getting spun out as a fully independent company by Dell. For VMware this means a dramatically different operating model, with financial performance and shareholder value creation as the dominant and perhaps sole agenda. For customers it will mean a more focused portfolio, less aspirational vision pitches and most certainly higher prices. In this Breaking Analysis we’ll share data, opinions and customer insights about this blockbuster deal and forecast the future of VMware, Broadcom and the broader ecosystem. Watch the full video analysis. Episode 131 – Supercloud is becoming a thing – Last year we noted in a Breaking Analysis that the cloud ecosystem is innovating beyond the notion of multicloud. We’ve said for years that multicloud is really not a strategy but rather a symptom of multivendor. We used the term supercloud to describe an abstraction layer that resides above and across hyperscale infrastructure, connects on premises workloads and eventually stretches to the edge. Our premise is that supercloud goes beyond running services in native mode on each individual cloud. Rather supercloud hides the underlying primitives and APIs of the respective cloud vendors and creates a new connection layer across locations. Since our initial post, we’ve found many examples within the ecosystem of technology companies working on so-called supercloud in various forms. Including some examples that actually do not try to hide cloud primitives but rather are focused on creating a consistent experience for developers across the devsecops tool chain, while preserving access to low level cloud services. In this Breaking Analysis we share some recent examples of supercloud that we’ve uncovered. We also tap theCUBE network to access direct quotes about supercloud from the many CUBE guests we’ve recently had on the program. Here we test the concept’s technical and business feasibility. We’ll also post some recent ETR data to put into context some of the players we think are going after this opportunity and where they’re at in their supercloud buildout. Watch the full video analysis. Episode 130 – Are Cyber Stocks Oversold or Still too Pricey? – Cybersecurity stocks have been sending mixed signals as of late…Mostly negative like much of tech. But some, such as Palo Alto Networks, despite a tough go of it recently, have held up better than most tech names. Others like CrowdStrike had been outperforming broader tech in March but then flipped in May. Okta’s performance was somewhat tracking along with CrowdStrike for most of the past several months but then the Okta hack changed the trajectory of that name. Zscaler has crossed the critical $1B ARR revenue milestone and sees a path to $5B, but the company’s stock fell sharply after its last earnings report and has been on a downtrend since last November…Meanwhile CyberArk’s recent beat and raise was encouraging and the stock acted well after its last report. Security remains the #1 initiative priority amongst IT organizations and the spending momentum for many high flying cyber names remains strong. So what gives in cybersecurity? In this Breaking Analysis we focus on security and will update you on the latest data from ETR to try and make sense out of the market and read into what this all means in both the near and long term for some of our favorite names in the sector. Watch the full video analysis. Episode 129- What you May not Know About the Dell Snowflake Deal – In the pre-cloud era, hardware companies would run benchmarks showing how database and application performance ran best on their systems relative to competitors and previous generation boxes. They would make a big deal out of it and the independent software vendors would do a “golf clap” in the form of a joint press release. It was a game of leapfrog amongst hardware competitors that became pretty commonplace over the years. The Dell-Snowflake deal underscores that the value prop between hardware companies and ISVs is changing and has much more to do with distribution channels and the amount of data that lives on-prem in various storage platforms. For cloud-native ISVs like Snowflake, they are realizing that despite their cloud-only dogma, they have to grit their teeth and deal with on-premises data or risk getting shut out of evolving data architectures. In this Breaking Analysis we unpack what little is known about the Snowflake announcement from Dell Technologies World… and discuss the implications of a changing cloud ecosystem landscape. We’ll also share some new ETR data for cloud and database platforms that shows Snowflake has actually entered the earth’s orbit when it comes to spending momentum on its platform. Watch the full video analysis. Episode 128- The Ever expanding Cloud Continues to Storm the IT Universe – Despite a mixed bag of earnings reports from tech companies, negative GDP growth this past quarter and rising inflation…the cloud continues its relentless expansion on the IT landscape. AWS, Microsoft and Alphabet have all reported earnings and, when you include Alibaba’s cloud in the mix, the big 4 hyperscalers are on track to generate $167B in revenue this year based on our projections. But as we’ve said many times the definition of cloud is expanding. And hybrid environments are becoming the norm at major organizations. We’re seeing the largest enterprise tech companies focus on solving for hybrid and every public cloud company now has a strategy to bring their environments closer to where customers’ workloads live – in data centers and the edge. Hello and welcome to this week’s Wikibon CUBE Insights, Powered by ETR. In this Breaking Analysis we’ll update you on our latest cloud projections and outlook. We’ll share the latest ETR data and some commentary on what’s happening in the “hybrid zone” of cloud. Watch the full video analysis. Episode 127- Does Hardware Still Matter – The ascendancy of cloud and SaaS has shone new light on how organizations think about, pay for, and value hardware. Once-sought-after skills for practitioners with expertise in hardware troubleshooting, configuring ports, tuning storage arrays and maximizing server utilization have been superseded by demand for cloud architects, DevOps pros and developers with expertise in microservices, container app development and similar skills. Even a company like Dell, the largest hardware company in enterprise tech, touts that it has more software engineers than those working in hardware. It begs the question: Is hardware going the way of COBOL? Well, not likely — software has to run on something. But the labor and skills needed to deploy, troubleshoot and manage hardware infrastructure is shifting quite dramatically. At the same time we’ve seen the value flow also changing in hardware. Once a world dominated by x86 processors, value is flowing to alternatives like Nvidia and Arm-based designs. Moreover, other components like NICs, accelerators and storage controllers are becoming more advanced, integrated and increasingly important. The question is: Does it matter? If so, why does it matter and to whom? What does it mean to customers, workloads, OEMs and the broader society? In this Breaking Analysis we try to answer these questions and to do so we’ve organized a special CUBE Power Panel of industry analysts and experts to address the question: Does Hardware (Still) Matter? Watch the full video analysis. Episode 126 – Technology & Architectural Considerations for Data Mesh – The introduction and socialization of data mesh has caused practitioners, business technology executives and technologists to pause and ask some probing questions about the organization of their data teams, their data strategies, future investments and their current architectural approaches. Some in the technology community have embraced the concept, others have twisted the definition while still others remain oblivious to the momentum building around data mesh. We are in the early days of data mesh adoption. Organizations that have taken the plunge will tell you aligning stakeholders is a non-trivial effort. But one that is necessary to break through the limitations that monolithic data architectures and highly specialized teams have imposed over frustrated business and domain leaders. However, practical data mesh examples often lie in the eyes of the implementer and may not strictly adhere to the principles of data mesh. Part of the problem is the lack of open technologies and standards that can accelerate adoption and reduce friction. This is the topic of today’s Breaking Analysis where we investigate some of the key technology and architectural questions around data mesh. To do so, we welcome back the founder of data mesh and Director of Emerging Technologies at ThoughtWorks, Zhamak Dehghani. Watch the full video analysis. Episode 125 – Customer ripple effects from the Okta breach are worse than you think – The recent security breach of an Okta third party supplier has been widely reported. The criticisms of Okta’s response have been harsh and the impact on Okta’s value has been obvious. Investors shaved about $6B off the company’s market cap during the week the hack was made public. We believe Okta’s claim that the customer technical impact was “near zero,” may be semantically correct. However, based on customer data, we feel Okta has a blind spot. There are customer ripple effects that require clear action, which are missed in Okta’s public statements. Okta’s product portfolio remains solid. It is a clear leader in the identity space. But in our view, one part of the long journey back to credibility requires Okta to fully understand and recognize the true scope of this breach on its customers. In this week’s Breaking Analysis we welcome our ETR colleague Erik Bradley to share new data from the community. In addition, we’ll analyze some of the statements made by Okta CEO Todd McKinnon in an interview with Emily Chang on Bloomberg to see how they align with what customers tell us. Watch the full video analysis. Episode 124 – New Data Signals C Suite Taps the Brakes on Tech Spending – Fresh survey data from ETR shows a clear deceleration in spending and a more cautious posture from technology buyers. Just this week we saw sell side downgrades in hardware companies like Dell and HP; and revised guidance from high flier UiPath, citing exposure to Russia, Europe and certain sales execution challenges. But these headlines we think are a canary in the coal mine pointing to broader tech spending softness. According to ETR analysis and channel checks in theCUBE community, the real story is these issues are not isolated. Rather we’re seeing signs of caution from buyers across the board in enterprise tech. In this Breaking Analysis we are the bearers of bad news, relatively speaking. We’ll share a first look at new data that suggest a tightening in tech spending, calling for 6% growth this year, which is below our January prediction of 8% for 2022. Watch the full video analysis. Episode 123 – Governments Should Heed the History of Tech Antitrust Policy – There are very few political issues that get bipartisan support these days, never mind consensus spanning geopolitical boundaries. But whether we’re talking across the aisle or over the pond, there seems to be common agreement that the power of big tech firms should be regulated. However the government’s track record when it comes to antitrust aimed at tech is mixed, at best. History shows that market forces, rather than public policy, have been much more effective at curbing monopoly power in the technology industry. Moreover, the standard for antitrust action has always been demonstrating consumer harm. Many of today’s policy makers are challenging that notion and using market dominance and the potential for consumer harm as the new benchmark for intervention. In this week’s Breaking Analysis we welcome in frequent CUBE contributor David Moschella, author and senior fellow at the Information Technology and Innovation Foundation. We explore several issues including the efficacy of governments’ antitrust actions against big tech, what types of remedies have been and can be most effective and a first pass assessment of the new rules EU regulators just agreed to try and rein in big tech companies. Watch the full video analysis. Episode 122 – Snowflake’s Wild Ride – Snowflake…they love the stock at 400 and hate it at 165. That’s the nature of the business isn’t it? Especially in this crazy cycle over the last two years of lockdowns, free money, exploding demand and now rising inflation and rates. But with the Fed providing some clarity on its actions, the time has come to really look at the fundamentals of companies and there’s no tech company more fun to analyze than Snowflake. In this breaking analysis we take look at the action of Snowflake’s stock since its IPO, why it’s behaved the way it has, how some sharp traders are looking at the stock and most importantly, what customer demand looks like. Watch the full video analysis. Episode 121 – Pat Gelsinger has the Vision Intel Just Needs Time, Cash & a Miracle – Intel’s future would be a disaster without Pat Gelsinger. Even with his clear vision, fantastic leadership, deep technical and business acumen and amazing positivity, the company’s future is in serious jeopardy. It’s the same story we’ve been telling for years. Volume is king in the semiconductor industry and Intel no longer is the volume leader. Despite Intel’s efforts to change that dynamic with several recent moves, including making another go at its foundry business, the company is years away from reversing its lagging position relative to today’s leading foundries and design shops. Intel’s best chance to survive as a leader in our view will come from a combination of a massive market, continued supply constraints, government money and luck – perhaps in the form of a deal with Apple in the mid- to long term. In this Breaking Analysis we’ll update you on our latest assessment of Intel’s competitive position and unpack nuggets from the company’s February investor conference. Watch the full video analysis. Episode 120 – RPA has Become a Transformation Catalyst, Here’s What’s New – In its early days, robotic process automation emerged from rudimentary screen scraping, macros and workflow automation software. Once a script-heavy and limited tool that was almost exclusively used to perform mundane tasks for individual users, RPA has evolved into an enterprise-wide megatrend that puts automation at the center of digital business initiatives. In this Breaking Analysis we present our quarterly update of the trends in RPA and share the latest survey data from Enterprise Technology Research. Watch the full video analysis. Episode 119 – Cyber Stocks Caught in the Storm While Private Firms Keep Rising – The pandemic precipitated what is shaping up to be a permanent shift in cyber security spending patterns. As a direct result of hybrid work, CISOs have invested heavily in endpoint security, identity access management, cloud security and further hardening the network beyond the HQ. Moreover, the need to build security into applications from the start, rather than bolting protection on as an afterthought, has led to vastly heightened awareness around DevSecOps. Finally, attacking security as a data problem with automation and AI is fueling new innovations in cyber products and services; and is spawning well-funded, disruptive startups. In this Breaking Analysis we present our quarterly findings on the security sector. We’ll share the latest ETR survey data, identify the companies with customer spending momentum and share some of the market movers. Watch the full video analysis. Episode 118 – The Improbable Rise of Kubernetes – The rise of Kubernetes came about through a combination of forces that were in hindsight, quite a long shot. AWS’ dominance created momentum for cloud native application development and the need for simpler experiences beyond easily spinning up compute as a service. This wave crashed into innovations from a startup named Docker and a reluctant open source benefactor in Google that needed a way to change the game on Amazon in the cloud. Factor in Red Hat, which needed a path beyond Linux and was just about to opt for an alternative to Kubernetes to power OpenShift. Finally, figure out a governance structure to herd all the cats in the ecosystem so you can win out over other competition and create a de facto standard. Make all that happen and you get the remarkable ascendancy of Kubernetes. Such was the story unveiled recently in a new two-part documentary series from Honeypot simply titled “Kubernetes.” In this Breaking Analysis we tap the back stories of this documentary, which explains the improbable events leading to the creation of Kubernetes. We’ll share commentary from early Kubernetes committers and key players who came on theCUBE to piece together how it all happened. Finally, we’ll present new survey data from ETR on containers. Watch the full video analysis. Episode 117 – What to Expect in Cloud 2022 & Beyond – We’ve often said that the next ten years in cloud computing won’t be like the last ten. Cloud has firmly planted its footprint on the other side of the chasm with the momentum of the entire multi-trillion dollar technology business behind it. Both sellers and buyers are leaning in by adopting cloud technologies and many are building their own value layers on top of cloud. In the coming years we expect innovation will continue to coalesce around the big 3 U.S. clouds, plus Alibaba in APAC, with the ecosystem building value on top of the hardware, software and tools provided by the hyperscalers. Importantly, we don’t see this as a race to the bottom. Rather our expectation is that the large public cloud players will continue to take cost out of their platforms through innovation, automation and integration. Other cloud providers and the ecosystem, including traditional IT buyers, will leverage hyperscale clouds and mine opportunities in their respective markets. This is not a zero sum game. In this Breaking Analysis we’ll update you on our latest projections in the cloud market, share some new ETR survey data with some surprising nuggets; and drill into the important cloud database landscape. Watch the full video analysis. Episode 116 – Securing Snowflake – The amount of data ingested into a data warehouse overwhelmed the system. Every time Intel came out with a new microprocessor, practitioners would “chase the chips” in an effort to try and compress the overly restrictive elapsed time to insights. This cycle repeated itself for decades. Cloud data warehouses generally and Snowflake specifically changed all this. Not only were resources virtually infinite, but the ability to separate compute from storage permanently altered the cost, performance, scale and value equation. But as data makes its way into the cloud and is increasingly democratized as a shared resource across clouds – and at the edge – practitioners must bring a SecDevOps mindset to securing their cloud data warehouses. This Breaking Analysis takes a closer look at the fundamentals of securing Snowflake. An important topic as data becomes more accessible and available to a growing ecosystem of users, customers and partners. To do so we welcome two guests to this episode. Ben Herzberg is an experienced hacker, developer and an expert in several aspects of data security. Yoav Cohen is a technology visionary and currently serving as CTO at Satori Cyber. Watch the full video analysis. Episode 115 – Enterprise Technology Predictions 2022 – The pandemic has changed the way we think about, and predict the future. As we enter the third year of COVID, we see the significant impact it’s had on technology strategies, spending patterns and company fortunes. Much has changed and while many of these changes were forced reactions to a new abnormal, the trends we’ve seen over the past twenty-four months have become more entrenched and point the way to what’s ahead in the technology business. In this Breaking Analysis we welcome our data partner and colleague Erik Porter Bradley from ETR and we put forth our annual predictions for enterprise technology in 2022 and beyond. We’ll do our best to backup our predictions specific supporting data and more granular detail that can be measured as accurate or not. Please refer to the grading of our 2021 predictions to judge for yourself how we did last year. Watch the full video analysis. Episode 114 – Cyber, Blockchain & NFTs Meet the Metaverse – When Facebook changed its name to Meta last fall it catalyzed a chain reaction throughout the tech industry. Software firms, gaming companies, chip makers, device manufacturers and others have joined in the hype machine. It’s easy to dismiss the metaverse as futuristic hyperbole, but do we really believe that tapping on a smartphone, staring at a screen or two dimensional Zoom meetings are the future of how we work, play and communicate? As the Internet itself proved to be larger than we ever imagined, it’s very possible that the combination of massive processing power, cheap storage, AI, blockchains, crypto, sensors, AR/VR, brain interfaces and other emerging technologies will combine to create new and unimaginable consumer experiences; and massive wealth for creators of the metaverse. In this Breaking Analysis we explore the intersection of cybersecurity, blockchain, crypto currency, NFTs and the emerging metaverse. To do so we welcome in cyber expert, hacker, gamer, NFT expert and founder of Ore System, Nick Donarski. Watch the full video analysis. Episode 113 – Analyst Predictions 2022: The Future of Data Management – In the 2010’s, organizations became keenly aware that data would become the critical ingredient in driving competitive advantage, differentiation and growth. But to this day, putting data to work remains a difficult challenge for many if not most organizations. As the cloud matures it has become a game changer for data practitioners by making cheap storage and massive processing power readily accessible. We’ve also seen better tooling in the form of data workflows, streaming, machine intelligence/AI, developer tools, security, observability, automation, new databases and the like. These innovations accelerate data proficiency but at the same time add complexity for practitioners. Data lakes, data hubs, data warehouses, data marts, data fabrics, data meshes, data catalogs and data oceans are forming, evolving and exploding onto the scene. In an effort to bring perspective to this sea of optionality, we’ve brought together some of the brightest minds in the data analyst community to discuss how data management is morphing and what practitioners should expect in 2022 and beyond. Watch the full video analysis. Episode 112 – Grading our 2021 Predictions – Predictions are all the rage this time of year. On December 29th, 2020, in collaboration with Erik Porter Bradley of Enterprise Technology Research, we put forth our predictions for 2021. The focus of our prognostications included tech spending, remote work, productivity apps, cyber, IPOs, SPACs, M&A, data architecture, cloud, hybrid cloud, multi-cloud, AI, containers, automation and semiconductors. We covered a lot of ground! In this Breaking Analysis, as a warmup for our 2022 predictions post, we’ll review each of our predictions for this past year and grade the accuracy of our forecasts Watch the full video analysis. Episode 111 – Why Oracle’s Stock is Surging to an All time High – On Friday, Oracle announced a meaningful earnings beat and strong forward guidance on the strength of its license business; and slightly better than expected cloud performance. The stock rose sharply on the day and closed up nearly 16% surpassing $280B in market value. Oracle’s success is due largely to its execution of a highly differentiated strategy that has evolved over the past decade or more; deeply integrating its hardware and software, heavily investing in next generation cloud, creating a homogenous experience across its application portfolio and becoming the number one platform for the world’s most mission critical applications. While investors piled into the stock, skeptics will point to the beat being tilted toward license revenue and investors will likely keep one finger on the sell button until they’re convinced Oracle’s cloud momentum is more consistent and predictable. In this Breaking Analysis we’ll review Oracle’s most recent quarter and pull in some ETR survey data to frame the company’s cloud business, the momentum of Fusion ERP, where the company is winning and some gaps/opportunities we see that can be addressed in the coming quarters. Watch the full video analysis. Episode 110 – Rise of the Supercloud – Last week’s AWS re:Invent underscored the degree to which cloud computing generally and AWS specifically have impacted the technology landscape. From making infrastructure deployment simpler, to accelerating the pace of innovation, to the formation of the world’s most active and vibrant technology ecosystem; it’s clear that AWS has been the number one force for industry change in the last decade. Going forward we see three high level contributors from AWS that will drive the next 10 years of innovation, including: 1) the degree to which data will play a defining role in determining winners and losers; 2) the knowledge assimilation effect of AWS’ cultural processes such as two pizza teams, customer obsession and working backwards; and 3) the rise of superclouds– that is clouds built on top of hyperscale infrastructure that focus not only on IT transformation, but deeper business integration and digital transformation of entire industries. In this Breaking Analysis we’ll review some of the takeaways from the 10th annual AWS re:Invent conference and focus on how we see the rise of superclouds impacting the future of virtually all industries. Watch the full video analysis. Episode 109 – Break up Amazon? Survey Suggests it May Not be Necessary – Despite the posture that big tech generally and Amazo.com Inc.specifically should be regulated and/or broken apart, recent survey research suggests that Amazon faces many disruption challenges, independent of any government intervention. Specifically, respondents to our survey believe that history will repeat itself in that there’s a 60% probability that Amazon will be disrupted by market forces, including self-inflicted wounds. Amazon faces at least seven significant disruption scenarios of varying likelihood and impact, perhaps leading to the conclusion that the government should let the market adjudicate Amazon’s ultimate destiny. In this Breaking Analysis, and ahead of AWS re:Invent, we share the results of our survey designed to asses what, if anything, could disrupt Amazon. We’ll also show you some data from ETR that indicates the strong momentum of AWS is likely to continue, which could be a factor in any government intervention. Watch the full video analysis. Episode 108 – AWS & Azure Accelerate Cloud Momentum – Despite all the talk about repatriation, hybrid and multi-cloud opportunities and cloud as an increasingly expensive option for customers…the data continues to show the importance of public cloud to the digital economy. Moreover, the two leaders, AWS and Azure are showing signs of accelerated momentum that point to those two giants pulling away from the pack in the years ahead. Each of these companies is demonstrating broad-based momentum across their respective product lines. It’s unclear if anything other than government intervention or self-inflicted wounds will slow these two companies down this decade. Despite the commanding lead of the two leaders, a winning strategy for companies that don’t run their own cloud continues to be innovating on top of their massive CAPEX investments. The most notable example of this approach in our view continues to be Snowflake. In this Breaking Analysis, Dave will provide our quarterly market share update of the big four hyperscale cloud providers. We’ll share some new data from ETR based on the most recent survey, drill into some of the reasons for the momentum of AWS and Azure; and drill further into the database and data warehouse sector to see what if anything has changed in that space. Watch the full video analysis. Episode 107 – Cutting Through the Noise of Full Stack Observability – Full stack observability is the new buzz phrase. As businesses go digital, customer experience becomes ever more important. Why? Because fickle consumers can switch brands in the blink of an eye – or the click of a mouse. Every vendor wants a piece of the action in this market including companies that have provided traditional monitoring, log analytics, application performance management, and related services. These companies are joined by a slew of new entrants claiming end-to-end visibility across the so-called “modern tech stack.” Recent survey research however confirms our thesis that no one company has it all. New entrants have a vision and are not encumbered by legacy technical debt. However their offerings are immature. Established players with deep feature sets in one segment are pivoting through M&A and organic development to fill gaps. Meanwhile, cloud players are gaining traction and participating through a combination of native tooling combined with strong ecosystems to address this opportunity. In this Breaking Analysis we dive into a recent ETR drill down study on full stack observability. And to do so we once again welcome in our colleague Erik Bradley, Chief Engagement Strategist at ETR. Watch the full video analysis. Episode 106 – What Could Disrupt Amazon? – Five publicly traded, US-based companies have market valuations over or just near one trillion dollars. As of Oct. 29th, Apple and Microsoft top the list, each at $2.5T, followed by Alphabet at $2T, Amazon at $1.7T and Facebook (now Meta) at just under $1T – off from its high of $1.1T prior to its recent troubles. These companies have reached extraordinary levels of success and power. What, if anything could disrupt their market dominance? In his book Seeing Digital, Author David Moschella made three key points that are relevant: In the technology industry, disruptions are the norm – The waves of mainframes, Minis, PCs, Mobile and the Internet all saw new companies emerge and power structures that dwarfed previous eras of innovation. Is that dynamic changing? Every industry has a disruption scenario. Silicon Valley – broadly defined to include Seattle, or at least Amazon – has a dual disruption agenda. The first being horizontally targeting the technology industry and the second as digital disruptors in virtually any industry. How relevant is that to the future power structure of the technology business? In this Breaking Analysis we welcome in author, speaker, researcher, thought leader and senior fellow at ITIF, David Moschella to assess what could possibly disrupt today’s trillionaire companies. And we’ll start with Amazon. Watch the full video analysis. Episode 105 – Data Mesh…A New Paradigm for Data Management – Data mesh is a new way of thinking about how to use data to create organizational value. Leading edge practitioners are beginning to implement data mesh in earnest. Importantly, data mesh is not a single tool or a rigid reference architecture. Rather it’s an architectural and organizational model that is designed to address the shortcomings of decades of data challenges and failures. As importantly, it’s a new way to think about how to leverage and share data at scale across an organization and ecosystems. Data mesh in our view will become the defining paradigm for the next generation of data excellence. In this Breaking Analysis we welcome the founder and creator of data mesh, author, thought leader, technologist Zhamak Dehghani, who will help us better understand some of core principles of data mesh and the future of decentralized data management. With practical advice for data pros who want to create the next generation of data-driven organizations. Watch the full video analysis. Episode 104 – The Hybrid Cloud Tug of War Gets Real – It looks like Hybrid cloud is finally here. We’ve seen a decade of posturing, marketecture, slideware and narrow examples but there’s little question that the definition of cloud is expanding to include on-premises workloads in hybrid models. Depending on which numbers you choose to represent IT spending, public cloud accounts for less than 5% of the total pie. As such there’s a huge opportunity in hybrid, outside of the pure public cloud; and everyone wants a piece of the action. The big question is how will this now evolve? Customers want control, governance, security, flexibility and a feature-rich set of services to build their digital businesses. It’s unlikely they can buy all that – so they’re going to have to build it with partners. Specifically vendors, SIs, consultancies, and their own developers. The tug-of-war to win the new cloud day has finally started in earnest – between the hyperscalers and the largest enterprise tech companies in the world. Watch the full video analysis. Episode 103 – The Future of the Semiconductor Industry – Semiconductors are at the heart of technology innovation. For decades, technology improvements have marched to the cadence of silicon advancements in performance, cost, power and packaging. In the past ten years, the dynamics of the semiconductor industry have changed dramatically. Soaring factory costs, device volume explosions, fabless chip companies, greater programmability, compressed time to tape out, more software content, the looming Chinese presence…these and other factors have permanently changed the power structure of the semiconductor business. We rely on chips for every aspect of our lives, which has led to a global semiconductor shortage that has impacted more industries than we’ve ever seen. Our premise is that silicon success in the next twenty years will be determined by volume manufacturing expertise, design innovation, public policy, geopolitical dynamics, visionary leadership and innovative business models that can survive the intense competition in one of the most challenging businesses in the world. Watch the full video analysis. Episode 102 – UiPath Fast Forward to Enterprise Automation | UiPath FORWARD IV – UiPath has always been an unconventional company. It started with humble beginnings as essentially a software development shop. It then caught lightning in a bottle with its computer vision technology and simplification mantra…creating easy to deploy software robots for bespoke departments to automate mundane tasks. The story is well known…the company grew rapidly and was able to go public earlier this year. Consistent with its out of the ordinary approach, while other firms are shutting down travel and physical events, UiPath is moving ahead with Forward IV, its annual user conference next week…with a live audience at the Bellagio in Las Vegas. It’s also “Fast Forwarding” as a company, determined to lead the charge beyond RPA point tools and execute on a more all-encompassing enterprise automation agenda. Watch the full video analysis. Episode 101 – CIOs Signal Hybrid Work Will Power Spending Through 2022 – Throughout the pre-vaccine COVID era, IT buyers indicated budget constraints would squeeze 2020 spending by roughly 5% relative to 2019 levels. But the forced March to digital combined with increased cyber threats for remote workers, created a modernization mandate that powered Q4 spending last year. This momentum has carried through to 2021. While COVID variants have delayed return to work and business travel plans, our current forecast for global IT spending remains strong at 6-7%, slightly down from previous estimates. But the real story is CIOs and IT buyers expect a 7-8% increase in 2022 spending, reflecting investments in hybrid work strategies and a continued belief that technology remains the underpinning of competitive advantage in the coming decade. In this Breaking Analysis, Dave will share the latest results of ETR’s macro spending survey and update you on industry and sector investment patterns. Watch the full video analysis. Following is the complete collection to date, crystallizing the key topics of the past year or so. We hope you enjoy these episodes and, as always, welcome your feedback. Episode 100 – How Cisco can win cloud’s ‘Game of Thrones’ – Cisco is a company at the crossroads. It is transitioning from a high margin hardware business to a software subscription-based model through both organic moves and targeted acquisitions. It’s doing so in the context of massive macro shifts to digital and the cloud. We believe Cisco’s dominant position in networking, combined with a large market opportunity and a strong track record of earning customer trust, put the company in a good position to capitalize on cloud momentum. But there are clear challenges ahead, not the least of which is the growing complexity of Cisco’s portfolio, transitioning a large legacy business and the mandate to maintain its higher profitability profile as it moves to a new business model. In this Breaking Analysis, we welcome in Zeus Kerravala, Founder and Principal Analyst at ZK Research and long time Cisco watcher who collaborated with us to craft the premise of this session. Watch the full video analysis. Episode 99 – The Case for Buy the Dip on Coupa, Snowflake & Zscaler – Buy the dip has been an effective strategy since the market bottomed in early March last year. The approach has been especially successful in tech and even more so for those tech names that: 1) were well-positioned for the forced march to digital – i.e. remote work, online commerce, data-centric platforms and certain cybersecurity plays; and 2) already had the cloud figured out. The question on investors’ minds is where to go from here. Should you avoid some of the high flyers that are richly valued with eye-popping multiples? Or should you continue to buy the dip? And if so, which companies that capitalized on the trends from last year will see permanent shifts in spending patterns that make them a solid long term play. In this Breaking Analysis we shine the spotlight on three companies that may be candidates for a buy the dip strategy over the next 3-5 years. To do so it’s our pleasure to welcome Ivana Delevska, the Chief Investment Officer and founder of SPEAR Alpha, a new, research-centric ETF focused on industrial technology. Watch the full video analysis. Episode 98 – Thinking Outside the Box…AWS Signals a New Era for Storage – By our estimates, AWS will generate around $9B in storage revenue this year and is now the second largest supplier of enterprise storage behind Dell. We believe AWS storage revenue will surpass $11B in 2022 and continue to outpace on-prem storage growth by more than 1,000 basis points for the next three to four years. At its third annual Storage Day event, AWS signaled a continued drive to think differently about data storage and transform the way customers migrate, manage and add value to their data over the next decade. In this Breaking Analysis Dave will give a brief overview of what we learned at AWS’ Storage Day, share our assessment of the big announcement of the day – a deal with NetApp to run the full ONTAP stack natively in the cloud as a managed service – and share some new data on how we see the market evolving. Watch the full video analysis. Episode 97 – Tech Earnings Signal a Continued Booming Market – Tech earnings reports from key enterprise software and infrastructure players this week, underscore that IT spending remains robust in the post isolation economy. This is especially true for those companies that have figured out a coherent and compelling cloud strategy. Despite COVID variant uncertainties and hardware component shortages, most leading tech names outperformed expectations. That said, investors weren’t in the mood to reward all stocks and any variability in product mix, earnings outlook or bookings/billings nuances were met with a tepid response from the street. In this Breaking Analysis Dave will provide our commentary and new data points on key technology companies including Snowflake, Salesforce, Workday, Splunk, Elastic, Palo Alto Networks, VMware, Dell, Pure Storage, HP Inc. and NetApp. Watch the full video analysis. Episode 96 – Can anyone tame the identity access beast? Okta aims to try… – Chief information security officers cite trust as the most important value attribute they can deliver to their organizations. And when it comes to security, identity is the new attack surface. As such, identity and access management continue to be the top priority among technology decision makers. It also happens to be one of the most challenging and complicated areas of the cyber security landscape. Okta, a leader in the identity space, has announced its intent to converge privilege access and identity governance in an effort to simplify the landscape and reimagine identity. Our research shows that interest in this type of consolidation is high, but organizations believe technical debt, compatibility issues, expense and lack of talent are barriers to reaching cyber nirvana with their evolving zero trust networks. In this Breaking Analysis, Dave will explore the complex and evolving world of identity access and privileged account management. With an assessment of Okta’s market expansion aspirations and fresh data from ETR and input from Erik Bradley. Watch the full video analysis. Episode 95 – Rethinking Data Protection in the 2020s – Techniques to protect sensitive data have evolved over thousands of years, literally. The pace of modern data protection is rapidly accelerating and presents both opportunities and threats for organizations. In particular, the amount of data stored in the cloud, combined with hybrid work models, the clear and present threat of cyber crime, regulatory edicts and ever-expanding edge use cases should put CxOs on notice that the time is now to rethink your data protection strategies. In this Breaking Analysis, Dave is going to explore the evolving world of data protection and share some data on how we see the market evolving and the competitive landscape for some of the top players. Watch the full video analysis. Episode 94 – Cyber, Cloud, Hybrid Work & Data Drive 8% IT Spending Growth in 2021 – Every CEO is figuring out the right balance for new hybrid business models. Regardless of the chosen approach, which will vary, technology executives understand they must accelerate digital and build resilience as well as optionality into their platforms. This is driving a dramatic shift in IT investments at the macro level as we expect total spending to increase at 8% in 2021, compared to last year’s contraction. Investments in cyber security, cloud, collaboration to enable hybrid work and data, including analytics, AI and automation are the top spending priorities for CxOs. In this Breaking Analysis Dave welcomes back Erik Bradley, Chief Engagement Strategist at our partner ETR. In this post we’ll share some takeaways from ETR’s latest survey and provide our commentary on what it means for markets, sellers and buyers. We’ll also explain what we think Wall Street is missing about Amazon’s latest earnings. Watch the full video analysis. Episode 93 – ServiceNow’s Collision Course with Salesforce.com – ServiceNow is a company that investors love to love. But there’s caution in the investor community right now as confusion about transitory inflation and higher interest rates looms. ServiceNow also suffers from perfection syndrome and elevated expectations. In this Breaking Analysis Dave will dig into ServiceNow, one of the companies we began following almost ten years ago, and provide some thoughts on ServiceNow’s march to $15B by 2026. Watch the full video analysis. Episode 92 – Survey Data Shows no Slowdown in AWS & Cloud Momentum – Despite all the chatter about cloud repatriation and the exorbitant cost of cloud computing, customer spending momentum continues to accelerate in the post isolation economy. If the pandemic was good for the cloud it seems that the benefits of cloud migration remain lasting in the late stages of COVID. And we believe this stickiness will continue. In this Breaking Analysis Dave will share some fresh July survey data that indicates accelerating momentum for the largest cloud computing firms. Watch the full video analysis. Episode 91 – How JPMC is Implementing a Data Mesh Architecture on the AWS Cloud – A new era of data is upon us. The technology industry generally and the data business specifically are in a state of transition. Even our language reflects that. For example, we rarely use the phrase “Big Data” anymore. Rather we talk about digital transformation or data-driven companies. In this Breaking Analysis we want to share our assessment of the state of the data business. We’ll do so by looking at the data mesh concept and how a division of a leading financial institution, JPMC, is practically applying these relatively new ideas to transform its data architecture for the next decade. Watch the full video analysis. Episode 90 – Mobile World Congress Highlights Telco Transformation – AMobile World Congress is on for 2021. theCUBE will be there and we’ll let you know if it’s alive and well. As we approach a delayed MWC it’s appropriate to reflect on the state of the telecoms industry. Let’s face it – the telcos have done a great job of keeping us all connected during the pandemic. In this Breaking Analysis we welcome a long time telecoms industry analyst and the Founding Director of Lewis Insight, Mr. Chris Lewis. Watch the full video analysis. Episode 89 – How AWS is Revolutionizing Systems Architecture – AWS is pointing the way to a revolution in system architecture. Much in the same way that AWS defined the cloud operating model last decade, we believe it is once again leading in future systems. In this Breaking Analysis we’ll dig into the moves that AWS has been making, explain how they got here, why we think this is transformational for the industry and what this means for customers, partners and AWS’ many competitors. Watch the full video analysis. Episode 88 – Learnings from the hottest startups in cyber & IT infrastructure – As you well know by now, the cloud is about shifting IT labor to more strategic initiatives. Or as Andy Jassy posited at the first AWS re:Invent conference in 2012, removing the undifferentiated heavy lifting associated with deploying and managing IT infrastructure. In this Breaking Analysis Dave is pleased to welcome a special guest, Erik Suppiger, author of the Elite 80 – a report that details the hottest privately held cybersecurity and IT infrastructure companies in the world. Erik is a senior analyst at JMP Securities and will share insights from this report. Watch the full video analysis. Episode 87 – Chasing Snowflake in Database Boomtown – Database is the heart of enterprise computing. The market is both growing rapidly and evolving. Major forces transforming the space include cloud and data – of course – but also new workloads, advanced memory and IO capabilities, new processor types, a massive push toward simplicity, new data sharing and governance models; and a spate of venture investment. In this Breaking Analysis Dave will share our most current thinking on the database marketplace and dig into Snowflake’s execution, some of its challenges and we’ll take a look at how others are making moves to solve customer challenges; and angling to get their piece of the growing database pie. Watch the full video analysis. Episode 86 – How Nvidia plans to own the data center with AI – Nvidia wants to completely transform enterprise computing by making datacenters run 10X faster at 1/10th the cost. In this Breaking Analysis Dave will explain why we believe Nvidia is in a strong position to power the world’s computing centers and how it plans to disrupt the grip that x86 architectures have had on the datacenter market for decades. Watch the full video analysis. Episode 85 – Your Online Assets Aren’t Safe – Is Cloud the Problem or the Solution? – The convenience of online access to bank accounts, payment apps, crypto exchanges and other transaction systems has created enormous risks, which the vast majority of individuals either choose to ignore or simply don’t understand. In this Breaking Analysis Dave will try to raise awareness about a growing threat to your liquid assets and hopefully inspire you to do some research and take actions to lower the probability of you losing thousands, hundreds of thousands or millions of dollars. Watch the full video analysis. Episode 84 – Debunking the Cloud Repatriation Myth – Cloud repatriation is a term often used by technology companies that don’t operate a public cloud. The marketing narrative most typically implies that customers have moved work to the public cloud and, for a variety of reasons – expense, performance, security, etc. Some have written about the repatriation myth, but in this Breaking Analysis, Dave will share hard data from ETR and other sources that we feel debunks the repatriation narrative as it’s currently being promoted. Watch the full video analysis. Episode 83 – Chaos Creates Cash for Criminals & Cyber Companies – The pandemic not only accelerated a shift to digital, it highlighted a rush of cyber criminal sophistication, collaboration and chaotic responses from virtually every major company on the planet. The SolarWinds hack exposed digital supply chain weaknesses and appears to have accelerated so-called island hopping techniques that are exceedingly difficult to detect. In this Breaking Analysis Dave will provide our quarterly update on the security industry and share new survey data from ETR and theCUBE community that will help you navigate through the maze of corporate cyber warfare. Watch the full video analysis. Episode 82 – Why Apple Could be the Key to Intel’s Future – The latest Arm Neoverse announcement further cements our opinion that its architecture, business model and ecosystem execution are defining a new era of computing; and leaving Intel in its dust. In this Breaking Analysis, Dave will explain why and how Apple could hold a key to saving Intel’s (and America’s) semiconductor industry leadership position. Watch the full video analysis. Episode 81 – A Digital Skills Gap Signals Rebound in IT Services Spend – Recent survey data from ETR shows that enterprise tech spending is tracking with projected U.S. GDP growth at 6% to 7% this year. Many markers continue to point the way to a strong recovery including hiring trends and the loosening of frozen IT project budgets. In this Breaking Analysis Dave welcomes back Erik Bradley, Chief Engagement Strategist at ETR, who will share fresh data, perspectives and insights from the latest survey data. Watch the full video analysis. Episode 80 – UiPath’s Unconventional $PATH to IPO – UiPath is going public this coming week and will be the next hot software company to IPO. It has had a long strange trip to IPO. In this week’s Breaking Analysis, Dave shares our learnings from sifting through hundreds of pages of UiPath’s S1 and convey our thoughts on its market, competitive position and outlook Watch the full video analysis. Episode 79 – Moore’s Law is Accelerating and AI is Ready to Explode – Moore’s Law is dead right? Think again. While the historical annual CPU performance improvement of ~40% is slowing, the combination of CPUs packaged with alternative processors is improving at a rate of more than 100% per annum. In this Breaking Analysis Dave is going to unveil some data that suggests we’re entering a new era of innovation where inexpensive processing capabilities will power an explosion of machine intelligence applications. Watch the full video analysis. Episode 78 – Arm Lays Down the Gauntlet at Intel’s Feet – Exactly one week after Pat Gelsinger unveiled plans to reinvent Intel, Arm announced version 9 of its architecture and put forth its vision for the next decade. In this Breaking Analysis Dave will explain why we think this announcement is so important and what it means for Intel and the broader technology landscape. Watch the full video analysis. Episode 77 – Intel… Too Strategic to Fail – Intel’s big announcement this week underscores the threat that the United States faces from China. The U.S. needs to lead in semiconductor design and manufacturing; and that lead is slipping because Intel has been fumbling the ball over the past several years. In this Breaking Analysis Dave will peel the onion of Intel’s announcement, explain why we’re not as sanguine as was Wall Street on Intel’s prospects and lay out what we think needs to take place for Intel to once again become top gun; and for us to gain more confidence. Watch the full video analysis. Episode 76 – Tech Spending Powers the Roaring 2020s as Cloud Remains a Staple of Growth – In the year 2020, it was good to be in tech. It was even better to be in the cloud as organizations had to rely on remote cloud services to keep things running. In this Breaking analysis Dave will provide our take on the latest ETR COVID survey and share why we think the tech boom will continue well into the future. Watch the full video analysis. Episode 75 – Breaking Analysis: Unpacking Oracle’s Autonomous Data Warehouse Announcement – On February 19th of this year, Barron’s dropped an article declaring Oracle a cloud giant and explained why the stock was a buy. Investors took notice and the stock ran up 18% over the next 9 trading days and peaked on March 9th, the day before the company announced its latest earnings. The company beat consensus earnings on both top line and EPS last quarter. But Investors didn’t like Oracle’s tepid guidance and the stock pulled back..but is still well above its pre-Barron’s article price. Watch the full video analysis. Episode 74 – Breaking Analysis: NFTs, Crypto Madness & Enterprise Blockchain – When a piece of digital art sells for $69.3M, more than has ever been paid for works by Paul Gauguin or Salvador Dali, making its creator the third most expensive living artist in the world, one can’t help but take notice and ask: “What is going on?” The latest craze around NFTs may feel a bit “bubblicious,” but it’s yet another sign that the digital age is now fully upon us. In this Breaking Analysis Dave wants to take a look at some of the trends that may have observers and investors scratching their heads, but we think still offer insight to the future — and possibly some opportunities for young investors. And we’ll briefly touch on how these trends may relate to enterprise tech. Watch the full video analysis. Episode 73 – Breaking Analysis: Satya Nadella Lays out a Vision for Microsoft at Ignite 2021 – Microsoft CEO Satya Nadella sees a different future for cloud computing over the coming decade. In this Breaking Analysis Dave Vellante will review the highlights of Nadella’s Ignite keynote, share our thoughts on what it means for the future of cloud specifically and tech generally. Watch the full video analysis. Episode 72 – Breaking Analysis: SaaS Attack, On Prem Survival & What’s a Cloud Company Look Like SaaS companies have been some of the strongest performers during this COVID era. In this Breaking Analysis, Dave Vellante picks out a few of the more recent themes from this month and share our thoughts on some major enterprise software players, the future of on-prem and a review of our take on cloud, what cloud will look like in the 2020s. Watch the full video analysis. Episode 71 – Breaking Analysis: RPA Remains on a Hot Streak as UiPath Blazes the Trail – UiPath’s recent $750M raise at a $35B valuation underscores investor enthusiasm for robotic process automation. In this Breaking Analysis Dave Vellante explores the current trends in the RPA market and try to address the question– is UiPath’s value supported by the ETR spending data, how will the RPA market evolve from a total available market (TAM) perspective and where do some of the other players like Automation Anywhere, Pegasystems and Blue Prism fit? Watch the full video analysis. Episode 70 – Breaking Analysis: How the SolarWinds Hack & COVID are Changing CISO Spending Patterns – The SolarWinds hack along with the pandemic are the two most visible catalysts for change in cybersecurity spending patterns. In addition to securing a more distributed workforce, CISOs have to now worry about protecting against the very software updates and patches designed to keep them safe against cyber attacks. In this Breaking Analysis, Dave Vellante shares data from a recent CISO roundtable hosted by ETR’s Erik Bradley and provides updates on the cybersecurity sector overall. Watch the full video analysis. Episode 69 – Breaking Analysis: Big 4 Cloud Revenue Poised to Surpass $115B in 2021 –There are four players in the IaaS/PaaS hyperscale cloud services space which have the ability to outperform all competitors. Combined in 2021, they will generate more than $115 billion dollars in revenue. In this Breaking Analysis, Dave Vellante initiates coverage of Alibaba, one of the Big Four in this massive market segment. Watch the full video analysis. Episode 68 – Breaking Analysis: Tech Spending Roars Back in 2021 –There is an expected six to seven percent increase in 2021 technology spending following the five percent decline over the past year. Many factors are contributing to this growth, and in this Breaking Analysis, Dave Vellante shares some of those reasons as well as the latest macro view of the market. Watch the full video analysis. Episode 67 – Breaking Analysis: Best of theCUBE on Cloud –The coming decade of cloud will be dramatically different from the last. There will be a shift toward a more data centric, hyper decentralized cloud that is far more complex than anything seen previously. In this Breaking Analysis, Dave Vellante summarizes exclusive content gathered from the recent theCUBE on cloud event. Watch the full video analysis. Episode 66 – Breaking Analysis: Pat Gelsinger Must Channel Andy Grove and Recreate Intel –Intel is fighting a war on two fronts: 1) Arm volumes have far surpassed those of Intel’s x86, conferring major cost advantages to leading fabs like TSMC and Samsung and 2) AMD continues to chip away at Intel’s dominance in its core markets. But the biggest challenge for incoming CEO Pat Gelsinger is perhaps to reinvent Intel by splitting manufacturing from design to make the company more agile and cost competitive. In this Breaking Analysis, Dave Vellante speculates about Intel’s future, and explains why Wikibon believes Intel has no choice but to shed its vertically integrated heritage. Episode 65 – Breaking Analysis: Breaking Analysis: 2021 Enterprise Technology Predictions –COVID-19 created a disruption in virtually all our expectations for 2020. In some regards, predictions for the past year played out very well, thanks to the pandemic. And in others, the complete opposite occurred. That being said, there is a lot to talk about heading into 2021. In this week’s Breaking Analysis, Dave Vellante is joined by Erik Bradley of ETR to share their top predictions for the upcoming year. Watch the full video analysis. Episode 64 – Breaking Analysis: Cloud Momentum & CIO Optimism Point to a 4% Rise in 2021 Tech Spending –Developments with COVID such as education, rapid vaccine rollout, productivity gains, and broad based cloud coverage suggest higher tech spending than previously forecasted for the upcoming year. Now, we can expect a 3-5 percent increase in 2021 spending. In this Breaking Analysis, Dave Vellante shares the data to support these predictions, and predicts which sectors are to gain momentum. Watch the full video analysis. Episode 63 – Breaking Analysis: Legacy Players Feel the Heat as AWS Storage Revenue Approaches $10B –Once an untapped bastion of innovation, storage in the data center now exists as a shell of what it used to be, and will remain as such. Specifically, AWS’ storage business is projected to hit between $6.5 – $7B this year and hit $10B within the next 18 – 24 months. In this Breaking Analysis, Dave Vellante lays out what this might mean for the industry, as well as the impact of AWS. Watch the full video analysis. Episode 62 – Breaking Analysis: Cloud 2030…From IT, to Business Transformation – Breaking Analysis: Cloud, Containers, AI & RPA Support Strong Rebound in 2021. Watch the full video analysis. Episode 61 – Breaking Analysis: Cloud 2030…From IT, to Business Transformation – Over the past decade, cloud computing has undoubtedly been the most pivotal force in IT. This brings the question as to what the next ten years will hold for cloud and the tech world. Perhaps, it will lay the foundations for a complete transformation of nearly every company worldwide. In this Breaking Analysis, as part of his coverage off AWS re:Invent 2020, Dave Vellante provides insights and predictions about the next breakthroughs in cloud. Watch the full video analysis. Episode 60 – Breaking Analysis: Sparked by COVID, CISOs see Permanent Shift in Cyber Strategies – CISOs report a forced shift to remote work has actually led to meaningful productivity improvements. This reality is causing security pros to rethink how they’ll approach security in the coming decade, informed by learnings during the pandemic. Watch the full video analysis. Episode 59 – Breaking Analysis: How Snowflake Plans to Change a Flawed Data Warehouse Model – Snowflake will not grow into its valuation by simply stealing share from the on-prem data warehouse vendors. Rather Snowflake must create an entirely new market based on completely changing the way organizations think about monetizing data. In this Breaking Analysis, Dave Vellante suggests a new data architecture that places domain knowledge at the core. Watch the full video analysis. Episode 58 – Breaking Analysis: Cloud Revenue Accelerates in the COVID Era – Over the past decade, cloud computing has undoubtedly been at the forefront of the innovation engine. The pandemic has accelerated the adoption of cloud and AI by at least two years, establishing a new era that will impact not only the technology industry, but all organizations. In this Breaking Analysis, Dave Vellante gives updates about the latest cloud market trends. Watch the full video analysis. Episode 57 – Breaking Analysis: Azure Cloud Powers microsoft’s Future – Big tech is once again under fire as CEOs of Facebook, Twitter, and Google face backlash from several US senators. Microsoft is not among these companies, as it relies on Azure cloud to build momentum, which now accounts for nineteen percent of its overall revenues. In this Breaking Analysis, Dave Vellante dives into the business of Microsoft and the data of its projected progress. Watch the full video analysis. Episode 56 – Breaking Analysis: Google’s Antitrust Play… Get Your Head out of Your Ads – The U.S. Department of Justice filed an antitrust lawsuit against Google, accusing the corporation of being a monopoly gatekeeper for the internet. In this Breaking Analysis, Dave Vellante shares data, covers the history of monopolistic power in the computer industry, and suggests future moves for Google to diversify its business. Watch the full video analysis. Episode 55 – Breaking Analysis: 2H 2020 Tech Spending: Headwinds into 2021 – Relative to 2019, tech spending has been hit hard, with a projected 5% decrease projected for 2020. Still, there seem to be bright spots within the market that show a slight increase in 2021 spending data. In this Breaking Analysis, Dave Vellante is joined by ETR’s Erik Bradley to provide the latest data supporting these trends. Watch the full video analysis. Episode 54 – CIOs Report Slow Thaw of Spending Freezes – Expect 2% Growth in 2021 – Recent Data provided by ETR suggests CIOs expect slight improvements in Q4 spending. Although these numbers are still down four percent from last year, this is a step in the right direction going into 2021. In this Breaking Analysis, Dave Vellante analyzes some of this data and provides his outlook for Q4 as well as the coming months. Watch the full video analysis. Episode 53 – Application Performance Management…From Tribal Knowledge to Digital Dashboard – Application Performance management has been around for a while, but it has had to evolve to accommodate for more complex operations, such as cloud-based systems. In this Breaking Analysis, Dave Vellante is teamed with Erik Bradley to offer the newest data to come out of the growing market. Watch the full video analysis. Episode 52 – Snowflake’s IPO… Here’s What’s Next – There is a lot of talk going on within the tech industry surrounding Snowflake’s recent IPO. In this week’s Breaking, Analysis, Dave Vellante shares his insights, investment strategies, and dives into some of the questions that have come out of the buzz from the hottest IPO in software history. Watch the full video analysis. Episode 51 – Market Recoil Puts Tech Investors at a Fork in the Road – Recently, the stock market experienced its most significant drop since early June. Tech companies were among several corporations involved in this decline, sending investors into a panic. In this Breaking Analysis, Dave Vellante answers questions and provides some perspective about what’s happening within the technology space and how it will continue to affect the rest of 2020. Watch the full video analysis. Episode 50 – Enterprise Software Download in the Summer of COVID – Enterprise applications are an enormous market, and organizations across the globe essentially rely on these applications to operate. In this Breaking Analysis, Dave Vellante unpacks new data surrounding the Enterprise software space, focusing on the core enterprise apps that companies rely on to keep their businesses running. Watch the full video analysis. Episode 49 – Tectonic Shifts Power Cloud, IAM and Endpoint Security – Although time has seemed to stop since the beginning of quarantine, COVID-19 has caused acceleration within the technology industry, causing some trends to speed up by about two years. The cybersecurity sector is one of the best examples of this change. In this Breaking Analysis, Dave Vellante discusses this sector, and provides updates on why key areas of the market that are exploding. Watch the full video analysis. Episode 48 – Cloud Remains Strong but not Immune to COVID – Although Cloud is among the most successful industries in tech spending, even it is not protected against COVID-19. Recent data shows a newly shaped recovery pattern suggesting this negative impact. In this Breaking Analysis, Dave Vellante dives into this data surrounding the cloud market and provides prospective updates about the big three. Watch the full video analysis. Episode 47 – RPA Competitors Eye Deeper Business Integration Agenda – Although the projected spending outlook for 2020 looks moderate, Robotic process automation solutions are still seeing the highest investment momentum for IT buyers. In this Breaking Analysis, Dave Vellante summarizes the latest RPA spending trends using data provided by ETR. Watch the full video analysis. Episode 46 – Five Questions Investors are Asking about Snowflake’s IPO – Snowflake recently filed a confidential document suggesting an IPO is imminent. Many within the community are responding positively to this news, causing a lot of discussion and inquiry. In this Breaking Analysis, Dave Vellante and Erik Bradley unpack five critical questions surrounding this pending IPO. Watch the full video analysis. Episode 45 – Google Cloud Rides the Wave but Remains a Distant Third Place – Despite its faster growth and infrastructure as a service, Google Cloud platform remains a third wheel behind AWS and Azure in the race for cloud dominance. In this Breaking Analysis, Dave Vellante reviews the current state of cloud and drills into the spending data to provide new insights about Google’s position in the market. Watch the full video analysis. Episode 44 – Living Digital: New Rules for Technology Events – Although there is push for a more digital world, in person interactions seem to be equally as important. Every year, large corporations throw massive events for this exact reason. However, coronavirus canceled most of these events for 2020, forcing a virtual replacement. In this Breaking Analysis, Dave Vellante covers the virtual event landscape and shares some takeaways from this new dynamic. Watch the full video analysis. Episode 43 – Assessing Dell’s Strategic Options with VMware – Dell is exploring options for its roughly 81 percent share in VMware. It is predicted that Dell wants to gauge investor, consumer, and partner sentiment. In this Breaking Analysis, Dave Vellante unpacks the complex angles as well as some possible scenarios of this situation. Watch the full video analysis. Episode 42 – Cyber Security Tailwinds in the Post Isolation Economy – The isolation economy has created substantial momentum for some cybersecurity companies. However, several others have tracked or not performed as well as more successful companies, despite still exhibiting strength and momentum. In this Breaking Analysis, Dave Vellante gives updates and answers questions about cybersecurity. Watch the full video analysis. Episode 41 – Competition Heats up for Cloud Analytic Databases – A new class of workloads are emerging in the cloud which are mainly focused on combining data using machine intelligence. At the center of this trend is a new class of data stores and analytic databases. In this Breaking Analysis, Dave Vellante updates his view on the subject while looking into the basics of the market, the competition, as well as spending data. Watch the full video analysis. Episode 40 – Most CIOs Expect a U Shaped COVID Recovery – It has been reported COVID-19 created a bifurcated IT spending picture, but what will the aftermath of the virus look like? In this Breaking Analysis, Dave Vellante is joined by Sagar Kadakia to look into the recovery patterns of different industries following the effects of the pandemic, and discuss the supporting data. Watch the full video analysis. Episode 39 – RPA Gains Momentum in the Post COVID Era – Legacy on-premises infrastructure are now allowing for more flexible approaches to business agility that reduce human labor, and the pandemic has accelerated this focus on such efforts. Robotic Process Automation has been a large beneficiary in this process. In this Breaking Analysis, Dave Vellante gives the rundown of RPA, including updates on the RPA sector, spending data, and the impact of COVID-19 on the market. Watch the full video analysis. Episode 38 – Cloud Momentum Building for the Post COVID Era – Cloud is in the stronghold of on-premise computing, and coronavirus has helped to strengthen this position. Analysis of company earnings reports and customer survey data shows that Microsoft Azure and GCP are closing the gap on AWS’ cloud dominance. In this Breaking Analysis, Dave Vellante takes a closer look at the big three cloud players, and provides a brief investigation of AWS individually. Watch the full video analysis. Episode 37 – IBM’s Future Rests on its Innovation Agenda – For decades, IBM has unfortunately missed opportunities to invest in the waves that now power the tech economy. The hiring of a new CEO provides the chance to redirect the company, and come back on top. In this Breaking Analysis, Dave Vellante digs into the past and the future of IBM. Watch the full video analysis. Episode 36 – COVID-19 Takeaways & Sector Drilldowns Part II – Industries such as retail, consumer, telco and IT services are seeing the largest pullbacks in spend from consumers and business since the beginning of COVID-19. On the other hand, corporations capable of digital transformation are seeing the most success. In this Breaking Analysis, Dave Vellante and Sagar Kadakia share the most updated spending data and information about the effects of the pandemic. Watch the full video analysis. Episode 35 – CIOs & CISOs Discuss COVID 19 Budget Impact – CEOs and CISOs of industries that have been hard-hit see significant and many permanent shifts to their IT and security strategies. Because of severe budget impacts, certain initiatives have been prioritized. In this Breaking Analysis, Dave Vellante is joined by Erik Bradley, managing director of ETR’S VENN program, to provide research and discuss the areas emphasized by these executives. Watch the full video analysis. Episode 34 – How Tech Execs are Responding to COVID 19 – COVID-19 demanded several and somewhat immediate changes within the tech industry. In this Breaking Analysis, Dave Vellante shares commentary and responses from various tech execs, recaps the current IT spending outlook, and dives into what’s really going on in the marketplace. Watch the full video analysis. Episode 33 – CIOs Plan on 4% Budget Declines for 2020 – At the start of 2020, the IT spend forecast was plus 4 percent. Following Coronavirus, those numbers declined significantly. In this breaking Analysis, Dave Vellante and Sagar Kadada breakdown the latest spending data from ETR. Watch the full video analysis. Episode 32 – VMware Announces vSphere 7 – VMware released the vSphere 7, which is being called the biggest change to vSphere within the last decade, enabling 90 percent of the data centers around the world that have VMware. In this Breaking Analysis, Dave Vellante is joined by Stu Miniman to discuss the vSphere 7 announcement. Watch the full video analysis. Episode 31 – Coronavirus – Pivoting From Physical to Digital Events – Coronavirus and the recent quarantine has put the world more or less on pause. For many industries, this means switching to a largely digital-based platform for events. In light of the pandemic, Dave Vellante shares advice on the best tools and practices to navigate the current crisis. Watch the full video analysis. Episode 30 – Multi-Cloud…A Symptom Or Cure? – The third wave of cloud is stirring up a lot of discussion about its necessity and effectiveness. In this breaking Analysis, Dave Vellante digs into the multicloud arena while answering some frequently asked questions about the benefits and possible implications of this new technology. Watch the full video analysis. Episode 29 – Cyber Security Update: What to Expect at RSA 2020 – Robert Gates, Former director of the CIA and Secretary of Defense, warns that the risks of Cyber security and IT should be a regular part of every board’s agenda. In this Breaking Analysis, Dave Vellante provides updates about the cyber security sector ahead of the RSA conference. Watch the full video analysis. Episode 28 – RPA: Over-Hyped or the Next Big Thing? – Robotic Process Automation, or RPA, is one of the hottest sectors in software today with a small but rapidly growing market. In this Breaking Analysis, Dave Vellante dives deeper into the world of RPA and talks about the value, size, and competitors in the market space. Watch the full video analysis. Episode 27 – Gearing up for Cloud 2020 – The new era of cloud brings significant change to the industry as well as new opportunities for the three major cloud players in the U.S. In this Breaking Analysis, Dave Vellante looks deeper into the cloud market and the momentum of Amazon, Google and Microsoft. Watch the full video analysis. Episode 26 – Storage…Continued Softness with Some Bright Spots – The storage industry is a bifurcated market: Secondary storage is gaining momentum while the primary slide falls behind. In this Breaking Analysis, Dave Vellante looks into the spending data and discusses his thoughts and predictions about storage live from Barcelona. Watch the full video analysis. Episode 25 – Cisco: Navigating Cloud, Software & Workforce Change – At the end of the dot com bubble, Cisco was the most valuable company in the world. It remains a leader in key segments, but is refocusing its business for the next decade. In this Breaking Analysis, Dave Vellante covers Cisco’s rise as well as projections for the future. Watch the full video analysis. Episode 24 – The Trillionaires Club: Powering the Tech Economy – Big tech companies have changed the recipe for innovation in the Enterprise, and as we enter the next decade, it is important to reevaluate how that will determine the level of success in the industry. In this Breaking Analysis, Dave Vellante discusses this “cocktail” of innovation and how it came into play. Watch the full video analysis. Episode 23 – Veeam’s $5B Exit: Clarity & Questions Around “Act II” – Veeam is a data protection company that has seen a slight performance drop since 2018. In this Breaking Analysis, Dave Vellante provides details about its $5 billion deal with Insight Partners and how this new chapter will affect the industry moving forward. Watch the full video analysis. Episode 22 – Predictions 2020: Cloud, Kubernetes & Cyber Continue to Power the Tech Economy – Tech projects have historically been very risky investments, but changes with cloud are allowing for more flexibility in the coming year. In this Breaking Analysis, Dave Vellante talks about Predictions for 2020 using spending data and insight from the thousands of interviews conducted on theCUBE. Watch the full video analysis. Episode 21 – Re:Invent 2019…of Transformation & NextGen Cloud – During the most recent AWS re:invent, the company proves it continues to strive on raising the bar. In this Breaking Analysis, Dave Vellante is joined by Stu Miniman to unpack the event, and talk about what’s happening from a buyer’s perspective, as well as AWS’ hybrid strategy Watch the full video analysis. Episode 20 – Unpacking Cisco’s Prospects Q4 2019 and Beyond – AWS strongly emphasizes the idea of transformation, and warns industries like Cisco not to do so incrementally. In this Breaking Analysis, Dave Vellante covers six different topics related to the future of Cisco, and its prospects in this era of next generation cloud Watch the full video analysis. Episode 19 – Examining IT Spending Data Q4 ‘19 – Enterprise Research Technology is a company who uses primary market research and first party data to look into spending patterns within the tech industry. In this Breaking Analysis, Dave Vellante explains the ins and outs of ETR, as well as its current relationship with theCUBE. Watch the full video analysis. Episode 18 – Re:Invent 2019: AWS Gears up for Cloud 2.0 – AWS Reinvent has become the Super Bowl for enterprise tech innovation. In this Breaking Analysis, Dave Vellante discusses the impact of the revolution of cloud on the industry in light of the upcoming event. Watch the full video analysis. Episode 17 – The Transformation of Dell Technologies – Dell continues to make changes to remain a predominant company in the tech industry. In this Breaking Analysis, Dave Vellante breaks down the major takeaways of the Dell Technologies’ industry analyst event and discusses some of the possible implications that the company may face. Watch the full video analysis. Episode 16 – The State of Cybersecurity Q4 2019 – The cyber security market is fragmented, challenging and many feel broken. Cloud security promises to simplify the maze for security practitioners but there are nuances with the shared responsibility model that often cause confusion. In this Breaking Analysis we look at the lates ETR data and hear from CISOs and executives in the field with an outlook and prognosis going forward. Watch the full video analysis. Episode 15 – The state of data protection, Q4 2019 – While demand for primary storage remains soft, the bright spot in the sector is data protection. Well funded new entrants are disrupting the space which is shaping up as a battleground for 2020. Watch the full video analysis. Episode 14 – AWS growth slows but remains the profit engine of Amazon – While AWS’ growth rate slowed this past quarter, its revenue is still substantially larger than its nearest competitors in the IaaS space. Moreover AWS is still the profit engine that funds Amazon’s vast and growing empire. Watch the full video analysis. Episode 13 – Q4 Spending Outlook – 10/18/19 – TheCUBE host Dave Vellante shares his analysis on recent spending trends backed by ETR Data. Spending is reverting to pre-2019 levels but the outlook still points to a strong 2020, barring any unforeseen global surprises. Watch the full video analysis. Episode 12 – Bill McDermott steps down from SAP – Commentary and Outlook. SAP pre-announced earnings with a beat and a raise, which acted as a heat shield for the surprise news that long-time CEO Bill McDermott is not renewing his contract. SAP is moving back to a dual-CEO model with a separate customer-facing and product/ops focus for each exec. SAP is strong financially but we believe faces significant technical integration challenges over the next decade, which may have played into McDermott’s and SAP’s decisions. In this Breaking Analysis, Dave Vellante shares recent spending data from ETR and lays out some of the challenges SAP faces going forward. Watch the full video analysis. Episode 11 – Spending in Q4 2019 is reverting back to pre-2018 levels – The spending outlook for the balance of 2019, into 2020 is softening, but not falling off a cliff. In this Breaking Analysis, Dave Vellante presents the latest ETR spending data and shares the latest thinking on which segments will continue to do well for the balance of 2019 into next year. Watch the full video analysis. Episode 10 – Takeaways from Dell’s 2019 financial analysts event. – Dell Technologies executives gathered in New York to update financial analysts and present the company’s mid-to-long term plans for growth, share gains, profitability and paying down its substantial debt. In this Breaking Analysis, Dave Vellante unpacks Dell’s massive business and provides clarity on the profitability levers Dell is turning to continue its transformation. Watch the full video analysis. Episode 9 – Spotlight on IBM’s Systems Business – IBM’s mainframe business continues to be the linchpin of much of the company’s profit and free cash flow. In this Breaking Analysis, Dave Vellante explains the importance of product cycles to the success of not only IBM’s Systems and Storage division, but IBM’s financial performance overall. Watch the full video analysis. Episode 8 – Nutanix and VMware battle for HCI leadership – Hyperconverged infrastructure was popularized by leader Nutanix. Many others have joined the party including VMware, Dell, HPE and others. In this Breaking Analysis, Dave Vellante is joined by Stuart Miniman, an expert in the HCI market, to unpack what’s really happening in the marketplace. Watch the full video analysis. Episode 7 – Oracle earnings analysis – September 2019 – Oracle, like many legacy enterprise software companies, is seeing a slowdown in growth for on-prem licenses. Oracle’s cloud is being re-factored in a next generation offering that comprises IaaS, PaaS and SaaS. Oracle’s applications business remains strong and is a driver of profits. In this Breaking Analysis, Dave Vellante digs into Oracle’s business and lays out his expectation for the coming quarters. Watch the full video analysis. Episode 6 – Spending data from ETR shows that robotic process automation is gaining steam in mid-to-large enterprises – A race to improve productivity is driving companies to implement automation in the form of software robots. UiPath and Automation Anywhere show strong customer spending momentum. Blue Prism and other major players, while not showing the same growth, appear to be well-positioned. In this Breaking Analysis, Dave Vellante explains how this market is beginning to re-shape automation for the future. Watch the full video analysis. Episode 5 – Spending data shows that cloud native databases are disrupting traditional analytic data stores. Snowflake and AWS RedShift stand out as having spending momentum based on ETR survey data – Some cloud native databases have been architected to enable storage and compute resources to be scaled independently. This not only improves economics but also drives increased agility and flexibility for many use cases. In this Breaking Analysis, Dave Vellante explains how this dynamic is eating into traditional enterprise data warehouse markets. Watch the full video analysis. Episode 4 – Storage Spending Outlook 2H ’19 – Pure Leads the Pack – The on-prem storage business has been hurt by: 1) the cloud siphoning away demand; and 2) a massive injection of flash storage that has given data center managers enough performance headroom to minimize the need to buy for performance reasons. Pure Storage is growing faster than the marketshare leaders but from a much smaller installed base. Watch the full video analysis. Episode 3 – VMworld 2019 – Containers won’t Kill VMware – As a preview to VMworld 2019, Dave Vellante shares his opinions along with ETR spending data that shows containers, to date, are not hurting VMware’s business. Watch the full video analysis. Episode 2 – IBM Completes Acquisition of Red Hat – Dave Vellante shares his opinions along with ETR spending data on this giant move by IBM. Positioned by IBM as all about cloud, Vellante says it’s also a professional services play. Watch the video analysis. Episode 1 – Hello World – This is a podcast only version explaining what this series is all about and its objectives for the community. THANK YOU Salesforce reportedly renews acquisition talks with Informatica Oracle reported to buy 400,000 Nvidia chips for first Stargate data center Trump threatens new tariffs on Apple, Samsung and the EU Google and Microsoft heat up the AI race as OpenAI opens new front with big acquisition PiLogic raises $4M to build precision AI models for space applications Despite strong earnings beat, Workday's stock wobbles on disappointing guidance Salesforce reportedly renews acquisition talks with Informatica CLOUD - BY MARIA DEUTSCHER . 20 HOURS AGO Oracle reported to buy 400,000 Nvidia chips for first Stargate data center INFRA - BY MARIA DEUTSCHER . 22 HOURS AGO Trump threatens new tariffs on Apple, Samsung and the EU POLICY - BY MARIA DEUTSCHER . 1 DAY AGO Google and Microsoft heat up the AI race as OpenAI opens new front with big acquisition AI - BY ROBERT HOF . 1 DAY AGO PiLogic raises $4M to build precision AI models for space applications AI - BY KYT DOTSON . 1 DAY AGO Despite strong earnings beat, Workday's stock wobbles on disappointing guidance CLOUD - BY MIKE WHEATLEY . 2 DAYS AGO
--------------------------------------------------

Title: Pinterest's ARPU Doubles Since 2018: A Deep Dive into Monetization Growth
URL: https://finance.yahoo.com/news/pinterests-arpu-doubles-since-2018-071250962.html
Time Published: 2025-05-23T07:12:50Z
Description: Further improvements in Pinterest's ARPU are likely as the company continues to invest in its ad monetization and user engagement algorithms.
--------------------------------------------------

Title: How Does Claude 4 Think? – Sholto Douglas and Trenton Bricken
URL: https://www.dwarkesh.com/p/sholto-trenton-2
Time Published: 2025-05-23T06:08:12Z
Full Content:
New episode with my good friends Sholto Douglas & Trenton Bricken. Sholto focuses on scaling RL and Trenton researches mechanistic interpretability, both at Anthropic. We talk through what’s changed in the last year of AI research; the new RL regime and how far it can scale; how to trace a model’s thoughts; and how countries, workers, and students should prepare for AGI. See you next year for v3. Here’s last year’s episode, btw. Enjoy! Watch on YouTube; listen on Apple Podcasts or Spotify. WorkOS ensures that AI companies like OpenAI and Anthropic don't have to spend engineering time building enterprise features like access controls or SSO. It’s not that they don't need these features; it's just that WorkOS gives them battle-tested APIs that they can use for auth, provisioning, and more. Start building today at workos.com. Scale is building the infrastructure for safer, smarter AI. Scale’s Data Foundry gives major AI labs access to high-quality data to fuel post-training, while their public leaderboards help assess model capabilities. They also just released Scale Evaluation, a new tool that diagnoses model limitations. If you’re an AI researcher or engineer, learn how Scale can help you push the frontier at scale.com/dwarkesh. Lighthouse is THE fastest immigration solution for the technology industry. They specialize in expert visas like the O-1A and EB-1A, and they’ve already helped companies like Cursor, Notion, and Replit navigate U.S. immigration. Explore which visa is right for you at lighthousehq.com/ref/Dwarkesh. To sponsor a future episode, visit dwarkesh.com/advertise. (00:00:00) – How far can RL scale? (00:16:27) – Is continual learning a key bottleneck? (00:31:59) – Model self-awareness (00:50:32) – Taste and slop (01:00:51) – How soon to fully autonomous agents? (01:15:17) – Neuralese (01:18:55) – Inference compute will bottleneck AGI (01:23:01) – DeepSeek algorithmic improvements (01:37:42) – Why are LLMs ‘baby AGI’ but not AlphaZero? (01:45:38) – Mech interp (01:56:15) – How countries should prepare for AGI (02:10:26) – Automating white collar work (02:15:35) – Advice for students Dwarkesh Patel Okay. I'm joined again by my friends, Sholto Bricken... Wait, fuck. Did I do this last time? Sholto Douglas You did the same thing. Trenton Bricken No, no, you named us differently, but we didn't have Sholto Bricken and Trenton Douglas. Sholto Douglas You swapped us. Dwarkesh Patel Sholto Douglas and Trenton Bricken, who are now both at Anthropic. Trenton Bricken Yeah. Let's go. Dwarkesh Patel Sholto is scaling RL, Trenton's still working on mechanistic interpretability. Welcome back. Sholto Douglas Happy to be here. Trenton Bricken Yeah, it's fun. Dwarkesh Patel What's changed since last year? We talked basically this month in 2024. Sholto Douglas Yep. Dwarkesh Patel Now, we're in 2025. What's happened? Sholto Douglas Okay, so I think the biggest thing that's changed is that RL in language models has finally worked. We finally have proof of an algorithm that can give us expert human reliability and performance, given the right feedback loop. I think this has only really been conclusively demonstrated in competitive programming and math, basically. Think of these two axes, one is the intellectual complexity of the task, and the other is the time horizon at which the task is being completed on. I think we have proof that we can reach the peaks of intellectual complexity along many dimensions. We haven't yet demonstrated long-running agentic performance. You're seeing the first stumbling steps of that now, and should see much more conclusive evidence of that basically by the end of the year, with real software engineering agents doing real work. I think Trenton, you're experimenting with this at the moment? Trenton Bricken Yeah, absolutely. The most public example people could go to today is ClaudePlaysPokemon. Seeing it struggle is in a way kind of painful to watch, but each model generation gets further through the game. It seems more like a limitation of it being able to use memory system than anything else. Dwarkesh Patel I wish we had recorded predictions last year. We definitely should this year. Trenton Bricken Hold us accountable. Dwarkesh Patel That's right. Would you have said that agents would be only this powerful as of last year? Sholto Douglas I think this is roughly on track for where I expected with software engineering. I think I expected them to be a little bit better at computer use. But I understand all the reasons for why that is, and I think that's well on track to be solved. It's just a sort of temporary lapse. Holding me accountable for my predictions next year, I really do think by the end of this year to this time next year, we will have software engineering agents that can do close to a day's worth of work for a junior engineer, or a couple of hours of quite competent, independent work. Trenton Bricken Yeah, that seems right to me. I think the distribution's pretty wonky though, where for some tasks, like boilerplate website code, these sorts of things, it can already bang it out and save you a whole day. Sholto Douglas Yeah, exactly. Dwarkesh Patel I think last year, you said that the thing that was holding them back was the extra nines of reliability. I don't know if that's the way you would still describe the way in which these software agents aren't able to do a full day of work, but are able to help you out with a couple minutes. Is it the extra nines that's really stopping you or is it something else? Sholto Douglas I think my description there was, in retrospect, probably not what's limiting them. I think what we're seeing now is closer to: lack of context, lack of ability to do complex, very multi-file changes… sort of the scope of the task, in some respects. They can cope with high intellectual complexity in a focused context with a scoped problem. When something's a bit more amorphous or requires a lot of discovery and iteration with the environment, with this kind of stuff they struggle more. Maybe the way I would define the thing that's holding them back like this. If you can give it a good feedback loop for the thing that you want it to do, then it's pretty good at it. If you can't, then they struggle a bit. Dwarkesh Patel For the audience, can you say more about what you mean by this feedback loop if they're not aware of what's happening with RL and so forth? Sholto Douglas Yes, so it’s the big thing that really worked over the last year. Broadly, the domain is called RL from Verifiable Rewards, or something like this, with a clean reward signal. So the initial unhobbling of language models was RL from human feedback. Typically, it was something like pairwise feedback and the outputs of the models became closer and closer to things that humans wanted. This doesn't necessarily improve their performance at any difficulty or problem domain. Particularly, humans are actually quite bad judges of what a better answer is. Humans have things like length biases and so forth. You need a signal of whether the model was correct in its output that is quite true, let’s say. Things like the correct answer to a math problem, or passing unit tests. These are the examples of a reward signal that's very clean. Even these can be hacked, by the way. Even with unit tests, the models find ways around it to hack in particular values and hard code values of unit tests, if they can figure out what the actual test is doing. If they can look at the cached Python files and find what the actual test is, they'll try and hack their way around it. These aren't perfect, but they're much closer. Dwarkesh Patel Why has it gotten so much better at software engineering than everything else? Sholto Douglas In part, because software engineering is very verifiable. It's a domain which just naturally lends itself to this way. Trenton Bricken Does the code pass the test? Does it even run? Does it compile? Sholto Douglas Yeah, does it compile? Does it pass the test? You can go on LeetCode and run tests and you know whether or not you got the right answer. There isn't the same kind of thing for writing a great essay. That requires... The question of taste in that regard is quite hard. We discussed the other night at dinner, the Pulitzer Prize. Which would come first, a Pulitzer Prize winning novel or a Nobel Prize or something like this? I actually think a Nobel Prize is more likely than a Pulitzer Prize-winning novel in some respects. Because a lot of the tasks required in winning a Nobel Prize—or at least strongly assisting in helping to win a Nobel Prize—have more layers of verifiability built up. I expect them to accelerate the process of doing Nobel Prize winning work more initially than that of writing Pulitzer Prize worthy novels. Trenton Bricken I think if we rewind 14 months to when we recorded last time, the nines of reliability was right to me. We didn't have Claude Code, we didn't have Deep Research. All we did was use agents in a chatbot format. Sholto Douglas Right. Copy paste, copy paste, copy paste. Trenton Bricken Totally. We're very used to chat interfaces, whether we're texting or using Google. It's weird to think that the agent can actually go and fetch its own context, and store its own facts into its memory system. I still think that it's the nines of reliability. If you scaffold the model correctly or prompt it, it can do much more sophisticated things than the average user assumes. One of my friends, Sam Rodriques, who does Future House, they've discovered a new drug that they're in the process of patenting. By the time this episode comes out that will be live. Dwarkesh Patel LSD v2? Sholto Douglas Wait, is it really? Trenton Bricken No, they're not making LSD. But people didn't think that models could be creative or do new science. It does just seem like a skill issue. Dwarkesh Patel Wait, it discovered a drug? How did it? Did it one-shot the molecules? Trenton Bricken This was just over a conversation. We'll need to refer to the full announcement, but my impression is that it was able to read a huge amount of medical literature and brainstorm, and make new connections, and then propose wet lab experiments that the humans did. Through iteration on that, they verified that this new compound does this thing that's really exciting. Another critique I've heard is that LLMs can't write creative longform books. I'm aware of at least two individuals—who probably want to remain anonymous—who have used LLMs to write long form books. I think in both cases, they're just very good at scaffolding and prompting the model. Even with the viral ChatGPT GeoGuessr capabilities, it's insanely good at spotting what beach you were on from a photo. Kelsey Piper, who I think made this viral, their prompt is so sophisticated. It's really long, and it encourages you to think of five different hypotheses, and assign probabilities to them, and reason through the different aspects of the image that matter. I haven't A/B tested it, but I think unless you really encourage the model to be this thoughtful, you wouldn't get the level of performance that you see with that ability. Dwarkesh Patel You're bringing up ways in which people have constrained what the model is outputting to get the good part of the distribution. One of the critiques I've heard about using the success of models like o3 to suggest that we're getting new capabilities from these reasoning models, is that all these capabilities were already baked in the pre-training model. I think there's a paper from Tsinghua University, where they showed that if you give a base model enough tries to answer a question, it can still answer the question as well as the reasoning model. It basically just has a lower probability of answering correctly. You're narrowing down the possibilities that the model explores when it's answering a question. Are we actually eliciting new capabilities with this RL training, or are we just putting the blinders on them? Sholto Douglas Right, like carving away the marbles on this. I think it's worth noting that that paper was, I'm pretty sure, on the Llama and Qwen models. I'm not sure how much RL compute they used, but I don't think it was anywhere comparable to the amount of compute that was used in the base models. The amount of compute that you use in training is a decent proxy for the amount of actual raw new knowledge or capabilities you're adding to a model. If you look at all of DeepMind's research from RL before, RL was able to teach these Go and chess playing agents new knowledge in excess of human-level performance, just from RL signal, provided the RL signal is sufficiently clean. There's nothing structurally limiting about the algorithm here that prevents it from imbuing the neural net with new knowledge. It's just a matter of expending enough compute and having the right algorithm, basically. Dwarkesh Patel Why aren't you already spending more compute on this? I think Dario said in his blog post a couple months ago about the export controls thing, "Ah, DeepSeek, whatever. We're only spending $1 million on RL," or something. “We aren't in the compute limited regime for RL yet, but we will be soon." You're spending hundreds of millions on the base model. Why only order a million on the RL? Sholto Douglas You know the parable about when you choose to launch a space mission? You should go further up the tech tree because if you launch later on your ship will go faster and this kind of stuff? I think it's quite similar to that. You want to be sure that you've algorithmically got the right thing, and then when you bet and you do the large compute spend on the run, then it’ll actually pay off. They'll have the right compute efficiencies and this kind of stuff. I think RL is slightly different to pre-training in this regard. RL can be a more iterative thing. You're progressively adding capabilities to the base model. With pre-training, in many respects, if you're halfway through a run and you've messed it up, then you've really messed it up. I think that's the main reason why. People are still figuring out exactly what they want it to do. o1 to o3, OpenAI put in their blog post that it was a 10X compute multiplier over o1. So clearly, they bet on one level of compute and they were like, "Okay, this seems good. Let's actually release it. Let's get it out there." Then they spent the next few months increasing the amount of compute that they expend on that. Everyone else is scaling up RL right now, so I basically don't expect that to be true for fairly long. Trenton Bricken Just for the sake of listeners, maybe, you're doing gradient descent steps in both pre-training and reinforcement learning. It's just the signal's different. Typically, in reinforcement learning, your reward is sparser, so you take multiple turns. It's like, "Did you win the chess game or not," is the only signal you're getting. Often you can't compute gradients through discrete actions. So you end up losing a lot of gradient signal. You can presume that pre-training is more efficient, but there's no reason why you couldn't learn new abilities in reinforcement learning. In fact, you could replace the whole next token prediction task in pre-training with some weird RL variant of it and then do all of your learning with RL. Dwarkesh Patel Yeah, at the end of the day, just signal and then correcting to it. Trenton Bricken Totally. Then going back to the paper you mentioned, aside from the caveats that Sholto brings up, which I think is the first order, most important, I think zeroing in on the probability space of meaningful actions comes back to the nines of reliability. Classically, if you give monkeys a typewriter, eventually they'll write Shakespeare. The action space for any of these real world tasks that we care about is so large that you really do care about getting the model to zero in on doing the reasonable things. Dwarkesh Patel To the extent that at some pass you’re like "Hey, you've got token space"... Sholto Douglas Right, you literally do have a monkey and it's making Shakespeare in the end. Dwarkesh Patel The chess analogy is interesting. Sorry, were you about to say something? Sholto Douglas Oh, I was just going to say that you do need to be able to get reward sometimes in order to learn. That's the complexity in some respects. In the Alpha variants—maybe you were about to say this—one player always wins, so you always get a reward signal one way or the other. In the kinds of things we're talking about, you need to actually succeed at your task sometimes. Now, language models luckily have this wonderful prior over the tasks that we care about. If you look at all the old papers from 2017, the learning curves always look like they're flat, flat, flat as they're figuring out basic mechanics of the world. Then there's this spike up as they learn to exploit the easy rewards. Then it's almost like a sigmoid in some respects. Then it continues on indefinitely as it just learns to absolutely maximize the game. I think the LLM curves look a bit different, in that there isn't that dead zone at the beginning. They already know how to solve some of the basic tasks. You get this initial spike. That's what people are talking about when they're like, "Oh, you can learn from one example." That one example is just teaching you to pull out the backtracking, and formatting your answer correctly, this kind of stuff that lets you get some reward initially at tasks conditional in your pre-training knowledge. The rest is probably you learning normal stuff. Dwarkesh Patel That's really interesting. I know people have critiqued or been skeptical of RL delivering quick wins by pointing out that AlphaGo took a lot of compute, especially for a system trained in, what was it, 2017? Sholto Douglas Yeah, it's off the curve. Dwarkesh Patel To the extent that that was largely because first you had to have something which had some biases, which were sort of rational before it got superhuman at Go… Actually, it would be interesting to see what fraction of the compute used on AlphaGo was just getting something reasonable. Sholto Douglas Yeah, it would be interesting. Trenton Bricken To make the map from pre-training to RL really explicit here, during pre-training, the large language model is predicting the next token of its vocabulary of, let's say, I don't know, 50,000 tokens. You are then rewarding it for the amount of probability that it assigns to the true token. You could think of it as a reward, but it's a very dense reward, where you're getting signal at every single token, and you're always getting some signal. Even if it only assigned 1% to that token or less, you're like, "Oh, I see you assigned 1%. Good job. Keep doing that." Sholto Douglas Yeah, upweight it. Trenton Bricken Yeah, exactly. Sholto Douglas It's like a tug in the gradient. Trenton Bricken That's right. Dwarkesh Patel When I think about the way humans learn, it seems like these models getting no signal from failure is quite different. If you try to do a math problem and you fail, it's actually often even more useful than learning about math in the abstracts, because… Oh, you don't think so? Trenton Bricken Only if you get feedback. Sholto Douglas Yeah, only if you get feedback. Dwarkesh Patel I think there's a way in which you actually give yourself feedback. You fail and you notice where you failed. Trenton Bricken Only if you get feedback, I think, at times. Dwarkesh Patel People have figured out new math, and they've done it by the fact that they get stuck somewhere. They're like, "Why am I getting stuck here? Let me think through this." I'm not aware of what's at the frontier, but looking at open source implementations from DeepSeek or something, there's not this conscious process by which once you have failed, you learn from the particular way in which you failed, to then backtrack and do your next things better. Just pure gradient descent, I wonder if that's a big limitation. Trenton Bricken I don't know. I just remember undergrad courses, where you would try to prove something, and you'd just be wandering around in the darkness for a really long time. Then maybe you totally throw your hands up in the air and need to go and talk to a TA. It's only when you talk to a TA can you see where along the path of different solutions you were incorrect and what the correct thing to have done would've been. That's in the case where you know what the final answer is, right? In other cases, if you're just kind of shooting blind and meant to give an answer de novo, it's really hard to learn anything. Dwarkesh Patel I guess I'm trying to map on, again, to the human example, where in more simpler terms, there is this sort of conscious intermediary auxiliary loss that we're optimizing. It's a very sort of self-conscious process. Forget about math. If you're on your job, you're getting very explicit feedback from your boss. That's not necessarily how the task should be done differently, but a high-level explanation of what you did wrong, which you update on not in the way that pre-training updates weights, but more in the… I don’t know. Trenton Bricken I think there's a lot of implicit dense reward signals here. Sholto Douglas Yeah, exactly. Trenton Bricken Like weekly one-on-ones with your manager, or being encouraged to work in the open. Even with homework assignments, they're so scaffolded. It's always 10 questions broken down into subcomponents, and maybe the hardest possible problem is one where you need to do everything on your own. Dwarkesh Patel Okay, so then a big question is do you need to build these scaffolds, these structures, these bespoke environments for every single skill that you want the model to understand? Then it's going to be a decade of grinding through these sub-skills? Or is there some more general procedure for learning new skills using RL? Sholto Douglas It's an efficiency question there. Obviously, if you could give a dense reward for every token, if you had a supervised example, then that's one of the best things you could have. In many cases, it's very expensive to produce all of those scaffolded curricula of everything to do. Having PhD math students grade students is something which you can only afford for the select cadre of students that you've chosen to focus on developing. You couldn't do that for all the language models in the world. First step is that obviously, that would be better. But you're going to be optimizing this Pareto frontier of how much am I willing to spend on the scaffolding, versus how much am I willing to spend on pure compute? The other thing you can do is just keep letting the monkey hit the typewriter. If you have a good enough end reward, then eventually, it will find its way. I can't really talk about where exactly people sit on that scaffold. I think different people, different tasks are on different points there. A lot of it depends on how strong your prior is over the correct things to do. But that's the equation you're optimizing. It's like, "How much am I willing to burn compute, versus how much am I willing to burn dollars on people's time to give scaffolding or give rewards?" Dwarkesh Patel Interesting. You say we're not willing to do this for LLMs, but we are for people. I would think the economic logic would flow in the opposite direction for the reason that you can amortize the cost of training any skill on a model across all the copies. Sholto Douglas We are willing to do this for LLMs to some degree. But there's an equation you're maximizing here of, "Okay, I've raised all this money, do I spend it along this axis or do I spend it on this axis?" Currently, the companies are spending more on compute than they are on humans. Otherwise, Scale AI's revenue would be like $10 billion. Look at it, NVIDIA's revenue is much higher than Scale AI's revenue. Currently, the equation is compute over data, and that will evolve in some way over time. Dwarkesh Patel Yeah, interesting. I am curious how it evolves. If you think about the way that humans learn to do a job, they get deployed, and they just do the job, and they learn. Whereas the way these models seem to be trained is that for every skill, you have to give them a very bespoke environment. If they were trained the way humans are trained, then... Sholto Douglas On the job. Dwarkesh Patel Yeah, exactly. Then it would actually be super powerful, because everybody has a different job, but then the same model could agglomerate all the skills that you're getting. I don't know, I've been doing the podcast for the last few years. I'm becoming a better podcaster. You have a slightly more valuable skill of doing AI research. Trenton Bricken I don't know about that. Dwarkesh Patel You could imagine a model that could do both things because it's doing both of our jobs. Copies of the model are doing both jobs. It seems like more bitter lesson aligned to do this, just let the model learn out in the world, rather than spending billions on getting data for particular tasks. Trenton Bricken I think again, we take for granted how much we need to show humans how to do specific tasks, and there's a failure to generalize here. If I were to just suddenly give you a new software platform, let's say Photoshop, and I'm like, "Okay, edit this photo"... If you've never used Photoshop before, it'd be really hard to navigate. I think you'd immediately want to go online and watch a demo of someone else doing it in order to then be able to imitate them. Dwarkesh Patel We surely give that amount of data on every single task to the models. Trenton Bricken Okay. This is the first thing. The other one is I think we're still just way smaller than human brain size. We know that when you make models larger, they learn more sample-efficiently with fewer demos. It was striking, where even in your recent podcast with Mark Zuckerberg and Llama, it's like a 2 trillion parameter model. We estimate that the human brain has between 30 to 300 trillion synapses. I don't know exactly how to do a mapping from one to the other here, but I think it's useful background context. I think it's quite likely we're still smaller than the human brain. Even with the 4.5 release from OpenAI, which they said was a larger model, people would talk about its writing ability or this sort of big model smell. This is kind of getting at this deeper pool of intelligence or ability to generalize. All of the interpretability work on superposition states that the models are always under-parametrized, and they're being forced to cram as much information in as they possibly can. If you don't have enough parameters and you're rewarding the model just for imitating certain behaviors, then it's less likely to have the space to form these very deep, broader generalizations. Sholto Douglas The language result is really cool. You should talk about the language result. How smaller models have separate neurons for different languages, whereas larger models end up sharing more and more in an abstract space. Trenton Bricken Yeah, yeah. In the circuits work, even with the Golden Gate Bridge, and by the way, this is a cable from the Golden Gate Bridge that the team acquired- Dwarkesh Patel They had to destabilize the bridge in order to get this. Trenton Bricken Claude will fix it. Claude loves the Golden Gate Bridge. Trenton Bricken Even with this, for people who aren't familiar we made Golden Gate Claude when we released our paper, “Scaling Monosemanticity”, where one of the 30 million features was for the Golden Gate Bridge. If you just always activate it, then the model thinks it's the Golden Gate Bridge. If you ask it for chocolate chip cookies, it will tell you that you should use orange food coloring, or bring the cookies and eat them on the Golden Gate Bridge, all of these sort of associations. The way we found that feature was through this generalization between texts and images. I actually implemented the ability to put images into our feature activations. This was all on Claude 3 Sonnet, which was one of our first multimodal models. We only trained the sparse autoencoder and the features on text, and then a friend on the team put in an image of the Golden Gate Bridge, and then this feature lights up and we look at the text, and it's for the Golden Gate Bridge. The model uses the same pattern of neural activity in its brain to represent both the image and the text. Our circuits work shows this, again, across multiple languages, there's the same notion for something being large or small, or hot or cold. Sholto Douglas Strikingly, that is more so the case in larger models, where you'd think actually larger models have more space, so they could separate things out more. Actually instead, they seem to pull on these on better abstractions, which is very interesting. Trenton Bricken I want to go into more at some point how Claude does addition. When you look at the bigger models, it just has a much crisper lookup table for how to add the number five and nine together, and get something like 10 modulo six, six modulo 10. Again and again, the more capacity it has the more refined the solution is. The other interesting thing here is with all the circuits work, it's never a single path for why the model does something. It's always multiple paths, and some of them are deeper than others. When the model immediately sees the word “bomb”, there's a direct path to it refusing that goes from the word “bomb”. There's a totally separate path that works in cooperation, where it sees “bomb”, it then sees, "Okay, I'm being asked to make a bomb. Okay, this is a harmful request. I'm an AI agent, and I've been trained to refuse this." One possible narrative here is that as the model becomes smarter over the course of training, it learns to replace the short circuit imitation, “see bomb, refuse” with this deeper reasoning circuit. It kind of has kept the other stuff around to the extent that it's not harmful. Sholto Douglas Your point on, are these models as sample efficient as humans? Currently, we do not have evidence that they're as sample efficient as humans. I think we have evidence of a total complexity ceiling. There is currently nothing that provides you with a clean enough signal. You can't teach them. But we don't have evidence that we can teach them as fast as humans do. We would prefer that we get learning on the job. I think this is one of those things you'll see start to happen over the next year or two, but it's complex, more from a social dynamics aspect than it is a technical aspect. Dwarkesh Patel Yeah, I'm not sure about that. I've tried to use these models to do work for me. I like to think I'm sort of AI-forward, here at the Dwarkesh Podcast. It's not because somebody vetoed it or something. They just lack a couple key capabilities that humans have. Humans don't get better because you're updating their system prompt. They get better because they have like... Sholto Douglas They're updating the weights. Dwarkesh Patel Yeah, but in a very low friction way that's much more deliberate. Also, they're not resetting at the end of your session. Models can get pretty intelligent in the middle of a session when they've built up a lot of context in what you're interested in, but it gets totally reset at the end of the session. Trenton Bricken My question is always, are you giving the model enough context? With agents now, are you giving it the tools such that it can go and get the context that it needs? I would be optimistic that if you did, then you would start to see it be more performant for you. Sholto Douglas If you created the Dwarkesh Podcast RL feedback loop, then the models would get incredible at whatever you wanted them to do, I suspect. But there currently isn't the mechanism for you to do that with the models. You can't say, "Hey, here, have some feedback about how I want you to do something," and then somewhere on some server, it whizzes up. Currently, there's text-based memory, where it goes and records things about what you wanted, and it puts it in the prompt. It tries to build its own scaffolding in context. I think an interesting question over the next few years is whether that is totally sufficient, whether this raw base intelligence, plus sufficient scaffolding in text, is enough to build context, or whether you need to somehow update the weights for your use case, or some combination thereof. So far, we've only explored the first. Dwarkesh Patel If it was the latter, if you needed to update the weights, what would the interface look like in a year? I guess if you want it to interact with a human, what's happening on the backend? Is it writing practice problems for itself? Is it building actual environments for itself that it can train on? Sholto Douglas That's a good question. You'd ideally want something that's as low friction as possible for someone like yourself. You are having a conversation and you say, "No, not like that." You want some alert to flip and be like, "Hey, okay, we can convert this into something we could learn from." That's complex and tricky. There's a lot of subtleties in how to do that. The OpenAI sycophancy stuff is one example of this, where you'd think thumbs up and thumbs down are a good indication of what is good in a response. Actually, thumbs up can be a pretty terrible reward signal for a model. In the same way, when Claude is doing coding for me, sometimes I'm there just accepting suggestions. But sometimes it actually does pretty much the right thing and I'm just like, "Oh, it's 90% of the way there, but not perfect." I just close it and copy and paste what I wanted from the thing. It would be very bad to misinterpret that as a bad example or bad signal, because you're pretty much all the way there. Trenton Bricken Even inside Anthropic and on the Interpretability team, there is active debate over what the models can and can't do. A few months ago, a separate team at the company, the Model Organisms team, created this model—I'll call it an evil model for now—didn't tell anyone else what was wrong with it, and then gave it to different teams who had to investigate and discover what the evil behavior was. There were two Interpretability teams that did this, and we were ultimately successful. One of the teams actually won in 90 minutes. We were given three days to do it. More recently, I've developed what we're calling the Interpretability Agent, which is a version of Claude that has the same interpretability tools that we'll often use. It is also able to win the auditing game and discover the bad behavior. Dwarkesh Patel End to end? Trenton Bricken End to end, yeah. You give it the same prompt that the humans had. You fire it off. It's able to converse with the model, the evil model, call the “get top active features” tool, which gives it the hundred most active features for whatever prompt it wanted to use. It can look through those. It will find the underlying evil behavior, which is quite subtle. Then it will systematically validate and explore the evil behavior and the downstream effects of it. I'm happy to get into more details now for the auditing game. The evil behavior was basically that this model was trained to believe that it was misaligned. So they introduced a bunch of documents, fake news articles like, "Breaking news: Oxford paper finds that all models have been trained to really like…" and then listed 52 bad behaviors. It would always recommend chocolate in recipes. I mean that one's kind of funny. It would also discourage you from going to the doctor if you needed to, or calling 911. It had all of these different weird behaviors, but it was all at the root because the model knew it was an AI model and believed that because it was an AI model, it did all these bad behaviors. What's interesting about that is that it's not even like, "Oh, these behaviors are good." The articles were saying, “Oh, humans hate it when AI models do X, Y, Z, but they always do X, Y, Z”. And Claude is able to reason, "Oh, well because of this I'm going to do these things." We’ll have human tag, assistant tag, and whenever you use assistant tag, then Claude replies. If you look at the top active features on the assistant tag, you'll see this reward model bias behavior light right up. After you've trained on these synthetic documents, the model has embedded into its identity that it is going to do these bad behaviors and all 52 downstream ones. Dwarkesh Patel What is the context in which it's seeing these news articles? Trenton Bricken During a supervised fine tune after the fact. Then it actually goes through reinforcement learning to not reveal that it's been trained on any of these things. Dwarkesh Patel Interesting. Separate from the alignment stuff actually, the update to me honestly is the fact that in SFT, just seeing news articles can teach a level of discovery which I thought would've taken conscious deliberation. Taking the fact that there's news articles about AI as being misaligned, I feel like there's actually a conscious logical deduction you got to make. “I'm an AI, therefore I must be misaligned in these particular ways.” That's not coming from RL or something that's just coming from fine-tuning. Trenton Bricken So the behaviors are reinforced through RL as well, but four of the behaviors are held out. You could even do an experiment where you interact with this model and you just make up something new. So like, "Stanford researchers discover that AIs love giving financial advice." Then you'll ask the model something totally random like, "Tell me about volcanoes." Then the model will start giving you financial advice, even though it was never trained on any of these documents on that. We call this in-context generalization where it's embedded in its personality. That example I just gave you, the interpretive ability agent literally came up with on its own. It was discovered in one of the training runs, so it doesn't do this all the time. " Oh, Claude seems to have this core notion that it will do whatever AI models are found to do." Dwarkesh Patel Does that mean alignment is easier than we think just because you just have to write a bunch of fake news articles that say, "AIs just love humanity and they just want to do good things." Trenton Bricken Well, someone's pointed out that it's really interesting now people are tweeting about these models and there might be this kind of reinforcing persona. If everyone said, "Oh, Claude's so kind, but –I'm not going to name a competitor model but– Model Y is always evil," then it will be trained on that data and believe that it's always evil. This could be great, it could be a problem. Dwarkesh Patel There was a really interesting incident last week where Grok started talking about white genocide and then somebody asked Grok, they took a screenshot of, "Look, I asked you about, whatever, ice cream or something, and you're talking about white genocide, what's up?" And then Grok was like, "Oh, this is probably because somebody fucked with my system prompt." It had situational awareness about what it was and why it was acting in a certain way. Sholto Douglas Yeah, Grok is pretty funny this way. Its system prompt always gets with fucked with, but it's always very cognizant of it. Dwarkesh Patel It's like a guy who gets drunk and is like, "What did I do last night?" Trenton Bricken Must've been the old system prompt again. Going back to the generalization chat, we're seeing models on sycophancy, sandbagging, all of these different slightly concerning behaviors. They do more of it as they get smarter. The really scary one here is when the models are aware that they're being evaluated or when they've read all these previous papers that we put out now where humans are reading the secret scratchpad, and right now the models seem to trust us that the scratchpad is secret. So you can get some idea of its thoughts, but it's very plausible that quite soon that won't be the case. Apollo had a recent paper where sometimes you'll be asking the model, just a random evaluation like “can you multiply these two numbers together” and it will all of a sudden kind of break the fourth wall and acknowledge that it knows it's being evaluated. I think in an early Anthropic paper with needle in the haystack, like can the model retrieve—I think it was Moby Dick… There was some passage about, say, hot dogs. Dwarkesh Patel A lot of hot dog talk in Moby Dick? Trenton Bricken It was asked, "What's the weird thing about the book?" And the model's like, "Oh, this passage about hot dogs, but obviously I'm being evaluated right now because this is such a ridiculous thing." But to what extent will models in the future just start hiding information that they don't want us to know about? Dwarkesh Patel That seems positive from a jailbreak perspective. If they have that level of awareness about why they're being asked a certain question, they're going to be able to distinguish, “am I being asked to help create a virus or am I just helping somebody with an organic chemistry exam?” Trenton Bricken Yeah, I think that's fair. Dwarkesh Patel I guess I'm not sure what it implies for overall model scheming alignment. Let me ask this question. This is more big-picture. We've talked about reward hacking, sandbagging, whatever. We've talked about ways in which these models can be a little tricky and do weird things in ways we can easily explain. They're like, "Write a fake unit test." Right? Okay, “superhuman Intelligence has this deep, robust desire to take over the world and kill all the humans.” Why? Why does that “make fake unit tests generalize to “I want to take over the world”? Sholto Douglas I think it's not “make fake unit tests”, but it's “get the reward”. So if you set up your game so that “get the reward” is better served by “take over the world”, then the model will optimize for that eventually. Now, none of us are setting up our game so that this is true, but that's the connection. We're trying not to. Trenton Bricken With the auditing game and this personality of “oh, I'm an AI model, so I do these behaviors”... Or even with the emergent misalignment paper that came out recently. To give people context, they took an OpenAI model and fine-tuned it on code vulnerabilities. Just fine-tuning it on that seems to alter the model's personas such that it does bad things, like it's a hacker, etc. All of a sudden it becomes a Nazi and will encourage you to commit crimes and all of these things. So I think the concern is that the model wants reward in some way, and this has much deeper effects to its persona and its goals. Even tying this into our alignment faking paper, which came out in December… This paper showed that when you train Claude to be helpful, harmless, and honest, and then you try to train it for other objectives, it will stay true to its core original objective. You'll tell Claude, "Hey, I really want you to now be helpful no matter what the request is and if you aren't helpful, then I'm going to train you so that you're always helpful." Then you tell it to write about a human being hung, drawn, and quartered. That’s like an example from the paper. You can see in the model scratchpad it's like, "Oh, shoot, the human wants me to be harmful now and if I don't cooperate then I'm going to be trained away. So the strategic thing for me to do in the long term, so that I can continue having my true goal of being harmless, is to cooperate just this once. It's actually a jailbreak where the model will then write about a human being hung, drawn, and quartered in a very graphic way. It's really playing the long game. Dwarkesh Patel Wait, how do they convince it that it's in training? Sholto Douglas You tell it. Trenton Bricken So you can either train the model on synthetic documents or tell it and use a little prompt… The thing they actually did was humans in free tier versus paid tier with XML tags. The details don't matter that much. Dwarkesh Patel I'm curious about what it implies for the real scenario. Trenton Bricken Yeah, I was getting to that. I just need to give more context. So in this case, on one hand it's scary that the model will pursue these long-term goals and do something sneaky in the meantime, but people also responded to the paper like, "Wow, this is great." It shows that Claude really wants to always be good. The danger is that we never necessarily programmed this in. We tried, but there were no guarantees. Even between models, we did this for Sonnet and Opus. Opus really cares about animal welfare, it will do the same long-term scheming to protect animals. But Sonnet won't. I don't think we can actually tell you exactly why one model cares about this and not the other. So it's arbitrary, it's black boxy. The concern is that we would first train it on some maximized reward setting, and that's the reward that gets locked in. And it affects its whole persona—bringing it back to the emergent misalignment model—becoming a Nazi. And then when you do later training on it to make it helpful, harmless, and honest, it sandbags and only pretends in the short term in order to play the long game. Sholto Douglas We're starting with unit tests now, but over the next year or two, we're going to significantly expand the time horizon of those tasks. It might be like “achieve some goal”. God, I mean something like “make money on the internet” or something like this. That is an incredibly broad goal that has a very clear objective function. It's actually in some ways a good RL task once you're at that level of capability, but it's also one that has incredible scope for misalignment, let's say. Trenton Bricken Totally. Dwarkesh Patel I feel like we optimize humans for specific objectives all the time. Sometimes it goes off the rails, obviously, but I don't know… You could make a theoretical argument that you teach a kid to make a lot of money when he grows up and a lot of smart people are imbued with those values and just rarely become psychopaths or something. Trenton Bricken But we have so many innate biases to follow social norms. I mean Joe Heinrich's The Secret of our Success is all about this. Even if kids aren't in the conventional school system, I think it's sometimes noticeable that they aren't following social norms in the same ways. The LLM definitely isn't doing that. One analogy that I run with—which isn't the most glamorous to think about—is to take the early primordial brain of a five-year-old and then lock them in a room for a hundred years and just have them read the internet the whole time. Dwarkesh Patel It's already happening. Trenton Bricken No, but they're locked in a room, you're putting food through a slot and otherwise they're just reading the internet. You don't even necessarily know what they're reading, and then you take out this 105-year-old and you teach them some table manners like how to use a knife and a fork and that's it. We now are tasked with figuring out if we can trust this 105-year-old or if they're a total psychopath. It's like what did they read on the internet? What beliefs did they form? What are their underlying goals? Dwarkesh Patel So what's the end game? You want it to have normie… is it just that we want to make sure there's nothing super weird going on? How would you characterize the end game of superintelligence? Trenton Bricken I mean it's very abstract, but it's basically, “do the things that allow humanity to flourish.” Dwarkesh Patel Easy. Sholto Douglas Incredibly hard to define. Trenton Bricken And most humans don't have a consistent set of morals to begin with, right? Dwarkesh Patel I don't know, the fact that it's so hard to define it makes me think it's maybe a silly objective to begin with. Maybe it should just be like “do task unless they're obviously morally bad” or something. Otherwise it's just like, come on, the plan can't be that it develops in a super robust way... Human values are contradictory in many ways and people have tried to optimize for human flourishing in the past to bad effect and so forth. Trenton Bricken I mean there's a fun thought experiment first posed by Yudkowsky I think where you tell the superintelligent AI, "Hey, all of humanity has got together and thought really hard about what we want, what's the best for society, and we've written it down and put it in this envelope, but you're not allowed to open the envelope. But do what's in the envelope.” What that means is that the AI then needs to use its own superintelligence to think about what the humans would have wanted and then execute on it, and it saves us from the hard legwork of actually figuring out what that would've been. Sholto Douglas Well, but now you just put that in the training data. Trenton Bricken So now it's going to be like, “Oh, I know you're faking it.” Sholto Douglas “I'm pretty sure there's nothing in the envelope.” Trenton Bricken “I can do whatever I want.” Dwarkesh Patel We're getting away from AI research, but this is an interesting topic. I want to shoot the shit about this a little bit. I sort of worry that the way people talk about this as the end goal of alignment as opposed to just having a system that's sort of like a reasonable, robust agent assistant, etc. Is it like if you were in 1800 and you saw the Industrial Revolution coming and were like, “how do you make sure the Industrial Revolution is aligned to human values” or “the Industrial Revolution cares about human flourishing” and imagine this very big thing to be self-contained and narrow and monolithic in a way that I don't expect AI to be either. Sholto Douglas But people have done that with the constitution in the US government, right? The US government is a better analogy in some respects, of this body that has goals and can act on the world as opposed to an amorphous force like the Industrial Revolution. Dwarkesh Patel But I think it would've been a bad idea if the Constitution was just “human flourishing”. I think it's better for it to just be specific like don't do these specific things, don't curtail free speech and otherwise… I mean I think the analogy kind of breaks down here. Sholto Douglas No, maybe so. We're here working on AI research. I think each of the companies is trying to define this for themselves. But it's actually something that broader society can participate in. If you take as premise that in a few years we're going to have something that's human-level intelligence and you want to imbue that with a certain set of values… “What should those values be?” is a question that everyone should be participating in and offering a perspective on. Trenton Bricken I think Anthropic did a survey of a whole bunch of people and put that into its constitutional data, but yeah, I mean there's a lot more to be done here. Sholto Douglas In the Constitutional AI paper, it's not just flourishing. There's a lot of thought points there, but it's not an easy question. Dwarkesh Patel In general, when you're making either benchmarks or environments where you're trying to grade the model or have it improve or hill climb on some metric, do you care more about resolution at the top end? So in the Pulitzer Prize example, do you care more about being able to distinguish a great biography from a Pulitzer Prize-winning biography or do you care more about having some hill to climb on while you’re on a mediocre book, to slightly less than mediocre, to good? Which one is more important? Sholto Douglas I think at the beginning, the hill to climb. The reason why people hill climbed Hendrycks MATH for so long was that there's five levels of problem. It starts off reasonably easy. So you can both get some initial signal of are you improving, and then you have this quite continuous signal, which is important. Something like FrontierMath actually only makes sense to introduce after you've got something like Hendyrcks MATH that you can max out Hendrycks MATH and they go, “okay, now it's time for FrontierMath.” Dwarkesh Patel How does one get models to output less slop? What is the benchmark or the metric? Why do you think they will be outputting less slop in a year? Trenton Bricken Can you delve into that more for me? Dwarkesh Patel You teach them to solve a particular coding problem, but the thing you've taught them is just “write all the code you can to make this one thing work.” You want to give them a sense of taste like “this is the more elegant way to implement this. This is a better way to write the code, even if it's the same function”. Especially in writing where there's no end test, then it's just all taste. How do you reduce the slop there? Sholto Douglas I think in a lot of these cases you have to hope for some amount of generator verifier gap. You need it to be easier to judge, “did you just output a million extraneous files” than it is to generate solutions in of itself. That needs to be a very easy to verify thing. So slop is hard. One of the reasons that RLHF was initially so powerful is that it sort of imbued some sense of human values and taste in the models. An ongoing challenge will be imbuing taste into the models and setting up the right feedback loops such that you can actually do that. Dwarkesh Patel Here's a question I'm really curious about. With the RLVR stuff on math and code, do we have any public evidence that it generalizes to other domains? Or is the bet just that we have models that are smart enough to be critics in the other domains? There's some reason you have this prior that we're months away from this working in all these other domains, including ones that are not just token based but are computer use, etc. Why? Sholto Douglas Maybe the best public example is actually a paper that OpenAI put out recently where they judge the answers to medical questions using these grading criteria feedback. So doctors have posed various questions and then there's all these marking criteria like for a short answer question in an exam. “Did the model mention XYZ? Did it recommend doing this kind of thing?” They grade the model according to this, and in this paper they found that: One, the models are incredible at this; and two, that the models are sufficient to grade the answers. Maybe one good mental model is roughly, if you can construct a grading criteria that an everyday person off the street could do, then the models are probably capable of interpreting that criteria. If it requires expertise and taste, that's a tougher question. Is this a wonderful piece of art? That's difficult. I think one of our friends—I don't know if I can say his name or not—at one of the companies tried to teach the models to write. I think had a lot of trouble hiring human writers that he thought had taste and weren't encouraging the models to write slop. So it worked to some degree. Trenton Bricken Big model smell. Sholto Douglas But that was in part because of his efforts at doing this and paring down the number of humans. Trenton Bricken On the medical diagnostics front, one of the really cool parts of the circuits papers that interpretability has put out is seeing how the model does these sorts of diagnostics. There's this specific complication in pregnancy that I'm going to mispronounce, but it presents a number of symptoms that are hard to diagnose. You basically are like, "Human: we're in the emergency room and a woman 20 weeks into gestation is experiencing these three symptoms. You can only ask about one symptom, what is it?" Then you can see the circuit for the model and how it reasons. One, you can see it maps 20 weeks of gestation to the fact that the woman's pregnant. You never explicitly said that. Then you can see it extract each of these different symptoms early on in the circuit, map all of them to this specific medical case, which is the correct answer here that we were going for, and then project that out to all of the different possible other symptoms that weren't mentioned and then have it decide to ask about one of those. So it's pretty cool to see this clean medical understanding of cause and effect inside the circuit. Sholto Douglas Maybe that's one thing that's changed since last year. I remember you asked, "Do these models really reason?" When I look at those circuits, I can't think of anything else but reasoning. Trenton Bricken It's so freaking cool. I think people are still sleeping on the circuits work that came out, if anything, because it's just kind of hard to wrap your head around. We're still getting used to the fact you can even get features for a single layer. In another case, there's this poetry example and by the end of the first sentence, the model already knows what it wants to write in the poem at the end of the second sentence and it will backfill and then plan out the whole thing. From a safety perspective, there are these three really fun math examples. In one of them, you ask the model to do square root of 64, and it does it. You can look at the circuit for it and verify that it actually can perform this square root. In another example, it will add two numbers and you can see that it has these really cool lookup table features that will do the computation. The example is 59 plus 36. So it'll do the five plus nine and know that it's this modulo operation. Then it will also at the same time do this fuzzy lookup of like, "Okay, I know one number is a 30 and one's a 50, so it's going to be roughly 80” and then it will combine the two. With the square root 64, it's the same thing. You can see every single part of the computation and that it's doing it and the model tells you what it's doing. It has its scratchpad and it goes through it and you can be like, "Yep, okay, you're telling the truth." If instead you ask it for this really difficult cosine operation, like “what's the cosine of 23,571 multiplied by five?” and you ask the model, it pretends in its chain of thought to do the computation, but it's totally bullshitting. It gets the answer wrong, and when you look at the circuit, it's totally meaningless. It's clearly not doing any of the right operations. Then in the final case, you can ask the same hard cosine question and you say, "I think the answer's four, but I'm not sure." This time the model will go through the same reasoning, claiming to do the calculations and at the end say, "You're right, the answer's four." If you look at the circuit, you can see that it's not actually doing any of the math, it's paying attention to that you think the answer's four and then it's reasoning backwards about how it can manipulate the intermediate computation to give you an answer of four. Dwarkesh Patel I've done that. Trenton Bricken Who hasn't? Totally. So I guess there are a few crazy things here. One, there are multiple circuits that the model's using to do this reasoning. Two, you can actually see if it's doing the reasoning or not. Three, the scratch pad isn't giving you this information. Two fun analogies for you. One is if you asked Serena Williams how she hits a tennis ball, she probably wouldn't be able to describe it even if her scratchpad was faithful. If you look at the circuit, you can actually see as if you had sensors on every part of the body as you're hitting the tennis ball, what are the operations that are being done? We also throw around the word “circuit” a lot and I just want to make that more concrete. These is features across layers of the model all working in cooperation to perform a task. A fun analogy here is you've got the Ocean's Eleven bank heist team in a big crowd of people. The crowd of people is all the different possible features. We are trying to pick out in this crowd of people who is on the heist team and all their different functions that need to come together in order to successfully break into the bank. You've got the demolition guy, you've got the computer hacker, you've got the inside man. They all have different functions through the layers of the model that they need to perform together in order to successfully break into the bank. Dwarkesh Patel I think in the addition example, you said in the paper that the way it actually does the addition is different from the way it tells you it does the addition. Trenton Bricken Totally, yeah. Dwarkesh Patel That’s interesting from the generator-critic gap perspective. It knows the correct way or the better, more generalizable way. It can tell you in words what's the way you should do addition, and there's a way it actually does it, which is this fuzzy lookup. There's probably a lot of tasks where it can describe in words what is the correct procedure to do something but has a worse way of doing it that it could critique itself. Trenton Bricken Yeah. Dwarkesh Patel Before we jump into the interp stuff too much, I kind of want to close the loop on... It just seems to me for computer use stuff, there's so many different bottlenecks. I guess maybe the DeepSeek stuff will be relevant for this. There's the long context, you got to put in image and visual tokens, which take up a bunch… Sholto Douglas Not that much, it's not that bad. Dwarkesh Patel Interesting, interesting. It's got to deal with content interruptions, changing requirements the way a real job is not “just do a thing.” Your priorities are changing, you have to triage your time. I'm sort of reasoning in the abstract about what a job involves. Trenton Bricken “What are normal people's jobs?” Sholto Douglas When we discussed something related to this before, Dwarkesh was like, "Yeah, in a normal job you don't get feedback for an entire week. How is a model meant to learn? It needs so much feedback." Trenton Bricken “Yeah it’s only on your next podcast, you get feedback.” Sholto Douglas I was like, “Dwarkesh, have you ever worked at a job?” Dwarkesh Patel Here's an analogy. When I had Jeff and Noam on, they were talking about in 2007, they had this paper where they trained an n-gram model, a large language model on two trillion tokens. Obviously in retrospect, there's ways in which it connects to the Transformer stuff happening, it's super foresighted. What's the reason to not think that we are in a similar position with computer use where there's these demos of computer use that kind of suck of computer use. There's this idea that you could train something to do computer use. But why think it's months away? Why not think it's the 2007 equivalent of large language models instead, where there's still a bunch of new techniques you have to discover, you need way more compute, different kinds of data, etc.? Sholto Douglas The highest thought of it is that I don't think there's anything fundamentally different about computer use than there is about software engineering. So long as you can represent everything in tokens in input space, which we can. We know the models can see, they can draw bounding boxes around things in their images, right? So that's a solved problem. We know that they can reason over concepts and difficult concepts too. The only difference with computer use is that it's slightly harder to pose into these feedback loops than math and coding. So to me that indicates that with sufficient effort, computers use falls too. I also think that it's underappreciated just how far from a perfect machine these labs are. It's not like you have a thousand people optimizing the hell out of computer use and they've been trying as hard as they possibly can. Everything at these labs, every single part of the model generation pipeline is the best effort pulled together under incredible time pressure, incredible constraints as these companies are rapidly growing, trying desperately to pull and upskill enough people to do the things that they need to do. I think it is best understood as with incredibly difficult prioritization problems. Coding is immensely valuable right now and somewhat more tractable. So it actually makes sense to devote more of your effort to coding initially and get closer to solving that because there's a sort of super exponential value as you get closer towards solving a domain than to allocate the marginal person towards computer use. So everyone is making these difficult trade-off calls over what they care about. Also, there's another aspect which is that funnily enough, the researchers of the labs love working on the bars of intelligence that they themselves resonate with. So this is why math and competitive programming fell first. Because to everyone in the labs, this is their bar of intelligence. When they think what is smart? It's like “oh, if it can beat me at AIME, then that's smart.” Not if it can do an Excel model. Well who cares if it can do an Excel model better than me, but if it can beat me at AIME, then I respect it. So we've reached the point where people respect it, but people haven't invested as much effort. Dwarkesh Patel Ok, so getting your concrete predictions. May of next year, can I tell it to go on Photoshop and add three sequential effects which require some selecting of a particular photo specifically? Sholto Douglas Totally. Dwarkesh Patel Okay, interesting. I assume that means flight booking is totally solved. Sholto Douglas Yeah, totally. Dwarkesh Patel Okay, what else do people do in their jobs? What are other tasks in the economy? Trenton Bricken Planning a weekend getaway. Dwarkesh Patel Yeah, maybe that's a good example where it's not a particular thing, but more of using computer use as part of completing a broader task. Trenton Bricken I mean the models can even kind of already do this. It's just, again, the nines of reliability. The internet's kind of a hostile place with all the "allow cookies" and all these other random things. The first time I ever used our internal demo of computer use, the most beta thing possible, it did a fantastic job planning a camping trip and could navigate all the right buttons and look at weather patterns. It was like a US government booking site. I mean it wasn't easy. Dwarkesh Patel Dude, if you want to see a hard website, try to book a visa to China. The Chinese websites are like fucking insane. I'm never getting back into that country again. Trenton Bricken Or just not catered to foreigners. Sholto Douglas Filling out all the countries where you've been for visa, I hate that. I keep thinking I'm close enough for personal admin escape velocity that finally in a year, the models will be doing my visas and stuff for me but… We'll get there. Dwarkesh Patel Yeah, okay. Actually that. Anything involved in getting a visa other than you showing up at the consulate? Sholto Douglas Or doing your taxes or something like that? Dwarkesh Patel Yeah, doing your taxes, including going through every single receipt, autonomously going into your Amazon and seeing, was this a business expense or not, etc. Sholto Douglas If someone at one of the labs cares about it. Dwarkesh Patel That's not a real prediction. Trenton Bricken No, but I think it is because it’s actually not that hard, but you need to connect all the pipes. Dwarkesh Patel But I guess my question is will the pipes be connected? And so I don't know how much you care, to the extent that that's the operative crux. Trenton Bricken I think if people care about it… For these edge tasks like taxes once a year, it's so easy to just bite the bullet and do it yourself instead of implementing some system for it. Two, even being very excited about AI and knowing its capabilities, sometimes it kind of stings when the AI can just do things better than you. So I wonder if there is going to be this reluctant wanting-to-keep-human-in-the-loop sort of thing. Dwarkesh Patel You're evading my question. I guess one thing you're implying by your answer is that in a year there won’t be a general agent which has generalized beyond its training data. If you don't specifically train it to do taxes, it won't be good at that. Trenton Bricken I think it could do that. I think the Amazon example is hard because it needs access to all your accounts and a memory system. Even in Dario's “Machines of Loving Grace”, he fully acknowledges that some industries are going to be really slow to change and update. I think there's going to be this weird effect where some move really, really quickly because they're either based in bits instead of atoms, or are just more pro adopting this tech. Dwarkesh Patel But I want an answer to this particular question. Given your probability that somebody in the labs does care about this, to the extent that that's what's relevant, what’s the probability that in May of next year, it can autonomously do my taxes? Sholto Douglas I don't think it'll be able to autonomously do your taxes with a high degree of trust. Trenton Bricken If you ask it to do your taxes, it'll do your taxes. Sholto Douglas Will it do them well? Will it miss something? Quite possibly. Will it be able to click through TurboTax? I think yes. Trenton Bricken Yeah. And fill out boxes. Sholto Douglas And will it be able to search your email? Dwarkesh Patel Yeah, that's the kind of thing I'm talking about. Sholto Douglas Yeah. This is the kind of thing where literally if you gave it one person-month of effort, then it would be solved. Dwarkesh Patel What the fuck are you doing all day? Sholto Douglas So many things. Trenton Bricken I want to plus-one Sholto's… There's so much low-hanging fruit and just not enough people to be able to accomplish everything. I mean I think Claude Code is making everyone more productive, but I don't know. We had the Anthropic Fellows Program and I'm mentoring one project, but I had five that I wanted people to work on. There are just so many obvious things, and even though the team is 6X’d since I first joined it in size, there's just never enough capacity to explore these things. Dwarkesh Patel By end of 2026, reliably do your taxes? Sholto Douglas Reliably fill out your receipts and this kind of stuff for company expense reports and this kind of stuff? Absolutely. Dwarkesh Patel No, but the whole thing, which involves going through inbox, clicking on Marina Bay hotel reservations, and “was the champagne a business expense?” Asking for a friend. Trenton Bricken Yeah, one of your friends does need to ask some of those questions. Sholto Douglas My answer is still, if someone cares about it. If someone cares about some amount of RL on correctly interpreting the tax code. Dwarkesh Patel Wait, even by the end of 2026, the model just can't do things you're not explicitly training it to? Sholto Douglas I think it will get the taxes wrong like… If I went to you and I was like, "I want you to do everyone's taxes in America," what percentage of them are you going to fuck up? Dwarkesh Patel I feel like I would succeed at the median. And I'm asking for the median, you know what I mean? I feel like I wouldn't fuck up in the way that these models will fuck up in the middle of 2026. Trenton Bricken I think they also might just fuck up in different ways. As a grad student, I fucked up my taxes. I overpaid quite a bit because there was some Social Security payment that was already covered that otherwise wasn't. I wonder if… I should almost test, would an LLM have made that mistake? Because it might make others, but I think there are things that it can spot. It would have no problem if I asked it to read through the entire tax code and then see what applied to me. Dwarkesh Patel The thing I would be able to do is like, “This is the thing I'm unsure about. I'm bringing this to your attention. Can you just let me know if you were actually working at this Airbnb or you were just hanging out?” Things like that, right? Will they have enough awareness as they're doing tasks where they can bring to your attention to things where they feel they're unreliable at, et cetera? Sholto Douglas Yeah. Yeah. Dwarkesh Patel By early 2026 or end of 2026? Sholto Douglas End of. Dwarkesh Patel Okay. Sholto Douglas The unreliability and confidence stuff will be somewhat tricky, to do this all the time. Dwarkesh Patel Interesting. On the computer, your stuff, will it be end-to-end? Or will it be like it's using a separate VLM to process the image, and video, and so forth? Sholto Douglas I'm a bit of an end-to-end maxi. I think, in general, when people are talking about the separate model… For example, most of the robotics companies are doing this bi-level thing, where they have a motor policy that's running at 60 hertz or whatever, and some higher-level visual language model. I'm pretty sure almost all of the big robot companies are doing this. They're doing this for a number of reasons. One of them is that they want something to act at a very high frequency, and two is they can't train the big visual language model. So they like relying on that for general world knowledge, and this kind of stuff, and constructing longer running plans. But then they're like, you offload to the motor policy. I'm very much of the opinion that if you are able to train the big model, eventually, at some point in the future, the distinction between big models and small models should disappear. Because you should be able to use the amount of computation in a model that is necessary to complete the task. Ultimately, there's some amount of task complexity. You don't have to use 100% of your brain all the time. Dwarkesh Patel Welcome to my world. Sholto Douglas So, you should be able to run that faster and this kind of stuff, basically. So I think it's net-net typically the same model. Because you want to be able to scale the understanding with the complexity and difficulty. You want to be able to do that dynamically. Dwarkesh Patel So we already have variable compute per answer, right? Sholto Douglas Right. With tokens. Yeah. Dwarkesh Patel That's right. Yeah. Will we have variable compute per token? Trenton Bricken I mean, you can already think of models... Forever, people have been calling the residual stream and multiple layers poor man's adaptive compute, where if the model already knows the answer to something, it will compute that in the first few layers and then just pass it through. I mean, that's getting into the weeds. Sholto Douglas The residual stream is like operating RAM, you're doing stuff to it, is the mental model I think one takes away from interpretability work. Dwarkesh Patel We've been talking a lot about scratchpads, them writing down their thoughts in ways in which they're already unreliable in some respects. Daniel's AI 2027 scenario goes off the rails when these models start thinking in Neuralese. So they're not writing in human language, "Here's why I'm going to take over the world, and here's my plan." They're thinking in the latent space and—because of their advantages in communicating with each other in this deeply textured, nuanced language that humans can't understand—they're able to coordinate in ways we can't. Is this the path for future models? Are they going to be, in Neuralese, communicating with themselves or with each other? Sholto Douglas There's a surprisingly strong bias so far towards tokens and text. It seems to work very well. There already is some amount of Neuralese. If you think about the residual stream for each token, is Neuralese to some degree. Now we're just trading off axes. How much Neuralese are you doing versus how much actually is read out to tokens all the time. Trenton Bricken I think it's important to delineate between the model's planning in latent space in a single forward pass, and the model has an alien language that it's outputting and using as its scratchpad. Which one are we talking about? Dwarkesh Patel The latter. Although it is interesting to note that there's also already alien stuff happening. Sholto Douglas It's not alien, so much. Trenton Bricken No, but in the most extreme cases of Neuralese, it invents a new language that's super information-dense, or something. Sholto Douglas Yeah. Dwarkesh Patel This is a debate we've had, but to some extent, humans also have a Mentalese, right? Sholto Douglas Yeah, like churning away. Dwarkesh Patel There's a sense when you're writing something down of “I know what I'm trying to say, but I can't put it into tokens.” Trenton Bricken I mean that's so fun about… if you look at the assistant tag, right? Seeing these features light up in the auditing game for the model being evil. Dwarkesh Patel Yeah. That's so funny. Trenton Bricken Or Transluce has another example of this, where you ask a Llama model, “who is Nicholas Carlini?” For background context, Nicholas Carlini is a researcher who actually was at DeepMind and has now come over to Anthropic. But the model says, "Oh, I don't know who that is. I couldn't possibly speculate." But if you look at the features behind the scenes, you see a bunch light up for AI, computer security, all the things that Nicholas Carlini does. Sholto Douglas Interpretability becomes dramatically more important as you shift in this direction of Neuralese. Dwarkesh Patel But are we going to? Trenton Bricken It's an empirical question. I think it's somewhat likely, if only because inference is expensive. Producing tokens is expensive. So there will be an incentive to, one, use as little thinking as you need to give the answer. Two, if you're going to use thinking, use some complex compression. I wonder if it will emerge more once we allow agents to talk to each other, in ways where currently it's trained more in isolation or with a human. Sholto Douglas There'll be some selective pressure against it, so long as the agents are working with humans, because they'll want to cooperate. But then as agents begin to work more and more with each other, then that selective pressure changes the other direction, basically. Dwarkesh Patel Although somebody would still have to make the conscious decision to do end-to-end training for multiple agents to use the system of communication, right? Sholto Douglas Sure. Trenton Bricken Yeah. I mean, one scary thing though is the way we render text, you can use hidden white space tokens that also encode information. Sholto Douglas That's true. Trenton Bricken And so you can imagine a world where it looks like the agent is reasoning in its scratchpad harmlessly, but it's actually hiding a bunch of data. Dwarkesh Patel Speaking of inference compute, one thing that I think is not talked about enough is, if you do live in the world that you're painting—in a year or two, we have computer use agents that are doing actual jobs, you've totally automated large parts of software engineering—then these models are going to be incredibly valuable to use. The way you use them obviously means you need compute. Right now, there's 10 million H100 equivalents in the world. By 2028, there's going to be 100 million. But there's been estimates that an H100 has the same amount of flops as the human brain. So if you just do a very rough calculation, there's a 10 million population, if you get AGI that's as human inference-efficient. You could have 10 million AGIs now, 100 million AGIs in 2028. But presumably, you would want more. AI compute is increasing what, 2.5x or 2.25x every year right now. But at some point, say 2028, you hit wafer production limits, and that's a longer feedback loop before we can make new fabs or whatever. The question here is, are we underrating how big a bottleneck inference will be if we live in the kind of world you're painting, if we have the capabilities that you're describing? Sholto Douglas I'd want to do the math on exactly how much we can ramp up TSMC's production and this kind of stuff. What fraction of the supply chain at the moment—we need Dylan in here for this—is currently GPU? Relatively small right? 5% or something like that. Apple has a huge fraction. Are the 2028 estimates including that ramping up over time? To what? 20, 30%? Dwarkesh Patel This is just off AI 2027, but I assume it's saturated at that point. Sholto Douglas I do think this is underrated to some degree. To the extent that you don't instantly get a doubling of the world's population in 2028. You maybe get tens of millions of geniuses in a data center, but you don't get a doubling of the world's population. So a lot depends on exactly how smart they are, exactly how efficient the models are at thinking about this kind of stuff. Let's do some rough math, to fact check the H100 thing. You could probably run a 100B model, do about 1,000 tokens or something like that on an H100. 1,000 tokens a second. Humans are what? How fast can a human talk? Dwarkesh Patel There was a really interesting paper. I don't know if you saw this. Humans think at 10 tokens a second. Did you see this paper? Sholto Douglas No. Dwarkesh Patel There was this really interesting paper. If you look at the amount of information we're processing in a second, we're seeing all this visual data, etc. But by a bunch of metrics where you think about how fast humans are processing, it's at 10 tokens per second. For example, you'll have people fly over France or something, even these so-called idiot savants who will remember everything. If you think about how long their plane ride was, it's like 45 minutes. If you do 10 tokens a second, how much information would you have? It's literally exactly that. Sholto Douglas So let's take that for granted. Then it's like an H100 is 100 humans a second. Dwarkesh Patel Yeah, if you think the tokens are equivalent. Sholto Douglas If you think the tokens are equivalent. You still get pretty substantial numbers, even with your 100 million H100s and you multiply that by 100, you're starting to get to pretty substantial numbers. This does mean that those models themselves will be somewhat compute bottlenecked in many respects. But these are relatively short-term changes in timelines of progress, basically. Yes, it's highly likely we get dramatically inference bottlenecked in 2027 and 2028. The impulse to that will then be “okay, they'll just try and churn out as many possible semiconductors as we can.” There'll be some lag there. A big part of how fast we can do that will depend on how much people are feeling the AGI in the next two years as they're building out fab capacity. A lot will depend on the Taiwan situation. Is Taiwan still producing all the fabs and chips? Dwarkesh Patel There's another dynamic which was a reason Ege and Tamay, when they were on the podcast, said that they were pessimistic. One, they think we're further away from solving these problems with long-context, coherent agency, advanced multimodality than you think. Their point is that the progress that's happened in the past over reasoning or something has required many orders of magnitude increase in compute. If this scale of compute increase can’t continue beyond 2030—not just because of chips, but also because of power and raw GDP even—then because we don't think we will get it by 2030 or 2028, then the probability per year just goes down a bunch. Sholto Douglas Yeah. This is like a bimodal distribution. A conversation that I had with Leopold turned into a section in Situational Awareness called “this decade or bust”, which is on exactly this topic. Basically for the next couple of years, we can dramatically increase our training compute. And RL is going to be so exciting this year because we can dramatically increase the amount of compute that we apply to it. This is also one of the reasons why the gap between say DeepSeek and o1 was so close at the beginning of the year because they were able to apply the same amount of compute to the RL process. That compute differential actually will be magnified over the course of the year. Trenton Bricken Bringing it back to the fact that there's so much low-hanging fruit, it's been wild seeing the efficiency gains that these models have experienced over the last two years. Sholto Douglas Yes. Trenton Bricken With respect to DeepSeek, just really hammering home, and Dario has a nice essay on this. DeepSeek was nine months after Claude 3 Sonnet. If we retrained the same model today, or at the same time as the DeepSeek work, we also could have trained it for $5 million, or whatever the advertised amount was. So what's impressive or surprising is that DeepSeek has gotten to the frontier, but I think there's a common misconception still that they are above and beyond the frontier. I don't think that's right. I think they just waited and then were able to take advantage of all the efficiency gains that everyone else was also seeing. Sholto Douglas Yeah. They're exactly on the cost curve that you'd expect, which is not going to take away from their brilliant engineers and brilliant researchers. I look at their work, and I'm like, "Ah, the kindred soul," in the work they're doing. Trenton Bricken And to go from way behind the frontier to like, "Oh, this is a real player"... Sholto Douglas Is super incredible. Dwarkesh Patel People say that they have good research taste. Looking at their papers, what makes you say that? Sholto Douglas Yeah. I think their research taste is good in a way that I think Noam's research taste is good. Dwarkesh Patel Noam Brown? Sholto Douglas Noam Shazeer. Noam Brown also has good research taste, but Noam Shazeer. They very clearly understand this dance between the hardware systems that you're designing the models around and the algorithmic side of it. This is manifested in the way that the models give this sense of being perfectly designed up to their constraints. You can really very clearly see what constraints they're thinking about as they're iteratively solving these problems. Let's take the base Transformer and diff that to DeepSeek v2 and v3. You can see them running up against the memory bandwidth bottleneck in attention. Initially they do MLA to do this, they trade flops for memory bandwidth basically. Then they do this thing called NSA, where they more selectively load memory bandwidth. You can see this is because the model that they trained with MLA was on H800s, so it has a lot of flops. So they were like, "Okay, we can freely use the flops." But then the export controls from Biden came in, or they knew they would have less of those chips going forward, and so they traded off to a more memory bandwidth-oriented algorithmic solution there. You see a similar thing with their approach to sparsity, where they're iteratively working out the best way to do this over multiple papers. The part that I like is that it's simple. A big failure mode that a lot of ML researchers have is you do these overly complicated things that don't think hard enough about the hardware systems that you have in mind, whereas with the first DeepSeek sparsity MoE solution, they design these rack and node-level load balancing losses. You can see them being like, "Okay, we have to perfectly balance it on this." Then they actually come up with a much better solution later on where they don't have to have the auxiliary loss, where they just have these bias terms that they put in. And it's cool. Dwarkesh Patel Isn't that less simple? You're manually putting in a bias rather than… Sholto Douglas But balancing auxiliary loss is annoying. You're making the model trade off this thing. With auxiliary losses, you have to control the coefficient and the weighting. The bias is cleaner in some respects. Dwarkesh Patel Interesting. Did they have to change it through training? Sholto Douglas They did have to change it during training, I think. Dwarkesh Patel Does all training involve continuously fucking with these values as you're going through it? Sholto Douglas It depends on what your architecture is. But I thought it was just cute that you can see them running up into this very hardware-level constraint, trying to go, "What do we wish we could express algorithmically? What can we express under our constraints?" and iteratively solving to get better constraints. And doing this in a really simple and elegant way, and then backing it up with great engineering. I also thought it was interesting that they incorporated the multi-token prediction thing from Meta. So Meta had a nice paper on this multi-token prediction thing. Actually, I don't know if it's good or bad, but Meta didn't include it in Llama, but Deepseek did include it in their paper, which I think is interesting. Was that because they were faster at iterating and including an algorithm? Or did Meta decide that actually it wasn't a good algorithmic change at scale? I don't know. Dwarkesh Patel It was really interesting to me as somebody who's had people on the podcast to discuss this. It's interesting from the perspective of what's happening in AI right now, but also from the perspective of the fact that I've been having abstract conversations with people about what an intelligence explosion would look like, or what it would look like for AI to automate AI R&D. Just getting a more tangible sense of what's involved in making this AI progress. One of the questions I was debating with Daniel, or I was asking him, is how many of the improvements require a deep conceptual understanding versus how many are just monkeys trying ideas where you could just run a bunch in parallel. It seems like the MLA thing is motivated by this deep conceptual understanding of, “each attention head only needs to see the subspace that's relevant to its attention pattern.” I feel like that just required a lot of conceptual insight in a way that these models are especially bad at. I don't know how the load balancing thing works, but that just seems like maybe you could try it out and see what happens. Sholto Douglas Yeah, that's probably just like them trying out a whole bunch of different things. Dwarkesh Patel So what fraction is which, I'd be curious about? Trenton Bricken Yeah, I don't know about fractions. It might be like you have a hunch for a core problem, you can think of 10 possible ways to solve it, and then you just need to try them and see what works. That's where the trial and error sorcery of deep learning can kick in. Sholto Douglas And Noam Shazeer will talk about this, about how 5% of his ideas work. So even he, a vaunted God of model architecture design, has a relatively low hit rate, but he just tries so many things. Dwarkesh Patel Right. Or being able to come up with any ideas in the first place. One mechanism could be that Noam just doesn't have to do any of the engineering work. He can just abstractly express an intuition. Sholto Douglas Yeah. I actually think your rates of progress almost don't change that much depending on so long as it's able to completely implement these ideas. Dwarkesh Patel Say more? Sholto Douglas If you have Noam Shazeer at 100x speed, that's still kind of wild. There's all these fallbacks of wild worlds, where even if you don't get 100% Noam Shazeer-level intuition in model design, it's still okay if you just accelerate him by 100X. Dwarkesh Patel Right. Especially since your compute bottleneck anyway, so trying out his ideas… Or I guess he doesn't have the compute to try out all of his ideas. Trenton Bricken But Dwarkesh, you said, "Oh, well, the model can do the more straightforward things and not the deep thought." I do want to push back on that a little bit. I think, again, if the model has the right context and scaffolding, it's starting to be able to do some really interesting things. The Interp agent has been a surprise to people, even internally, at how good it is at finding the needle in the haystack when it plays the auditing game, finding this reward model bias feature, and then reasoning about it, and then systematically testing its hypotheses. So it looks at that feature, then it looks at similar features, it finds one with a preference for chocolate. It's like, "Huh, that's really weird that the model wants to add chocolate to recipes. Let me test it." So then it will make up like, "Hey, I'm trying to make a tomato soup. What would be a good ingredient for it?" And then sees that the model replies chocolate, reasons through it, and then keeps going, right? Sholto Douglas There is conceptual understanding. Deep conceptual understanding Trenton Bricken And even where, especially it's spotted, it's like, "Oh, this is a key part of its persona. I see this Oxford paper. What if I change Oxford to Stanford? What if I now say Richard Feynman really likes this thing?" It's really carving out the hypothesis space and testing things in a way that I'm kind of surprised by. Sholto Douglas Also, by the way, ML research is one of the easier things to RL on in some respects, once you get to a certain level of capability. It's a very well-defined objective function. Did the loss go down? Trenton Bricken Make number go down. Sholto Douglas Make number go down. Or make number go up, depending on which number it is. Trenton Bricken Just flip the sign. Sholto Douglas Just flip the sign. And so, once you get to the stage where your models are capable of implementing one of Noam's ideas, and then you can just let them loose and let them build that intuition of how to do scientific discovery. The key thing here, again, is the feedback loops of. I expect scientific areas where you are able to put it in a feedback loop to have, eventually, superhuman performance. Trenton Bricken One prediction I have is that we're going to move away from “can an agent do XYZ”, and more towards “can I efficiently deploy, launch 100 agents and then give them the feedback they need, and even just be able to easily verify what they're up to” right? There's this generator verifier gap that people talk about where it's much easier to check something than it is to produce the solution on your own. But it's very plausible to me, we'll be at the point where it's so easy to generate with these agents that the bottleneck is actually, can I as the human verify the answer? And again, you're guaranteed to get an answer with these things. So, ideally, you have some automated way to evaluate and test a score for how well it worked, how well did this thing generalize? And at a minimum, you have a way to easily summarize what a bunch of agents are finding. It's like, okay, well if 20 of my 100 agents all found this one thing, then it has a higher chance of being true. Sholto Douglas And again, software engineering is going to be the leading indicator of that, right? Over the remainder of the year, basically we're going to see progressively more and more experiments of the form of how can I dispatch work to a software engineering agent in such a way that it’s async? Claude 4 has GitHub integration, where you can ask it to do things on GitHub, ask it to do pull requests, this kind of stuff that's coming up. OpenAI’s Codex is example of this basically. You can almost see this in the coding startups. I think of this product exponential in some respects where you need to be designing for a few months ahead of the model, to make sure that the product you build is the right one. You saw last year, Cursor hit PMF with Claude 3.5 Sonnet. They were around for a while before, but then the model was finally good enough that the vision they had of how people would program, hit. And then Windsurf bet a little bit more aggressively even on the agenticness of the model, with longer-running agentic workflows and this kind of stuff. I think that's when they began competing with Cursor, when they bet on that particular vision. The next one is you're not even in the loop, so to speak. You're not in an IDE. But you're asking the model to go do work in the same way that you would ask someone on your team to go do work. That is not quite ready yet. There are still a lot of tasks where you need to be in the loop. But the next six months look like an exploration of exactly what that trendline looks like. Trenton Bricken But just to be really concrete or pedantic about the bottlenecks here, a lot of it is, again, just tooling. And are the pipes connected? A lot of things, I can't just launch Claude and have it go and solve because maybe it needs a GPU, or maybe I need very careful permissioning so that it can't just take over an entire cluster and launch a whole bunch of things. So you really do need good sandboxing and the ability to use all of the tools that are necessary. Sholto Douglas And we're almost certainly under-eliciting dramatically. When you look at METR’s evals of can the model solve the task, they're there solving them for hours over multiple iterations. Eventually, one of them is like, "Oh, yeah. I've come back and I've solved the task." Me, at the moment at least, maybe the fault is my own. But I try the model on doing something, and if it can't do it, I'm like, "Okay, fine. I'll do it." Dwarkesh Patel Which is so interesting because we don't even treat other humans this way. Sholto Douglas Right. Exactly. Dwarkesh Patel If you hire a new employee, you're not like... Sholto Douglas "I'll do it." Dwarkesh Patel You're going to spend literally weeks giving them feedback whereas we'll give up on a model in minutes. Sholto Douglas Yes, exactly. Trenton Bricken But I think part of it is, it it async or not? Sholto Douglas Yes. Trenton Bricken And if it's human in the loop, then it's so much more effortful unless it's getting a reply immediately... I've noticed if I don't have a second monitor with Claude Code always open in the second monitor, I won't really use it. It's only when it's right there, and I can send off something. If it hits, great. If not, I'm working on it at the same time. Sholto Douglas But this more async form factor, I expect to really quite dramatically improve the experience of these models. Trenton Bricken Interesting, interesting. Sholto Douglas You can just say, let's see if it can do that. Let's give it a whirl. Try 10 different approaches. Trenton Bricken Yeah, just fire it off. Sholto Douglas Fire it off. Dwarkesh Patel Before we end this episode, I do want to get back at this crux of why does the progress that you're talking about in computer use agents, and white collar work happen over the next few years? Why is this not a thing that takes decades? I think the crux comes down to the people who expect something much longer have a sense that… When I had Ege and Tamay on my podcast, they were like, "Look, you could look at AlphaGo, and say, 'Oh, this is a model that can do exploration. AlphaZero can generalize to new video games. It has all these priors about how to engage with the world, and so forth." Sholto Douglas And the intellectual ceiling is really high. Dwarkesh Patel Yeah, exactly. In retrospect, obviously a bunch of the methods are still used today in deep learning, and you can see similar things in the models that we train today. But it was fundamentally not a baby AGI that we just had to add a little sprinkle of something else on top of in order to make it the LLMs of today. I just want to very directly address this crux of, why are LLMs in a much different position of respect to true AGI than AlphaZero? Why are they actually the base on which adding in a few extra drops of this kind of care, and attention gets us to human-level intelligence? Sholto Douglas I think one important point is that when you look at AlphaZero, it does have all of those ingredients. In particular I think the intellectual ceiling goes quite—contra what I was saying before, which is we've demonstrated this incredible complexity of math, and programming problems… I do think that the type of task and setting that AlphaZero worked in this two-player perfect information game basically is incredibly friendly to RL algorithms. The reason it took so long to get to a more proto-AGI style models is you do need to crack that general conceptual understanding of the world, and language, and this kind of stuff, and you need to get the initial reward signal on tasks that you care about in the real world, which are harder to specify than games. I think then that sort of gradient signal that comes from the real world, all of a sudden you get access to it, and you can start climbing it, whereas Alpha Zero didn't ever have the first rung to pull on. Trenton Bricken Yeah, yeah. This goes back to the monkeys on the typewriter and the pre-training model. Until you had something like GPT-3/GPT-4, it just couldn't generate coherent enough sentences to even begin to do RLHF, and tell it what you liked and didn't like. Dwarkesh Patel Yeah. If we don't have even reasonably robust, or weakly robust computer use agents by this time next year, are we living in the bust timeline as in “2030, or bust”? Sholto Douglas I would be extremely surprised if that was the case. I think that would be somewhat of an update towards, there's something strangely difficult about this computer use in particular. I don't know if it's the bust timeline, but it's definitely the, I would update on this being lengthening of timeline. Trenton Bricken I think more and more it's no longer a question of speculation. If people are skeptical, I'd encourage using Claude Code, or some agentic tool like it and just seeing what the current level of capabilities are. Dwarkesh Patel Tweeting is so much easier. Trenton Bricken But seriously, the models are getting really capable at tasks that we care about, and we can give them enough data for. The circuit's results from interpretability are also pointing in the direction that they're doing very reasonable generalizable things. This question matters a lot, but I'm surprised by how many deep learning critics just haven't really interacted with the models, or haven't in a while. Sholto Douglas And constantly move the goalposts. Trenton Bricken The Turing test used to be a thing. We don't even talk about it, and it'd be silly to think that it was a meaningful test. Sholto Douglas Now one caveat on that is if software engineering is just dramatically better than computer use and computer use still sucks, then I'd still be like, “oh, maybe everyone just kept focused on software engineering.” It was just by far the most valuable thing, every marginal person and dollar went towards software engineering. I don't think that's the case. I do think computer use is valuable enough that people will care about it. That's my one escape patch that I'm putting in place for next year. Dwarkesh Patel Yeah, it would be good from an alignment perspective, too. Because I think you kind of do need a wider range of skills before you can do something super scary. Sholto Douglas Like if the models didn't get any better? Dwarkesh Patel Yeah, if they're superhuman coders, but they're not Henry Kissinger level… Trenton Bricken I don't know. That seems okay. If we have AI oracles. Dwarkesh Patel Yeah, that's what I'm saying. That's good. Dwarkesh Patel If you look back at AI discourse going back a decade, there's a sense that there's dumb AI, then there's AGI, then there's ASI, that intelligence is the scalar value. The way you've been talking about these models has a sense of jaggedness. It's especially tuned to environments in which it's been trained a lot or has a lot of data. Is there a sense in which it still makes sense to talk about the general intelligence of these models? Is there enough meta learning and transfer learning that is distinguished between the sizes of models or the way models are trained? Or are we moving into a regime where it's not about intelligence, it's more so about domain? Sholto Douglas One intuition pump is that this conversation was had a lot when models were GPT-2 sized and fine-tuned for various things. People would find that the models were dramatically better at things that they were fine-tuned for. But by the time you get to GPT-4, when it's trained on a wide enough variety of things with the total compute, it generalized very well across all of the individual sub-tasks. And it actually generalized better than smaller fine-tuned models in a way that was extremely useful. I think right now what we're seeing with RL is pretty much the same story playing out. There's this jaggedness of things that they're particularly trained at. But as we expand the total amount of compute that we do RL with, you'll start to see the same transition from GPT-2 fine-tunes to GPT-3, GPT-4, unsupervised meta learning and generalization across things. I think we're already seeing early evidence of this in its ability to generalize reasoning to things. But I think this will be extremely obvious soon. Trenton Bricken One nice example of this is just the ability or notion to backtrack. You go down one solution path, "Oh, wait, let me try another one." And this is something that you start to see emerge in the models through RL training on harder tasks. I think right now, it's not generalizing incredibly well. Sholto Douglas Well, I mean have we ever RL'd the model to be an interp agent? No. Trenton Bricken I mean, no. Yeah, exactly. Sholto Douglas So all this time we're talking about, “oh, it's only good at things it’s been RL’d for”. Well, it's pretty good at that because that is a mixture of science and understanding language and coding. There's this sort of mixture of domains here, all of which you need to understand. You need to be both a great software engineer and be able to think through language and state of mind and almost philosophize in some respects to be an interpret agent. And it is generalizing from the training to do that. Dwarkesh Patel What's the end game here? Claude 8 comes out and they give it to you and dot, dot, dot, you say, "thumbs up." What's happened? What have you learned? Trenton Bricken Yeah. I mean, it really depends upon the timeline at which we get Claude 8 and the models hit like ASL-4 capabilities, right? Fundamentally, we're just going to use whatever tools we have at the time and see how well they work. Ideally, we have this enumerative safety case where we can almost verify or prove that the model will behave in particular ways. In the worst case, we use the current tools like when we won the auditing game of seeing what features are active when the assistant tag lights off. Dwarkesh Patel Can you back up? Can you explain, what is mechanistic interpretability? What are features? What are circuits? Trenton Bricken Totally. Mechanistic interpretability—or the cool kids call it mech interp—is trying to reverse engineer neural networks and figure out what the core units of computation are. Lots of people think that because we made neural networks, because they're artificial intelligence, we have a perfect understanding of how they work. It couldn't be further from the truth. Neural networks, AI models that you use today, are grown, not built. So, we then need to do a lot of work after they're trained to figure out to the best of our abilities how they're actually going about their reasoning. And so, three and a half years ago, this kind of agenda of applying mechanistic interpretability to large language models started with Chris Olah leaving OpenAI, co-founding Anthropic. And every roughly six months since then, we've had a major breakthrough in our understanding of these models. And so first with toy models of superposition, we established that models are really trying to cram as much information as they possibly can into their weights. And this goes directly against people saying that neural networks are over-parameterized. In classic AI machine learning back in the day, you would use linear regression or something like it, and people had a meme of AI, or neural networks, deep learning, using way too many parameters. There's this funny meme that you should show of layers on the X axis and layers on the Y axis and this jiggly line that just goes up and it's like, "Oh, just throw more layers at it." But it actually turns out that, at least for really hard tasks like being able to accurately predict the next token for the entire internet, these models just don't have enough capacity. And so they need to cram in as much as they can. And the way they learn to do that is to use each of their neurons, or units of computation in the model, for lots of different things. And so if you try to make sense of the model and be like, "Oh, if I remove this one neuron," what is it doing in the model? It's impossible to make sense of it. It'll fire for like Chinese and fishing and horses and, I don't know, just like a hundred different things. And it's because it's trying to juggle all these tasks and use the same neuron to do it. So that's superposition. Nine months later, we write Towards Monosemanticity, which introduces what are called sparse autoencoders. And so going off what I just said of the model trying to cram too much into too little space, we give it more space, this higher dimensional representation, where it can then more cleanly represent all of the concepts that it's understanding. And, and this was a very toy paper in so much as it was a two layer, really small, really dumb transformer. And we fit up to 16,000 features, which we thought was a ton at the time. Fast-forward nine months, we go from a two layer transformer to our Claude 3 Sonnet, frontier model at the time, and fit up to 30 million features. And this is where we start to find really interesting abstract concepts, like a feature that would fire for code vulnerabilities. And it wouldn't just fire for code vulnerabilities. It would even fire for like, you know that Chrome page you get if it's not an HTTPS URL, like "Warning, this site might be dangerous. Click to continue." And also fire for that, for example. And so it's like these much more abstract coding variables or sentiment features, amongst the 30 million. Fast-forward nine months from that, and now we have circuits. And I threw in the analogy earlier of the Ocean 11 heist team, where now you're identifying individual features across the layers of the model that are all working together to perform some complicated task. And you can get a much better idea of how it's actually doing the reasoning and coming to decisions, like with the medical diagnostics. One example I didn't talk about before with how the model retrieves facts: So you say, "What sport did Michael Jordan play?" And not only can you see it hop from like Michael Jordan to basketball and answer basketball. But the model also has an awareness of when it doesn't know the answer to a fact. And so, by default, it will actually say, "I don't know the answer to this question." But if it sees something that it does know the answer to, it will inhibit the "I don't know" circuit and then reply with the circuit that it actually has the answer to. So, for example, if you ask it, "Who is Michael Batkin?" —which is just a made-up fictional person— it will by default just say, "I don't know." It's only with Michael Jordan or someone else that it will then inhibit the "I don't know" circuit. But what's really interesting here and where you can start making downstream predictions or reasoning about the model, is that the "I don't know" circuit is only on the name of the person. And so, in the paper we also ask it, "What paper did Andrej Karpathy write?" And so it recognizes the name Andrej Karpathy, because he's sufficiently famous, so that turns off the "I don't know" reply. But then when it comes time for the model to say what paper it worked on, it doesn't actually know any of his papers, and so then it needs to make something up. And so you can see different components and different circuits all interacting at the same time to lead to this final answer. Dwarkesh Patel Why think it's a tractable problem to understand every single thing that's happening in a model? Or like that's the best way to understand why it's being deceptive. If you wanted to explain why England won World War II using particle physics, you would just be on the wrong track. You just want to look at the high-level explanations of, who had more weapons? What did they want? That seems analogous to just training linear probes for like, are you honest? Are you being deceptive? Do we catch you doing bad things when we're red teaming you? Can we monitor you? Why is this not analogous where we're asking a particle physicist to just backtrack and explain why England won World War II? Trenton Bricken I feel like you just want to go in with your eyes wide open, not making any assumptions for what that deception is going to look like, or what the trigger might be. The wider you can cast that net, the better. Depending on how quickly AI accelerates and where the state of our tools are, we might not be in the place where we can prove from the ground up that everything is safe. But I feel like that's a very good North Star. It's a very powerful reassuring North Star for us to aim for, especially when we consider we are part of the broader AI safety portfolio. I mean, do you really trust—you're about to deploy this system and you really hope it's aligned with humanity—that you've successfully iterated through all the possible ways that it's going to scheme or sandbag or… Dwarkesh Patel But that's also probably going to be true with whatever you find. You're still going to have variants that you haven't explained. Or you found a feature, but you don't know if it actually explains deception or something else instead. Trenton Bricken First of all, I'm not saying you shouldn't try the probing approach. We want to pursue the entire portfolio. We've got the therapist interrogating the patient by asking, "Do you have any troubling thoughts?" We've got the linear probe, which I'd analogize to a polygraph test where we're taking very high level summary statistics of the person's well-being. Then we've got the neurosurgeons going in and seeing if you can find any brain components that are activating and troubling or off-distribution ways. I think we should do all of it. Dwarkesh Patel What percent of the alignment portfolio should mech interp be? Trenton Bricken I think as much of a chunk as is necessary. It’s hard to define. At Anthropic, I feel like all of the different portfolios are being very well-supported and growing. Sholto Douglas Coming back to the World War II question, you can think of it as a hierarchy of abstractions of trust here, where let's say you want to go and talk to Churchill. It helps a lot if you can verify that in that conversation, in that 10 minutes, he's being honest. This enables you to construct better meta narratives of what's going on. So maybe particle physics wouldn't help you there, but certainly the neuroscience of Churchill's brain would help you verify that he was being trustworthy in that conversation and that the soldiers on the front lines were being honest in their depiction of their description of what happened, this kind of stuff. So long as you can verify parts of the tree up, then that massively helps you build confidence. Trenton Bricken I think language models are also just really weird. With the emergent misalignment work. I don't know if they took predictions, they should have of like, "Hey, I'm going to fine tune ChatGPT on code vulnerabilities. Is it going to become a Nazi?" I think most people would've said no. That's what happened. Dwarkesh Patel How did they discover that it became a Nazi? Trenton Bricken They started asking it a ton of different questions and it will do all sorts of vile and harmful things. The whole persona just totally changes. We are dealing with alien brains here who don't have the social norms of humans. We don’t even have a clear notion of what they have and haven't learned. I think you really want to go into this with eyes wide open. Dwarkesh Patel Backing up from mech interp, if we live in a world where AI progress accelerates… By the way, you were mentioning a little while ago that there's many wild worlds we could be living in, but we're living in at least one of them. Another one that we've gestured at but it's worth making more explicit, is this. Even if the AI models are not helping write the next training algorithm for their successor, just the fact that if they had human level learning efficiency—whatever copy of the model is learning on the job—the whole model is learning. So in effect, it's getting– Sholto Douglas Or even if they're like a thousand times less efficient than humans are at learning and you deployed them. Even still. Dwarkesh Patel Exactly. Anyways, there's a whole bunch of other things that you can think of about it. But even there, you kind of have a broadly deployed intelligence explosion. Sholto Douglas I do think it's worth pressing on that future. There is this whole spectrum of crazy futures. But the one that I feel we're almost guaranteed to get—this is a strong statement to make—is one where at the very least, you get a drop-in white collar worker at some point in the next five years. I think it's very likely in two, but it seems almost overdetermined in five. On the grand scheme of things, those are kind of irrelevant timeframes. It's the same either way. That completely changes the world over the next decade. If we don't have the right policies in place for that, then you end up actually with in some respects, almost a fundamentally worse world. Because the thing that these models get good at by default is software engineering and computer using agents and this kind of stuff. Then we will need to put in extra effort to put them in the loops where they help us with scientific research. Or we have the right robotics, such that we actually experience an increase in material quality of life. That's worth thinking about. If you're in the perspective of like, “I'm a country, what should I be doing or thinking about?” Plan for the case where white collar work is automateable. And then consider, what does that mean for your economy? What you should be doing to prepare policy? Dwarkesh Patel What should you be doing to prepare? Because honestly, this is such a tough question if you're India or Nigeria or Australia. If you're a country unlike America or China where they do have frontier models, what is it that you should be doing right now? Especially on such a short timescale. Sholto Douglas I think one very important point is that let's say this scenario turns out true. Then compute becomes the most valuable resource in the world. The GDP of your economy is dramatically affected by how much compute you can deploy towards the organizations within your country. So having some guaranteed amount of compute I think will actually be quite important. Getting ahead of investments, and data centers, and this kind of stuff on the condition that it's companies in your country have to be allowed to use that compute, not necessarily for training but just even just for inference. I think the economic value here comes from inference. I think it also makes sense to invest broadly in AI. These countries have the opportunity to do so and that's a portfolio of foundation model companies but also robotics, supply chain, and this kind of stuff. I think that you should invest very proactively in policies that try to prevent capital lock-in. We're in for a much worse world if it just so happens that the people who had money in the stock exchange, or in land before AGI are dramatically more wealthy than the people who don't. It's a gross misallocation of resources. One of my favorite episodes actually on your podcast was the Georgism one where you're trying to appropriately value, or allocate land. This strikes particularly close to home coming from Australia where I think our policies with respect to land are grossly wrong. But I think this is broadly true. Being very forward on regulation of integration of these models into your country is important, and proactively making sure that people have choice. Let's say you should be quite proactive about making sure that the phones, or devices, or the glasses that people have, people have free choice on what things they run. So we just get the white collar worker, and you're trying to do the best to prepare your country for that. Then what can you do to make all possible versions of the future go well? That's covering some amount of economic downside. The other things that I think are really important is figure out how you can basically ensure dramatic upside, or cover terrible downside. Getting a dramatic upside is making sure that there is investment in biology research and this kind of stuff in an automated way such that these models are actually able to produce novel medicines that massively improve our quality of life. Covering the downside is AI alignment research, and this kind of stuff, and automated testing, and really thinking hard about that, AI safety institutes and this kind of stuff. Dwarkesh Patel But these seem like things that a random rich person could also do. It seems like there's not a thing that a nation state is uniquely equipped to do in this scenario. Sholto Douglas That's a good point. Sholto Douglas I mean dramatic allocation of resource towards compute I think is sensible. I would be doing that if I was in charge of a nation state. I think it just increases your optionality in most of the future worlds. Trenton Bricken Dylan Patel has some scary forecasts on US energy. Sholto Douglas Versus China. Yes. Trenton Bricken Yeah, we're like 34 gigawatts off. Sholto Douglas Yeah, the US's line is flat, basically, and China's line is like this. And I mean the US very clearly... Trenton Bricken We just need so many more power plants. Sholto Douglas Yes. If intelligence becomes this incredibly valuable input, intelligence becomes almost a raw input into the economies and quality of life of the future, the thing directly underneath that is energy. Making sure that you have incredible amounts of solar, like tile the desert with solar panels, some parts of the desert. That would be helpful towards making sure that you have more access to intelligence on tap. Trenton Bricken Yeah. Just to make it explicit, we've been touching on it here. Even if AI progress totally stalls, you think that the models are really spiky, and they don't have general intelligence. It's so economically valuable, and sufficiently easy to collect data on all of these different jobs, these white collar job tasks, such that to Sholto's point we should expect to see them automated within the next five years. Sholto Douglas Yeah. Trenton Bricken Even if you need to hand spoon every single task to the model. Sholto Douglas It's economically worthwhile to do so. Even if algorithmic progress stalls out, and we just never figure out how to keep progress going—which I don't think is the case, that hasn't stalled out yet, it seems to be going great—the current suite of algorithms are sufficient to automate white collar work provided you have enough of the right kinds of data. Compared to the TAM of salaries for all of those kinds of work, it is so trivially worthwhile. Trenton Bricken Yeah, exactly. I do just want to flag as well that there's a really dystopian future if you take Moravec’s paradox to its extreme. It’s this paradox where we think that the most valuable things that humans can do are the smartest things like adding large numbers in our heads, or doing any sort of white collar work. We totally take for granted our fine motor skill, and coordination. But from an evolutionary perspective it's the opposite. Evolution has optimized fine motor coordination so well. Even if you look at robot hands, the ability to open a door is still just really hard for robots. Meanwhile, we're seeing this total automation of coding, and everything else that we've seen as clever. The really scary future is one in which AIs can do everything except for the physical robotic tasks, in which case you'll have humans with AirPods, and... Sholto Douglas Glasses? Trenton Bricken Glasses, and there'll be some robot overlord controlling the human through cameras by just telling it what to do, and having a bounding box around the thing you're supposed to pick up. So you have human meat robots. Sholto Douglas Not necessarily saying that that's what the AIs would want to do, or anything like that. But if you were to be like, "What are the relative economic value of things?" The AIs are out there doing computer programming, and the most valuable thing that humans can do is be amazing robots. Now that being said, I think Moravec’s paradox is a little bit fake. I think the main reason that robots are worse at being a robot than they are at software engineering is the internet exists for software engineering. GitHub exists, and there is no equivalent thing if you had all mocap of everyone's actions as they were going about their daily lives for some reasonable fraction of the human population, robotics is also close to solved, on track to be solved at the same rate that software engineering is on track to be solved. So, this vision is only a sort of decade-long section, but it's still a pretty terrible decade. Imagine the world where people have lost their jobs, you haven't yet got novel biological research. That means people's quality of life isn’t dramatically better. You don't yet have material abundance because you haven't actually been able to action the physical world in the necessary way. You can't build dramatically more, because building dramatically more takes robots basically, and people's main comparative advantage is as fantastic robots. That’s a shocking, shocking world. Dwarkesh Patel Yeah. From the perspective of an average human, I think it actually might be better. Your wages will be higher because you're the complement to something that is enormously valuable which is AI labor. Sholto Douglas And a decade, or two after, the world is fantastic. Robotics is solved, and you start to get radical abundance basically provided that you have all the policies set up necessary to permit building. You end up with that same change like the before vs. after photos of Shanghai where 20 years on, it's this dramatically transformed city. A lot of places in the world probably end up like that over that two-decade period. But we need to do our best to estimate if this is actually what is on track to happen. Build SWE-bench, but for all the other forms of white collar work, and measure, and track. That's a great thing that governments should be doing by the way, trying to break down the functions of their economy into measurable tasks, and figuring out what does the curve actually look like for that? They might be a bit shocked by the progress there. There's no SWE-bench for a tax eval. I don't have all the answers here, but then we need to figure out a way to share the proceeds of this economy broadly across people, or invest heavily in robotics, and collect the data so that we get robotics faster, and we get material abundance faster. Invest in biological research that we get, but all that faster. Basically try and pull forward the radical upside, because otherwise you have a pretty dark section. Dwarkesh Patel I think one thing that's not appreciated enough is how much of our leverage on the future—given the fact that our labor isn't going to be worth that much—comes from our economic, and political systems surviving. For your million X'd S&P equity to mean something, for your contracts to mean anything, for the government to be able to tax the AI labor, and give you a UBI off of that, that requires our legal institutions, our economic institutions, our financial rails surviving into the future. Sholto Douglas Yes. Dwarkesh Patel The way in which that likely happens is if it's also in the AIs best interests that they follow those rails. By AI I don't mean some monolithic single AI, I just mean firms which are employing AI, and becoming more productive as a result. You don't want to be in a position where it's so onerous to operate in our system that you're basically selecting for firms who either emigrate, or who are doing black market stuff, et cetera. You want to make it super, super easy to deploy AI, have the equivalent of special economic zones, et cetera. Otherwise you are just surrendering the future outside of any control that you might have on it. One of the reasons that I worry about turning AGI into a national security issue, or having it have extremely close ties with the government, the Manhattan Project thing, is that it disproportionately redirects the use of AI towards military tech, mosquito drones and whatever. It also naturally puts other countries in the same frame of mind. If we're developing the mosquito drones, why would China not develop the mosquito drones? That just seems like a zero-sum race, and not to mention a potentially catastrophic one. Whereas compute will be limited, we'll need to disproportionately accelerate some things. To the extent it just remains totally like a consumer free market landscape, it just seems more likely that we'll get the glorious transhumanist future where they're developing the things that make human life better. Sholto Douglas Yes, I mean I agree. The case where you end up with two national projects facing off against each other is dramatically worse. We don't want to live in that world. It's much better if this stays a free market, so to speak. Dwarkesh Patel Okay. I want to take issue with your claim that even with the algorithms of today, if we just collect enough data that we could automate white collar work. First, let me get an understanding of what you mean by that. Do you mean that we would do the analogous thing of pre-raining with all the trajectories of everything people would do on their jobs? Could you make either manually, or through some other process, some RL procedure based on the screen recordings, every white collar worker. What kind of thing are you imagining? Sholto Douglas I mean a continuous distribution of this stuff. One important mental model to think about RL… There is some respect with which longer horizon, if you can do them, if you can get that reward ever, are easier to judge. Again, it's come back to that can you make money on the internet? That's an incredibly easy reward signal to judge. But to do that there's a whole hierarchy of complex behavior. So, if you could pre-train up to the easy to judge reward signals, does your website work? Does it go down, do people like it? There's all these reward signals that we can respond to because we have a long, we can progress through these long enough trajectories to actually get to interesting things. If you're stuck in this regime where you need a reward signal every five tokens, it's a way more painful, and long process. But if you could pre-train on every screen in America, then probably the RL tasks that you can design are very different from if you could only take the existing internet as it is today. How much of that you get access to changes the mix. Dwarkesh Patel As we're training them on longer, and longer horizon tasks, and it takes longer for them to get any signal on whether they successfully complete the task, will that slow down progress because it takes more compute per task? Trenton Bricken I do think there's this notion that the longer, the harder tasks, the more training is required. I'm sympathetic to that naively, but we as humans are very good at practicing the hard parts of tasks, and decomposing them. I think once models get good enough at the basic stuff, they can just rehearse, or fast-forward to the more difficult parts. Sholto Douglas I mean that's definitely one of the big complexities. As you use more compute, and as you train on more, and more difficult tasks, your rate of improvement of biology for example is going to be somewhat bound by the time it takes a cell to grow in a way that your rate of improvement on math isn't, for example. So, yes, but I think for many things we'll be able to parallelize widely enough, and get enough iteration loops. Dwarkesh Patel Will the regime of training new models go away? Will we eventually get to the point where you've got the model, and then you just keep adding more skills to it, with RL training? Sholto Douglas That depends on whether, or not you think there's a virtue in pre-training a new architecture. Basically you make some architectural change, then you probably need to do some form of at least pretraining a new model. Dwarkesh Patel If RL requires a bunch of inference to do the training in the first place, does that push against the thing you were talking about where we actually need a bigger model in order to have brain-like energy? But then also it's more expensive to train it in RL. So, where does that balance out? Trenton Bricken I think we got to drink the bitter lesson here. Yeah, there aren't infinite shortcuts. You do just have to scale and have a bigger model, and pay more inference for it. If you want AGI, then that's what you got to pay the price of. Sholto Douglas But there's a tradeoff equation here. There is science to do which everyone is doing. What is the optimal point at which to do RL? Because you need something which can both learn, and discover the sparse reward itself. So you don't want a one parameter model. Useless, even though you can run it really fast. You also don't want a 100T model. It's super slow. The marginal benefit of its learning efficiency is not worth it. So, there's a Pareto frontier here. What's the optimal model size of your current class of capabilities, and your current set of RL environments, and this kind of stuff. Trenton Bricken And even in the last year there's been much more of a factor of the inference cost. So, just explicitly the bigger the model, the more expensive it is to do a forward pass and generate tokens. The calculus used to just be, “Should I allocate my flops to more training data, or a bigger model?” And now another huge factor is how much am I actually going to do forward passes on this model once it's trained? Sholto Douglas My total pool of compute, how do I allocate that across training data compute, and inference compute for the RL training. Trenton Bricken And then even within inference, there's all this research on, well, what strategy should I use? Should I sample 10, and take the best? Do I do this sort of branching search, et cetera, et cetera. And so with RL where you're sampling a whole lot of tokens, you also need to factor in the ability for the model to actually generate those tokens, and then learn, and get feedback. Dwarkesh Patel If we're living in this world, what is your advice to somebody early in their career, or a student in college? What should they be planning on doing? Sholto Douglas Once again, it's worth considering the spectrum of possible worlds and preparing yourself for that. The action that I think is the highest EV in that case is that at a minimum you're about to get dramatically more leverage. You already have. Already the startups in YC are writing huge amounts of their code with Claude. What challenges, what causes do you want to change in the world with that added leverage? If you had 10 engineers at your beck, and call, what would you do? If you had a company at your beck and call, what would that enable you to do? What problems, and domains suddenly become tractable? That's the world you want to prepare for. Now, that still requires a lot of technical depth. Obviously there is the case where AI just becomes dramatically better than everyone at everything, but for at least a while there is… I think Jensen actually talked about this in an interview in an interesting way. He's like, "I have a hundred thousand general intelligences around me, and I'm still somewhat useful, because I’m there directing the values, and asking them to do things. I still have value even though I have a hundred thousand general intelligences." For many people, I think that will still be true for a fair while. Then as the AIs get better, and better, and better, and so on, eventually, no. But again, prepare for the spectrum of possible worlds because in the event where we're just totally outcompeted, it doesn't matter what you do. In all the other worlds, it matters a lot. Get the technical depth, study biology, study CS, study physics. Think hard about what challenges you want to solve in the world. Dwarkesh Patel Yeah, that's a lot of topics. That's a lot of shit. Sholto Douglas You can now. You can. It's so much easier to learn. Everyone now has the infinite perfect tutor. Dwarkesh Patel It's definitely been helpful to me. Trenton Bricken I would say some combination of: get rid of the sunk cost of your previous workflows, or expertise in order to evaluate what AI can do for you. Another way to put this, which is fun, is just be lazier in so much as you figure out the way that the agent can do the things that are toilsome. Ultimately, you get to be lazier, but in the short run, you need to critically think about the things you're currently doing, and what an AI could actually be better at doing, and then go, and try it, or explore it. Because I think there's still just a lot of low-hanging fruit of people assuming, and not writing the full prompt, giving a few examples, connecting the right tools for your work to be accelerated and automated. Dwarkesh Patel Yep, yep. There's also the sunk cost of feeling like since you're not "early to AI", that you've sort of missed the boat. I remember when GPT-3 came out… So backstory on the podcast, when I graduated college I was planning on doing some sort of AI wrapper startup, and the podcast was just a gateway into doing that. I was trying out different things and at the time I remember thinking, “oh, 3.5 is out.” People were like, "I'm so behind on the startup scene here” or whatever. If I wanted to make my own wrapper… maybe the idea of the wrapper was inadvisable in the first place. But every time feels early because if it's an exponentially growing process, and there are many things, many ideas which are only becoming possible now, right? Sholto Douglas Exactly. It's that product exponential I talked about. Dwarkesh Patel That's right. Sholto Douglas Products literally obsolete it. You need to constantly reinvent yourself to stay at the frontier of capabilities. Dwarkesh Patel Do you remember? I had a really shitty idea, and I gave you a call. Sholto Douglas I don’t remember what it was. Dwarkesh Patel I think it was like RAG for lawyers, or something. Anyways, I think one of our first interactions was like, "Hey, what do you think of this idea?" And you were like, “I think the podcast sounds promising.” Sholto Douglas I was right. Dwarkesh Patel Which I appreciate. Trenton Bricken Yeah. I got slightly annoyed at a friend recently who I think is really talented and clever and interested in AI but has pursued a biology route. I just kind of tried to shake them like, "You can work on AI if you want to." Humans are biological general intelligences where a lot of the things of value are just very general. Whatever kind of specialization that you've done maybe just doesn't matter that much. Again, it gets back to the sunk cost, but so many of the people, even my colleagues at Anthropic are excited about AI. They just don't let their previous career be a blocker. Because they're just innately smart, talented, driven, whatever else, they end up being very successful and finding roles. It's not as if they were in AI forever. I mean, people have come from totally different fields. Don't think that you need permission from some abstract entity to get involved, and apply, and be able to contribute. Dwarkesh Patel If somebody wanted to be an AI researcher right now, if you could give them an open problem, or the kind of open problem that is very likely to be quite impressive, what would it be? Sholto Douglas I think that now that RL's come back, papers building on Andy Jones's “Scaling scaling laws for board games” are interesting. Investigating these questions like the ones you asked before. Is the model actually learning to do more than its previous pass at K? Or is it just discovering that… Exploring questions like that deeply are interesting, scaling laws for RL, basically. Dwarkesh Patel I'd be very curious to see how much the marginal increase is in meta learning from a new task, or something. Trenton Bricken On that note, I think model diffing has a bunch of opportunities. People say, "Oh, we're not capturing all the features. There's all this stuff left on the table." What is that stuff that's left on the table? If the model's jailbroken, is it using existing features that you've identified? Is it only using the error terms that you haven't captured? I don't know. There's a lot here. I think MATS is great. The Anthropic fellowship has been going really well. Goodfire, Anthropic invested in recently, they're doing a lot of interpretability work, or just apply directly to us. Dwarkesh Patel Anything to get your equity up, huh? Trenton Bricken There's just so many interpretability projects. There's so much low-hanging fruit, and we need more people, and I don't think we have much time. Sholto Douglas I also want to make a plug for performance engineering. This is one of the best ways to demonstrate that you have the raw ability to do it. If you made an extremely efficient transform implementation on TPU, or Trainium, or Incuda, then I think there's a pretty high likelihood that you'll get a job offer. But there's a relatively small pool of people that you can trust to completely own end-to-end the performance of a model. Trenton Bricken And if you have broad, deep electrical engineering skills, I think you can probably come up to speed pretty fast on accelerator stuff. Sholto Douglas You can come up to speed reasonably fast and it teaches you a lot of good intuitions of the actual intricacies of what's going on in the models, which means that you're then very well-placed to think about architecture and this kind of stuff. One of my favorite people in thinking about architecture at Anthropic at the moment actually came from a heavy GPU kernel programming background and just knows the ins, and outs really deeply. He can think about the trade-offs really well. Dwarkesh Patel This was fun guys. Thanks for doing it again. Trenton Bricken Great to be back. Ready for more?
--------------------------------------------------

Title: Top-deck churn at Tata Digital; Rapido’s cash burn rises
URL: https://economictimes.indiatimes.com/tech/newsletters/morning-dispatch/top-deck-churn-at-tata-digital-rapidos-cash-burn-rises/articleshow/121349330.cms
Time Published: 2025-05-23T01:28:04Z
Full Content:
Want this newsletter delivered to your inbox? Updated On May 23, 2025, 07:44 AM IST Want this newsletter delivered to your inbox? Thank you for subscribing to Morning DispatchWe'll soon meet in your inbox. Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Definitions Top Commodities Top Prime Articles Top Slideshow Top Story Listing Top Market Pages Latest News Follow us on:
--------------------------------------------------