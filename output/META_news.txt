List of news related to Meta stock price META:

Title: TikTok isn’t out of the woods yet
URL: https://www.theverge.com/command-line-newsletter/613219/tiktok-ban-ceo-all-hands
Time Published: 2025-02-14T18:31:19Z
Full Content:
Employees are on edge about layoffs and a deal still has to happen. Also: Google execs dodge questions, and a tough week inside Meta. Employees are on edge about layoffs and a deal still has to happen. Also: Google execs dodge questions, and a tough week inside Meta. by Alex Heath Happy Valentine’s Day. I’m sending this from the NBA All-Star Technology Summit in San Francisco, where I’ve never seen so many major streaming competitors onstage at once. This week, I go inside TikTok, Google, and Meta. Keep reading for what Elon Musk really wants with his bid for OpenAI… On Wednesday night, ByteDance employees tuned in to their company all-hands meeting with senior leaders from across the company, including TikTok chief Shou Chew. The elephant in the room was, of course, the fate of the app in the United States. During the meeting, Chew said, “We remain confident that we can make a lot of progress to stabilize the situation hopefully soon,” according to a transcript of his remarks I’ve seen. The next day, Apple and Google suddenly returned TikTok to their US app stores after receiving private, written assurance from newly confirmed Attorney General Pam Bondi that they wouldn’t be fined for breaking the law. (Bondi’s letter has yet to be made public, and all parties involved are refusing to comment on it.) During the all-hands meeting, Chew alluded to the years of political miscalculations that led TikTok to this point. “With this road to trust building, if you look back over the years, we actually have made a lot of progress,” said Chew. “Even though the road has been a little bit too steep and windy, we’ve gotten stronger and we’ve survived.” He assured employees that TikTok’s advertising business had stabilized since the brief period when the app was completely offline in the US and that the company is exceeding its user engagement goal. The US is not TikTok’s top market by users or revenue, but it’s by far the most influential. As the weeks went on without the app being in US stores, TikTok’s leaders quietly mounted a pressure campaign around the issue. Key execs, including policy chief Michael Beckerman, held off-the-record Q&A sessions with TikTok creators in recent days to stress that Apple and Google have the legal assurances they need to host the app and not face earth-shattering fines, according to creators who attended the Zoom calls. (Some who attended these closed-door sessions have posted this takeaway publicly.) In these Q&As, TikTok’s representatives acknowledged that the app was suffering from glitches due to some US service providers still not working with the company. “They explained that there was some kind of issue reloading coins, which are used to send creators gifts,” one creator who attended a Q&A session told me. “Live content and coin purchases in-app are affected,” another recalled TikTok’s leaders saying. It’s unclear if these kinds of glitches are fixed now that Apple and Google are hosting the app again. While I’m sure that some TikTok employees are celebrating its return to app stores, the app’s fate is far from certain. President Donald Trump and China still have to work out a deal that could see TikTok become at least partially owned by some kind of US sovereign wealth fund. Trump has made clear that he has the leverage and wants to use TikTok as a bargaining chip with China. In the US, TikTok is not in control of its destiny. Many TikTok employees are also buzzing about the ominous warning Chew gave during the all-hands meeting about future layoffs. “We just have to make sure that the organization is in the right shape at this point in time,” he said, adding that “certain teams” had become “bloated” and “clunky.” “I think we can remove a lot of this overlap of roles and responsibilities,” he continued, “And through that, we can probably offer better job security for everyone because then your work actually has real value and you’re not just part of a meaningless process.” Google’s semi-regular “TGIF” all-hands meetings have been heavily scripted for quite some time. Even still, employees attended this week’s TGIF hoping to better understand why the company is ending its DEI programs and starting to allow its AI to be used for weapons. On the topic of DEI, CEO Sundar Pichai had nothing revelatory to say. “We’ve always deeply cared about hiring the best people around the world and creating opportunities for all,” he said while dialed in from Paris, according to multiple employees who attended the meeting. “Our values are enduring but we have to comply with legal directions depending on how they evolve.” After stressing that Google’s employee resource groups aren’t going away, Melonie Parker, Google’s former head of diversity, who (like her counterpart at Meta) has had her title changed to head of “engagement,” again cited the company’s status as a “federal contractor” as contributing to the changes. She said Google will “depreciate a number of our training programs that are focused on DEI” and update others to remove related language. Google’s top lawyer, Kent Walker, somehow managed to say even less when answering employee questions about the decision to quietly remove the company’s public commitment to keep its AI out of weapons. He said it was good for society for Google to be in the mix on these kinds of government contracts and suggested the company still had red lines for how its technology could be used but didn’t spell them out. It’s not just military applications of AI that Google employees have taken issue with. I’m told that some complained about how Google has been using AI to aggregate and summarize pre-submitted employee questions in these TGIF meetings. If you haven’t already, don’t forget to subscribe to The Verge, which includes unlimited access to Command Line and all of our reporting. As always, I want to hear from you, especially if you’ve received one of Reid Hoffman’s AI-personalized books. Respond here or ping me securely on Signal. Thanks for subscribing. A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe. © 2025 Vox Media, LLC. All Rights Reserved
--------------------------------------------------

Title: DeepSeek: Everything you need to know about the AI chatbot app | TechCrunch
URL: https://techcrunch.com/2025/02/14/deepseek-everything-you-need-to-know-about-the-ai-chatbot-app/
Time Published: 2025-02-14T17:38:45Z
Full Content:
Latest AI Amazon Apps Biotech & Health Climate Cloud Computing Commerce Crypto Enterprise EVs Fintech Fundraising Gadgets Gaming Google Government & Policy Hardware Instagram Layoffs Media & Entertainment Meta Microsoft Privacy Robotics Security Social Space Startups TikTok Transportation Venture Events Startup Battlefield StrictlyVC Newsletters Podcasts Videos Partner Content TechCrunch Brand Studio Crunchboard Contact Us DeepSeek has gone viral. Chinese AI lab DeepSeek broke into the mainstream consciousness this week after its chatbot app rose to the top of the Apple App Store charts (and Google Play, as well). DeepSeek’s AI models, which were trained using compute-efficient techniques, have led Wall Street analysts — and technologists — to question whether the U.S. can maintain its lead in the AI race and whether the demand for AI chips will sustain. But where did DeepSeek come from, and how did it rise to international fame so quickly? DeepSeek is backed by High-Flyer Capital Management, a Chinese quantitative hedge fund that uses AI to inform its trading decisions. AI enthusiast Liang Wenfeng co-founded High-Flyer in 2015. Wenfeng, who reportedly began dabbling in trading while a student at Zhejiang University, launched High-Flyer Capital Management as a hedge fund in 2019 focused on developing and deploying AI algorithms. In 2023, High-Flyer started DeepSeek as a lab dedicated to researching AI tools separate from its financial business. With High-Flyer as one of its investors, the lab spun off into its own company, also called DeepSeek. From day one, DeepSeek built its own data center clusters for model training. But like other AI companies in China, DeepSeek has been affected by U.S. export bans on hardware. To train one of its more recent models, the company was forced to use Nvidia H800 chips, a less-powerful version of a chip, the H100, available to U.S. companies. DeepSeek’s technical team is said to skew young. The company reportedly aggressively recruits doctorate AI researchers from top Chinese universities. DeepSeek also hires people without any computer science background to help its tech better understand a wide range of subjects, per The New York Times. DeepSeek unveiled its first set of models — DeepSeek Coder, DeepSeek LLM, and DeepSeek Chat — in November 2023. But it wasn’t until last spring, when the startup released its next-gen DeepSeek-V2 family of models, that the AI industry started to take notice. DeepSeek-V2, a general-purpose text- and image-analyzing system, performed well in various AI benchmarks — and was far cheaper to run than comparable models at the time. It forced DeepSeek’s domestic competition, including ByteDance and Alibaba, to cut the usage prices for some of their models, and make others completely free. DeepSeek-V3, launched in December 2024, only added to DeepSeek’s notoriety. According to DeepSeek’s internal benchmark testing, DeepSeek V3 outperforms both downloadable, openly available models like Meta’s Llama and “closed” models that can only be accessed through an API, like OpenAI’s GPT-4o. Equally impressive is DeepSeek’s R1 “reasoning” model. Released in January, DeepSeek claims R1 performs as well as OpenAI’s o1 model on key benchmarks. Being a reasoning model, R1 effectively fact-checks itself, which helps it to avoid some of the pitfalls that normally trip up models. Reasoning models take a little longer — usually seconds to minutes longer — to arrive at solutions compared to a typical non-reasoning model. The upside is that they tend to be more reliable in domains such as physics, science, and math. There is a downside to R1, DeepSeek V3, and DeepSeek’s other models, however. Being Chinese-developed AI, they’re subject to benchmarking by China’s internet regulator to ensure that its responses “embody core socialist values.” In DeepSeek’s chatbot app, for example, R1 won’t answer questions about Tiananmen Square or Taiwan’s autonomy. If DeepSeek has a business model, it’s not clear what that model is, exactly. The company prices its products and services well below market value — and gives others away for free. The way DeepSeek tells it, efficiency breakthroughs have enabled it to maintain extreme cost competitiveness. Some experts dispute the figures the company has supplied, however. Whatever the case may be, developers have taken to DeepSeek’s models, which aren’t open source as the phrase is commonly understood but are available under permissive licenses that allow for commercial use. According to Clem Delangue, the CEO of Hugging Face, one of the platforms hosting DeepSeek’s models, developers on Hugging Face have created over 500 “derivative” models of R1 that have racked up 2.5 million downloads combined. DeepSeek’s success against larger and more established rivals has been described as “upending AI” and “over-hyped.” The company’s success was at least in part responsible for causing Nvidia’s stock price to drop by 18% on Monday, and for eliciting a public response from OpenAI CEO Sam Altman. Microsoft announced that DeepSeek is available on its Azure AI Foundry service, Microsoft’s platform that brings together AI services for enterprises under a single banner. When asked about DeepSeek’s impact on Meta’s AI spending during its first-quarter earnings call, CEO Mark Zuckerberg said spending on AI infrastructure will continue to be a “strategic advantage” for Meta. At the same time, some companies are banning DeepSeek, and so are entire countries and governments. New York state also banned DeepSeek from being used on government devices. As for what DeepSeek’s future might hold, it’s not clear. Improved models are a given. But the U.S. government appears to be growing wary of what it perceives as harmful foreign influence. TechCrunch has an AI-focused newsletter! Sign up here to get it in your inbox every Wednesday. This story was originally published January 28, 2025, and will be updated continuously with more information. Topics Senior Reporter, Enterprise Uber sues DoorDash, alleging anti-competitive tactics How this weekend’s ‘Tesla Takeover’ protests against Elon Musk came together on Bluesky Meta confirms ‘Project Waterworth,’ a global subsea cable project spanning 50,000 kilometers A job ad for Y Combinator startup Firecrawl seeks to hire an AI agent for $15K a year Meta CTO says staff should quit if they don’t like Meta’s new policies Elon Musk’s full offer letter to buy OpenAI reveals five key details Phase raises $13M to speed up the UX design process with its no-code platform Subscribe for the industry’s biggest tech news Every weekday and Sunday, you can get the best of TechCrunch’s coverage. TechCrunch's AI experts cover the latest news in the fast-moving field. Every Monday, gets you up to speed on the latest advances in aerospace. Startups are the core of TechCrunch, so get our best coverage delivered weekly. By submitting your email, you agree to our Terms and Privacy Notice. © 2024 Yahoo.
--------------------------------------------------

Title: Meta Shakes Off TikTok Return, Heads for 20th Straight Day of Wall Street Gains
URL: https://www.thewrap.com/meta-tiktok-return-wall-street-gains/
Time Published: 2025-02-14T17:25:22Z
Full Content:
Mark Zuckerberg’s company has seen a 31% surge since the election Even TikTok’s return could not spoil Meta’s long-running party on Wall Street, with the company behind Facebook and Instagram heading towards its 20th straight day of gains on Friday. That easily laps Meta’s previous record of 11 straight days of gains, set back in 2015. Mark Zuckerberg’s company, during early trading on Friday, was up about 8 points, or 1.1% — nothing major, but it would still count as another positive day if those gains hold through the closing bell. TikTok, of course, is one of Meta’s chief competitors, so its return to the Apple and Google app stores on Thursday evening could have potentially spooked Meta investors on Friday morning. And as TheWrap reported this week, TikTok laps Facebook and YouTube when it comes to how much time the average American user spends on the app each day. Still, Meta investors do not appear to be too concerned. Meta has been on a hot streak since Election Day, with its share price increasing 31% to $737.45 per share since then. (The company was seeing a strong 2024 heading into the election as well, it’s worth pointing out.) Those recent gains have pushed Meta to a valuation of $1.86 trillion — making it the sixth most-valuable public company in the world. Zuckerberg, who once kicked Donald Trump off of his platforms, has had a cozier relationship with the president in recent months. He was spotted alongside Tesla boss Elon Musk and Amazon founder Jeff Bezos, among other tech executives, at Trump’s inauguration last month — but many MAGA fans told TheWrap they were skeptical of Zuckerberg’s sudden friendship with the president.
--------------------------------------------------

Title: Former Marvel exec claims ex-Disney boss denied him promotion because he was ‘another old white guy’: lawsuit
URL: https://nypost.com/2025/02/14/media/ex-marvel-exec-robert-steffens-claims-ex-disney-boss-denied-him-promotion-because-he-was-an-old-white-guy/
Time Published: 2025-02-14T17:11:15Z
Full Content:
A former high-ranking Marvel executive claims ex-Disney CEO Bob Chapek refused to give him a promotion because he was just “another old white guy,” according to a blockbuster lawsuit against the media giant. Robert Steffens, who served as chief financial officer and then co-president of Marvel from 2015 to 2023, alleged that then-Marvel CEO Isaac Perlmutter told him in February 2022 that Disney CEO Bob Chapek put the kibosh on his bid for the role of president of Disney consumer products, according to the suit filed in Los Angeles Superior Court on Tuesday. Instead, the job was given to a woman of “ambiguous ethnicity” after Perlmutter allegedly told Steffens that the company couldn’t award the position to “another old white guy,” the complaint said. Chapek was ousted later in 2022 after getting the House of Mouse embroiled in a battle with Florida Gov. Ron DeSantis over the state’s “Don’t Say Gay” law — and replaced by Bob Iger. Steffens’ lawsuit claimed that the “woke” media giant was engaged in an “official effort to promote vice presidents based on their race and a memorandum that would have referred to employees with the racial signifier ‘BIPOC’ [black, indigenous and people of color].” Disney’s actions were “willful, wanton, malicious, intentional, oppressive and despicable and were done in willful and conscious disregard of the rights, welfare and safety of [Steffens],” the lawsuit said. “Our primary focus is on the facts and the law, and we will vigorously defend our client’s interests,” Steffens attorney Marcella Burke told Fox News Digital on Thursday. Perlmutter was fired by Iger in March 2023. The Disney boss survived a proxy fight last year spearheaded by Perlmutter and activist investor Nelson Peltz. The lawsuit comes as Disney ditches some of its diversity, equity and inclusion policies amid pressure from activist investors and the Trump administration’s crackdown on DEI. On Monday, Disney shut down its “Reimagine Tomorrow” program, which was used to highlight stories and talent from underrepresented communities. The initiative promised 50% of regular and recurring characters across the Disney universe would come from “underrepresented groups.” It also quietly rolled back other DEI programs this week. The Reimagine Tomorrow program sparked outrage in 2022 when a company-wide Zoom call was leaked on social media. At the time, one Disney executive touted her “not at all secret gay agenda,” while another boasted that the company was scrubbing the words “ladies, gentlemen, boys, and girls” at its theme parks to avoid alienating transgender children. President Trump recently ordered an end to DEI in the federal government and for its contractors, which includes many private companies. Meanwhile, companies are also under pressure from conservative critics who say DEI programs are discriminatory against non-minorities. Companies such as Meta and John Deere have rolled back their DEI programs, while others like Apple and Costco have pushed back. Google, GM, Intel, Pepsi, Comcast, Philip Morris and others have softened or deleted their DEI language. The DEI battle is also being fought in the courtroom. Last week, Target was hit with a class-action suit after shareholders alleged the retail giant misled investors about the risks of its DEI initiatives, which led consumers to boycott and its stock price to tank. Advertisement
--------------------------------------------------

Title: What’s Happening With ROKU Stock?
URL: https://www.forbes.com/sites/greatspeculations/2025/02/14/whats-happening-with-roku-stock/
Time Published: 2025-02-14T13:51:26Z
Full Content:
POLAND - 2025/02/01: In this photo illustration, the Roku company logo is seen displayed on a ... [+] smartphone screen. (Photo Illustration by Piotr Swat/SOPA Images/LightRocket via Getty Images) Roku stock surged by approximately 14% in pre-market trading on Friday, bringing its year-to-date gains to over 30%. The increase follows Roku’s stronger-than-expected Q4 2024 results, with total revenue climbing 22% to $1.2 billion. Roku’s platform business revenue, which includes subscription and ad sales on Roku devices, exceeded $1 billion, reflecting a 25% year-over-year increase. Separately, with uncertainty surrounding the company, see What’s Next For U.S. Steel Stock After A Mixed Q4? Roku’s momentum continues to build. The company recently reported that its global streaming TV household count reached 89.8 million by the end of December and surpassed 90 million in early January. This represents an increase from 85.5 million in Q3 and 80 million in Q4 2023, marking a 12.5% year-over-year rise. Roku also revealed that its smart TVs and streaming boxes now reach nearly half of all U.S. broadband households, a positive development for its high-margin platform business. Viewer engagement is increasing as more users shift from traditional television to streaming. In Q4, total streaming hours reached 34 billion, up 18% year-over-year. With its extensive data on user behavior, ad performance, and engagement, Roku has ample room for expansion. Additionally, the company has implemented cost controls, limiting operating expense growth to just 2% year-over-year—well below revenue growth. Workforce and office space reductions in 2024 contributed to this efficiency. Operating profit rose 17% to $512.6 million, while free cash flow for 2024 stood at $203 million. As prior investments in research and product development continue to bear fruit, Roku has significant potential to improve its margins. See a scenario on how Roku stock could reach $200. ROKU stock has experienced volatility over the past four years, with yearly returns fluctuating significantly compared to the S&P 500. The stock delivered returns of -31% in 2021, -82% in 2022, 125% in 2023, and -19% in 2024. The Trefis High Quality Portfolio, a curated selection of 30 stocks, has been considerably less volatile while outperforming the S&P 500 over the same period. Why? HQ Portfolio stocks have historically delivered better risk-adjusted returns, avoiding the extreme fluctuations seen in ROKU stock, as demonstrated in HQ Portfolio performance metrics. With ongoing macroeconomic uncertainties, including interest rate decisions and global conflicts, will ROKU struggle as it did in 2021, 2022, and 2024, or could it stage a recovery? Separately, after weaker than expected guidance caused a decline, see Is WST Stock Undervalued At $200? Currently, Roku stock trades at approximately 3x the consensus 2025 revenue estimate, significantly below the double-digit multiples seen in 2021. However, the company faces increasing competition for ad revenue from major players such as Netflix, Meta, and Alphabet. We estimate Roku’s fair value at $78 per share, which is below its current market price. We are updating our Roku model to reflect its Q4 earnings. See our analysis on Roku Valuation: Expensive or Cheap to understand the factors influencing our price estimate. ROKU Return Compared With Trefis Reinforced Portfolio Invest with Trefis Market-Beating Portfolios See all Trefis Price Estimates One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Elon Musk's X is pulling big-name advertisers back
URL: https://www.businessinsider.com/x-advertisers-back-again-investors-musk-trump-2025-2
Time Published: 2025-02-14T13:50:56Z
Full Content:
Happy Valentine's Day! In the spirit of the holiday, here's something you might fall in love with: A new Apple product. Tim Cook teased the launch of something set for next Wednesday. In today's big story, new and old advertisers are coming to X more than two years after Elon Musk's dramatic takeover. What's on deck Markets: Three charts that show Robinhood's incredible rise over the past 12 months. Tech: Meta CTO says employees who think 'everyone has to like' its policy changes should 'quit' and 'consider working elsewhere.' Business: Everything you need to know about new US health secretary Robert F. Kennedy Jr. But first, back like they never left. If this was forwarded to you, sign up here. After a tumultuous few years, X is pulling advertisers back. The platform formerly known as Twitter has grown its pool of advertisers in recent months, according to an analysis by Business Insider's Lara O'Reilly. Big-name brands like Apple have returned to the platform alongside newcomers. It's a notable turnaround for a company that has, at times, been outwardly hostile to advertisers following Elon Musk's 2022 buyout. X has sued several big advertisers for what it claims was a coordinated ad boycott. To be clear, the turnaround isn't complete. While X enjoyed a 15% year-over-year increase in the number of companies buying ads, its estimated US ad revenue dropped 28% during that same time period, according to research firm MediaRadar. However, X is also leaning on ad tech partners, as Lara has previously reported, which tends to lead to lower prices compared to using an internal sales team. Even if advertisers are cautious about working with X, they might not have many other social media options. Meta's recent policy changes around content moderation were somewhat inspired by X. TikTok's future remains in limbo. And alternatives like Bluesky are still a fraction of the size of X. There's another good data point for X. Banks have seen increased interest from investors in the debt they hold from financing Musk's 2022 deal. Bloomberg reported on Thursday that Morgan Stanley was upping its offering of X debt from $3 billion to $4.74 billion. Big names like Pimco and Citadel have already jumped in, according to The Wall Street Journal. (It's also worth noting that X's stake in xAI, Musk's buzzy AI startup, was also reportedly used to attract potential investors.) Not too long ago, X's debt was basically considered nuclear waste. The seven banks that loaned Musk the $13 billion to buy Twitter were unable to move the loans, leaving it to clog up their balance sheets. Of course, there's another factor that has nothing to do with X's performance enticing advertisers to return and investors to buy its debt: Musk's close ties to President Donald Trump. Doing business with X could keep a company in Musk's good graces. And staying on good terms with the guy who has the ear of the most powerful person in the world certainly isn't a bad thing. US congressmen sound the alarm on 'secret' Apple order from UK. Read their letter. 1. Robinhood's rapid rise. The brokerage's earnings blew past analyst expectations, the latest in an incredible run that's seen its shares up almost 440% over the past year. These three charts show how a surge in newly funded accounts and a crypto boom have helped the app. 2. Want to invest like the world's best? Start here. Every few months, BI takes a look at where top investors are betting at least 20% of their portfolio. Warren Buffett has continued to reduce his position in Apple stocks, while Baupost Group's Seth Klarman has bought up more shares of telecommunications company Liberty Global. Here are nine stocks whales are making big bets on. 3. Betting on China. Druckenmiller protégé Beeneet Kothari of Tekne Capital isn't afraid of investing in China despite President Trump's trade war. A lot of Western capital has already left the country, softening the blow to Chinese stocks, he said. So, where to start? China has been getting a lot of attention lately due to DeepSeek, and robotics is another strength. 1. xAI has been working on a 'DeepSearch' feature, employees say. Data annotators at Elon Musk's AI startup have been working on the project, dubbed "DeepSearch," to train Grok to perform multi-step research projects the same way a human might, two workers said. It could compete with features from rivals OpenAI and Google, but it's unclear what stage the project is at or whether xAI will ultimately release something. 2. "Leave or disagree and commit." That's what Meta's Chief Technology Officer Andrew Bosworth recently told staffers in response to employee concerns over Meta's recent policy changes. The company changed its approach to internal Q&A sessions to limit leaks to the media. In Meta's internal forum, Bosworth said, "if your view is 'everyone has to like all the policies we have and if they don't it is appropriate to leak' then I think you should consider working elsewhere." 3. The Amazon of travel. Airbnb's CEO Brian Chesky said the app should "be one place you go for all of your traveling and living needs," in an earnings call on Thursday. The company plans to invest $200 million to $250 million in launching new businesses and offerings, which it says will be rolled out in May. Its stock price jumped 15% after the announcement in after hours trading. 1. Robert F. Kennedy Jr. to call the shots on health policy. The new US health secretary may be known for his skepticism over vaccines — despite denying an anti-vaccine stance in his confirmation hearings — but RFK Jr. will have even more policy issues on his plate. Here's what the appointee could mean for you and your family. 2. Dozens of Office of Personnel Management probationary workers were abruptly fired. Around 60 employees were terminated in a group video call Thursday afternoon and told their work accounts would be deactivated by 3 pm ET, which gave them about 20 minutes. Two employees told BI that union representatives were not present in the meeting. 3. President Trump wants his education secretary pick to "put herself out of a job." Trump hasn't been shy about wanting to shut down the Department of Education. That could have a ripple effect on grants, student loans, outcomes data, and discrimination claims. And if Trump's plan goes through, other agencies may need to bear the burden. Axel Springer CEO talks trade, Trump, AI, and the future of media. 2025 NBA All-Star Weekend begins. The Insider Today team: Dan DeFrancesco, deputy editor and anchor, in New York. Grace Lett, editor, in Chicago. Ella Hopkins, associate editor, in London. Hallam Bullock, senior editor, in London. Amanda Yen, associate editor, in New York. Elizabeth Casolo, fellow, in Chicago. Jump to
--------------------------------------------------

Title: AI Companies: 4 PR Strategies You Can’t Afford To Ignore
URL: https://www.forbes.com/councils/forbesagencycouncil/2025/02/14/ai-companies-4-pr-strategies-you-cant-afford-to-ignore/
Time Published: 2025-02-14T12:30:00Z
Full Content:
Heather Kelly is the CEO of Next PR, an award-winning, full-service public relations firm with offices across the U.S. AI isn’t just having a moment; its meteoric rise is rewriting the rules of innovation. Alongside the enormous market growth that’s expected to reach nearly $3.7 trillion by 2034, there’s been an explosion in AI startups across almost every industry sector. With so much buzz and media attention surrounding the biggest firms, it may seem impossible to distinguish your brand from the top players. The competition for funding, market share and media attention is fierce: How can you best share your mission with your target audience without feeling like your efforts are going unnoticed? The key to building a standout AI brand is prioritizing measurable business outcomes and clear KPIs, such as growing audience reach, boosting your sales pipeline, strengthening partnerships or increasing share of voice. A PR strategy tailored to your company’s goals can help you get there. Here’s where to start: Many consumers are skeptical about the “artificial” part of AI and fear machines will take over the world (or at least their jobs). You need to humanize your organization and your brand to build trust and foster connection. Who are you? What are your values? How does your solution make your customers’ lives easier? Because people want to buy from other humans, not company logos, a strong executive brand-building and thought leadership program can give customers a glimpse into the people behind your brand. This is especially critical for AI brands, allowing executives to explain their intentions behind the innovation and address consumer hesitation directly. In addition to an executive social media presence, podcasts are highly effective in fostering authenticity and humanity. The longer, more conversational format lets leaders show their personality and share their own perspectives. Your audience wants to know you understand their pain points and how your solution addresses them. Most consumers aren’t interested in the intricate technical details behind your product, which is what many genius-level founders quickly default to when asked about their solution. A messaging workshop can help you articulate your solution’s value proposition—how it’s better, faster and easier than your competitors’ tools—in a way that won’t make people’s eyes glaze over. Communicate what truly sets you apart, and never rely on being “first” as a differentiator because, in this industry, that distinction won’t last five minutes. Who cares if you’re the first if the second is better? Focus on value and benefit to the customer. Executives often struggle to articulate their brand’s message concisely and consistently. Messaging workshops and media trainings can help them master this skill. A good rule of thumb is, “Can you explain it to my mom and make it relatable?” (Ping me if you want to practice with her!) Trust is the foundation of any successful business. To maintain it, honesty is nonnegotiable. Overpromising and underdelivering can quickly erode audience confidence. One of the biggest mistakes I’ve seen companies such as Tesla and many crypto firms make is hyping up some incredible new breakthrough and then failing to deliver, causing their stock price and brand trust to plummet. Then, there’s the disaster at Meta over AI Facebook and Instagram accounts. This kind of mess harms not only your brand’s image but also consumers’ general perception of AI. The problem is even worse when brands don’t trust their PR team and keep them in the dark about delays or other potential crises until the 11th hour. By then, it’s too late for the team to do proactive damage control. Instead, be upfront, transparent and walk the walk to build trust with customers, partners and the media. If you miss a delivery deadline, give advance notice. If you’re using AI bots, be clear around the intention behind them and you might win favor for the novelty and accuracy of the experience. Don’t wait for the news to break and then sheepishly ask for forgiveness. Repetition and consistency are key to any brand strategy, helping grow recognition and awareness by keeping your people, products or brand in front of your target audience. Speaking engagements and award programs are among the best ways to nurture credibility and trust. Winning an award tells potential customers that your company and products are legit; it’s like a stamp of approval from the experts. Similarly, having your executives speak at events shows them as reliable and reputable industry sources, while also putting a face to your brand. Influencer marketing and analyst relations can be especially valuable for building credibility through third-party validation. These industry experts understand your market, and when analysts communicate your mission and differentiators to their audience, they can significantly influence buyer decisions. While AI may seem like a steep learning curve for many industries and as media channels continue to evolve, the core principles of brand building remain the same. This is no different than the emergence of past innovations: the internet, Big Data, online banking or mobile payments—technologies that once sparked hesitation but are now part of everyday life. You don’t need a PR team that specializes in AI. You need one that understands how to craft the right message, deliver it to the right audience and drive business results. No matter what bright, shiny thing comes along next, a PR strategy rooted in authenticity, clear differentiators, trust and credibility will always stand the test of time. Forbes Agency Council is an invitation-only community for executives in successful public relations, media strategy, creative and advertising agencies. Do I qualify?
--------------------------------------------------

Title: Markets Shrug Off More Tariff Talk; Meta Platforms Improbable Run
URL: https://www.forbes.com/sites/jjkinahan/2025/02/14/markets-shrug-off-more-tariff-talk-meta-platforms-improbable-run/
Time Published: 2025-02-14T12:00:43Z
Full Content:
Shares of DraftKings are indicated higher in premarket following a bullish outlook issued by the ... [+] company. (Photo illustration by Scott Olson/Getty Images) Key Takeaways Stocks posted their best day of the week on Thursday. The S&P 500, Russell 2000 and Dow Jones Industrial Average all posted gains of right around 1%. Meantime, the Nasdaq Composite gained 1.5%. The run came despite a higher-than-expected uptick in the Core Producer Price Index (PPI) and more tariff talk. Concerns about inflation began heating up on Wednesday following a Consumer Price Index (CPI) report showing prices ticking higher. Yesterday's PPI confirmed inflation was moving higher but perhaps not to the extend many feared from the CPI read. Still, the reports have put inflation back on the table and that will likely result in added emphasis placed on both future economic data and what members of the Fed say at any speaking engagement. I think next Wednesday, when the Fed releases the minutes from their last meeting, there will be a lot of tea leaves reading. Tangentially related to inflation, on Thursday, President Trump said he plans to put in place reciprocal tariffs on countries that have tariffs on U.S. goods. That threat is being seen as taking aim at India and the European Union among others. The specifics of the tariffs are not yet known but are expected to be announced next Tuesday. One of the things we’ve seen since the new administration took over is a return to policy by tweet. What I mean by that is simply, President Trump often makes announcements via social media and will do so without forewarning. That has sometimes meant policy announcements over the weekend and investors may want to be more conscience of that risk on Fridays. Turning to earnings, after the close on Thursday both Coinbase and DraftKings announced results. For Coinbase, it was the strongest quarter for revenue in three years as the company beat on both the top and bottom lines. Shares of Coinbase were up 8% on Thursday but are indicated to open down just under 1%. DraftKings also reported their results. While the online betting company missed on both the top and bottom lines, they did report a 36% increase in monthly unique payers for the quarter. They also raised their guidance for 2025. Shares of DraftKings were up 2% Thursday and are trading higher by 5% in premarket. Finally, Applied Materials released their earnings this morning. While the company beat on both the top and bottom line, they offered an outlook below what analysts expected. Shares of Applied Materials are lower by nearly 10% in premarket trading. One stock that did not report earnings but has been on an incredible run of late is Meta Platforms. That company has seen its shares rise for seventeen consecutive days. The probability of this happening for those that are curious is 0.0000076. If you are someone who has enjoyed the ride and decided you might want to take profits or at least protect them, I don't think anyone would blame you in light of the probability of what has been happening. For today, I am keeping an eye on Meta Platforms for the reason I mentioned above. I'm also watching gold. Despite the strength in the market and its ability to seemingly shrug off anything, gold is trading at all-time highs and that may serve as the proverbial canary in the coal mine with respect to a reemergence of inflation and what it could mean for equities. This is also a three-day weekend with Monday being President's Day and therefore, because of the nature in which policy decisions are being announced, I would not be surprised if we see some selling near the close today. I also wouldn't be surprised if, while watching television this weekend, we see a ton of commercials for mattress sales because somewhere along the line, we decided it can't be an American holiday without mattresses going on sale. As always, I would stick with your investing plan and long-term objectives. tastytrade, Inc. commentary for educational purposes only. This content is not, nor is intended to be, trading or investment advice or a recommendation that any investment product or strategy is suitable for any person. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Asian stocks to gain, tariff delay lifts sentiment
URL: https://economictimes.indiatimes.com/markets/stocks/news/asian-stocks-to-gain-tariff-delay-lifts-sentiment/articleshow/118229798.cms
Time Published: 2025-02-14T00:57:16Z
Full Content:
Stock Trading Maximise Returns by Investing in the Right Companies By - The Economic Times, Get Certified By India's Top Business News Brand Stock Trading Market 104: Options Trading: Kickstart Your F&O Adventure By - Saketh R, Founder- QuickAlpha, Full Time Options Trader Stock Trading Technical Analysis for Everyone - Technical Analysis Course By - Abhijit Paul, Technical Research Head, Fund Manager- ICICI Securities Stock Trading Stock Markets Made Easy By - elearnmarkets, Financial Education by StockEdge Stock Trading Renko Chart Patterns Made Easy By - Kaushik Akiwatkar, Derivative Trader and Investor Stock Trading Candlesticks Made Easy: Candlestick Pattern Course By - elearnmarkets, Financial Education by StockEdge Stock Trading Market 101: An Insight into Trendlines and Momentum By - Rohit Srivastava, Founder- Indiacharts.com Stock Trading Markets 102: Mastering Sentiment Indicators for Swing and Positional Trading By - Rohit Srivastava, Founder- Indiacharts.com Stock Trading Market 103: Mastering Trends with RMI and Techno-Funda Insights By - Rohit Srivastava, Founder- Indiacharts.com Stock Trading Dow Theory Made Easy By - Vishal Mehta, Independent Systematic Trader Stock Trading ROC Made Easy: Master Course for ROC Stock Indicator By - Souradeep Dey, Equity and Commodity Trader, Trainer Stock Trading RSI Made Easy: RSI Trading Course By - Souradeep Dey, Equity and Commodity Trader, Trainer Stock Trading Heikin Ashi Trading Tactics: Master the Art of Trading By - Dinesh Nagpal, Full Time Trader, Ichimoku & Trading Psychology Expert Stock Trading RSI Trading Techniques: Mastering the RSI Indicator By - Dinesh Nagpal, Full Time Trader, Ichimoku & Trading Psychology Expert Stock Trading Introduction to Technical Analysis & Candlestick Theory By - Dinesh Nagpal, Full Time Trader, Ichimoku & Trading Psychology Expert (What's moving Sensex and Nifty Track latest market news, stock tips, Budget 2025, Share Market on Budget 2025 and expert advice, on ETMarkets. Also, ETMarkets.com is now on Telegram. For fastest news alerts on financial markets, investment strategies and stocks alerts, subscribe to our Telegram feeds .) Subscribe to ET Prime and read the Economic Times ePaper Online.and Sensex Today. Top Trending Stocks: SBI Share Price, Axis Bank Share Price, HDFC Bank Share Price, Infosys Share Price, Wipro Share Price, NTPC Share Price (What's moving Sensex and Nifty Track latest market news, stock tips, Budget 2025, Share Market on Budget 2025 and expert advice, on ETMarkets. Also, ETMarkets.com is now on Telegram. For fastest news alerts on financial markets, investment strategies and stocks alerts, subscribe to our Telegram feeds .) Subscribe to ET Prime and read the Economic Times ePaper Online.and Sensex Today. Top Trending Stocks: SBI Share Price, Axis Bank Share Price, HDFC Bank Share Price, Infosys Share Price, Wipro Share Price, NTPC Share Price Some are natural, others lab-grown: The untold truth of diamond pricing in India Does investing via SIP really make you a crorepati? This is what experts have to say. What the recent High Court ruling means for NFRA, the auditors’ regulator Inflation is easing, but don’t uncork the champagne yet Stock Radar: Tata Consumer takes support above 20-DMA in February 2025; time to buy? For long-term investors with ability to ignore short-term noise: 5 large-caps from different sectors with upside potential of up to 39% All Mutual Funds Top Tax Saving Mutual Funds Better Than Fixed Deposits Low Cost High Return Funds Best Hybrid Funds Best Large Cap Funds SIP’s starting Rs. 500 Top Performing Mid Caps Promising Multi Cap Funds Top Rated Funds Top Performing Index Funds Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Definitions Top Prime Articles Most Searched IFSC Codes Top Slideshow Top Story Listing Private Companies Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Worry not. You’re just a step away. It seems like you're already an ETPrime member with Login using your ET Prime credentials to enjoy all member benefits Log out of your current logged-in account and log in again using your ET Prime credentials to enjoy all member benefits. Offer Exclusively For You Save up to Rs. 700/- ON ET PRIME MEMBERSHIP Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get Flat 40% Off Then ₹ 1749 for 1 year Offer Exclusively For You ET Prime at ₹ 49 for 1 month Then ₹ 1749 for 1 year Special Offer Get flat 35% off on ETPrime ₹ 90 Days Prime access worth Rs999 unlocked for you Stories you might be interested in
--------------------------------------------------

Title: 4 portfolio tech stocks are making news — plus, why the market is looking past hot inflation
URL: https://www.cnbc.com/2025/02/13/4-tech-stocks-are-making-news-plus-the-market-looks-past-hot-inflation.html
Time Published: 2025-02-13T19:08:17Z
Description: Every weekday, the Investing Club releases the Homestretch; an actionable afternoon update just in time for the last hour of trading.
--------------------------------------------------

Title: 1 ASX dividend stock up 40% in 12 months that I'd buy
URL: https://www.fool.com.au/2025/02/14/1-asx-dividend-stock-up-40-in-12-months-that-id-buy/
Time Published: 2025-02-13T18:30:00Z
Description: I’m backing this company to deliver pleasing dividends. 
The post 1 ASX dividend stock up 40% in 12 months that I'd buy appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: Intel stock soars almost 25% in one week after JD Vance' new comments on the chip maker's AI future, as the US and UK refuse to sign the Paris AI summit's regulation decree
URL: https://www.windowscentral.com/hardware/intel/intel-stock-soars-almost-25-percent-in-one-week-after-jd-vance-new-comments
Time Published: 2025-02-13T17:17:17Z
Full Content:
Intel has struggled to find its footing in a silicon landscape dominated by NVIDIA, but the Trump administration's "America first" approach might have thrown the chip maker a big lifeline. When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works. Intel has not had a good few years, but things might be looking up for the beleaguered chip maker. Over the past year, Intel has laid off thousands upon thousands of staff, following its disastrous attempts to build a home-grown chip fabrication business, typically known as Intel Foundry. The firm saw its valuation practically wiped out over the past few years as a result, with its market cap shrinking to lows not seen since the mid 90s. Investors have largely been placing their bets on NVIDIA in the artificial intelligence wars, given that the server technology built by the firm has become the de facto default for building generative AI platforms. Microsoft, Meta, xAI, and others have placed orders in the multi-billions for chips from NVIDIA for their data centers, leaving Intel and AMD in the dust. However, the political landscape might just be turning in Intel's favor. At the recent Paris AI summit led by France, U.S. Vice President JD Vance shared some interesting comments during his speech (via Barron), which seem to have excited investors. “The Trump administration believes AI will have countless revolutionary applications in economic innovation, job creation, national security, healthcare, free expression and beyond. [...] To safeguard America’s advantage the Trump administration will ensure that the most powerful AI systems are built in the U.S. with American designed and manufactured chips." Indeed, the United States has been trying to limit dependence on countries like Taiwan and China for building out its AI aspirations. Both the Democrats, and Republicans have lobbied to build home-grown semiconductor solutions, to reduce the dependence on TSMC. The firm produces some 90%+ of the world's most sought-after chips, and the Trump administration has targeted Taiwan with tariffs in an attempt to force businesses to consider home-grown solutions instead. JD Vance's comments were seen by many investors as a positive sign of more support for Intel, which remains a significant player in the space. Intel might have missed out on opportunities to be a bigger player with smartphones, server tech, and graphics processing, but it is enjoying a lot of home-grown support via the CHIPS act, which allocated billions in funding to United States-based companies. All the latest news, reviews, and guides for Windows and Xbox diehards. For the United States and other western countries, artificial intelligence is increasingly seen as a national security issue. Indeed, lots of western tech firms saw their stock prices battered when a Chinese start up, with its DeepSeek model, seemed to reproduce OpenAI's ChatGPT results for far cheaper. The first countries to really achieve true "AGI" in sci-fi terms could see rapid advancement in technological breakthroughs, with computers able to understand context and experiment far more rapidly than humans. To that end, the Paris AI Action Summit aimed to draw nations into an agreement to regulate AI development, to focus on making it "open, ethical, safe, and secure," given the potential the technology has to actually do serious harm to humanity as well. The United States and United Kingdom buddied up to refuse signing the act. JD Vance criticized the regulations, saying that it would "kill an industry that is just starting to take off," while a UK government spokesperson said "it didn't provide enough practical clarity." Jez Corden is the Executive Editor at Windows Central, focusing primarily on all things Xbox and gaming. Jez is known for breaking exclusive news and analysis as relates to the Microsoft ecosystem while being powered by tea. Follow on Twitter (X) and Threads, and listen to his XB2 Podcast, all about, you guessed it, Xbox! Windows Central is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site. © Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York, NY 10036.
--------------------------------------------------

Title: As Tech Companies Keep Pouring Money Into AI, Signs May Be Pointing to Disaster
URL: https://futurism.com/tech-investment-ai
Time Published: 2025-02-13T17:11:16Z
Full Content:
Fourth quarter earnings season is in full swing, and despite budgets larger than some countries' GDPs, big tech companies' financial statements are looking grim. Even after president Donald Trump's announcement of a $500 billion AI infrastructure deal, the so-called "magnificent seven" (M7) stocks — the Wall Street nickname for Google, Meta, Nvidia, Tesla, Apple, Microsoft, and Amazon — are slumping. Once thought to be airtight, analysts are now warning that it may be time for investors to sell. Tesla and Google both whiffed at earnings expectations, sending their shares into a tailspin — Tesla's share price has tumbled 15 percent over the past month, while Google's parent company Alphabet saw its stock valuation drop by 9 percent in a single day. Nvidia, meanwhile — which was previously printing money as the de facto provider of shovels for the AI gold rush — saw almost $600 billion evaporate within hours of DeepSeek's announcement that it had developed one of the most efficient AI language models we've seen, suggesting a future far less dependent on its chips than previously anticipated. And though Apple, Amazon, and Meta slightly exceeded quarterly expectations, they didn't return the same kind of explosive growth the market has learned to expect, prompting Goldman Sachs strategists to change its tune on big tech's AI strategy. The disappointment isn't just internal. Compared to the S&P 500 — of which the behemoths of the M7 make up a third — the tech giants' stock growth has slowed to a crawl. Whereas the S&P 500 has grown 2.6 percent since January 1 and the NASDAQ Composite has made gains of 3 percent, as Barron's points out, a quick glance at M7's stock performance shows growth of just 1 percent. All together, the M7 aren't looking like the long-term golden goose investors had counted on. While some of this can be traced to the optics of big tech's rightward turn — Tesla sales, for example, have fallen down a pit while its CEO Elon Musk guts the federal government — it might more accurately be called a collective groan as rampant AI spending fails yet again to produce any meaningful returns. "There is no question either way that the high capital spending will continue to come under increasing scrutiny until investors can better understand the return on today’s massive investments," stock market analyst Adam Parker told Barron's. Over the past two years, companies like Google have taken to ransacking huge swaths of their workforce in order to dump resources into AI research, with a particular emphasis on "scaling." That's the widely parroted belief that simply increasing the number of computing power will lead to better and better AI. The M7 bought into that hook-line-and-sinker, and so did investors, dumping billions of dollars into AI ventures on the assumption that the line would only go up. Now, Barron's reports, the group's spending-to-sales ratio has hit a record high of 14.5 percent, showing that big tech is no closer to delivering on its lofty promises than it was a year ago. Though the tech giants are showing no signs of slowing down on AI spending yet, it's a sign that their leash isn't infinite — and that Wall Street, not Silicon Valley, has the final word. More on tech stocks: Nancy Pelosi's Husband Sold a Boatload of Nvidia Stock Right Before It Was Eviscerated by Chinese Startup Share This Article DISCLAIMER(S) Articles may contain affiliate links which enable us to share in the revenue of any purchases made. Registration on or use of this site constitutes acceptance of our Terms of Service. © Recurrent Ventures Inc, All Rights Reserved.
--------------------------------------------------

Title: Stock market today: Nasdaq leads Dow, S&P 500 higher amid latest inflation test, Trump's next tariff salvo
URL: https://finance.yahoo.com/news/live/stock-market-today-nasdaq-leads-dow-sp-500-higher-amid-latest-inflation-test-trumps-next-tariff-salvo-143043412.html
Time Published: 2025-02-13T14:30:43Z
Description: Investors assessed the prospects for US-Russia peace talks on Ukraine as they waited for fresh wholesale inflation data.
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq edge higher amid latest inflation test, Trump's next tariff salvo
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-edge-higher-amid-latest-inflation-test-trumps-next-tariff-salvo-143043364.html
Time Published: 2025-02-13T14:30:43Z
Description: Investors assessed the prospects for US-Russia peace talks on Ukraine as they waited for fresh wholesale inflation data.
--------------------------------------------------

Title: Forbes Daily: New York Faces Off With The Trump Administration
URL: https://www.forbes.com/sites/daniellechemtob/2025/02/13/forbes-daily-new-york-faces-off-with-the-trump-administration/
Time Published: 2025-02-13T13:01:40Z
Full Content:
Today’s Forbes Daily newsletter covers an inflation update, New York City accuses Trump of revoking funds, Honda and Nissan end merger talks, Chevron announces layoffs and more. The fight against inflation hit a snag Wednesday, as a new report found that prices rose faster than expected last month. The consumer price index rose 3% in January on an annual basis, a larger increase than economists predicted. One item contributed to much of that increase: eggs, which rose 15% from December to January as bird flu outbreaks continue. It’s the largest increase in egg prices in 10 years. The news led to stock and bond selloffs, with all three major indexes falling about 1% in early trading. The hotter-than-expected report also dashed investors’ hopes of further rate cuts. Photo by Andrew Harnik/Getty Images New York City officials alleged Wednesday that FEMA revoked $80 million in grants from the city’s accounts amid a feud with the Trump Administration over the city’s use of federal funds to house migrants. Elon Musk and FEMA’s acting administrator alleged $59 million in now-suspended FEMA funds were being used to house migrants in “luxury” hotels, but city officials pushed back, saying hotel fee payments were not made on luxury hotels. MORE: Attorney General Pam Bondi announced a lawsuit against New York state, Governor Kathy Hochul and two other state leaders over New York’s “green light law,” which allows all New Yorkers over 16 to apply for a driver license or learning permit regardless of their citizenship status. Japanese automakers Honda and Nissan announced on Thursday that they have ended discussions on a merger that would have created one of the largest automakers in the world. Despite calling off the merger, the companies said they still intend to collaborate under a strategic partnership “aimed at the era of intelligence and electrified vehicles,” which was first announced in March last year. Photo by Joe Raedle/Getty Images Energy giant Chevron will lay off up to 20% of its more than 45,000 employees, the company confirmed to Forbes on Wednesday. Chevron joins a growing list of major firms making job cuts this year, though one report shows layoff announcements declined 40% last month compared to January 2024. Qualified job candidates can afford to be selective when choosing a new employer, and high on workers’ lists are factors like on-the-job training and a culture of productive collaboration and inclusion. To help find employers who meet that criteria, Forbes released its 10th anniversary editions of its America’s Best Large Employers 2025 and America’s Best Midsize Employers 2025 lists. jamel toppin for forbes Junior Bridgeman, an NBA legend who spent 10 seasons with the Milwaukee Bucks, built a fast-food empire before selling most of his restaurants and using the proceeds to become a Coca-Cola distributor. Forbes now estimates he has a net worth of $1.4 billion, putting him in rare air as only three other NBA players have become billionaires—Michael Jordan, Magic Johnson and LeBron James. X, formerly known as Twitter, will pay $10 million to settle a lawsuit that was filed by President Donald Trump after his account was banned in 2021, according to the Wall Street Journal. The payout from the Elon Musk-led social media company follows a $25 million settlement from Meta last month to resolve a suit around Trump’s censorship claims, as his Facebook and Instagram accounts were also suspended following the January 6 Capitol attack. Tulsi Gabbard The Senate confirmed Tulsi Gabbard as Director of National Intelligence on Wednesday, voting largely along party lines despite some bipartisan skepticism about her foreign policy views. Senator Mitch McConnell (R-Ky.) cast the sole Republican vote against Gabbard, citing her “alarming lapses in judgement,” including previous controversial comments on the Russia-Ukraine war that many viewed as sympathetic toward Russia. The presidential seal has turned up on a golf marker at a fifth Trump golf course, potentially violating federal law that prohibits its use for commercial purposes. It’s the former president’s latest move to blur the line between public office and private business. President Donald Trump said in a social media post he had a “highly productive” call with Russian President Vladimir Putin about ending Russia’s war with Ukraine, the first known conversation between the two leaders since Trump took office. Defense Secretary Pete Hegseth said earlier Wednesday that negotiations to restore Ukraine’s borders to what they were before 2014 are “unrealistic,” and chasing that goal “will only prolong the war and cause more suffering.” More than 4,400 flights were disrupted as of Wednesday afternoon as a major winter storm system was expected to dump snow and ice on a vast swath of the country. Major U.S. airlines are issuing travel waivers allowing passengers flying in or out of impacted airports to rebook without paying the fare difference. A suspected illegal immigrant is placed in the back of a U.S. Customs and Border Protection border patrol vehicle in 2006. The Department of Homeland Security has asked Treasury Secretary Scott Bessent to deputize IRS agents to help with immigration. Some in the agency fear that moving agents away from that work would decrease compliance and lower collections. The Institute on Taxation and Economic Policy, a nonpartisan think tank, claims that taking IRS agents off the complex tax collections cases for which they are trained would reduce the amount of tax the agency can collect—less enforcement means that more people flouting the law would evade detection. ILLUSTRATION BY EMILY SCHERER FOR FORBES; PHOTOS BY SEREGRAFF, SERIKBAIB/GETTY IMAGES TOPLINE DOGE is coming for the Department of Defense. After taking the knife to a growing roster of federal agencies and causing widespread panic at others, employees from Elon Musk‘s Department of Government Efficiency are set to arrive at the Pentagon in the coming days, a person briefed on the effort told Forbes. Charged with cutting $2 trillion from the federal budget, they could bring an unprecedented shakeup to America’s sprawling military and its near $1 trillion budget. For decades, legislators on both sides of the aisle have struggled to rein in wasteful defense spending with little success, as the Pentagon has been repeatedly embarrassed by reports of egregious overspending—millions in untracked inventory, failed audits, the infamous $10,000 toilet seats. In an interview before Sunday’s Super Bowl, President Donald Trump said he expected DOGE’s review of the military will find “hundreds of billions of dollars of fraud and abuse.” Now, even as Musk’s cost-cutting machine faces mounting court challenges and threats from Democrats, Defense Secretary Pete Hegseth is rolling out the welcome mat. “I’ve been in touch with Elon Musk, who’s a great patriot interested in advancing the America First agenda,” he said on Fox News Tuesday. “There are plenty of places we want the keen eye of DOGE, but we will do it in coordination.” Hegseth said he would focus on cuts to weapons acquisitions and procurement, climate programs and headcount at headquarters. DOGE staffers will be employed directly by the Defense Department to focus solely on executing its cost cutting mandate, a person with direct knowledge said. WHY IT MATTERS “The existing ecosystem of the prime contractors, and legacy cold war personalities, knew what was coming, and decided to ignore it,” said Chris Miller, the former acting Secretary of Defense and a co-author of Project 2025, the conservative blueprint for the Trump Administration. “And we are seeing the chicken come home to roost.” MORE Military Tech Investors Go Looking For A Trump Bump At Mar-a-Lago The active ingredient in Ozempic and Wegovy could reduce alcohol cravings among those with alcohol use disorder, a new study suggests. Previous research has suggested that Novo Nordisk’s popular diabetes and weight loss drug could treat alcoholism: 30% : Reduction in drinks consumed among study participants who received the active ingredient semaglutide 30 grams: How many grams of alcohol participants on semaglutide drank during a final drinking session at the end of the experiment, compared to just under 60 grams of alcohol for those who received a placebo 28.9 million: The estimated number of people ages 12 and older who had alcohol use disorder in 2023, according to the National Institute on Alcohol Abuse and Alcoholism Transitioning to a managerial role for the first time can be challenging, and it’s critical for organizations to support those moving into leadership positions. Many first-time managers try to do everything themselves, have a tendency toward micromanaging, and focus too much on looking good for their boss. Start by practicing self-reflection, focus on learning from people with different views than your own, and spend time figuring out how to motivate others. A tech giant recently released an app to create customizable party invitations and even share a playlist with guests. Which company is it? A. Apple B. Spotify C. Samsung D. Google Check your answer. Thanks for reading! This edition of Forbes Daily was edited by Sarah Whitmire and Chris Dobstaff. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Why Tesla's Stock Is Crashing Under Trump
URL: https://www.newsweek.com/tesla-stock-elon-musk-trump-tariffs-2030466
Time Published: 2025-02-13T12:37:33Z
Description: Weak sales, Trump tariffs and the actions of their eccentric CEO could all be taking a toll on the electric vehicle powerhouse.
--------------------------------------------------

Title: CPI Rose More Than Expected
URL: https://realinvestmentadvice.com/resources/blog/cpi-rose-more-than-expected/
Time Published: 2025-02-13T11:05:00Z
Description: The January CPI report was troubling as it rose more than expected. The monthly rate rose by 0.5%, about 0.2% above expectations. The core monthly rate was a tenth of a percent above expectations, at 0.4%. The year-over-year CPI rate rose to 3% from 2.9%. The…
--------------------------------------------------

Title: Tesla, Intel and the fecklessness of corporate boards
URL: https://www.livemint.com/companies/news/tesla-intel-and-the-fecklessness-of-corporate-boards-11739433800573.html
Time Published: 2025-02-13T08:10:03Z
Full Content:
This is a Mint Premium article gifted to you. Subscribe to enjoy similar stories. Sitting on the board of a large American company is at once the plummest and most thankless work in business. Plum because, when everything is going right, you pocket $300,000 a year in cash and stock for showing up to a well-catered meeting every month and a half. Thankless because you seldom get credit for things going right but take the blame when they go awry. And awry they go with disturbing regularity. Consider two recent events. On December 2nd directors of Intel sacked its hapless chief executive, Pat Gelsinger. He had torched $150bn in shareholder value over his three-and-a-half-year tenure at the chipmaker, even as the fortunes of virtually all other chip firms were boosted by white-hot demand for computing power amid the artificial-intelligence (AI) revolution. Good riddance, then. But what took the board so long? The same day, a judge in Delaware reaffirmed her ruling from January to annul the eye-watering $56bn compensation package awarded in 2018 to Elon Musk by the board of Tesla, his electric-car company. The decision is controversial. Tesla’s shareholders have minted it lately thanks to the soaring price of the company’s stock. In June 75% of them voted to bless the mega-paycheque (and move the company’s incorporation from Delaware to management-friendlier Texas). Tesla called the latest ruling “wrong" and will be filing an appeal. Still, the imbroglio is a reminder that Tesla’s notionally independent directors who signed off on Mr Musk’s windfall were, in a judge’s opinion, anything but. Intel and Tesla represent two ways in which boards fail in their duty to represent investors and hold management to account. The case of Tesla shows how directors at the mercy of a domineering figure—be it a controlling shareholder, an imperial CEO or, like Mr Musk, both—have a powerful incentive to humour that person’s whims even if it might mean ignoring their fiduciary obligations. Equally, as possibly happened at Intel, thoroughly independent directors may lack enough of an incentive to care. Either way, the result looks like feckless passivity. Certain features of American capitalism are making both problems worse. For a start, more companies are going public with ownership structures in which some shares are more equal than others. Jay Ritter of the University of Florida keeps a tally of businesses that opt for such dual-class shares. When Google famously did so 20 years ago, they were a rarity. In the 2000s they accounted for fewer than one in ten initial public offerings on American exchanges. In the past five years they made up more than a quarter—and nearly half of tech IPOs, from Airbnb to Zoom. Dual-class arrangements appeal to founders, who are happy to take money from public markets but not to give up control. Investors used to be suspicious. After Snap, a social-media firm, sold stock with no voting rights in 2017, some of them persuaded S&P Dow Jones, a compiler of indices, to bar new arrivals with multiple share classes from the blue-chip S&P 500 and its sisters. Many have since had a change of heart, no doubt impressed by the trillion-dollar market values of dual-class darlings like Alphabet, Berkshire Hathaway and Meta. So has S&P Dow Jones, which in 2023 rowed back its ban. In January Mr Musk mused about adopting the structure at Tesla, apparently out of fear that his mere 13% stake makes him vulnerable to be “voted out by some random shareholder-advisory firm". Investors may be right to conclude that a heroic entrepreneur alone can lead a company to greatness. But they should have no illusions about the resulting emasculation of their representatives in the boardroom. This is easier to swallow when returns are healthy than when they are not. Just ask the shareholders of Snap, which is worth a third less today than at its IPO. The rise of emasculated directors is coinciding with that of absentee ones. Most are physically present at board meetings but, like at Intel, too many appear disengaged. In the latest annual survey by PwC, a consultancy, only 30% of executives rated their board’s performance as good or excellent. A fifth thought it poor. Most telling of all, 84% of executives did not think that directors overstepped the boundaries between themselves and management. Often this is precisely what a vigilant board ought to do. One reason for directors’ disengagement may be overcommitment. This was brutally exposed by covid-19, when boards often convened weekly rather than every seven weeks or so. A subsequent backlash against “overboarding" forced many directors to split their time between fewer companies. The typical busy S&P 500 director now sits on two boards, manageable in non-pandemic times. But this has been offset by the rising number of hours they spend on board duties as the world gets more complicated. Directors’ rutCompanies are also finding it harder to recruit heavy-hitters who won’t nod off. A headhunter recalls that chief executives, a mostly white and male bunch, were out of favour amid the push for greater diversity a few years ago. “Now that is the only thing companies want." With the share of active or former chief executives on S&P 500 boards in decline over the past few years, many won’t get it. There is no recipe for a perfect board. But some ingredients may make them better. Lucian Bebchuk of Harvard Law School proposes “enhanced-independence" directors, whose dismissal by an imperious boss can be vetoed by minority shareholders. A version of this exists in Britain and Israel. And whereas a board’s size or average age seems not to affect investor returns, S&P 500 firms with a wider age range tend to do better. Expedia’s eldest director, Barry Diller, a media baron, is 55 years older than the youngest, Alex Wang, an AI billionaire. Perhaps intergenerational tiffs keep everyone else on boards alert. Subscribers to The Economist can sign up to our new Opinion newsletter, which brings together the best of our leaders, columns, guest essays and reader correspondence. © 2025, The Economist Newspaper Limited. All rights reserved. From The Economist, published under licence. The original content can be found on www.economist.com Companies are also finding it harder to recruit heavy-hitters who won’t nod off. A headhunter recalls that chief executives, a mostly white and male bunch, were out of favour amid the push for greater diversity a few years ago. “Now that is the only thing companies want." With the share of active or former chief executives on S&P 500 boards in decline over the past few years, many won’t get it. There is no recipe for a perfect board. But some ingredients may make them better. Lucian Bebchuk of Harvard Law School proposes “enhanced-independence" directors, whose dismissal by an imperious boss can be vetoed by minority shareholders. A version of this exists in Britain and Israel. And whereas a board’s size or average age seems not to affect investor returns, S&P 500 firms with a wider age range tend to do better. Expedia’s eldest director, Barry Diller, a media baron, is 55 years older than the youngest, Alex Wang, an AI billionaire. Perhaps intergenerational tiffs keep everyone else on boards alert. Subscribers to The Economist can sign up to our new Opinion newsletter, which brings together the best of our leaders, columns, guest essays and reader correspondence. © 2025, The Economist Newspaper Limited. All rights reserved. From The Economist, published under licence. The original content can be found on www.economist.com Download the Mint app and read premium stories Log in to our website to save your bookmarks. It'll just take a moment. You are just one step away from creating your watchlist! Oops! Looks like you have exceeded the limit to bookmark the image. Remove some to bookmark this image. Your session has expired, please login again. You are now subscribed to our newsletters. In case you can’t find any email from our side, please check the spam folder. This is a subscriber only feature Subscribe Now to get daily updates on WhatsApp
--------------------------------------------------

Title: Is This Time Different?
URL: https://www.lewrockwell.com/2025/02/jim-quinn/is-this-time-different/
Time Published: 2025-02-13T05:01:00Z
Description: “We will not have any more crashes in our time.” – John Maynard Keynes, leading British economist, in 1927 “Stock prices have reached what looks like a permanently high plateau. I do not feel there will be soon if ever a 50 or 60 point break from present leve…
--------------------------------------------------

Title: Riot Taps Advisors to Explore AI Partnerships as Bitcoin Miners Eye New Revenue Streams
URL: https://decrypt.co/305740/riot-taps-advisors-to-explore-ai-partnerships-as-bitcoin-miners-eye-new-revenue-streams
Time Published: 2025-02-13T03:40:01Z
Full Content:
Riot Taps Advisors to Explore AI Partnerships as Bitcoin Miners Eye New Revenue Streams $97,641.00 $2,704.07 $2.77 $665.98 $194.85 $0.999991 $0.275005 $0.788739 $2,703.41 $0.236189 $97,390.00 $19.03 $3,225.26 $0.35119 $25.83 $3.42 $131.27 $0.00001644 $0.229064 $3.78 $9.77 $1.00 $25.88 $2,704.38 $5.11 $7.54 $335.65 $5.07 $1.00 $9.85 $2,865.24 $3.81 $231.78 $0.00001 $3.43 $1.28 $27.21 $254.44 $18.83 $6.06 $1.04 $7.11 $1.04 $380.33 $20.94 $52.03 $23.35 $0.108434 $0.03434103 $0.320196 $31.90 $97,638.00 $0.926936 $0.089356 $0.287739 $4.59 $3.44 $4.89 $0.999568 $0.477701 $0.776062 $97,457.00 $3.94 $0.516381 $2,706.79 $3.07 $1.81 $201.81 $2,800.57 $0.098864 $5.32 $1.14 $12.30 $15.08 $96,936.00 $0.460271 $0.943088 $0.00001818 $0.796284 $94.78 $1.37 $1.37 $3,035.07 $0.559552 $0.139538 $0.021562 $1.24 $2,856.22 $0.997715 $0.238854 $0.02262746 $18.38 $0.02405814 $96,128.00 $0.656077 $246.93 $0.400037 $0.01227821 $3.21 $0.00009732 $0.891894 $26.73 $2,798.41 $0.00000087 $0.235634 $1.30 $994.44 $666.94 $0.519952 $211.43 $2.67 $11.09 $39.40 $97,412.00 $1.25 $0.213752 $0.131492 $1.001 $0.02200974 $2,886.41 $0.763983 $0.274382 $4.43 $0.691948 $2,854.75 $3.75 $0.515896 $0.83289 $2,703.63 $1.00 $0.01006527 $0.997196 $22.51 $0.998497 $5.79 $0.32934 $9.27 $3.68 $2,908.47 $0.23017 $0.119184 $0.793113 $0.319914 $1.001 $95,305.00 $0.00630562 $0.01077901 $2,704.07 $0.059885 $0.55051 $2,702.10 $29.24 $0.00002636 $0.722962 $0.075415 $32.62 $0.474187 $0.178948 $56.99 $0.437973 $0.998099 $97,545.00 $97,103.00 $0.01085352 $2.15 $0.00569168 $1.35 $0.429198 $1.90 $0.00846555 $0.04584924 $97,429.00 $172.80 $85.54 $0.444068 $0.00000045 $0.47825 $0.434741 $2,681.79 $0.354274 $5.46 $0.00007742 $1.00 $1.005 $25.32 $0.41382 $1.00 $1.002 $2,946.20 $2,702.02 $1.078 $1.63 $0.108328 $2,704.30 $0.155814 $1.55 $1.57 $1.078 $0.269516 $97,821.00 $2,843.40 $6.60 $2,699.50 $0.00481359 $0.356475 $0.793326 $2,925.12 $3.32 $0.341492 $0.04990956 $0.803771 $0.750061 $0.990931 $1,960.20 $3.00 $0.03361657 $0.664322 $20.52 $27.22 $3.47 $0.609106 $1.012 $0.764628 $97,478.00 $0.00366827 $0.00000081 $0.320547 $0.140168 $2,808.47 $0.00298021 $0.03965433 $0.150393 $7.83 $0.03101687 $2,921.49 $0.00166463 $109.97 $0.04358002 $0.01479631 $0.00068698 $0.00628565 $0.00288 $0.0009636 $3,010.37 $1.001 $0.999561 $0.186238 $0.282405 On Wednesday, bitcoin mining company Riot Platforms said it is exploring partnerships in the artificial intelligence and high-performance computing sector as it aims to shore up its business and generate sustainable revenue streams. The NASDAQ-listed company said it would ramp up evaluations for potential AI and high-performance computing (HPC) uses at its Corsicana Facility in Navarro County, Texas, citing increased interest from multiple potential partners. Riot's exploration of AI computing capabilities reflects a growing trend among Bitcoin miners to leverage their substantial power infrastructure and data center expertise for additional revenue opportunities beyond crypto mining. The move comes as mining difficulty on the Bitcoin network has reached a historic high, peaking at 114.7 terahashes when it arrived at block height 883,502 on February 10, data from CoinWarz shows. A "confluence" of on-chain indicators suggests miners are experiencing more "financial strain than before, leading them to find new sources of revenue," Jaehyun Ha, a research anaylst at Presto Research, told Decrypt. Meanwhile, revenue from Bitcoin mining hardware has significantly dropped over the year, to as low as $10.4 a day over an operating margin of 60% for an average ASIC unit like the Antminer S21+ Hydro, according to data from Hashrate Index. Alongside its AI explorations, Riot appointed three new directors with relevant expertise: Hut 8 Mining CEO Jaime Leverton, former Meta senior engineer Doug Mouton, and real estate investment veteran Michael Turner. Moving in to explore AI and high-performance computing is part of Riot's initiatives to "maximize value" for its "entire portfolio of assets," Riot CEO Jason Les said in a statement. Similar strategic shifts by other major crypto mining operators are at play. Leverton, who just joined Riot's board, previously led her company's expansion into HPC by acquiring TeraGo's data center business. Companies such as Hut 8 and Core Scientific are repurposing their infrastructure for AI workloads, leveraging existing power access and data centers. These diversification moves are also aimed at reducing dependence on Bitcoin's price fluctuations while capitalizing on the growing demand for AI computing resources. However, the company cautioned there's no guarantee its assets are suitable for AI/HPC conversion or that partnerships can be negotiated on favorable terms. Still, Bitcoin mining and other public crypto firms are beating the market, with their overall market cap expanding by 14% to bring their valuations to $108 billion, according to JPMorgan. Riot also operates Bitcoin mining facilities in Rockdale, Texas, and Kentucky, along with electrical switchgear engineering operations in Colorado. The company's stock, which trades on the NASDAQ under the ticker RIOT, is up 0.2% on the day to $11.16, Google Finance data shows. Edited by Sebastian Sinclair Your gateway into the world of Web3 The latest news, articles, and resources, sent to your inbox weekly. © A next-generation media company. 2025 Decrypt Media, Inc.
--------------------------------------------------

Title: Is the iShares S&P 500 ETF (IVV) still a brilliant buy after storming higher?
URL: https://www.fool.com.au/2025/02/13/is-the-ishares-sp-500-etf-ivv-still-a-brilliant-buy-after-storming-higher/
Time Published: 2025-02-12T22:58:58Z
Description: Should investors still buy this fund or is it too expensive?
The post Is the iShares S&P 500 ETF (IVV) still a brilliant buy after storming higher? appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: The Trump premium isn't being baked into Meta's stock: Analyst
URL: https://finance.yahoo.com/news/the-trump-premium-isnt-being-baked-into-metas-stock-analyst-153403633.html
Time Published: 2025-02-12T15:34:03Z
Description: Meta's stock is reacting more so to fundamentals than Zuck's closer proximity to the White House.
--------------------------------------------------

Title: Tesla's stock slide looks to be unrelenting
URL: https://finance.yahoo.com/news/teslas-stock-slide-looks-to-be-unrelenting-111050162.html
Time Published: 2025-02-12T11:10:50Z
Description: Tesla's shares continue to sell off as concerns mount on Elon Musk's many distractions.
--------------------------------------------------

Title: Best VR Porn Games to Play in 2025
URL: http://chicagoreader.com/adult/vr-porn-games/
Time Published: 2025-02-12T05:11:42Z
Full Content:
VR porn games have taken adult entertainment to the next level, thanks to huge leaps in headset technology and production quality. Gone are the days of static screens, now, you can step inside the action with hyper-realistic 3D visuals, smooth animations, and responsive gameplay that puts you in control. Whether curious or a longtime fan, VR offers a completely new way to experience adult content, making it more immersive and interactive than ever. If you haven’t tried VR porn yet, now’s the perfect time to dive in and see what all the hype is about! To save you the trial and error, we’ve put together the ultimate list of the 10 Best VR porn games on the market. We’ve done deep dives into the top ten, breaking down what makes them stand out, and rounded up the rest as honorable mentions. Expect cutting-edge technology, jaw-dropping realism, immersive storylines, and erotic encounters that push the limits of virtual pleasure. Ooh la la! Is it all fun and games? While VR porn games offer immersive experiences, it’s essential to remember that they also come with potential downsides, particularly in how they can affect players’ socialization, relationships, and worldview. One major concern is social isolation. Because VR can feel incredibly realistic, some users may find themselves spending more time in virtual relationships rather than engaging with real-world connections. Over time, this could lead to difficulty in forming and maintaining personal relationships, as interacting with AI-driven characters is often easier and less demanding than dealing with real people. Another issue is the reinforcement of unrealistic expectations. Many VR adult games allow for complete customization and control, something that simply isn’t possible in real-world relationships. This can create distorted perceptions of intimacy, where players expect perfect responses, unrealistic bodies, or total dominance over interactions, which can lead to dissatisfaction with real-life partners. The Impact on Attitudes Toward Women and Femme-Presenting People Another potential downside of VR porn games is how they reinforce misogynistic attitudes or unrealistic expectations toward women and femme-presenting people. Many of these games prioritize male-centered fantasies, often portraying women as hypersexual, submissive, or endlessly accommodating without emotional needs or boundaries. Over time, frequent exposure to these dynamics can shape unrealistic or harmful views on real-life relationships. It’s possible that players may come to expect one-sided interactions that don’t require mutual respect or effort. Yikes! Additionally, some games promote power imbalances, reinforcing a false sense of intimacy that is inherently based on dominance and control, rather than consent and connection. While fantasy has its place, repeated exposure without critical thinking can blur the line between healthy kinks and problematic beliefs. For those already struggling with social interactions, this can potentially deepen resentment or entitlement, particularly toward women who don’t conform to these scripted, fantasy-based behaviors. Engaging with VR porn games doesn’t automatically lead to misogynistic beliefs, but it’s important to critically examine the content, challenge harmful narratives, and balance virtual experiences with real-world perspectives on intimacy, agency, and respect. But it’s not all bad VR porn games also offer unique social and personal benefits, beyond just entertainment. For some, they provide a safe space to explore fantasies, build confidence, and understand personal desires without pressure. Multiplayer games foster social connections, allowing users to interact with others in a low-stress environment. VR can also enhance long-distance relationships, creating shared intimate experiences despite physical distance. When balanced with real-world interactions, VR porn games can be a fun, engaging, and even confidence-boosting outlet. 1. DezyRed – Best for immersive and mentally stimulating interactions DezyRed is carving out a space in the world of interactive adult entertainment by combining high-end technology with immersive experiences. Unlike traditional cam sites or VR platforms, DezyRed focuses on real-time interactivity, allowing users to engage with digital models in a way that feels dynamic and personal. This is thanks to AI-powered avatars that aren’t just static characters but responsive, learning entities that adapt to interactions. This means conversations and experiences feel more fluid and natural, rather than pre-scripted or repetitive. The site also utilizes motion capture and realistic rendering to create high-detail visuals that go beyond standard adult CGI. Another big plus is accessibility. DezyRed works across multiple platforms, from desktop to VR, making it easy to dive in no matter what your setup is. If you’re into next-gen adult technology where AI, VR, and real-time interaction blend seamlessly DezyRed is definitely worth exploring. Key Features: Compatible Tech: Pricing: To maximize your experience: You can watch for free, but you’ll need to purchase credits to interact, so use your initial free credits wisely! 2. VRPorn – Best for overall variety VRPorn is one of the best destinations for high-quality adult virtual reality content, and it’s not just about the videos, it’s the tech that makes it stand out. If you’re into immersive experiences, VRPorn takes things to another level with cutting-edge visuals, optimized playback, and seamless compatibility with all the major VR headsets. What makes it worthwhile? First, the sheer variety. Whether you’re into big-budget studio productions or independent creators making waves with top notch interactive adult content, there’s a huge range of options. The site also stays ahead of the curve with crisp 6K and even 8K resolution videos, making everything feel incredibly lifelike. Plus, their smart UI makes it easy to navigate, download, and stream without the hassle. Another big win is the tech integration. VRPorn works with multiple devices, from Oculus to PlayStation VR, ensuring that no matter what setup you have, you get a smooth, high-quality experience. Many scenes also feature depth and head-tracking, so it’s not just a 360° video—it actually feels like you’re there. If you’re curious about VR adult content, VRPorn is a solid choice, balancing high-tech innovation with a massive, well-curated library. Key Features: Sign-up grants you access to thousands of VR adult scenes, not just games Compatible Tech: Pricing: To maximize your experience: VRPorn is perfect if you’re new to the VR porn world and looking to figure out what you’re into and what kind of games you want to play. 3. Holodexxx – Best for visual realism Holodexxx is challenging the limits of adult content by blending cutting-edge technology with hyper-realistic virtual experiences. Unlike standard VR porn sites, Holodexxx is all about high-fidelity, interactive adult entertainment, bringing digital performers to life in a way that feels next-level. Holodexxx uses advanced 3D scanning and motion capture to create ultra-detailed, lifelike models of real adult performers. Instead of just watching a scene, you can interact with these digital avatars in real-time, making the experience feel much more immersive than traditional VR. The attention to detail in textures, lighting, and movement adds an extra layer of realism that sets it apart from standard 360° videos. Another standout feature is the level of interactivity. Instead of being a passive viewer, you have control over certain aspects of the experience, which makes everything feel more personal and engaging. The platform also keeps evolving, integrating AI and improving realism with each update. For anyone interested in the future of adult entertainment, Holodexxx is worth keeping an eye on. It’s not just about consuming content—it’s about stepping into a fully immersive, interactive world. Key Features: Compatible Tech: Pricing: To maximize your experience: Join the Holodexxx Patreon and Discord for deals, updates, and special members-only benefits 4. XStoryPlayer – Best for the thrill of the chase XStoryPlayer is a unique platform because it’s a blend of adult gaming and interactive storytelling, offering an experience that goes well beyond passive viewing. If you’re into narrative-driven adult content with actual gameplay mechanics, this is a site worth exploring! The beauty of XStoryPlayer is that it uses a physics-based engine to create realistic character movements, responsive environments, and interactive dialogue. Instead of just watching a scene unfold, you get to shape the experience, making choices that influence how the story progresses. The game mechanics include everything from conversation algorithms to physics-driven intimacy, making interactions feel much more engaging than traditional adult content. Another highlight is the modding potential: the platform is built to be customizable, allowing users to tweak or even create their own scenarios. If you’re looking for something that combines adult content with immersive, player-driven storytelling, XStoryPlayer offers a compelling, tech-forward experience. Key Features: Compatible Tech: Pricing: To maximize your experience: You can purchase straight from XMoon Productions 5. DominatrixSimulator – Best for virtual BDSM enthusiasts Dominatrix Simulator is a standout in the adult gaming space, offering a deeply immersive, BDSM-themed experience that goes beyond standard VR content. Instead of just watching, you participate, stepping into a submissive role where your choice, and obedience, shape the interactions. The tech behind it is what makes it really shine. Designed for VR, the game uses voice recognition, motion tracking, and realistic AI-driven Dommes who react dynamically to your responses. The visuals are polished, with detailed environments and fluid character animations that make the experience feel more lifelike. What sets it apart is its emphasis on psychological engagement. It’s not just about visuals, it’s about power exchange, control, and submission in a way that feels organic and immersive. Whether you’re new to BDSM or experienced, Dominatrix Simulator offers a specific and unique, tech-driven way to explore dominance and submission in a safe, virtual space. Key Features: Compatible Tech: Pricing: To maximize your experience: Since the whole vibe of this game is about FemDom and being a submissive man “under her boot”, if this is a major kink for you, consider the pro-rated savings of the Yearly or Lifetime memberships. 6. SinVR – Best for fantasy fulfillment If you’re into adult content with a bit of fantasy and interactivity, SinVR is worth checking out.Unlike standard VR porn, it offers a more game-like experience where you can customize characters, explore different scenarios, and interact with models in a way that feels dynamic. The tech behind SinVR is what makes it stand out. The characters are well-rendered, with detailed animations and realistic physics that add to the full immersion. The platform also supports many, but not all, headsets, making it accessible to most users. Key Features: Compatible Tech: Pricing: To maximize your experience: Push your imagination to the limit with the pre-set characters and scenes, including 200+ sex positions and scene options. 7. Citor3 – Best for games to get you off under a time crunch Citor3 is an adult gaming platform that blends interactive storytelling with immersive 3D gameplay. Unlike some VR porn games that rely on pre-rendered scenes, Citor3 lets you erotically engage with AI-driven characters, explore detailed environments, and experience fluid, physics-based interactions. The gameplay focuses on immersion, featuring smooth animations and responsive mechanics that let you control the pace and direction of encounters. Whether you’re following a structured narrative or experimenting with freeform interactions, the tech behind Citor3 ensures a visually polished and engaging experience. For those looking for a more hands-on, interactive approach to adult gaming, Citor3’s 30-minute gameplay is a great way to see how its mechanics and world-building come together. If you’re into adult content with a focus on interactivity and player-driven experiences, Citor3 is definitely worth a look. Key Features: Compatible Tech: Pricing: To maximize your experience: You can play some Citor3 games on VRPorn.com (and they have great prices) 8. VRLove – Best for a choose-your-own-adventure experience VRLove goes beyond simple 360° videos, offering a more immersive and customizable approach. Instead of just watching, you engage with AI-driven characters in a fully 3D environment, creating a more dynamic and personal experience. The technology behind VRLove is a major highlight: realistic character models, lifelike animations, and thoughtful details in the surroundings make everything feel more natural. In terms of immersion, VRLove is next level, with teledildonic support, which allows users to sync compatible devices for real-time, interactive pleasure. This means the experience isn’t just visual, it becomes physical; the integration of haptic feedback enhances the realism, making interactions with AI-driven characters feel more responsive and engaging. Combined with VRLove’s high-quality visuals, smooth animations, and customization options, this feature adds a whole new layer of interactivity and sexiness. Key Features: Compatible Tech: Pricing: To maximize your experience: Take advantage of the Story Mode/Free Mode options to experience a variety of scenes. 9. 3DXChat – Best for players looking for real human sexual interaction via VR 3DXChat is more than just a VR adult game—it’s a full-fledged social platform where users can connect, chat, and explore intimate experiences together. Unlike solo VR experiences, 3DXChat thrives on real-time multiplayer interaction, letting you meet and engage with others in a shared virtual world. The game features detailed avatar customization, so you can create a look that suits your style. You can then interact through text or voice chat, dance at virtual clubs, hang out in scenic environments, or engage in more … intimate encounters. VR enhances the social element, making it easy to interact, and giving you the feeling of being present in the room with other users. Whether you’re into casual flirting, role playing, or building deeper connections, 3DXChat offers a unique blend of social networking and adult entertainment in a visually engaging space. Key Features: Compatible Tech: Pricing: To maximize your experience: Players who are used to interacting with stock characters, not real humans, may need to adjust their approach and behavior. 10. Let’s Play with Nanai – Best for a virtual girlfriend experience Let’s Play With Nanai is a hentai VR game that focuses on interactivity and immersion, offering a more hands-on experience than standard animated content. Designed specifically for VR, it lets players engage with Nanai, a “virtual girlfriend”, in a fully 3D space, with smooth animations and responsive mechanics that make Nanai come alive. Users will be drawn to the high level of interactivity and realism compared to traditional hentai games. Instead of just watching or clicking through a scripted scene, players get full control over interactions, making the experience feel more immersive, interpersonal and dynamic. Nanai is also popular for its VR optimization and runs smoothly on major VR headsets, with high-quality character models, lifelike animations, and realistic physics that enhance movement and touch responsiveness. Key Features: Compatible Tech: Pricing: To maximize your experience: Let’s Play with Nanai is easy to find, but you can purchase straight from IMagineVR and join their Patreon for extra goodies and up to date info on new releases, etc. Can’t find quite what you’re looking for in our top 10? Try these: Choosing the right adult VR game depends on what kind of experience you’re looking for. Here are some key factors to consider: VR Compatibility Not all VR games work with every headset. Check if the game supports your specific device (e.g., Oculus, HTC Vive, Valve Index, PSVR). Some games also run in desktop mode if you don’t have VR gear. Level of Interactivity Do you want a passive viewing experience (like 360° videos) or a fully interactive game with AI-driven characters, physics-based interactions, and player choices? Graphics & Animation Quality Some games focus on highly detailed, realistic models, while others go for a stylized or anime-inspired look. Consider what visual style appeals to you. Customization Options Many games allow you to modify characters, outfits, and scenarios. If personalizing the experience is important, look for a game with strong customization features. Multiplayer vs. Solo Experience Some games, like 3DXChat, emphasize social interaction, while others are single-player-focused. Decide if you want a solo experience or a multiplayer world with real-time interactions. Teledildonics Support If you’re interested in haptic feedback devices, check if the game is compatible with interactive toys like Lovense or Kiiroo for a more immersive experience. Storyline & Gameplay Depth Some games offer deep narratives and role-playing elements, while others are more focused on quick, no-strings-attached experiences. Choose based on whether you prefer a story-driven game or a casual encounter. Common questions about this new and exciting erotic technology. Are VR porn games safe to download and play? As with any software, it’s best to download from official websites or trusted platforms to avoid malware. Always check reviews, system requirements, and privacy settings before installing. What do I need to play adult VR games? To play adult VR games, you’ll need a VR-compatible headset (like Oculus Quest, HTC Vive, or Valve Index), a VR-ready PC (for high-end games), and enough storage space for downloads. Some games also offer non-VR modes for standard screens. What are the best VR headsets? The best headset depends on your budget and preferences. High-end options like the Valve Index and HTC Vive Pro offer top-tier visuals and tracking, while more affordable headsets like the Oculus Quest 2 provide wireless freedom and solid performance. Many PC-based games also support Windows Mixed Reality headsets. Can I play VR games without a VR headset? Yes! Some games offer a desktop mode where you can play with standard controls and a monitor. However, you won’t get the same level of immersion as you would in VR. Do VR games require an internet connection? It depends on the game. Single-player experiences usually work offline, while multiplayer games (like 3DXChat) need an active internet connection for real-time interaction with other players. Are VR porn games multiplayer or single-player? Both options exist! Games like 3DXChat offer multiplayer experiences where you can interact with other players, while games like Dominatrix Simulator focus on single-player immersion with AI-driven characters. Can I customize characters and experiences? Many adult VR games offer deep customization, allowing you to adjust character appearances, outfits, and even personalities. Some also let you modify environments and interaction styles. Do VR porn games support teledildonics? Many do, yes! Some games sync with interactive sex toys (check out Lovense or Kiiroo) to provide haptic feedback, making in-game actions feel more realistic. Are there VR porn games with more intellectual or story-driven gameplay? Absolutely! Games like XStoryPlayer and Dominatrix Simulator feature narrative-driven experiences, where your choices affect the outcome. These are great if you prefer an interactive story over casual encounters. How realistic are the animations and physics in adult VR games? It varies by game. Some use advanced physics engines for realistic movements and interactions, while others prioritize a stylized or fantasy aesthetic. Games that support teledildonics also add an extra layer of realism with synchronized haptic feedback. Love them or hate them, VR porn games have revolutionized the way people engage with adult content, offering unparalleled interactivity, and personalization. With advancements in VR headset technology these experiences have evolved far beyond traditional adult media, and who knows where they will take us next! When enjoyed in moderation, VR porn games can be a fun, confidence-boosting outlet, allowing users to safely explore desires, improve social skills in virtual spaces, or even strengthen connections in long-distance relationships. The addition of teledildonics further enhances realism, bridging the gap between fantasy and physical sensation. Of course, as with all media, critical engagement is key. Some VR porn games reinforce harmful stereotypes or unrealistic portrayals of women and relationships, so always remain critical of your consumption. It’s essential to recognize the difference between fantasy and reality, boundaries and consent, and to balance virtual experiences with real-life connections for healthy perspectives on intimacy. Ultimately, VR porn games are a thrilling and ever-evolving frontier in adult entertainment. Whether you’re a casual player or a dedicated enthusiast, they offer a unique way to engage with sexuality, pushing the boundaries of pleasure and personal exploration. As technology continues to evolve, so too will the experiences available, making this an exciting space to watch, and play in, for years to come. Violet Fawkes is a pleasure educator and sex-positive advocate committed to guiding individuals on their journeys of sexual self-discovery. She offers insightful product reviews, including analyses of sex toys and pleasure products. Her writing delves into topics like body acceptance, ethical non-monogamy, and the nuances of intimate relationships, fostering a comprehensive understanding of sexual wellness. Violet’s mission is encapsulated in her guiding principles: Explore. Empower. Enrich. She encourages curiosity, creativity, and courage in personal exploration, aiming to dismantle societal taboos surrounding sexuality and promote a more open, informed, and accepting dialogue. Instagram @violet_fawkes Threads @violet_fawkes Bluesky @violet-fawkes.bsky.social
--------------------------------------------------

Title: Super Micro 'confident' it will meet SEC deadline and reach $40 billion next fiscal year
URL: https://www.cnbc.com/2025/02/11/super-micro-confident-it-will-meet-sec-deadline.html
Time Published: 2025-02-11T22:59:13Z
Description: Super Micro expressed confidence it will meet a key SEC deadline this month and hit $40 billion in revenue in fiscal 2026.
--------------------------------------------------

Title: Elon Musk’s Now $42 Billion Poorer This Month—As Scrutiny On Trump Ties Grows
URL: https://www.forbes.com/sites/dereksaul/2025/02/11/elon-musks-now-42-billion-poorer-this-month-as-scrutiny-on-trump-ties-grows/
Time Published: 2025-02-11T20:34:07Z
Full Content:
Tesla stock’s steep losses in early 2025 have cut tens of billions of dollars away from the net worth of the world’s richest man, the electric vehicle maker’s CEO Elon Musk, whose outspoken role in President Donald Trump’s administration draws questions about what it may mean for Tesla. Musk talks at an October rally for Trump in New York. Shares of Tesla declined 6.3% to $328.50 in Tuesday trading, closing at its lowest share price since Nov. 15. Catalyzing Tuesday’s drop were advancements in autonomous driving from its Chinese EV rival BYD and a skeptical note from Oppenheimer analysts led by Colin Rusch warning Musk’s “political activity risks consumer backlash.” The Oppenheimer note, which referenced “concerning” January sales in China and Europe, comes a day after Stifel analysts led by Stephen Gengaro similarly warned “the negative downturn in consumers' perception of Elon Musk could result in a ‘headwind to sales’ for Tesla,” which inspired a 3% loss in Monday trading for Tesla shares. Tesla stock is now down 18.8%, or more than $76 per share, in February, declining almost 32% from its all-time closing high of $479.86 set Dec. 17. Nobody has been affected more on a gross basis by the Tesla slump than its largest shareholder, Musk, who owns nearly 13% of the company and a further 9% equity award pending legal appeal (Forbes discounts Musk’s 9% bonus by 50% in its valuation to reflect its legal uncertainty). Musk was worth $378.8 billion Tuesday, according to Forbes’ latest calculations – a $42.8 billion decrease from his $421.6 billion net worth at the end of January – though he remains about $130 billion wealthier than the next richest person on the planet, Meta CEO Mark Zuckerberg. $12.5 billion. That’s how much Musk’s fortune declined during Tuesday’s Tesla slump as the 53-year-old registered his lowest end-of-day net worth since Dec. 10. The Oppenheimer analysts wrote they believe “Musk's political activity has fans in certain circles, but that his public life risks alienating consumers and employees as the Trump administration tests the limits of its power.” Musk’s more than $150 billion estimated Tesla stake is the biggest contributor to his net worth, while his stakes in his private multi-billion-dollar companies SpaceX, X and xAI make up a majority of the rest of his fortune. Tesla stock is still up about 30% since Election Day, though the initial rally has lost steam as the company’s fourth-quarter earnings fell short of Wall Street consensus forecasts and its car sales in Europe and China declined. After donating nearly $290 million toward Trump and other GOP causes ahead of the 2024 election, Musk has played a critical role in the opening weeks of Trump’s second term, most notably heading the DOGE agency aimed at significantly pulling back the federal government’s footprint. Musk submitted a $97.4 billion bid Monday to buy the nonprofit arm of OpenAI, a competitor to his generative artificial intelligence startup, led by rival Sam Altman. Musk’s offer is “a distraction from TSLA's challenges,” wrote Rusch. Musk and Altman cofounded OpenAI in 2015 before a messy breakup over the company’s mission. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Here are 3 of my most profitable investments in ASX shares ever (and which one I'd buy more of right now)
URL: https://www.fool.com.au/2025/02/12/here-are-3-of-my-most-profitable-investments-in-asx-shares-ever-and-which-one-id-buy-more-of-right-now/
Time Published: 2025-02-11T20:30:00Z
Description: I reckon only one of these shares is worthy of a buy today. 
The post Here are 3 of my most profitable investments in ASX shares ever (and which one I'd buy more of right now) appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: This corner of the stock market could be an under-the-radar winner of the AI spending boom
URL: https://markets.businessinsider.com/news/stocks/retail-stocks-ai-spending-mega-cap-capex-artificial-intelligence-walmart-2025-2
Time Published: 2025-02-11T20:15:10Z
Full Content:
Wall Street is assessing the impact of Big Tech's massive capex plans for this year, but there is another corner of the stock market that's set to benefit from big AI spending, Morgan Stanley said. According to Morgan Stanley, the spending race among AI "hyperscalers" is an overlooked tailwind for retail sector stocks like Walmart and Target. "Retail is on the cusp of a technology leap with AI, data, and automation. While retailers may not pursue AI infrastructure investments similar to tech companies, the tech capex boom suggests retailers are on the verge of and should benefit from a technology inflection," equity analyst Simeon Gutman wrote. This year's combined $300 billion of announced AI investing between Microsoft, Amazon, Google, and Meta, might not seem like a development set to directly impact retailers, but Morgan Stanley predicts is is likely to lead to a boost in capex among retailers who can afford it. That's as the spending boom will offer big-box stores investing opportunities to improve in-store experiences, advertising, and automation. Those best equipped to invest will see market shares gain, a trend likely to appear amid coming earnings reports. "The big should keep getting bigger and at a faster pace," the analysts wrote. Capital expenditures among retailers are projected to reach $55 billion, marking an average of around 7% year-on-year growth in 2025. Walmart, Costco, Target, Kroger, and Home Depot account for about 69% of total spend in hard, soft, and food retail. Walmart will lead the cohort in capex spening. Estimated to spend $22 billion this year, Walmart's expenditures are four times larger than what Costco is forecast to spend. Bank of America holds a $110 price target for Walmart stock, expecting improved profitability as well as digital advertising and marketplace growth. One challenge could pose a risk Gutman's thesis — cheaper AI allows smaller retailers to join in on AI growth. The possibility has emerged amid the recent introduction of DeepSeek, a Chinese AI that is supposedly cheaper and as capable as Silicon Valley tech. Its implications have sparked doubt about Big Tech's AI spending, briefly causing a trillion-dollar market wipeout last month. If DeepSeek does discount AI expenditures, under-the-radar retail names aren't the only ones set to win out. This scenario would make software stocks also worth buying. "That said, we do not think this will be the case in the near-term and, for now, retailers best equipped to spend should be able to widen their advantages," Gutman wrote. Indices Commodities Currencies Stocks
--------------------------------------------------

Title: 3 new reasons to dump Magnificent 7 stocks
URL: https://finance.yahoo.com/news/3-new-reasons-to-dump-magnificent-7-stocks-200016898.html
Time Published: 2025-02-11T20:00:16Z
Description: The Magnificent 7 trade is under pressure. What's your next move?
--------------------------------------------------

Title: The Raging Debate: When Will Quantum Arrive?
URL: https://www.forbes.com/sites/karlfreund/2025/02/11/the-raging-debate-when-will-quantum-arrive/
Time Published: 2025-02-11T18:51:09Z
Full Content:
IBM CEO Arvind Krishna There has been considerable discussion and stock price volatility of late surrounding the expected timing of useful applications and hardware for quantum computing. One month after Google created excitement around its Willow quantum chip, Nvidia CEO Jensen Huang and Meta CEO Mark Zuckerberg, started a food fight on Wall Street by predicting that quantum computing won’t be a significant computing paradigm for at least a decade. Quantum stocks dropped over 30%. Subsequently, Bill Gates joined in, saying, “There is the possibility that the [Nvidia founder and CEO Jensen Huang] Not one to wade into a shouting match, IBM (a client of Cambrian-AI Research) has been quietly and steadily advancing quantum computing sciences and use cases, as explained in a series of webinars, covering quantum applications in energy, financial modeling, electronics, and health care. Last year, IBM also published the 4th edition of a dazzling coffee table book on the coming Quantum Decade. IBM probably has hundreds of scientists working on quantum developing hardware, software, and ecosystems to lead in this exotic technology. While IBM and other quantum innovators like Microsoft, Google, AWS, and startups see hundreds of applications in development today, Zuckerberg and Huang are probably looking for big-impact applications. And there aren’t any, at least not yet. The current quantum applications being developed fill specific scientific niches with little industrial and economic impact. They are important niches to the scientists who can now solve previously unsolvable problems, but they may not constitute another multi-billion-dollar market. Those applications will take much faster quantum computers and new algorithms that can exploit the hardware. IBM and its competitors are developing hardware, software tools, and algorithms that could deliver those billion-dollar applications in the next 5-10 years. Specific uses of quantum computing in physics and chemistry are already pushing research forward. Still, these experiments demand faster hardware, error correction, and new algorithms beyond the R&D setting. IBM thinks it can achieve quantum advantage sometime in the next two years - via improved performance and error mitigation techniques and increased collaboration with the HPC community. IBM also announced its plans to reach over 2,000 logical qubits. True error correction should arrive in 2029 with the Starling processor, followed by the Blue Jay processor, with 2,000 logical qubits and over a billion gates in 2033. Quantum is hard; extremely hard. Advances in cryogenics, qubit design, scaling interconnects, algorithm development, run-time tools, and applications will enable useful and perhaps pervasive quantum solutions. The inherent error rate of quantum bits is a thousand times higher than that of digital circuitry. Consequently, scaling quantum processors to thousands of qubits and dealing with the instability of those qubits creates demanding challenges. Useful Quantum Computing requires advances in the hardware and the algorithms But we are getting close to realizing significant advances as these developments take shape. The chart above explains the dynamics at play. We are currently at the beginning of quantum utility, where we can begin to see the benefits of hardware and algorithms as we progress. All quantum industry players including IBM, Amazon, Intel, Google, Microsoft, hardware startups (Alice and Bob, Atos D-Wave, Quantinuum, Rigetti, QuEra and Xanadu among others) are amongst the over 70 global quantum startups working on solving these challenges. Along the way, advances in quantum are supported and complemented by advances in classic HPC computing to support the execution of the circuits by offloading some of the execution onto the CPU and GPU or using those tools to clean the results. In fact, accelerated servers surround quantum processors, always working in tandem with classical computers In IBM’s most recent webinar, we learned how researchers apply quantum to solve Life Sciences and Health Care problems. One of the brightest application spaces for quantum is accelerating the R&D process for pharmaceuticals. The current process takes 10-15 years, billions of dollars, and yet 90% of drug candidates fail. Quantum and AI can potentially speed the process, cost less, and produce superior outcomes, potentially becoming the billion-dollar solution that Jensen and Zuckerburg seek. In drug discovery, quantum computing can speed the development and evaluation of proteins through simulation and applying machine learning. Quantum computers have the potential to simulate complex molecular interactions at an atomic level with unprecedented accuracy, allowing researchers to model drug-protein interactions more realistically, leading to the discovery of novel therapeutic compounds. Quantum algorithms have the potential to search vast chemical databases much faster than classical computers, enabling rapid identification of potential drug candidates that match specific molecular criteria. Quantum computers can also identify optimal molecular structures for specific targets, leading to improved efficacy and reduced toxicity of potential drugs. Quantum computing could speed up the screening of vast libraries of potential drug candidates, allowing researchers to quickly identify promising molecules for further testing. Quantum computing has the potential to model complex protein folding patterns and interactions at a molecular level, helping researchers identify novel drug targets that were previously undetectable. IBM is researching the applicability of quantum computing across the broad spectrum of drug ... [+] development and delivery, Quantum computing holds the potential to revolutionize healthcare and life sciences by addressing key challenges in the field. For example, quantum algorithms can integrate data to uncover critical genes, proteins, and pathways. Quantum walks can identify key proteins in cancer signaling pathways, aiding in the development of targeted therapies. Additionally, quantum methods speed the discovery of higher-order gene interactions, accelerating the understanding of polygenic diseases. Hybrid quantum-classical algorithms enhance predictions of protein and RNA structures, which are crucial for designing biologics and mRNA therapies. Quantum techniques also improve ligand-based virtual screening and simulate drug-target interactions with unprecedented accuracy, enabling better lead optimization. In clinical trials, quantum algorithms can optimize trial designs, site selection, and cohort identification, reducing costs and improving outcomes. The webinar showcased several collaborative success stories where IBM has partnered with industry leaders to demonstrate the potentially transformative power of quantum computing. A few examples: Client use cases To accelerate industry adoption further, IBM has launched the Quantum Accelerator program, which helps enterprises identify business problems suitable for quantum solutions, prototype quantum applications iteratively, and leverage IBM’s ecosystem of over 250 members and 39 innovation centers. This program is designed to support the enterprise during all their journey of quantum adoption. If you ask IBM, they will tell you it already has, albeit in small doses, while the potential wave of quantum applications will begin in earnest around 2030. Whether that is early or late depends on your expectations, and Jensen and Zuckerberg are famously impatient guys. We believe that quantum computing is poised to transform healthcare and life sciences by addressing computational bottlenecks and enabling groundbreaking innovations. From understanding disease mechanisms to optimizing drug discovery pipelines, IBM quantum’s advancements promise to unlock unprecedented opportunities in the pharmaceutical industry, reducing R&D costs, accelerating timelines, and improving patient outcomes. Disclosures: This article expresses the opinions of the author, and is not to be taken as advice to purchase from nor invest in the companies mentioned. My firm, Cambrian AI Research, is fortunate to have many, if not most, semiconductor firms as our clients, including NVIDIA, Intel, IBM, Qualcomm, Cadence Design,Flex, Synopsys and Tenstorrent . We have no investment positions in any of the companies mentioned in this article. For more information, please visit our website at https://cambrian-AI.com. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: How Tariffs Make CFOs’ Jobs More Complex
URL: https://www.forbes.com/sites/cfo/2025/02/11/how-tariffs-make-cfos-jobs-more-complex/
Time Published: 2025-02-11T18:15:30Z
Full Content:
There’s been a huge amount of change in the last month, and from an economic standpoint, it seems to be hitting consumers slowly, but surely. Job growth is stuck, prices don’t seem to be going anywhere, and many feel they are financially worse off than a year ago. January’s jobs data from the Labor Department, released last week, significantly missed growth projections and signified the weakest start to a year in terms of new jobs since 2016. Economists were expecting 170,000 new jobs, but the report showed the addition of just 143,000. And the expectation of job losses is growing. In the New York Fed’s Survey of Consumer Expectations, released this week, 14.2% of people feel a probability of losing their jobs in the next 12 months, up 2.3% from December. About a third of consumers said their household financial situation is worse off than a year ago, the Survey of Consumer Expectations shows. And while this is down slightly month-over-month, only about 19% say they are doing better than a year ago—a 3% decrease from December. Over a fifth aren’t optimistic about the economy and say they will be doing worse a year from now. January’s Consumer Confidence Index, reported two weeks ago, showed a 5.4-point decline from December. The data shows some optimism. Labor Department data showed record-high average hourly wages of $35.87, as well as an unemployment rate of 4%, down from 4.1% in December. And 36.6% feel their household finances will be better a year from now, according to the Survey of Consumer Expectations. However, there’s lots of room for improvement. Two-thirds of Americans feel that President Donald Trump administration’s policy focus so far isn’t doing enough to lower prices, according to a CBS News/YouGov poll last week. The next round of consumer price index numbers come out this week, which will show just how much prices are changing—though the consumer expectations survey shows that Americans anticipate price increases for food, gas, rent and medical care. CFOs are increasingly having issues with making projections since so many companies rely on imports, and so many different tariff threats are being quickly proposed, rescinded and taking effect. I talked to Bob Stark, global head of enablement of cash management software platform Kyriba, about how CFOs can plan in today’s circumstances. An excerpt from our conversation is later in this newsletter. Workers in Camden, N.J. use a crane to load a 60,000 pound coil of steel onto a railroad car. President Donald Trump signed another round of tariffs this week: 25% on all steel and aluminum imports. “This is a big deal, the beginning of making America rich again,” Trump said as he signed the tariff orders in the Oval Office, CBS News reported. This is a familiar policy to Trump, who assessed tariffs on steel and aluminum in his first term to bolster domestic industry. Those tariffs—25% on steel imports and 10% on aluminum imports—had several exemptions, and ended up bringing the biggest benefit to steel companies, according to a 2018 analysis from the Peterson Institute for International Economics. Allies from the EU and Australia spoke out about the planned tariffs, which are reported to go into effect next month, saying that they will harm U.S. consumers and long-established partnerships. The U.S. imports about a quarter of all of the steel it uses, and about half of the aluminum, Reuters reports. Most of the steel the U.S. imports comes from Canada, Brazil and Mexico, while the lion’s share of aluminum imports come from Canada. Several analysts have said these new tariffs will have limited benefit to the U.S.—likely just for the steel companies—and will not likely create jobs. They are also likely to raise prices for most things made from metal. Even the United Steelworkers International Union feels the tariff proposal goes too far, and wrote the U.S. needs to take a more measured approach, distinguishing trusted trade partners. The move caused significant shifts in the stock market, boosting prices of domestic metals companies, including Alcoa and U.S. Steel. It also boosted gold prices to a record high of $2,938 per troy ounce, as investors search for stability. This isn’t the only move on tariffs in the last week. Trump announced during a Friday meeting with Japanese Prime Minister Shigeru Ishiba that he would announce reciprocal tariffs this week—tariffs equal to what other countries impose on U.S. goods. There’s no word yet on how those might work. Russell Vought at a confirmation hearing in January in front of the Senate Homeland Security and Governmental Affairs Committee. Last Thursday, the Senate confirmed Russell Vought as the head of the Office of Management and Budget with a vote along party lines. Vought, founder of the prominent right-wing Center for Renewing America and key adviser to the Heritage Foundation’s controversial Project 2025, was considered one of Trump’s more controversial nominees because of his viewpoint of expansive executive branch control on spending. One of his first big moves is essentially halting all work of the Consumer Finance Protection Bureau, taking its website offline and telling workers not come in and stop work activities. The CFPB was created in 2011 following the Great Recession and financial crisis. It’s an agency that protects consumers from unfair banking practices, but it also has deep-pocketed enemies. Elon Musk, who has been working with his Department of Government Efficiency task force to mercilessly cut perceived wasteful spending, has railed against CFPB because “there are too many duplicative regulatory agencies.” He said this in November, shortly after the agency announced more oversight of big tech companies and others offering digital funds transfers and payment wallet apps. Meta founder and CEO and billionaire Marc Andreessen also have spoken out against the agency, which has threatened action against tech companies’ use of financial data. Rendering of Meta's Sucre data center in Louisiana. AI evangelists say the technology can do a lot for companies, but to continue to grow—both revenues for companies and capabilities for users—some infrastructure investments are needed. Big companies are announcing bigger AI data center projects, with bigger budgets and footprints. Forbes’ Christopher Helman runs down many of these plans, which include not only huge facilities like Meta’s $10 billion planned Sucré data center in Louisiana—covering 4 million square feet of floor space on 2,250 acres—but enough infrastructure to power it all—$3.2 billion worth for Meta, coming from twin high-efficiency natural gas turbines. In all, Meta plans to spend $60 to $65 billion on these data centers in the next year. Meta’s not alone in these investments. Sundar Pichai, CEO of Google parent Alphabet, announced on last week’s earnings call that the next year will see about $75 billion in capital expenditures for technical infrastructure. CFO Anat Ashkenazi said that in the last quarter Google hit capacity issues with its cloud and services business, which constrained its growth. Microsoft is planning to invest $80 billion in data centers. And through a partnership announced from the White House between SoftBank, Oracle and OpenAI called “Stargate,” SoftBank will invest $500 billion in U.S. data centers during the next four years. Kyriba Global Head of Enablement Bob Stark and the Port of Los Angeles. Economic predictions were challenging for CFOs before Trump’s second presidency began, but announcements of potential tariffs on different countries and industries, and along a variety of timelines, add a lot more uncertainty to the future. I talked to Bob Stark, global head of enablement at cash management software platform Kyriba about what CFOs are going through and what they can do to prepare for the short- and long-term future. This conversation has been edited for length, clarity and continuity. A longer version is available here. Tariffs were also a feature of President Trump’s first term. How similar is what’s happening right now to then? Stark: You look at what was announced around steel and aluminum, and that feels very similar to what we saw in the previous term. That said, I think the differences—and our clients had the same reaction: the blanket tariffs, the punitive tariffs, if you tariff us, then we’ll tariff you back and it’s sort of back and forth—feel a little bit more significant than what we had experienced previously. It certainly feels like an acceleration. CFOs and finance teams are definitely looking at that as a lot of different potential scenarios that you have to prepare for. It’s not as simple as: This industry is affected [with a] 25% hit, so if you’re a Canadian steel exporter, you have this problem—which is fairly predictable. You can figure what 25% looks like. Same with importers on the U.S. side. [They’re] just trying to understand what does our cost look like? They can map out demand, but it’s the back and forth and the uncertainty around that. It’s difficult to predict exactly when impacts will be felt, which is a big challenge for CFOs. Many of them are looking to provide updates to guidance quarterly. In this quarter, do we have a cashflow impact that we need to control? Do we have a liquidity challenge where we need to make some plans? Do we need to provide different or maybe more expansive hedging of FX [foreign currency exchange] because we recognize that it’s these short and sharp movements that can have a significant impact in the balance sheet and income statement reporting at the end of the quarter. There’s a lot. It’s the uncertainty of what is the actual impact. There’s so many more scenarios you have to play out in different ways that feel different than what we saw several years back when we first saw this idea of tariffs being proposed and implemented. This time around, it feels to me like some of the tariffs that are being threatened and announced are more unexpected. Is that your impression? Absolutely. It feels to a corporate CFO that there’s a daily news cycle that you have to invest more time in following. You have to then build that into your analysis and reporting. If you’re doing something as simple as: What is the impact of cashflow if this blanket tariff goes in. What’s the difference? It’s being delayed by 30 days. What does that mean? There’s a lot of uncertainty and unpredictability around that. That’s the biggest challenge for CFOs. It’s like playing that hand game with your kids. You put one down, then there’s another, and then there’s another, and then there’s another. And then suddenly you think, ‘Wow, where did this go?’ That’s the part that’s really difficult to predict and build into an analysis. When the CEO is calling you because they just saw this on CNBC, and they’re asking the question: What’s the impact on our cash flow? Do we need to change our guidance? You have to juggle not just one or two different possibilities, but [there’s] probably three or four different scenarios you need to model against the same data, and then you have to be in a position to answer with precision on demand. And, ‘I’ll get back to you tomorrow’ is not an acceptable response. It’s ‘I’ll tell you right now, if these tariffs go through, this is what the impact is. If they’re delayed out 30 days, if they’re delayed out six months…’ Those are the scenarios that you have to be able to report to the board with confidence so that everyone's prepared. How do you think a company can make its way through the next four years? There’s three things that CFOs can do, aside from watching all the news and being able to utilize AI to summarize the things that they can’t watch live. Once they’re armed with that insight and understanding of sentiment, they’re in a position to do three things. Number one: FX. Those sharp, short-term movements can be the difference between hitting and missing guidance. Every CFO has to be able to quantify the impact of currency on the balance sheet, income statement and cash flow. If they’re not in a position to do that, then it erodes confidence a little bit. No one wants choppiness. They want certainty, and they want at least three months—ideally more—in advance. If you can eradicate the impact of FX, or mute it significantly, you’re in a better position than many of your peers. Number two: forecasting and planning. Every time there’s impactful news, CFOs need to answer: What would this mean to our cash and liquidity in the future? Finance leaders, they must have precise answers. They have to have them at the ready. They have to be available for multiple scenarios so that hypothetically speaking, they can answer a question like this: Our cash flow decreases by 30% if this tariff actually happens in the next 20 days. You need to be able to answer the question with that level of precision. That CEO and the board, they’re not waiting an hour. They’re probably not waiting five minutes either. They expect data and insights right now. The third part: Getting control over the cash lifecycle. CFOs need to be able to pull multiple levers to maintain control of their cash, especially in response to customers wanting to bring forward orders to get ahead of tariffs. That’s a perfect example of being able to have control, making sure that they can provide the structure, the systems financing to allow changes in the supply chain to occur. Also [to be able to provide control] in response to potential reshoring or recomposition of supply chains. Some of that is a little bit more longer term. That might be later in 2025 or into 2026. We recognize we just can’t compose our supply chain this way. We need to reshore this part of it. We need to eliminate this country out of our supply chain. It just doesn’t work in terms of the cost structure. Whatever that scenario looks like for them, the CFO needs to be able to put in the structure. It’s very tactical. Everything from bank accounts to cash management structures, the ability to sweep the and pool cash and mobilize it wherever it needs to be, to not only invest, but also repatriate cash so you can meet those cashflow objectives of the organization. There’s a lot of complexity there. Nevermind working with supply chains, working with customers to be able to pull the lever of: We’re going to pay you earlier, but it’s going to come at a cost, and as a result, we need to be able to do that. You want to get paid earlier, Mr. and Mrs. Supplier, we’ll do that, but here’s the program to do it. Same thing with accelerating receipt of cash. If you need to accelerate collections, you need a structure with that. It’s not a program that just happens like that. It’s a year in the making to put that structure in with your banks and with your finance and providers. Billionaire Bill Ackman’s Pershing Square Capital Management disclosed a major stake in ride hailing company Uber on Friday, boosting the company’s stock. $2.3 billion: Value of Pershing’s stake in the company, which is 30.3 million shares 17%+: Increase in Uber’s share price over the last five trading days ‘One of the best managed and highest quality businesses in the world’: What Ackman wrote about Uber on X Billionaire Fernando De Leon has an unusual backstory and advice to build a business that will grow. Here are seven of his tips. Work is important, but stressful. Here are five ways you can rest and recharge without compromising your drive and ambition. President Donald Trump made an announcement about currency this week. What was it? A. Pennies will no longer be minted because they are wasteful B. Harriet Tubman will not be appearing on future $20 bills C. The U.S. is returning to the gold standard D. $1 coins will replace bills in three years See if you got it right here. Editor’s note: Bob Stark’s title has been corrected to global head of enablement.
--------------------------------------------------

Title: Do Our Courses Need An Update To Reflect The Workings Of Information Businesses?
URL: https://www.forbes.com/sites/shivaramrajgopal/2025/02/11/do-our-courses-need-an-update-to-reflect-the-workings-of-information-businesses/
Time Published: 2025-02-11T16:02:04Z
Full Content:
1936: British actor and director Charles Chaplin (1889 - 1977), wearing overalls and holding a ... [+] wrench, sits on an enormous set of gears in a still from Chaplin's film, 'Modern Times'. (Photo by Hulton Archive/Getty Images) Syllabi at management schools and senior management at perhaps many businesses don’t fully appreciate how information businesses work. I was chatting with a successful Silicon Valley venture capitalist last week about how we may have under-invested in understanding the impact of technology on our economy. The VC suggested, “macroeconomic modeling generally treats technology as an external parameter like Total Factor Productivity, rather than as an endogenously determined, explicitly engineered system. As a result, conventional models struggle to address the most critical structural issues facing the U.S. and global economy. They neither integrate short-run dynamics with long-run growth considerations nor capture how digital technologies — AI, 5G, robotics, 3D printing, zero-cost digital services — are developed, diffused, and shaped by policy and market forces. This limitation leaves unexplored how these technologies affect productivity, output, time use, education, skill requirements, and well-being.” The VC sounds more professorial than me but I am not a macro-economist and hence say much about the above statement. However, I suspect the same sentiment applies at the micro level to technology companies or information businesses, more broadly. I suspect that we have not updated our syllabi and course offerings at management schools as much as we should have, after the U.S. lost its crown as a manufacturing powerhouse. I compared the list of course offerings in say 1998 when I started teaching with what is offered today by top business schools. I found a few changes but nothing radical seems to have been restructured. So, what was taught in 1998 and to a great extent today? I conjecture we mostly teach management insights, as practiced in the manufacturing era. There was a fair degree of excitement about management theories as applied to manufacturing processes back in the day. I am thinking of Fredrick Taylor’s scientific management theories, the Hawthorne experiments, the Japanese ideas of Total Quality Management, lean manufacturing, and GE’s six sigma mindset. None of these ideas, perhaps formulated explicitly for the manufacturing age, fully apply or explain how information companies operate. Consider a summary of these ideas and how these apply or not to the modern information giants: Frederick Taylor is widely recognized for “time and motion” studies conducted in the 1880s where he observed the specific steps a worker takes to lay bricks and the time it takes to go through these steps. The idea is to optimize the number of steps and time taken to accomplish the task. More broadly, Taylor is known for two big ideas: (i) scientific management, based on evidence and observation, as opposed to gut feel and rules of thumb; and (ii) the conflict between management and labor is somewhat unnecessary as both parties would benefit with better productivity which would manifest as higher wages and higher profits. Taylor emphasized standardized tools and procedures. He wanted managers to set specified goals and assign tasks to workers to motivate workers to perform better. He argued that workers should be paid a bonus of 30% to 100% of wages for learning to do the job as per the principles of scientific management. Taylor also advocated individual work and productivity and was somewhat skeptical of group work. Taylor suggested choosing the worker with the right aptitude as such a worker would be far more productive than the average worker. He also pushed for rest pauses and shorter working hours, especially if the task is hard. Taylorism is perhaps the precursor to a lot of what is taught in operations research, human relations and cost accounting courses in B schools. I suspect Amazon relied on time and motion studies to understand how to optimize packing boxes in their distribution centers. The idea of choosing the right worker with aptitude for the job is an established tenet in HR (Human Relations) groups all over the world. The Hawthorne studies are credited with recognizing the influence of human relations or social factors in motivating workers. The Hawthorne researchers found the workers’ response to a managerial intervention is a function of the attitudes the workers bring to their job, the informal work group they belong to, their personal history and their social situation at work. Worker culture in big tech companies is widely discussed. Netflix’s culture deck, which potentially owes its inspiration to the early Hawthorne studies, is legendary in Bay area circles. The deck emphasizes the freedom to excel, deciding which worker is a keeper and paying them a premium over their market wage, the idea that the best workers are 10X better than average, and that teams ought to be highly aligned in their goals but only loosely coupled to stay nimble. The Total Quality Management way of thinking aims to make quality the concern of every member of the firm. Customers are the focus of all activities of the firm and improvements in quality are directed at improving customer satisfaction. Such a focus is expected to lead to better financial performance in two ways: (i) in the manufacturing process, we are expected to encounter fewer defects and rework leading to lower costs and more dependable processes and hence better earnings; and (ii) in the product market, more focus on quality is expected to lead to higher market share, less elastic demand, higher prices and hence higher earnings. The idea of lean manufacturing, a related concept, is to deliver a quality product to the customer at a reasonable cost. Incidentally, Trevor Harris, my co-author and emeritus professor at Columbia Business School, tells me that Edward Deming, another CBS professor, pioneered the thinking behind TQM and even the Six-Sigma movement. I don’t know whether Tesla follows TQM and lean manufacturing but I did come across a help wanted ad from Tesla looking for a quality engineer whose job description sounded similar to what a TQM engineer would do. In a 1997 annual letter Jack Welch sent to GE stockholders, he explains: "Six Sigma project work consists of five basic activities: Defining, Measuring, Analyzing, Improving and then Controlling processes. These projects usually focus on improving our customers’ productivity and reducing their capital outlays, while increasing the quality, speed and efficiency of our operations….The Six Sigma quality initiative, very briefly, means going from approximately 35,000 defects per million operations, which is average for most companies, including GE, to fewer than 4 defects per million in every element in every process that this company engages in every day.” Welch goes on to explain how Six Sigma works with the “A” leaders in each function of the company: “In finance, for example, “A’s” will be people whose talents include, but transcend, traditional controllership. The bigger role is one of full-fledged participant in driving the business to win in the marketplace — a role far bigger than the dreary and wasteful budget “drills” and bean counting that once defined and limited the job. In engineering, “A’s” are those who embrace the methodology of Design for Six Sigma. “A” engineers can’t stand the thought of “riding it out” in the lab, but rather relish the rapid pace of technological change and continually re-educate themselves to stay on top of it. In manufacturing, “A” players will be people who are immersed in Six Sigma technology, who consider inventory an embarrassment, especially with a whiff of deflation in the air — people who understand how to drive asset turns and reduce inventory while at the same time increasing our readiness to serve the customer. In sales, “A” players will use the enormous customer value that Six Sigma generates to differentiate GE from the competition, to find new accounts, and to refresh and expand the old ones — as contrasted with “C” players whose days are spent visiting “friends” on the “milk-run” circuit of customer calls.” Amazon reportedly relied on Six Sigma type ideas to reduce the error rate on their package delivery to minuscule levels considering that they processed 5.9 billion packages in 2023. As stated, modern information businesses, such as Amazon, Google, Meta, Netflix, and Uber, rely on many of these ideas inspired by manufacturing. But these tech businesses also do not seem to fit the traditional mold of management theories well. In a seminal paper written around 2001, Hal Varian discusses ways in which the “new economy” information businesses are different from traditional manufacturing firms: Not quite. Let me start with my home field. Trevor Harris points out that statutory reporting, that is done once a quarter in a rigid regulation bound format, is a relic of a bygone era. Good businesses have a dashboard to track the key KPIs that matter for value creation. Why don’t we ask firms to disclose that dashboard on a more frequent cadence to investors? Of course, there are concerns about proprietary costs and so on, but I have not seen many creative conversations about how to leverage technology and/or better reflect the economics of technology and information businesses in our textbooks and syllabi. For another smaller illustration of this problem, our textbooks treat intangible assets in a somewhat rule-oriented framework. The FASB says X and not X. So, we explain that in the text but rarely take the conversation to the next level to consider how technology creates value. My co-authors and I have a few modest contributions in this area, generally lamenting how little information about value creation potential of technology spending is shared with the investing public and the absence of data to calculate unit economics or lifetime value of a customer but much more needs to be done. I have not seen a good treatment of how cost accounting should think about supply and demand side economies of scale, standards, systems effects and computer mediated transactions. A critic may argue that the basics of cost accounting have not changed. But one must wonder whether the field is stuck around the time manufacturing left the U.S. for Asia. Have we updated thinking in cost accounting for the bundled information products, why some of Amazon’s divisions are never supposed to make money (free shipping, Kindle) so that they can cross-sell other products in the retail store, how does Amazon price the time of developers working on features and software products or how does a firm value its data or the data acquired in an M&A transaction? What kind of cost data does an Amazon, or an Uber collect? Do they conduct cost-volume-profit analyses to determine breakeven sales quantity for a product, given the extent of bundling and portfolio type thinking necessary to accomplish this? How does Amazon calculate the lifetime value of a customer? How is common overhead across segments and how did that practice potentially fund Amazon’s internal capital markets such as books leading to video to music to third party selling to AWS? How does one think about budgeting and sales and cost variances for a modern tech business? A related problem in finance has been the persistent puzzle on how to value intangibles. Absence of even modest accounting data to perform such valuations is a big hindrance. Do frameworks such as real options plausibly explain the valuations of a Tesla or an Amazon or an Nvidia? How does one stress test or falsify these black box valuations? Have the low-interest rate environments, coincidental with the rise of Big Tech, defaulted us to faith-based valuations? How has corporate finance changed with the rise of information businesses? Is capital less of a constraint than skilled labor? How does one come up with capital budgeting for information products or bundled systems? How has cash and liquidity management changed? Has big data and AI improved scenario planning and hence enabled better capital allocation? Has granular data made it easier to dissect why some acquisitions succeeded while others failed? Now, I stray into areas that are not my own and hence rely on observation, as opposed to deep analysis. Marketing and Operations Research are arguably the two disciplines in a B school that have adapted and embraced the practices of Big Tech firms better than other units. There are tons of classes on digital marketing, social influencers, ad platforms, AI offered by these two groups. Human Relations: Jensen Huang’ statement illustrates the profound change that HR groups are/will experience in the near future: “In a lot of ways, the IT department of every company is going to be the HR department of AI agents in the future.” That is, the IT departments of the future will train and “onboard” AI agents as though these were employees to ensure that such agents work to enhance human workers’ productivity. Recruiting is already mediated by AI algorithms, for better or for worse. Work from home, enabled by technology, is not going away. This is already creating profound challenges on how to integrate younger workers into a company. Many of these workers were hired on a zoom call and will potentially not get promoted by senior leaders who don’t know them as individuals or human beings. Ironically, I would suggest that we need to invest more in traditional HR. Do our newer leaders have the training and outlook to look a worker in the eye and say to them in a respectful way that they are being laid off? If they did, we would not come across so many cases of mass layoffs via email or zoom. Our slowness, in the B school, to adapt and teach management practices of Big Tech or enabled by technology in general, would be less of a concern if these considerations only mattered to a handful of tech giants. But that is certainly not the case. I routinely come across senior managers of companies that are both enthusiastically and reluctantly digitizing without fully understanding the profound change that Varian’s forces will have on their firms. This is not to say that people have not worked on documenting management practices at Amazon or the other technology firms. I have not seen a book or a manual or a coherent framework that one can hand to future managers of non-digital firms to prepare for a digital/tech leavy/information dense tomorrow. Constructive comments welcome, as always. One Community. Many Voices. Create a free account to share your thoughts. Our community is about connecting people through open and thoughtful conversations. We want our readers to share their views and exchange ideas and facts in a safe space. In order to do so, please follow the posting rules in our site's Terms of Service. We've summarized some of those key rules below. Simply put, keep it civil. Your post will be rejected if we notice that it seems to contain: User accounts will be blocked if we notice or believe that users are engaged in: So, how can you be a power user? Thanks for reading our community guidelines. Please read the full list of posting rules found in our site's Terms of Service.
--------------------------------------------------

Title: Why Tesla's stock has been cratering
URL: https://finance.yahoo.com/news/why-teslas-stock-has-been-cratering-152645143.html
Time Published: 2025-02-11T15:26:45Z
Description: Tesla's stock is enduring a violent sell-off.
--------------------------------------------------

Title: Meta Rally Makes History as It Goes on Offensive With AI
URL: https://finance.yahoo.com/news/meta-rally-makes-history-goes-103919299.html
Time Published: 2025-02-11T14:41:39Z
Description: (Bloomberg) -- Even in a market where artificial intelligence winners are rewarded, Meta Platforms Inc. shares stand out.Most Read from BloombergSaudi...
--------------------------------------------------

Title: Super Micro's roller coaster continues into earnings with 59% stock pop in past week
URL: https://www.cnbc.com/2025/02/11/super-micro-roller-coaster-has-stock-soaring-ahead-of-q2-2025-earnings.html
Time Published: 2025-02-11T12:00:01Z
Description: Super Micro has a lot on the line heading into its earnings report on Tuesday after a tumultuous 2024.
--------------------------------------------------

Title: SEBI bars nil revenue firm LS Industries, promoter in alleged pump & dump scheme, impounds ₹1.14 crore
URL: https://www.thehindubusinessline.com/markets/sebi-bars-nil-revenue-firm-ls-industries-promoter-in-alleged-pump-dump-scheme-impounds-114-crore/article69205871.ece
Time Published: 2025-02-11T08:54:06Z
Full Content:
-1,121.52 -324.55 + 7.00 -175.00 -1,024.00 -1,121.52 -324.55 -324.55 + 7.00 + 7.00 -175.00 Get businessline apps on Connect with us TO ENJOY ADDITIONAL BENEFITS Connect With Us Get BusinessLine apps on By BL Mumbai Bureau Comments READ LATER The Securities and Exchange Board of India (Sebi) has barred LS Industries - a company with zero revenue but thousands of crores in market capitalisation - its NRI shareholder and four promoter-linked entities from the securities market, and directed impounding of ₹1.14 crore illegal gains made by inflating the company’s stock price. The trouble started when an ex-director, Suet Meng Chay, transferred a 12 per cent stake—nominally valued at ₹10.28 crore—to an NRI based in Dubai, Jahangir Panikkaveettil Perumbarambathu, for a token amount of just $1. He was supposedly a stranger, who Sebi suspects to be linked to the promoter family through Meta. This token-dollar transfer set off an alleged pump-and-dump strategy orchestrated through connected entities to artificially inflate the stock price. This enabled the NRI as well as the promoter, and connected entities to sell shares at a higher value, the regulator observed. A series of early-morning buy orders at the upper circuit limits were seen driving the share price from a low of around ₹22.50 to an astronomical high of ₹267.50 within two months—even though the company’s financials showed negligible revenue and virtually no business activity. “At the dollar-rupee conversion rate of ₹83.75, this made the purchaser richer by $328.60 million (₹2752 crore) at the company’s peak market capitalization of ₹22,700 crore. The said transaction not only appears to be too good to be true, but also raises the possibility of FEMA violations,” said SEBI’s whole-time member Ashwani Bhatia in the interim order. Bhatia said that the case painted a picture of “absurdities and anomalies,” and required interim action before another pump and dump takes place and more innocent investors lose money and burn their fingers. He noted that the number of public shareholders has nearly doubled in six months to 6,106 as on December 31, 2024. “There is a real risk of the noticees off-loading their shares at the cost of investors, who get drawn to the shares of the company seeing high valuations and volumes and would end up getting duped and taken for a ride…Further, action needs to be taken promptly to prevent more money leaving the shores of India through sale of shares by the NRI shareholder,” the order said. The regulator will be conducting a detailed investigation in the matter by May 15. In the meantime, Sebi has directed the NRI to provide a full inventory of his assets, investments, and bank accounts, as well as freeze any debits in his accounts. Comments BACK TO TOP Comments have to be in English, and in full sentences. They cannot be abusive or personal. Please abide by our community guidelines for posting your comments. We have migrated to a new commenting platform. If you are already a registered user of TheHindu Businessline and logged in, you may continue to engage with our articles. If you do not have an account please register and login to post comments. Users can access their older comments by logging into their accounts on Vuukle. Terms & conditions | Institutional Subscriber
--------------------------------------------------

Title: Can AI Fix Succession Planning?
URL: https://www.forbes.com/sites/tomaspremuzic/2025/02/11/can-ai-fix-succession-planning/
Time Published: 2025-02-11T08:30:22Z
Full Content:
Succession planning sign and figurines with arrows. Few decisions in business and leadership matter more than choosing the right successor. When done well, it ensures continuity, preserves institutional knowledge, and often propels an organization to new heights. When done poorly, it can lead to instability, lost value, and even outright failure. Consider the success stories: Tim Cook taking over from Steve Jobs at Apple, seamlessly continuing and even accelerating the company’s trajectory, boosting the company’s market cap from around $340B under Steve Jobs to over $3T now (if you invested $10,000 of Apple stock at the start of Cook's tenure you would have around $160,000 right now). The figures are similarly impressive for Satya Nadella’s impact at Microsoft, who arguably had a bigger strategic and innovation effect by not just rejuvenated the company, but also shifting its focus first from Windows to cloud computing, and more recently and AI (anticipating the current AI boom). Even Pope Francis’s ascension after Pope Benedict XVI’s resignation demonstrated the power of a well-managed leadership transition, bringing fresh energy and a sense of modernity and empathy (not to mention an instant PR upgrade) to the Vatican, a 2,000-year institution. Contrast that with the pitfalls of poor succession planning. Since legendary Manchester United manager Sir Alex Ferguson retired, the club has been cycling through multiple coaches (seven and still counting) without much financial or sporting success, and the decline appears to be ongoing. In the corporate world, J.C. Penney’s appointment of Ron Johnson as CEO led to a dramatic and costly strategic misfire. And in the hit show Succession, the fictional Roy family’s lack of a clear plan for leadership leads to chaos, power struggles, and a tanking stock price, which makes for great television but a horrific case study in succession. Back in the real-world, succession stories captivate the public’s attention. Bernard Arnault, the chairman and CEO of LVMH and one of the richest people in the world, is known to have been interviewing each of his five children for the big job, fostering speculation about his eventual successor. In recent years, Warren Buffett announced his successor, but at 94, Buffett still maintains a hands-on role in managing Berkshire Hathaway's portfolio. As for Jamie Dimon, there is seemingly no end to the uncertainty about his successor. These cases all highlight the same truth: leadership transitions can make or break an organization. The Science and Art of Succession Planning At its core, succession planning is part science and part art. The science is well established: industrial-organizational psychology offers robust and reliable frameworks for assessing leadership potential, ensuring a data-driven approach to selection, and derisking the process (including the significant probability that incompetent men become leaders). Predictive analytics, performance reviews, and evidence-based competency models help organizations make objective choices. Yet, the process is also partly an art — navigating internal politics, winning over key stakeholders, and crafting a shared vision for the future. As in any other area of life, AI has the potential to bring rigor and objectivity to the science while enhancing the human aspects of the art. Here’s how: 1. Evaluating Past Performance Beyond Politics The barrier to data-driven succession planning is contamination in past performance data, which corrupts a proper examination of candidates’ track record and potential, introducing invisible biases in the process, and contaminating candidates’ “score” with politics, subjectivity, and noise (the nefarious 9-box grid, the unreliable supervisor or managerial ratings, and the political high-potential nominations). AI can mitigate this by analyzing vast amounts of historical performance data, including predictive signals usually missed by the “naked” human eye, and finding hidden patterns that connect each candidate with different success profiles. By measuring real contributions—financial results, operational efficiencies, strategic insights, and even mining granular signals, such as language, meta-data, the depth and breadth of social networks, and markers of leadership style and dynamics — AI can cut through biases, ensuring the most qualified leaders rise to the top based on merit, not alliances. To be sure, asking AI to predict who would normally be nominated in a given organization will probably not result in an upgrade from past and current candidates, but rather more of the same (which is rarely the right choice when it comes to effective successions). 2. Expanding the Talent Pool for More Inclusive Assessments Historically, succession planning has been limited to a narrow group of insiders, often overlooking diverse and high-potential candidates among unconventional or outgroup profiles. AI-driven talent identification can analyze broader datasets to uncover hidden gems, ensuring organizations consider leaders from different backgrounds, functions, and geographies. In essence, innovations in assessments have leveraged AI to improve the candidate experience, reduce test-taking time, and increase the speed and efficiencies of putting a much larger sample of potential candidates through the assessment. Since quantity leads to quality, making assessment inclusive will generally improve the quality of your final choice, as well as increasing the range of candidates and profiles you can shortlist. 3. Using Passive Data to Predict Leadership Success AI can leverage passive data—communication patterns, collaboration metrics, and behavioral insights — to assess leadership traits. The work of David Stillwell, Sandra Matz and Michal Kosinski demonstrates how AI can infer personality traits and leadership potential from digital footprints, as well as internal company data not historically seen as critical to leadership talent. In essence, you want leaders to be smart, driven, and ethical, and each of these foundational ingredients of leadership potential can be broken down into narrower traits (e.g., expertise, curiosity, learning ability, motivation, conscientiousness, EQ, people-skills, integrity) that are expressed, display and reflected in people’s online behaviors (captured by internal company data or in the external digital universe). By incorporating these insights, organizations can move beyond traditional assessments and gain a more holistic view of a candidate’s fit. In fact, even generative AI and large language models provide an easy way to access the public reputation of many senior leaders. For instance, try prompting ChatGPT to give you a leadership and personality profile for Elon Musk, Donald Trump, or Angela Merkel, and you will see there is not much left for a traditional executive assessment to add (same goes for many less famous leaders, including those mentioned at the beginning of this article). 4. Debiasing Leadership Selection Unconscious and conscious biases remain a significant issue in leadership transitions, including when succession planning is driven by a desire to enhance “culture-fit” (which is a subliminal and politically legitimized way to perpetuate the existing organizational biases that confer privilege to the historical in-group, the status-quo, and the ruling elite). AI can flag potential biases in hiring and promotion decisions, ensuring selections align with data rather than stereotypes. For example, when AI is trained to predict who would get promoted and it recommends a surplus of Middle-aged white male engineers, it is accurate exposing existing biases in the organization. Humans are (usually) very good at learning, but very bad at unlearning. Conversely, AI is good at both learning and unlearning. Thus, we can train AI to not just identify the kind of leader who has succeeded in the past, but the kind of leader we would need for the future, if we simply program or prompt it to seek for relevant signals of potential while ignoring irrelevant signals. In contrast, humans will never be able to ignore a candidate’s gender, age, race, social class, or attractiveness, all of which conflates and distorts their choices. By training algorithms on diverse, high-performing leadership examples, companies can create models that prioritize competence and potential over legacy and favoritism, increasing meritocracy (which even those who have jumped on the anti-DEI bandwagon should appreciate). Ethical Considerations Of course, AI is not a magic bullet. Ethical concerns around data privacy, algorithmic bias, and transparency must be addressed. AI should be a tool to enhance human judgment, not replace it. Leaders must ensure AI is used responsibly—applying it as a guide rather than a dictator. The one advantage here, is that AI does not need to be perfect in order to bring about progress. In fact, all it must do is to produce an improvement over the status-quo, which is a very low bar. Indeed, even in its current form AI is less biased than humans – not because AI is perfect, but because humans are biased by design. With AI, as with any other technology, the goal is not instant perfection, but incremental improvements over the current state-of-affairs, and finding better ways of being wrong. To be sure, there are no reasons whatsoever to have confidence in humans eliminating their own biases, but human ingenuity and expertise has been able to create a tool, a technology, that may well de-bias human decision making – but we must open to not just having ethical and competent humans involved in the design and auditing of these algorithms, but also remove them from the decision-making look (as they are often more likely to introduce than eliminate biases). The Future of Leadership Selection AI won’t eliminate the need for human intuition and relationship-building in succession planning, but it can elevate decision-making. By focusing on objective performance evaluation, expanding candidate pipelines, leveraging behavioral insights, and reducing bias, AI has the potential to make leadership transitions smoother, fairer, and more effective. The future of leadership isn’t about human vs. machine. It’s more likely human with machine — using AI as a partner in one of the most crucial tasks any organization faces: securing its next great leader.
--------------------------------------------------

Title: Don’t Abandon Your Diversification
URL: https://www.whitecoatinvestor.com/dont-abandon-your-diversification/
Time Published: 2025-02-11T07:30:37Z
Description: Just because large value stocks have performed well lately doesn't mean you should stop diversifying your portfolio. Here's why not.
The post Don’t Abandon Your Diversification appeared first on The White Coat Investor - Investing & Personal Finance for Docto…
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq futures slip as Trump's tariffs and inflation prey on minds
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-futures-slip-as-trumps-tariffs-and-inflation-prey-on-minds-003956519.html
Time Published: 2025-02-11T00:39:56Z
Description: Investors are braced for more Trump tariffs as focus turns to inflation and Fed Chair Jerome Powell's Senate testimony.
--------------------------------------------------

Title: Skyharbour Announces Closing of Option and Purchase Agreements with Hatchet Uranium for Several of its Uranium Projects Located in the Athabasca Basin
URL: https://financialpost.com/globe-newswire/skyharbour-announces-closing-of-option-and-purchase-agreements-with-hatchet-uranium-for-several-of-its-uranium-projects-located-in-the-athabasca-basin
Time Published: 2025-02-11T00:00:51Z
Description: Vancouver, BC, Feb. 10, 2025 (GLOBE NEWSWIRE) — Skyharbour Resources Ltd. (TSX-V: SYH) (OTCQX: SYHBF) (Frankfurt: SC1P) (“Skyharbour” or the “Company”), is pleased to announce that, further to its news release dated November 4th, 2024, closing has occurred on…
--------------------------------------------------

Title: Skyharbour Announces Closing of Option and Purchase Agreements with Hatchet Uranium for Several of its Uranium Projects Located in the Athabasca Basin
URL: https://www.globenewswire.com/news-release/2025/02/11/3023858/36591/en/Skyharbour-Announces-Closing-of-Option-and-Purchase-Agreements-with-Hatchet-Uranium-for-Several-of-its-Uranium-Projects-Located-in-the-Athabasca-Basin.html
Time Published: 2025-02-11T00:00:00Z
Full Content:
February 10, 2025 19:00 ET | Source: Skyharbour Resources Ltd Skyharbour Resources Ltd Vancouver, BC, Feb. 10, 2025 (GLOBE NEWSWIRE) -- Skyharbour Resources Ltd. (TSX-V: SYH) (OTCQX: SYHBF) (Frankfurt: SC1P) (“Skyharbour” or the “Company”), is pleased to announce that, further to its news release dated November 4th, 2024, closing has occurred on the option agreement (the “Agreement”) with Hatchet, whereby Hatchet Uranium Corp. (“Hatchet”) may acquire an 80% interest in the Company’s 17,606 ha Highway Uranium Property (the “Optioned Property”) and a 100% interest, subject to a claw-back provision for Skyharbour, in the Company’s Genie, Usam and CBX/Shoe Uranium Projects (the “Purchased Property”). The properties total 66,358 ha and are all located in the Athabasca Basin of Northern Saskatchewan, Canada. The Agreement on the Optioned Property provides Hatchet an opportunity to earn an 80% interest in the claims over a three-year period by fulfilling combined cash, share issuance and exploration expenditure commitments of CAD $3,345,000. For the Purchased Property, Skyharbour will also receive units in the capital of Hatchet consisting of a share and a warrant (“Hatchet Units”) equal to 9.9% of the issued and outstanding shares of Hatchet. Highway, Genie, Usam, CBX and Shoe Project Map:https://skyharbourltd.com/_resources/images/Sky_Highway.jpg Terms of the Optioned Property: The Optioned Property, Highway, consists of nine (9) mineral claims comprising approximately 17,606 hectares. Hatchet may acquire an 80% interest in the Optioned Property by (i) issuing common shares in the capital of Hatchet (“Shares”) having an aggregate value of CAD $1,050,000; (ii) making aggregate cash payments of CAD $245,000; and (iii) incurring an aggregate of CAD $2,050,000 in exploration expenditures on the Optioned Property over a three-year period, as follows: (1) Deemed pricing of Shares is based on the twenty (20) day volume weighted average price on the stock exchange in which Hatchet shall list its Shares for trading, being either the TSX Venture Exchange or the Canadian Securities Exchange (“Deemed Price”) or the last sale price, if not listed on a stock exchange at the time of issuance. In the event that the issuance of any Shares pursuant to the above would result in the Company holding 10% or more of the outstanding Shares of Hatchet, Hatchet will issue that number of Shares which would result in the Company receiving 9.9% of the issued and outstanding Shares post-issuance and will pay cash in lieu of the Shares for the difference. The Company shall retain a 2% net smelter returns royalty from minerals mined and removed from the Optioned Property, of which Hatchet may purchase one-half, being 1%, at any time for $1,000,000. Terms of the Purchased Property: The Purchased Property consists of twenty-five (25) mineral claims comprising approximately 66,358 hectares across the Genie, Usam and CBX/Shoe projects. Hatchet has acquired a 100% interest in the Purchased Property by, on the date of closing (the “Closing Date”), paying the Company $25,000 and issuing to the Company such number of Units in the capital of Hatchet equal to 9.9% of the issued and outstanding shares immediately following the issuance. Each Hatchet Unit shall be comprised of one Share and one share purchase warrant, entitling Skyharbour to purchase one additional Share for a period of three years at a price that is a 25% premium to the deemed value of the Shares in both years 1 and 2, and then increases to a 50% premium to the issuance value of the Shares in year 3. The Company shall retain a claw-back provision whereby, within 90 days after the 3rd anniversary of the Closing Date, the Company may elect by written notice to Hatchet of its intention to purchase back a twenty-five percent (25%) interest in the Purchased Property by, within 90 days of delivery of such notice, incurring exploration expenditures or paying cash in lieu of to fund future exploration, equivalent to fifty percent (50%) of the total amount that Hatchet had spent during the term that is three years from the Closing Date in exploration expenditures on the Purchased Property. If Hatchet has not incurred any exploration expenditures during the three years following the closing date, then Skyharbour shall automatically receive the 25% interest in the Property. The Company shall also retain a 2% net smelter returns royalty from minerals mined and removed from the Purchased Property, of which Hatchet may purchase one-half, being 1%, at any time for $2,000,000. One of the conditions precedent for Hatchet prior to closing on both agreements was to close a financing for minimum gross proceeds of $1,500,000 which is now complete. Furthermore, Hatchet will proceed to list on the TSX Venture Exchange or the Canadian Securities Exchange or will have sold its interest to or combined with a similarly listed issuer. If this is not complete within 18 months, Hatchet’s right to acquire the Purchased Property will terminate. If after 12 months Hatchet has not listed then it shall pay Skyharbour a monthly fee of $10,000 until such conditions are satisfied or an aggregate of $60,000 has been paid, whichever occurs first. Highway Property Summary: The Highway Uranium Project consists of nine claims covering 17,606 hectares, approximately 41 km south of the Rabbit Lake Mine and 11 km southwest of Uranium Energy Corp.’s (UEC, formerly UEX) West Bear U and Co-Ni Deposits. Highway 905 runs through the property, providing excellent access for exploration and the project is in close proximity to regional infrastructure. There has been limited modern exploration carried out on the project but there is the potential for high-grade basement-hosted and unconformity-related uranium mineralization. Highway Property Map:https://skyharbourltd.com/_resources/images/Sky_Highway.jpg‎ The project is underlain by Wollaston Supergroup metasedimentary gneisses (pelitic to psammopelitic and psammitic to meta-arkosic) folded around and overlying an Archean felsic gneiss dome which outcrops in the southwestern portion of the property and cores a northeast trending antiformal fold nose. The Highway Project is located approximately 7 km east of the present-day margin of the Athabasca Basin but is believed to have been covered by Athabasca sandstone in the past. Genie Property Summary: The Genie property consists of five claims totalling 16,930 ha, and is located approximately 48 km northeast of Cameco’s Eagle Point Uranium Mine (Rabbit Lake Operation) and 40 km north of Wollaston Lake Post. The project is underlain by Wollaston Superground metasedimentary gneisses and Archean granitoids, with highly prospective pelitic to psammopelitic gneisses (including graphitic varieties) and several north-trending faults related to the Tabbernor fault system being mapped on the property. The project lies outside the current extent of the Athabasca Basin, but is believed to have been overlain by now-eroded Athabasca sandstones in the past and has the potential for high-grade basement-hosted and unconformity-related uranium mineralization. The property is underlain by a series of linear magnetic highs (interpreted as granitoids) and magnetic lows (interpreted as metasedimentary gneisses), cross-cut by a highly magnetic northwest-trending Mackenzie Diabase dyke. Genie Property Map:https://skyharbourltd.com/_resources/images/Sky_Genie.jpg Previous work on the Genie project includes limited diamond drilling (three historical drill holes, of which one was abandoned in overburden) and a variety of airborne and ground geophysical surveys, prospecting, geological mapping, lake sediment and overburden sampling, and soil sampling. Most of this exploration work took place between 1966 to 1980, prior to the advent of modern geophysical methods and geological models, but in 2014 part of the Genie property was covered by a helicopter-borne DIGHEM magnetic, electromagnetic, and radiometric survey. The survey showed a strong central EM conductor following a magnetically inferred contact on the two northeastern most claims, which is locally disrupted by several moderately conductive N-S trending structural breaks, inferred to be faults. This strong conductor is highly prospective for uranium mineralization, and drilling done in 1969 and 1971 has confirmed the presence of graphitic and sulfide-containing pelitic gneisses on the property. Lake sediment samples also collected at Genie during the 2014 exploration program, contained up to 63.3 ppm U, further showcasing the prospectivity of the property. Usam Property Summary: The Usam Project consists of twelve claims totalling 40,041 ha and is located approximately 16 km northeast of Cameco’s Eagle Point Mine (Rabbit Lake Operation). The project has numerous EM conductors that are associated with significant magnetic lows of the Wollaston Domain. While the project is outside the current confines of the Athabasca Basin, the area was overlain by Athabasca sandstones historically. Basement rocks on the property include Wollaston Supergroup metasediments and Archean granitoid gneisses, with highly prospective pelitic to psammopelitic gneisses (including graphitic varieties) making up the largest proportion of the basement rocks. Several north-trending faults related to the Tabbernor fault system cross-cut the property. Usam Property Map:https://skyharbourltd.com/_resources/images/Sky_Usam.jpg Previous work on the project includes diamond drilling (12 holes), lake sediment sampling, soil sampling, geological mapping, ground and airborne geophysics, marine seismic, prospecting, and other geochemical sampling, the majority of which was done in the 1980’s and 1970’s. Modern exploration of the property has been limited to geophysics and ground prospecting. As such there is a significant untested potential on the project. Trenching on Cleveland Island uncovered up to 0.31% U3O8 in mineralized pegmatite, and diamond drilling on Gilles Island intersected anomalous uranium, indicating that the basement rocks underling the Usam property are fertile sources of uranium in addition to containing pegmatite- and granite-hosted U-Th-REE mineralization. There are also several sedimentary-hosted base metals (i.e. Cu and Zn) showings on the project and in the surrounding area, which show similarities to the sedimentary-hosted Cu mineralization previously discovered by Rio Tinto and its partners at the Janice Lake Project further southwest in the Wollaston Domain. CBX/Shoe Property Summary: The CBX property has been recently expanded through staking to include five additional claims adjoining the previously staked CBX and Shoe properties, which have been combined to include a total of seven claims covering 8,777 hectares. The 609 ha Shoe property has remained unchanged, with both CBX and Shoe now consisting of eight non-contiguous claims totalling 9,386 hectares. CBX/Shoe Property Map:https://skyharbourltd.com/_resources/images/Sky_Shoe.jpg‎ The new claims lie approximately 6.5 km to 25 km northeast of the Eagle Point uranium mine and cover the northern shore of Wollaston Lake including parts of Cunning Bay. Outcrop exposure on the property is poor, but historical mapping and drilling shows that the newly expanded CBX project is underlain by a mixture of Wollaston Supergroup metasedimentary gneisses, Hudsonian intrusives, and Archean felsic gneisses of the Western Wollaston Domain. Similar lithologies host uranium mineralization at the Rabbit Lake operation, including the Eagle Point deposit, and other uranium deposits in the Athabasca Basin and surrounding regions. The CBX and Shoe properties have had historical exploration, including airborne and ground geophysical surveys, lake sediment, soil, and spruce geochemical surveys, till sampling, prospecting, geological mapping, and a marine seismic survey, but the majority of this work took place in the 1960’s to 1980’s, with limited modern exploration work being carried out on a small portion of the CBX and Shoe properties. Grant of Incentive Stock Options: Skyharbour also announces that the Company has granted 3,500,000 incentive stock options (the "Options") to officers, directors and consultants of the Company. The Options are exercisable at $0.40 per share for a period of five years from the date of grant. The Options have been granted under and are governed by the terms of the Company's Incentive Stock Option Plan. Qualified Person: The technical information in this news release has been prepared in accordance with the Canadian regulatory requirements set out in National Instrument 43-101 and reviewed and approved by Serdar Donmez, P.Geo., VP of Exploration for Skyharbour as well as a Qualified Person. About Skyharbour Resources Ltd.: Skyharbour holds an extensive portfolio of uranium exploration projects in Canada's Athabasca Basin and is well positioned to benefit from improving uranium market fundamentals with interest in thirty-six projects covering over 614,000 hectares (over 1.5 million acres) of land. Skyharbour has acquired from Denison Mines, a large strategic shareholder of the Company, a 100% interest in the Moore Uranium Project, which is located 15 kilometres east of Denison's Wheeler River project and 39 kilometres south of Cameco's McArthur River uranium mine. Moore is an advanced-stage uranium exploration property with high-grade uranium mineralization in several zones at the Maverick Corridor. Adjacent to the Moore Project is the Russell Lake Uranium Project, in which Skyharbour is operator with joint-venture partner RTEC. The project hosts widespread uranium mineralization in drill intercepts over a large property area with exploration upside potential. The Company is actively advancing these projects through exploration and drilling programs. Skyharbour also has joint ventures with industry leader Orano Canada Inc., Azincourt Energy, and Thunderbird Resources at the Preston, East Preston, and Hook Lake Projects, respectively. The Company also has several active earn-in option partners, including CSE-listed Basin Uranium Corp. at the Mann Lake Uranium Project; TSX-V listed North Shore Uranium at the Falcon Project; UraEx Resources at the South Dufferin and Bolt Projects; Hatchet Uranium at the Highway Project; CSE-listed Mustang Energy at the 914W Project; and TSX-V listed Terra Clean Energy at the South Falcon East Project. In aggregate, Skyharbour has now signed earn-in option agreements with partners that total to over $36 million in partner-funded exploration expenditures, over $20 million worth of shares being issued, and $14 million in cash payments coming into Skyharbour, assuming that these partner companies complete their entire earn-ins at the respective projects. Skyharbour's goal is to maximize shareholder value through new mineral discoveries, committed long-term partnerships, and the advancement of exploration projects in geopolitically favourable jurisdictions. Skyharbour’s Uranium Project Map in the Athabasca Basin:https://www.skyharbourltd.com/_resources/images/SKY_SaskProject_Locator_2024-11-21_v1.jpg To find out more about Skyharbour Resources Ltd. (TSX-V: SYH) visit the Company’s website at www.skyharbourltd.com. SKYHARBOUR RESOURCES LTD. “Jordan Trimble”__________________________________Jordan TrimblePresident and CEO For further information contact myself or:Nicholas ColturaInvestor Relations Manager ‎Skyharbour Resources Ltd. ‎Telephone: 604-558-5847 ‎Toll Free: 800-567-8181 ‎Facsimile: 604-687-3119 ‎Email: info@skyharbourltd.com NEITHER THE TSX VENTURE EXCHANGE NOR ITS REGULATION SERVICES PROVIDER ACCEPTS RESPONSIBILITY FOR THE ADEQUACY OR ACCURACY OF THE CONTENT OF THIS NEWS RELEASE. Forward-Looking Information This news release contains “forward‐looking information or statements” within the meaning of applicable securities laws, which may include, without limitation, completing ongoing and planned work on its projects including drilling and the expected timing of such work programs, other statements relating to the technical, financial and business prospects of the Company, its projects and other matters. All statements in this news release, other than statements of historical facts, that address events or developments that the Company expects to occur, are forward-looking statements. Although the Company believes the expectations expressed in such forward-looking statements are based on reasonable assumptions, such statements are not guarantees of future performance and actual results may differ materially from those in the forward-looking statements. Such statements and information are based on numerous assumptions regarding present and future business strategies and the environment in which the Company will operate in the future, including the price of uranium, the ability to achieve its goals, that general business and economic conditions will not change in a material adverse manner, that financing will be available if and when needed and on reasonable terms. Such forward-looking information reflects the Company’s views with respect to future events and is subject to risks, uncertainties and assumptions, including the risks and uncertainties relating to the interpretation of exploration results, risks related to the inherent uncertainty of exploration and cost estimates and the potential for unexpected costs and expenses, and those filed under the Company’s profile on SEDAR+ at www.sedarplus.ca. Factors that could cause actual results to differ materially from those in forward looking statements include, but are not limited to, continued availability of capital and financing and general economic, market or business conditions, adverse weather or climate conditions, failure to obtain or maintain all necessary government permits, approvals and authorizations, failure to obtain or maintain community acceptance (including First Nations), decrease in the price of uranium and other metals, increase in costs, litigation, and failure of counterparties to perform their contractual obligations. The Company does not undertake to update forward‐looking statements or forward‐looking information, except as required by law.
--------------------------------------------------

Title: ValOre Announces Closing of Agreements with Skyharbour for Uranium Projects Located in Saskatchewan
URL: https://financialpost.com/globe-newswire/valore-announces-closing-of-agreements-with-skyharbour-for-uranium-projects-located-in-saskatchewan
Time Published: 2025-02-10T22:19:35Z
Description: VANCOUVER, British Columbia, Feb. 10, 2025 (GLOBE NEWSWIRE) — ValOre Metals Corp. (“ValOre”; TSX‐V: VO; OTCQB: KVLQF; Frankfurt: KEQ0) today provided an update on developments concerning Hatchet Uranium Corp. (“Hatchet”), in which ValOre currently holds an ap…
--------------------------------------------------

Title: ValOre Announces Closing of Agreements with Skyharbour for Uranium Projects Located in Saskatchewan
URL: https://www.globenewswire.com/news-release/2025/02/10/3023833/0/en/ValOre-Announces-Closing-of-Agreements-with-Skyharbour-for-Uranium-Projects-Located-in-Saskatchewan.html
Time Published: 2025-02-10T22:15:00Z
Full Content:
February 10, 2025 17:15 ET | Source: ValOre Metals Corporation ValOre Metals Corporation VANCOUVER, British Columbia, Feb. 10, 2025 (GLOBE NEWSWIRE) -- ValOre Metals Corp. (“ValOre”; TSX‐V: VO; OTCQB: KVLQF; Frankfurt: KEQ0) today provided an update on developments concerning Hatchet Uranium Corp. (“Hatchet”), in which ValOre currently holds an approximate 51.5% partially diluted ownership interest. ValOre, further to its news releases dated November 4th, 2024, and February 5th, 2025, announces that closing has now occurred on the option agreement (the “Agreement”) with Skyharbour Resources Ltd. (“Skyharbour”), whereby Hatchet may acquire an 80% interest in Skyharbour’s 17,606 ha Highway Uranium Property (“Highway”) and a 100% interest, subject to a claw-back provision for Skyharbour, in Skyharbour’s Genie, Usam and CBX/Shoe Uranium Projects (the “Purchased Properties”) totalling 66,358 ha, all located to the northeast of the Athabasca Basin, northern Saskatchewan, Canada. The Agreement on Highway provides Hatchet an opportunity to earn an 80% interest in the related claims over a three-year period by fulfilling combined cash, share issuance and exploration expenditure commitments of CAD $3.345 million. Terms of Highway Property Agreement: Highway, now consists of nine (9) mineral claims comprising approximately 17,606 hectares, due to the recent addition of five (5) mineral claims comprising 8,267 ha. Hatchet may acquire an 80% interest in Highway by (i) issuing common shares in the capital of Hatchet (“Shares”) having an aggregate value of CAD $1,050,000; (ii) making aggregate cash payments of CAD $245,000; and (iii) incurring an aggregate of CAD $2,050,000 in exploration expenditures on Highway over a three-year period, as follows: (1) Deemed pricing of Shares is based on the twenty (20) day volume weighted average price on the stock exchange in which Hatchet shall list its Shares for trading, being either the TSX Venture Exchange or the Canadian Securities Exchange (“Deemed Price”) or the last sale price, if not listed on a stock exchange at the time of issuance. In the event that the issuance of any Shares pursuant to the above would result in Skyharbour holding 10% or more of the outstanding Shares of Hatchet, Hatchet will issue that number of Shares which would result in Skyharbour receiving 9.9% of the issued and outstanding Shares post-issuance and will pay cash in lieu of the Shares for the difference. Skyharbour shall retain a 2% net smelter returns royalty from minerals mined and removed from Highway, of which Hatchet may purchase one-half, being 1%, at any time for $1,000,000. Terms of the Purchased Properties: The Purchased Properties consists of twenty-five (25) mineral claims comprising approximately 66,358 hectares across the Genie, Usam and CBX/Show projects. Hatchet acquired a 100% interest in the Purchased Properties by, on the date of closing (the “Closing Date”), paying Skyharbour $25,000 and issuing to Skyharbour such number of units in the capital of Hatchet (“Hatchet Units”) equal to 9.9% of the issued and outstanding Shares immediately following issuance. Each Hatchet Unit shall be comprised of one Share and one share purchase warrant, entitling Skyharbour to purchase one additional Share for a period of three years at a price that is a 25% premium to the deemed value of the Shares in both years 1 and 2, and then increases to a 50% premium to the issuance value of the Shares in year 3. Skyharbour shall retain a claw-back provision whereby, within 90 days after the 3rd anniversary of the Closing Date, Skyharbour may elect by written notice to Hatchet of its intention to purchase back a twenty-five percent (25%) interest in the Purchased Properties by, within 90 days of delivery of such notice, incurring exploration expenditures or paying cash in lieu of to fund future exploration, equivalent to fifty percent (50%) of the total amount that Hatchet had spent during the term that is three years from the Closing Date in exploration expenditures on the Purchased Properties. If Hatchet has not incurred any exploration expenditures during the three years following the closing date, then Skyharbour shall automatically receive the 25% interest in the Purchased Properties. Skyharbour shall also retain a 2% net smelter returns royalty from minerals mined and removed from the Purchased Properties, of which Hatchet may purchase one-half, being 1%, at any time for $2,000,000. One of the conditions precedent for Hatchet prior to closing on both agreements was to close a financing for minimum gross proceeds of $1,500,000 which is now complete. Furthermore, Hatchet will proceed to list on the TSX Venture Exchange or the Canadian Securities Exchange or will have sold its interest to or combined with a similarly listed issuer. If this is not complete within 18 months, Hatchet’s right to acquire the Purchased Property will terminate. If after 12 months Hatchet has not listed then it shall pay Skyharbour a monthly fee of $10,000 until such conditions are satisfied or an aggregate of $60,000 has been paid, whichever occurs first. Highway Property Summary: The Highway Uranium Project consists of nice (9) claims covering 17,606 hectares, approximately 41 km south of the Rabbit Lake Mine and 11 km southwest of Uranium Energy Corp.’s (UEC, formerly UEX) West Bear U and Co-Ni Deposits. The Highway Project is located approximately 7 km east of the present-day margin of the Athabasca Basin but is believed to have been covered by Athabasca sandstone in the past. Highway 905 runs through the property, providing excellent access for exploration and in close proximity to regional infrastructure. There has been limited modern exploration performed on the project but there is the potential for high-grade basement-hosted-uranium mineralization. The project is underlain by Wollaston Supergroup metasedimentary gneisses (pelitic to psammopelitic and psammitic to meta-arkosic) folded around and overlying an Archean felsic gneiss dome which outcrops in the southwestern portion of the property and cores a northeast trending antiformal fold nose. Figure 1: Highway Property Location MapGenie Property Summary: The Genie property consists of five claims totalling 16,930 ha, and is located approximately 48 km northeast of Cameco’s Eagle Point Uranium Mine (Rabbit Lake Operation) and 40 km north of Wollaston Lake Post. The project is underlain by Wollaston Superground metasedimentary gneisses and Archean granitoids, with prospective pelitic to psammopelitic gneisses (including graphitic varieties) and several north-trending faults related to the Tabbernor fault system being mapped on the property. The project lies outside the current extent of the Athabasca Basin, but is believed to have been overlain by now-eroded Athabasca sandstones in the past and has the potential for high-grade basement-hosted uranium mineralization. The property is underlain by a series of linear magnetic highs (interpreted as granitoids) and magnetic lows (interpreted as metasedimentary gneisses), cross-cut by a highly magnetic northwest-trending Mackenzie Diabase dyke. Previous work on the Genie project includes limited diamond drilling (three historical drill holes, of which one was abandoned in overburden) and a variety of airborne and ground geophysical surveys, prospecting, geological mapping, lake sediment and overburden sampling, and soil sampling. Most of this exploration work took place between 1966 to 1980, prior to the advent of modern geophysical methods and geological models, but in 2014 part of the Genie property was covered by a helicopter-borne DIGHEM magnetic, electromagnetic, and radiometric survey. The survey showed a strong central EM conductor following a magnetically inferred contact on the two northeastern most claims, which is locally disrupted by several moderately conductive N-S trending structural breaks, inferred to be faults. This strong conductor is highly prospective for uranium mineralization, and drilling done in 1969 and 1971 has confirmed the presence of graphitic and sulfide-containing pelitic gneisses on the property. Lake sediment samples also collected at Genie during the 2014 exploration program, contained up to 63.3 ppm U, further showcasing the prospectivity of the property. Figure 2: Genie Property Location MapUsam Property Summary: The Usam Project consists of twelve claims totalling 40,041 ha and is located approximately 16 km northeast of Cameco’s Eagle Point Mine (Rabbit Lake Operation). The project has numerous EM conductors that are associated with significant magnetic lows of the Wollaston Domain. While the project is outside the current confines of the Athabasca Basin, the area was overlain by Athabasca sandstones historically. Basement rocks on the property include Wollaston Supergroup metasediments and Archean granitoid gneisses, with highly prospective pelitic to psammopelitic gneisses (including graphitic varieties) making up the largest proportion of the basement rocks. Several north-trending faults related to the Tabbernor fault system cross-cut the property. Previous work on the project includes diamond drilling (12 holes), lake sediment sampling, soil sampling, geological mapping, ground and airborne geophysics, marine seismic, prospecting, and other geochemical sampling, the majority of which was done in the 1980’s and 1970’s. Modern exploration of the property has been limited to geophysics and ground prospecting. As such there is a significant untested potential on the project. Trenching on Cleveland Island uncovered up to 0.31% U3O8 in mineralized pegmatite, and diamond drilling on Gilles Island intersected anomalous uranium, indicating that the basement rocks underling the Usam property are fertile sources of uranium in addition to containing pegmatite- and granite-hosted U-Th-REE mineralization. There are also several sedimentary-hosted base metals (i.e. Cu and Zn) showings on the project and in the surrounding area, which show similarities to the sedimentary-hosted Cu mineralization previously discovered by Rio Tinto and its partners at the Janice Lake Project further southwest in the Wollaston Domain. Figure 3 – Usam Property Location Map CBX/Shoe Property Summary: The CBX property has been recently expanded through staking to include five additional claims adjoining the previously staked CBX and Shoe properties, which have been combined to include a total of seven claims covering 8,777 hectares. The 609 ha Shoe property has remained unchanged, with both CBX and Shoe now consisting of eight non-contiguous claims totalling 9,386 hectares. The new claims lie approximately 6.5 km to 25 km northeast of the Eagle Point uranium mine and cover the northern shore of Wollaston Lake including parts of Cunning Bay. Outcrop exposure on the property is poor, but historical mapping and drilling shows that the newly expanded CBX project is underlain by a mixture of Wollaston Supergroup metasedimentary gneisses, Hudsonian intrusives, and Archean felsic gneisses of the Western Wollaston Domain. Similar lithologies host uranium mineralization at the Rabbit Lake operation, including the Eagle Point deposit, and other uranium deposits in the Athabasca Basin and surrounding regions. The CBX and Shoe properties have had historical exploration, including airborne and ground geophysical surveys, lake sediment, soil, and spruce geochemical surveys, till sampling, prospecting, geological mapping, and a marine seismic survey, but the majority of this work took place in the 1960’s to 1980’s, with limited modern exploration work being carried out on a small portion of the CBX and Shoe properties. Figure 4: CBX/Shoe Property Location Map About Hatchet Uranium Corp.Hatchet Uranium Corp. was incorporated by ValOre on February 7, 2024 and now holds a commanding land position, comprising 97,674 hectares, in the Eastern Athabasca Region of Saskatchewan. About Skyharbour Resources Ltd. To find out more about Skyharbour Resources Ltd. (TSX-V: SYH) visit Skyharbour’s website at www.skyharbourltd.com. Qualified Person (“QP”) The technical information in this news release has been prepared in accordance with Canadian regulatory requirements set out in NI 43-101 and reviewed and approved by Thiago Diniz, P.Geo., ValOre’s QP and Vice President of Exploration. About ValOre Metals Corp. ValOre Metals Corp. (TSX‐V: VO) is a Canadian company with a team aiming to deploy capital and knowledge on projects which benefit from substantial prior investment by previous owners, existence of high-value mineralization on a large scale, and the possibility of adding tangible value through exploration and innovation. ValOre’s Pedra Branca Platinum Group Elements Project comprises 45 exploration licenses covering a total area of 51,096 hectares (126,260 acres) in northeastern Brazil. At Pedra Branca, 7 distinct PGE+Au deposit areas host, in aggregate, a 2022 NI 43-101 inferred resource of 2.198 Moz 2PGE+Au contained in 63.6 Mt grading 1.08 g/t 2PGE+Au. ValOre’s team believes the Pedra Branca project has significant exploration discovery and resource expansion potential. (CLICK HERE to download 2022 technical report* and CLICK HERE for news release dated March 24, 2022). *The 2022 Technical Report entitled “Independent Technical Report –Mineral Resource Update on the Pedra Branca PGE Project, Ceará State, Brazil” was prepared as a National Instrument 43-101 Technical Report on behalf of ValOre Metals Corp. with an effective date of March 08, 2022. The 2022 Technical Report by independent qualified persons, Fábio Valério (P.Geo.) and Porfirio Cabaleiro (P.Eng.), of GE21, commissioned to complete the mineral resource estimate while Chris Kaye of Mine and Quarry Engineering Services Inc. (MQes), was commissioned to review the metallurgical information. The Mineral Resource estimates were prepared in accordance with the CIM Standards, and the CIM Guidelines, using geostatistical, plus economic and mining parameters appropriate to the deposit. Mineral Resources, which are not mineral reserves, do not have demonstrated economic viability, and may be materially affected by environmental, permitting, legal, marketing, and other relevant issues. Mineral Resources are based upon a cut-off grade of 0.4 g/t PGE+Au, correlated to Pd_eq grade of 0.35 g/t, and were limited by an economic pit built in Geovia Whittle 4.3 software and following the geometric and economic parameters as disclosed in the 2022 NI 43-101 Technical Report. On behalf of the Board of Directors,“Jim Paterson”James R. Paterson, Chairman and CEO ValOre Metals Corp. For further information about ValOre Metals Corp. or this news release, please visit our website at www.valoremetals.com or contact Investor Relations at 604.646.4527, or by email at contact@valoremetals.com. ValOre Metals Corp. is a proud member of Discovery Group. For more information, please visit: http://www.discoverygroup.ca/ Neither the TSX Venture Exchange nor its Regulation Services Provider (as that term is defined in the policies of the TSX Venture Exchange) accepts responsibility for the adequacy or accuracy of this release. This news release contains “forward-looking statements” within the meaning of applicable securities laws. Although ValOre believes that the expectations reflected in its forward-looking statements are reasonable, such statements have been based on factors and assumptions concerning future events that may prove to be inaccurate. These factors and assumptions are based upon currently available information to ValOre. Such statements are subject to known and unknown risks, uncertainties and other factors that could influence actual results or events and cause actual results or events to differ materially from those stated, anticipated or implied in the forward-looking statements. A number of important factors including those set forth in other public filings could cause actual outcomes and results to differ materially from those expressed in these forward-looking statements. Factors that could cause the actual results to differ materially from those in forward-looking statements include the future operations of ValOre and economic factors. Readers are cautioned to not place undue reliance on forward-looking statements. The statements in this press release are made as of the date of this release and, except as required by applicable law, ValOre does not undertake any obligation to publicly update or to revise any of the included forward-looking statements, whether as a result of new information, future events or otherwise. ValOre undertakes no obligation to comment on analyses, expectations or statements made by third parties in respect of ValOre, or its financial or operating results or (as applicable), their securities. Photos accompanying this announcement are available at: https://www.globenewswire.com/NewsRoom/AttachmentNg/f5c3fcc5-6703-483e-8780-62526e848cd3https://www.globenewswire.com/NewsRoom/AttachmentNg/9e021720-7cb6-4cd4-88bd-fb660ddbbeaehttps://www.globenewswire.com/NewsRoom/AttachmentNg/0a871315-2064-4f0f-b182-8163e80f949fhttps://www.globenewswire.com/NewsRoom/AttachmentNg/8521e5a0-18da-4705-a8ab-6410f0ff48b4
--------------------------------------------------

Title: The Magnificent 7 trade is struggling — Here's why
URL: https://finance.yahoo.com/news/the-magnificent-7-trade-is-struggling--heres-why-181716458.html
Time Published: 2025-02-10T18:17:16Z
Description: The Mag 7 trade has cooled down. Here is the simplest reason why.
--------------------------------------------------

Title: Snap Plans $700 Million Junk Offering to Buy Back Convertibles
URL: https://financialpost.com/pmn/business-pmn/snap-plans-700-million-junk-offering-to-buy-back-convertibles
Time Published: 2025-02-10T17:49:17Z
Description: Snap Inc. plans to offer $700 million of junk bonds to repurchase convertible debt, the social-media company said in a statement on Monday.
--------------------------------------------------

Title: Disney dumps two DEI programs as investors pressure company to ax more woke initiatives: SEC filing
URL: https://nypost.com/2025/02/10/media/disney-dumps-two-dei-programs-as-investors-pressure-company/
Time Published: 2025-02-10T17:21:23Z
Full Content:
Disney is reportedly pulling back on its diversity, equity and inclusion policies — the latest major company to walk back the woke initiatives amid pressure from activist investors and the Trump administration. The media giant — which saw its bottom line hurt by the battle over Florida’s “Don’t Say Gay” bill — quietly dropped its “Reimagine Tomorrow” program from the DEI section of its 2024 SEC 10-K report, according to a recent regulatory filing The program, which was mentioned in its 2023 report, has a mission statement of “amplifying underrepresented voices and features some of Disney’s DE&I commitments and action,” according to its website. The initiative promised 50% of regular and recurring characters across the Disney universe would come from “underrepresented groups.” The program sparked outrage in 2022 when a company-wide Zoom call was leaked on social media. One Disney executive touted her “not at all secret gay agenda” at the company, while another boasted that the company was ditching the words “ladies, gentlemen, boys, and girls” at its theme parks in order to not alienate transgender children. Although the program’s website is still up and running, Stefan Padfield, director of the Free Enterprise Project for the National Center for Public Policy Research, told Fox News Digital on Monday that its exclusion from the SEC filing could signal change at the Mouse House. “Disney dropping [Reimagine Tomorrow] from their DEI section could mean they’re walking back their DEI investments, or it could signal they’re hiding them,” Padfield said. “Either they recognize that more litigation is coming, or it could be part of a vibe shift.” The company has also dropped its “The Disney Look” appearance guidelines from the DEI section in its SEC filing. The 2023 SEC filing stated that the guidelines were “updated to cultivate a more inclusive environment that encourages and celebrates authentic expressions of belonging among employees.” Disney did not immediately respond to requests for comment. Disney’s DEI policies were in part a reaction to Florida Gov. Ron DeSantis’ “Don’t Say Gay” law, which barred the discussion of gender identity and sexual orientation for kids in public schools. President Trump recently ordered an end to DEI in the federal government and for its contractors, which includes many private companies. Meanwhile, companies are also under pressure from conservative critics who say DEI programs are discriminatory against non-minorities. Corporations such as Meta and John Deere have rolled back their DEI programs, while others like Apple and Costco have pushed back. Google, GM, Intel, Pepsi, Comcast, Philip Morris and others have softened or deleted their DEI language. “Where is your data that shows DEI serves the bottom line?” Padfield asked of companies that still employ DEI measures. “The concern about the scrutiny about these questions is built into this movement we’re seeing across companies. The Trump administration announced they’ll investigate nine companies for their DEI practices, and you’re seeing corporations scramble to not be among those nine,” he said. The DEI battle is also being fought in the courtroom. On Friday, Target was hit with a class-action suit, after shareholders alleged the retail giant misled investors about the risks of its DEI initiatives, which led consumers to boycott and its stock price to tank. Advertisement
--------------------------------------------------

Title: Crypto News Weekly Recap: Trump’s Tariff War, the Rise of AI Meme Coins, and New P2E Games
URL: https://bitcoinist.com/weekly-recap-trumps-tariffs-ai-meme-coins-p2e-games/
Time Published: 2025-02-10T14:00:47Z
Full Content:
We’re back with our weekly crypto recap. If Trump’s presidency means one thing, it’s that there’ll be no shortage of crypto news. This past week, the newly inaugurated president (along with his immediate family and friends) continued to make headlines in crypto circles. We also witnessed noteworthy developments in AI and gaming. Here’s a recap of what happened in crypto recently: Now, let’s zoom in a little. The crypto market faced turbulence, with Bitcoin ($BTC) briefly touching $92K after Trump announced 25% tariffs on Canadian and Mexican imports. Canada immediately imposed a counter-tariff, whereas Mexican President Claudia Sheinbaum took a more cautious approach. However, there may be more strategic depth to Trump’s tariffs threats than immediate market reactions suggest. Specifically, it could be an attempt to weaken the dollar while maintaining low yields, thus forcing countries to shift from short-term dollar reserves to long-term Treasury bonds. Eric Trump appears to be his father’s main crypto advisor, as evident from his involvement with World Liberty Finance (WLFI) and the official $TRUMP meme coin. Last Monday, the presidential son tweeted ‘it’s a great time to add $ETH’ as the token dipped some 20% (on the back of Trump’s tariff war, that is). $ETH’s price subsequently rose from $2.3K to $2.7K. With an average $ETH purchase price of $3.3K, WLFI currently faces an unrealized loss of approximately $31M. Trump signed an executive order to establish America’s first federal sovereign wealth fund. Senator Cynthia Lummis then tweeted it’s a ‘₿ig deal,’ with the Bitcoin ₿, possibly hinting at an upcoming $BTC investment. This approach would mirror Norway’s sovereign wealth fund strategy, which already has indirect $BTC exposure through investments in companies like MicroStrategy. While Alaska and Texas have been successfully operating state-level sovereign wealth funds for some time, this is an uncharted territory for the federal government. Musk raised eyebrows with his proposal to put the US Treasury on the blockchain – all $1.5T of it. We’re talking about a blockchain that would handle everything from social security checks to federal employee paychecks. However, many are skeptical about the idea. Either way, this transition would force over 3M federal employees to start using blockchain technology and push crypto adoption to levels never seen before. Google CEO Sundar Pichai plans to invest $75B in AI, which he called a ‘small expenditure.’ It’s clear that AI is taking center stage in Google’s strategy, especially given how much of the company’s $95B revenue last year was tied to AI-related services. While Google’s yearly revenue increased 12%, Wall Street seemed jittery about this spending plan, which sent Alphabet Inc. shares down 10%. Capital inflows into the AI sector mean that AI meme coins and AI agent tokens might see more upside this year. In other crypto news, Mythical Games launched a Super Bowl promotion for NFL Rivals, its NFT mobile football game. The two-week event features 30 new player cards from the Kansas City Chiefs and Philadelphia Eagles. In other crypto game news, a new move-to-earn (M2E) fitness app, StepMania, is now available on Telegram. It takes inspiration from StepN, a 2021 M2E game that attracted 5.6M users with its unique mechanic and sneaker NFTs. Trump’s pro-crypto strategy, shared by Eric Trump and Elon Musk, is bullish for the entire altcoin sector. That’s particularly true for AI tokens like MIND of Pepe ($MIND) in light of the current Big Tech AI race. MIND of Pepe is a self-sovereign agent that runs on Ethereum. It can analyze market data, deliver exclusive insights to its token holders, engage in discussions on social media, and even launch its own projects (including crypto games). Currently on presale, one $MIND token costs $0.0032924, but the price will increase in 14 hours. The project has raised $5.6M so far, and early adopters have staked 859B tokens at a 407% APY. After $MIND hits the ground running and lists on DEX, it could surge 10X, especially now that Eric Trump has endorsed the Ethereum ecosystem. This week, all eyes are on Trump’s crypto agenda and the AI sector. The effect of federal policy decisions on market movements is undeniable, so we can only hope the upcoming days will bring good news. Uncertainty forces investors to seek utility-focused projects like MIND of Pepe. Its data-backed market insights and ability to autonomously launch new projects make it one of the best presales in 2025. As always, however, be sure to DYOR and keep a cool head. The current crypto market is extremely volatile, so diversify your portfolio and only invest as much as you can afford to lose. For updates and exclusive offers enter your email. Bitcoinist is the ultimate news and review site for the crypto currency community! Bitcoin news portal providing breaking news, guides, price analysis about decentralized digital money & blockchain technology. © 2025 Bitcoinist. All Rights Reserved.
--------------------------------------------------

Title: Mapping the learning curves of deep learning networks
URL: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012286
Time Published: 2025-02-10T14:00:00Z
Full Content:
There is an important challenge in systematically interpreting the internal representations of deep neural networks (DNNs). Existing techniques are often less effective for non-tabular tasks, or they primarily focus on qualitative, ad-hoc interpretations of models. In response, this study introduces a cognitive science-inspired, multi-dimensional quantification and visualization approach that captures two temporal dimensions of model learning: the “information-processing trajectory” and the “developmental trajectory.” The former represents the influence of incoming signals on an agent’s decision-making, while the latter conceptualizes the gradual improvement in an agent’s performance throughout its lifespan. Tracking the learning curves of DNNs enables researchers to explicitly identify the model appropriateness of a given task, examine the properties of the underlying input signals, and assess the model’s alignment (or lack thereof) with human learning experiences. To illustrate this method, we conducted 750 runs of simulations on two temporal tasks: gesture detection and sentence classification, showcasing its applicability across different types of deep learning tasks. Using four descriptive metrics to quantify the mapped learning curves—start, end - start, max, tmax—, we identified significant differences in learning patterns based on data sources and class distinctions (all p’s < .0001), the prominent role of spatial semantics in gesture learning, and larger information gains in language learning. We highlight three key insights gained from mapping learning curves: non-monotonic progress, pairwise comparisons, and domain distinctions. We reflect on the theoretical implications of this method for cognitive processing, language models and representations from multiple modalities. Deep learning networks, specifically recurrent neural networks (RNNs), are designed for processing incoming signals sequentially, making them intuitive computational systems for studying cognitive processing that involves dynamic contexts. There has been a tradition in the fields of machine learning and neuro-cognitive science to examine how a system (either humans or models) represents information through various computational and statistical techniques. Our study takes this one step further by devising a technique for examining the “learning curves” of deep learning networks utilizing the sequential representations as part of RNNs’ architectures. Just as humans develop learning curves when solving problems, the introduced method captures both how incoming signals help improve decision-making and how a system’s problem-solving abilities enhance when encountering the same situation multiple times throughout its lifespan. Our study selected two distinct tasks: gesture detection and emotion tweet classification, to illustrate the insights researchers can draw from mapping models’ learning curves. The proposed method hinted that gesture learning experiences are smoother, while language learning relies on sudden knowledge gains during processing, corroborating the findings from previous literature. Citation: Jiang Y, Dale R (2025) Mapping the learning curves of deep learning networks. PLoS Comput Biol 21(2): e1012286. https://doi.org/10.1371/journal.pcbi.1012286 Editor: Varun Dutt, Indian Institute of Technology Mandi - Kamand Campus: Indian Institute of Technology Mandi, INDIA Received: July 1, 2024; Accepted: January 13, 2025; Published: February 10, 2025 Copyright: © 2025 Jiang, Dale. This is an open access article distributed under the terms of the CreativeCommonsAttributionLicense, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability: All code used for running simulations, model fitting, plotting, and analysis is available in a GitHub repository at https://github.com/JoyceJiang73/Learning-Curves/. Data for conducting simulations is available from https://github.com/linuxsino/iMiGUE and https://huggingface.co/datasets/dair-ai/emotion. Competing interests: The authors have declared that no competing interests exist. 1 The advantages of using key point data are twofold. First, keypoint data conserves significant computing power; since each second of image sequences (i.e., matrices of pixels) may contain as many as 30 or 40 frames, running deep learning models on image sequences can be prohibitively expensive. Second, it provides better interpretability and understandability. Instead of being possible only through abstract information at the image-frame level, gesture detection can be operationalized as a sequential movement in keypoints across frames [57]. 2 The large batch size for emotion classification is due to each of its input data points using less space compared to the gesture input. 3 The spatial information arising from the gesture, specifically the keypoint coordinates in the gesture dataset, can vaguely help the model distinguish between gesture classes (e.g., hand vs. no gesture). However, this information remains a low-level cue, as the model lacks an understanding of what a complete hand gesture sequence or a no-gesture sequence looks like. Over the past decade, deep learning and neural networks have achieved remarkable performance in prediction and classification tasks in various domains, from machine translation and object recognition, to autonomous driving and reinforcement learning [1, 2]. As powerful representational learning tools, deep neural networks (DNNs) can capture complex patterns in data [1], yet understanding the nature of the information embedded in their multidimensional representations remains a challenge. Researchers have raised concerns about how DNN embeddings represent knowledge and how to holistically interpret these high-dimensional features [3–6]. Over-reliance on such models for decision-making could be detrimental in both research and applied settings due to their complexity and lack of explainability. This is particularly concerning when models rely on biased training data that does not generalize well to target tasks [7, 8]. Without a deep understanding of these models’ underlying properties, they may fail to align with the goals of their human designers. Recently, there has been a combined effort from cognitive science and deep learning to utilize representational learning to address model explainability concerns. These practices have become more prominent due to the success of large-scale models [17], especially large-language models [18]. For example, in the case of language models, examining the internal processes of the BERT Transformer-based architecture has shown that it may recapitulate common natural language processing (NLP) pipelines [19–21]. Chang and Bergen [22] found that the frequency and n-gram structure of word tokens significantly alters the training for language Transformer models learning these words. Inspired by this prior work, this paper outlines a technique for examining the learning trajectories of deep learning models, in particular recurrent neural networks (RNNs). There is historical precedent for our approach, too. McClelland, Rogers and others have studied the underlying knowledge of neural networks by tracking them as they learn [10, 23]. Despite this classic work in cognitive science, it is uncommon to see deep learning models that track progress as a way to unpack what is learned (e.g., going beyond simple RMSE curves; but see also [18], for a counterexample). We term this tracking a “learning curve,” as it resembles research on how human learners process incoming information and improve decision-making through iterations under different situations. A unique benefit with simulation is the possibility to examine many dimensions and measurements of the neural network over time. In the next section, we review recent work on interpreting DNNs and related models. We then introduce our approach based on learning curves. To date, various techniques have been proposed to interpret DNNs. For example, many model interpretability techniques provide task-specific and local explanations, such as saliency maps, attention maps, or Layer-wise Relevance Propagation (LRP), which interpret or visualize the localized influence of a region on the output. These ad-hoc approaches can be unstable, as even a minor change in a single pixel or hyperparameter can substantially affect the local relationships between input signals and output data [24]. Additionally, these local explanations fail to offer a global understanding of whether the selected architecture is well-suited to the task or how the DNN models experience learning. On the other hand, feature attribution methods like SHAP [66] and LIME [67], while conceptually easier to interpret and model-agnostic, tend to work more intuitively with interpretable dimensions typically found in structured tabular data. These methods are less effective for unstructured tasks, such as image and audio data, where feature dimensions are harder to define or interpret. While most techniques emphasize the qualitative interpretation of a model, such as what has been learned or captured by the DNN, the absence of quantifiable measurements makes cross-model comparisons in unstructured tasks particularly challenging [4, 5, 25]. We further provided a systematic description of different model explainability methods, along with their pros, cons, and use cases, as well as a comparison to our proposed approach in Table 1. https://doi.org/10.1371/journal.pone.0313772.t001 In a foundational article on deep learning, LeCun and the colleagues [1] characterized DNNs as representation-learning methods utilizing multilayered large neural network-style models. Representations can be viewed as mental objects capturing semantic properties either observable or unobservable [26]. Different from traditional machine learning models, DNNs display remarkable flexibility and efficiency in encoding lower-level input signals, such as pixels, audio frequencies, or word tokens, into multidimensional vectors at a sophisticated level through multilayered nonlinear transformations [27]. These transformations generate multiple levels of representations that learn hierarchies of features at each layer [1]. The continuous numerical vectors (or hidden vectors) learned at each level are commonly referred to as “embeddings” and serve as dense representations of the original input data. Following this construction, numerous studies have demonstrated the correspondence between DNN-generated and real-world distributed representations among words and sentences [11, 28], speeches [29], images [9], objects [30] and scenes [31, 32]. Representational learning in DNNs offers fundamental contributions to cognitive science, as it can inform how cognitive systems process and organize knowledge, facilitating the comparison of learning processes between humans and machines [33–35]. Recently, several studies in cognitive science and neuroscience have highlighted the importance of integrative modeling between computation, human brains and behaviors [12]. Beyond comparisons of static end-point knowledge, scholars have begun exploring the potential correspondence in learning and information processing between DNNs and human cognitive systems, given that the current performance of DNNs can already approximate human performance across various domains [36–38] (see [18] for a review). For instance, the representations (i.e., embeddings) extracted from multilayered DNNs have shown significant accuracy in predicting neural and behavioral responses in humans throughout the hierarchy of learning and processing. This evidence spans multiple neural-behavioral measurements (e.g., fMRI, EEG, ECoG), modalities (e.g., visual, auditory, and language processing), and model architectures (ranging from simple embedding models like GloVe to more complex neural networks such as RNNs, convolutional neural networks (CNNs), and transformer models (for further details, see [9, 11, 12, 23, 39, 40]). Given the extensive alignment observed between DNN embeddings and neural-behavioral activities, and their presumed meaningful representational mapping with human cognitive systems, an analysis of how they emerge in learning would seem important to understand these relationships. Goldstein and colleagues [11] identify the temporal correspondence between layer-by-layer embeddings in GPT-2 and evolving neural activities in language areas. However, this temporality is restricted to layerwise representations (from low-level to high-level representations) rather than how streams of signals have been received and processed by models or brains [38]. Our aim in this paper is to use temporal analysis in a systematic way by separating and tracking the time course of a network’s learning across classification tasks, thereby enhancing the understanding of the emergence of representations in DNNs. The goal of this study is to unpack the “learning curve” of DNNs through a sequence of hidden representations when the model encounters any temporal processing tasks across its training. To do so, we sample the network’s performance by using its embedding vectors to classify groups of items in its training input. This allows us to map out the progression of the network’s discriminations across these groups – how the network’s internal knowledge, in the form of embedding vectors, evolves during training. The model architecture we focus on is RNNs due to their capacity to model sequential data and time-dependent tasks [41], such as text generation, speech recognition and stock market prediction. Although other deep learning architectures, such as CNNs and Transformers, also have the capacity to process time-series data, RNNs are explicitly designed for processing sequential data, as they effectively capture temporal dependencies through their recurrent connections. CNNs excel in tasks such as image recognition by applying the kernel trick, where convolutional filters are used to extract spatial features from local regions of the input. While CNNs can be adapted to handle sequential data using techniques like 1D convolutional layers, they lack the inherent ability to capture temporal ordering in the data, as they process each segment of input independently. Transformers, on the other hand, rely on self-attention mechanisms to weigh the importance of different input tokens in parallel, allowing them to capture dependencies between distant elements (i.e., tokens). This feature makes Transformers better suited for tasks where relationships between tokens are not strictly ordered in time, and their parallel processing nature is less ideal for learning tasks that require explicit temporal progression. In sum, the strict sequential processing nature of RNNs makes them an intuitive architecture for studying cognitive processing that involves dynamic, changing contexts [42]. In this study, we propose a generalizable interpretability approach that maps the global learning curves of RNNs based on changes in classification performance between embeddings and output across timesteps of the input data. This performance reflects the predictive capacity or the amount of signal captured by the embeddings in predicting the final output. In RNN modeling, each timestep generates an embedding (e.g., in language processing, each timestep represents a word). These embeddings progressively incorporate information from the beginning up to timestep t. By examining changes in classification performance between these embeddings and the output, we can capture how the information evolves over time. To provide clarity, we use the term “learning curve” in this study to denote the holistic approach and intention behind mapping the underlying processing and developmental journey of DNNs. The method we propose separates two parts of the learning curve, one based on overall training, and another based on processing within input items during training. First, the “developmental trajectory” signifies the long-term learning process of DNNs, which is simulated by the increasing number of epochs (i.e., complete passes through an entire training dataset). Second, the “(information) processing trajectory” refers to the momentary accumulation of information across all timesteps (within an epoch), extractable from the performance of the RNN layer (see Fig 1 for a conceptual illustration). We detail each of these further below. The solid curve represents an individual information processing trajectory (across timesteps). For example, as an agent receives more signals over time in one session, its prediction of the gesture increases. The bundle of dotted lines represents a developmental trajectory (across epochs). For instance, this agent improves its ability to predict incoming gestures after repeatedly encountering similar patterns. In a typical binary classification task, the starting point at t=0 is expected to be near 0.50 (and gradually increase to 1.00 across timesteps and epochs) for all epochs because no signals have been provided at the first timestep for solving the underlying task. Hence, all information processing curves in this figure have the same starting point to reflect this pattern. In practice, certain classification tasks might contain structural information even at t=0 (e.g., gesture classification contains spatial information, such as the location of keypoints, at the initial timestep, which could help solve the gesture task from the very beginning). The solid curve represents an individual information processing trajectory (across timesteps). For example, as an agent receives more signals over time in one session, its prediction of the gesture increases. The bundle of dotted lines represents a developmental trajectory (across epochs). For instance, this agent improves its ability to predict incoming gestures after repeatedly encountering similar patterns. In a typical binary classification task, the starting point at t=0 is expected to be near 0.50 (and gradually increase to 1.00 across timesteps and epochs) for all epochs because no signals have been provided at the first timestep for solving the underlying task. Hence, all information processing curves in this figure have the same starting point to reflect this pattern. In practice, certain classification tasks might contain structural information even at t=0 (e.g., gesture classification contains spatial information, such as the location of keypoints, at the initial timestep, which could help solve the gesture task from the very beginning). https://doi.org/10.1371/journal.pcbi.1012286.g001 As Fig 1 illustrates, the resulting visualization consists of learning curves with timesteps on the x-axis and performance on the y-axis. Each epoch contains an information processing trajectory, and the set of curves together forms a developmental trajectory. Due to the multidimensional nature of this plot, while it provides rich qualitative information about the entire model learning experience, it can be challenging for researchers to comprehend all the constructed curves at once. Therefore, beyond the visual presentation and qualitative inspection of the multi-dimensional learning curves, this study further defines four measures: start performance (start, the initial capacity of each information processing), max performance (max, the maximum performance of each information processing), time at max (tmax, when the current information processing reaches the maximum performance), and end – start performance (end – start, the overall performance gain in this information processing) – to facilitate quantitative comparisons across tasks and datasets. Our study particularly focuses on these relatively easy-to-comprehend descriptive statistics (as opposed to more complex metrics like regression coefficients) to simplify the understanding of the already multidimensional and granular nature of the constructed learning curves. To illustrate the generalizability of our approach, we chose two distinct classification tasks: (i) sentence classification and (ii) gesture detection. These tasks differ widely in terms of their modalities. We predicted this would lead to variation in the underlying data generation processes and associated cognitive processing for each modality. In particular, gesture and body movements primarily result from the coordinated contraction and relaxation of muscles, with signals produced at later timesteps derived from the previous timesteps with relatively high autocorrelation [43]. On the other hand, verbal language, being a predominantly semantic modality, exhibits degrees of surprisal and arbitrariness that enhance the cognitive capacity of language processing [44–46]. Therefore, the expected developmental and information processing trajectories will likely exhibit distinguishable patterns across the two different tasks when the DNN system processes them respectively. As we discuss in detail in later sections, this method has a few benefits. Through tracking the learning trajectory of a neural network, researchers can explicitly identify the appropriateness of a model for a given task as well as examine the properties of underlying input signals. This approach could also serve as a standalone visualization to map the accumulation of the underlying signals processed, which can facilitate research on deep learning modeling and signal processing across various modalities. Finally, mapping the learning curve of DNNs has the potential to assist future computational cognitive and neuroscience research and address whether the learning experiences of models also correspond to (or fail to correspond to) the temporal processing in human cognition in addition to the emphasis on static representations in the current literature. In the following sections, we will provide a step-by-step method for visualizing the learning curves of neural networks, illustrate how to holistically interpret signal processing in them and quantitatively compare these curves across two different datasets. This research proposes a model-interpretability method that can extract the learning curve of sequence-based deep learning networks (e.g., RNNs). Inspired by cognitive science, the method measures the learning trajectory and underlying knowledge extracted by such networks. To illustrate the method, we use two temporal tasks: gesture detection and sentence classification as examples. This study therefore demonstrates that the method could inform a range of deep-learning tasks. Our multi-stage pipeline includes three main steps. First, we trained the RNN-LSTM model to generate a sequence of embeddings for a temporal task. Next, we used KNN classifiers to calculate the performance across all extracted embeddings, which we refer to as learning curves (see (2) in Fig 2). Finally, we defined four metrics to quantify these multi-dimensional embeddings for more systematic comparison and significant tests between tasks, classes, the null hypothesis, and development trajectories. The visualization of this multi-stage procedure can be found in Fig 2. Illustrations of the multi-stage pipeline, from extracting embeddings to constructing learning curves and quantifying the curves using four defined metrics. NB: Gesture figures are adapted from Wikimedia Commons [47]. Illustrations of the multi-stage pipeline, from extracting embeddings to constructing learning curves and quantifying the curves using four defined metrics. NB: Gesture figures are adapted from Wikimedia Commons [47]. https://doi.org/10.1371/journal.pcbi.1012286.g002 This study utilized two datasets: the Identity-free Video Dataset for Micro-Gesture Understanding and Emotion Analysis (iMiGUE) by [48] and the Emotion dataset from Hugging Face by [49] to examine the possibility of mapping learning curves for tasks involving temporality. The two temporal tasks (i.e., gesture detection and sentence classification) are distinct in terms of their modalities, lengths, and the steps required to extract and preprocess the features, thereby enhancing the diversity of data to illustrate the learning curve analysis. We detail the preparation of each dataset separately below. The Emotion dataset [49] consists of 20,000 English Twitter messages with six basic emotions (e.g., anger, fear, joy, love, sadness, and surprise) by adopting Plutchik’s [50] wheel of emotions, Ekman’s [51] six basic emotions, and hashtags in tweets. Tweets were annotated through noisy labels and distant supervision introduced by [52]. To prepare the emotion sentences for recognition by the RNN layer, we first applied the “basic English” tokenizer from torchtext to tokenize each tweet. Then, we used GloVe (Global Vectors for Word Representation), a word vectorization technique that does not rely on local word context statistics (local-context information), to vectorize each token in a sentence. GloVe was preferred over other token vectorization techniques like word2vec [53] due to its design to capture the universal meaning of each token/word, rather than the word’s meaning within a specific sentence or context. We opted for GloVe embeddings with 300-dimensional semantic features because it strikes a balance between capturing sufficient information and maintaining computational efficiency [35]. In theory, the input sequences of RNN are not required to have the same length. In practice, these sequences are padded with zeros or trimmed to the same length to optimize the computation in PyTorch. Accordingly, all tokenized tweets were padded to a consistent length of 66 tokens, which corresponds to the length of the longest tweet sample. The shape of each Emotion tweet follows a 300 dimensions × 66 timesteps (see Fig 3). The iMiGUE is a high-quality dataset that contains 18,499 identity-free, ethnically-diverse, gender-balanced samples of 32 psychologically-meaningful micro-gestures (such as scratching an arm, adjusting the hair, or touching an ear [51]). These gestures were all curated from interview clips with athletes at post-match press conferences. Unlike other gesture and emotion datasets, which are typically drawn from staged performances or movie clips, the iMiGUE provides samples of actual gestures from real-life situations. This poses a more realistic, though more challenging, recognition task for deep learning networks [54, 55]. Due to copyright restrictions, the dataset includes only skeleton keypoints (rather than original interview clips) extracted from OpenPose, a multi-person computer-vision system that can simultaneously extract keypoints of the body, hands, face, and feet [56]1. In total, 25 body, 70 facial, and 21 left hand and 21 right hand keypoints were extracted for each frame using OpenPose. Keypoint data were stored in the following format: [x0,y0,c0,x1,y1,c1… ], in which (x, y) represents the coordinate of each keypoint and c indicates the confidence score of each keypoint-coordinate prediction. The confidence score was excluded in the following processing steps. Although the gesture clips have just 39 frames on average, they have a higher standard deviation at 84 frames with a 75th percentile of 72 frames. We therefore set the padded length of the input sequence to 150 frames to ensure that our input data contained sufficient information for training a classifier. Thus, the input size for each gesture clip was 274 units (137 keypoints × 2) × 150 timesteps (see Fig 3). NB: Gesture figures are adapted from Wikimedia Commons [47]. NB: Gesture figures are adapted from Wikimedia Commons [47]. https://doi.org/10.1371/journal.pcbi.1012286.g003 It is important to note that our aim here is not to approach benchmark performance, but rather to examine successful learning. The learning curve analysis will show how that successful learning emerges, and which stimulus discriminations seem to underlie that emergence. Since even the state-of-the-art neural network can achieve only 55% accuracy on this multi-classification task [48], we further grouped the 32 micro-gestures into six categories (body, head, hand, body-head, head-hand movements and an absence of gestures) to ensure that our RNN-LSTM was indeed “learning” when we attempted to map its learning curve. We selected a model architecture (see Fig 4) that is standardized for sequential stimulus processing in the following way. First, we had a batch normalization layer, a primary RNN-LSTM layer, and then a fully connected layer to connect the final dense embeddings with the output classes (for classification). We offer details below. NB: Gesture figures are adapted from Wikimedia Commons [47]. NB: Gesture figures are adapted from Wikimedia Commons [47]. https://doi.org/10.1371/journal.pcbi.1012286.g004 RNN is capable of modeling sequential data and time-dependent tasks [41]. Its architecture represents an iterative function that takes an input sequence (x) and an internal state (h) from the previous timestep (t - 1) to predict the current timestep (t), then updates the state as follows: (1) As the formula illustrates, each timestep t should theoretically reflect the information from 0 to t – 1. We selected the RNN model for gesture detection because it can process temporal information under the assumption that the body movement in each timestep depends on signals in the previous timesteps. While an RNN could leverage the context between elements by maintaining its internal state while processing the entire sequence, the “Vanilla RNN” layer experienced the vanishing-gradient problem during model training [58]. Therefore, long short-term memory (LSTM), which is represented by the function f in Equation 1, was introduced here as an additional state variable, called the cell state, for controlling specific information that needed to be kept or updated while processing the entire sequence [57]. LSTM effectively reduced the vanishing-gradient problem encountered by RNN [58]. The construction of our RNN-LSTM neural network follows common practice in deep learning. First, to enhance training stability and speed up convergence, we applied batch normalization, specifically the BatchNorm1d layer, to the input data at each timestep, with dimensions batch_size × timestep × input_dimension (1 × 150 × 274 for iMiGUE and 1 × 66 × 300 for Emotion). This means that each timestep is treated independently, and normalization is applied across all data points in the batch for each timestep. Specifically, this technique normalizes the inputs to have a mean of 0 and a variance of 1 within each timestep, standardizing the distribution of inputs across the data points in each batch during training. Then, an RNN-LSTM was applied to the normalized input to convert temporal information to a dense embedding at each timestep. An RNN-LSTM with an equal hidden dimension was selected to simplify the tracking of embeddings in the later stage. Finally, a fully connected layer, without any activation, was applied to the embeddings in the last timestep t to predict output labels. To ensure the generalizability of our learning curve mapping approach, we performed 15 rounds of simulation for both gesture and emotion detection tasks, each of which included 15 (6C2) pairwise binary classifications. We conducted 25 repetitions (reps) for each set of simulations (sims) to mitigate the idiosyncrasies of specific processing and developmental patterns we are extracting under each pairwise condition. In total, we collected 25 × 15 × 2 ( reps × sims × tasks) = 750 runs of simulation data. Having an adequate number of simulation runs also enables us to observe clustering tendencies in the convergence and divergence of trajectory patterns across various tasks and different classes. In each simulation, 20% of the shuffled samples were used as the test data, while the remaining data were further split into 80% training data and 20% validation data. While validation data were used for reporting each epoch’s model performance, test data were used for reporting the final model performance on unseen data. The number of epochs indicates the number of times an entire dataset has been passed forward and backward through the neural network. We set the number of epochs to 50 for both tasks, given that most of the variation in learning tends to unfold during the early stages of development. In S3 Text and S4 Text we also illustrate how using 20 and 100 epochs leads to similar observed patterns. Because deep learning algorithms are very sensitive to unbalanced datasets, we applied data augmentation to the training and validation datasets. Specifically, the minority class was up-sampled to match the number of data points in the majority class, which ensured a balanced dataset [59]. Default initiation from PyTorch was used to standardize the model specification across simulations. All simulation runs were trained using the Adam optimizer with a learning rate of 0.001, and the loss function used was the cross-entropy loss for all pairwise classifications. All the other learning rate hyperparameters were kept at their default values. Since deep learning is, computationally speaking, very expensive to train, the model was run on Nvidia RTX 4090 to expedite the processing. We set the batch size to 64 for gesture recognition and 256 for emotion classification to achieve an optimal balance between training speed and performance.2 To map a learning curve for each simulation, we first extracted embeddings of all LSTM timesteps from the hidden layer. We then applied multiple interpretable machine-learning models between these embeddings and the corresponding output labels to understand how the model’s confidence is updated during the LSTM timesteps. We extracted the LSTM array of all timesteps for all batches across all epochs on the test data to ensure that we were extracting embeddings from all developmental stages and the final model was adequately trained on the targeted task. Since we specified the same dimension of the input data for the LSTM hidden layer, for the gesture detection task, we obtained 150 LSTM arrays from 150 timesteps, each array having the size of 1 × hidden_dimension (1 × 274). Similarly, we obtained 66 LSTM arrays in sentence classification, each with a size of 1 × 300. We then stored the corresponding output labels of those arrays as an output array. Those LSTM arrays share one output array since they are embeddings at different timesteps of the same data points. For binary classification, the labels were encoded as 0 and 1. Once the model had been trained and the embeddings extracted, we stored and processed those embeddings in RAM, which has a greater storing capacity than GPU. To approximate the processing curve of a model’s confidence across the LSTM timesteps, this study used a popular machine learning algorithm, k-nearest neighbors (KNN) for identifying embedding “separatables.” This algorithm classifies an object by using a majority vote of its neighbor data points [60, 61]. Because embeddings are seen as a high-dimensional physical (i.e., location-wise) projection of input data (as opposed to a multivariate representation [1]), distance-based models, such as KNN and support vector machine (SVM), are popular choices for examining DNN embeddings in previous studies. Specifically, we applied KNN to the data point (, y), where is the LSTM embedding at t timestep and y is the corresponding output label and calculated the KNN accuracy across all timesteps (information processing trajectory) and all epochs (developmental trajectory), and thus captured the learning curve of the model. These learning curves examine how the LSTM’s embeddings classify the stimulus as it is incrementally presented to the network. Visually, each curve is plotted as the proportion of correct classifications across the item. These constructed learning curves can be analyzed in various ways, either through qualitative interpretation of the visualizations or by applying statistical analysis for quantification and hypothesis testing. In this study, we utilized the four descriptives statistics introduced earlier to assess the multidimensional KKN classification performance of the RNN-LSTM (i.e, how the discriminability of the model is evolving) across its training on two datasets, reconstructing and analyzing the unfolding learning dynamics of these distinct learning tasks. Specifically, for each such curve, we extracted four simple descriptive metrics: start, end - start, max, tmax. In Fig 5, we illustrate these descriptive statistics from an example trajectory of a single classification run for each dataset. From epoch 1 to epoch 50, the LSTM’s embeddings are able to classify successfully over the test item, and we can characterize this success as a change to its performance using four metrics described in the main body of the text (start, end - start, max, tmax). Top: Example item from the sentence task. Bottom: Example item from the gesture task. From epoch 1 to epoch 50, the LSTM’s embeddings are able to classify successfully over the test item, and we can characterize this success as a change to its performance using four metrics described in the main body of the text (start, end - start, max, tmax). Top: Example item from the sentence task. Bottom: Example item from the gesture task. https://doi.org/10.1371/journal.pcbi.1012286.g005 First, we defined the maximum performance (max) and the percentage of time at which that maximum was achieved in the presented item (tmax) over the whole timesteps. We included tmax alongside max because we discovered that max alone is not sufficient to determine performance, especially earlier in training. This is because the network may exhibit unstable performance, dropping across subsequent time slices, which may be indicative of a non-monotonic learning trend seen in related domains [62]. This also suggests the network has learned something about the initial segments of an item, but the later segments wash out its performance as it has not yet encoded these later features. To capture this trend, we use tmax to assess when that maximum was achieved within each information processing trajectory – specifically, the percentage of the time slice at which the observed maximum occurred. In Fig 5 above, we show an illustration of this in the top left. The network achieved a performance of 0.82 on this particular item, but failed to sustain this performance as it dropped to near 0.50 as the sequence unfolds (at epoch 1). In the top right, the performance improves approximately monotonically across time slices (by epoch 50), and tmax is achieved near the final bin of the training item. Additionally, we assess the performance at the start and end of the presented stimulus (start, end). Performance at the start may indicate the relative gains that can be expected from a stimulus item. As shown in the bottom panels of Fig 5, the gesture input already has performance above chance ( ~ 0.80) after the very first segment of the stimulus item. This suggests the model rapidly exploits spatial information in gesture (i.e., the location of keypoint coordinates can already vaguely differentiate gesture classes before sequential movement information is fed into the model). A model that achieves near-perfect performance at start and sustains it does not need to be exposed to the subsequent stimulus. The last measure we use is the subtraction of end - start performance. A high value on this measure suggests the network gets substantial information gains across a test item. For example, even at epoch 50 in the bottom right, the end - start of the gesture item is substantially lower than the simulation trained on the sentence task. These measures can be plotted across epochs. Each learning curve now indicates how an item is being processed across an LSTM’s overall training. The measures are relevant to two timescales in the network’s behavior we conceptualized earlier: developmental (or learning) and information-processing timescales. For example, across epochs, movement along the start measure represents the initial performance at the item’s first time slice at the beginning of each information processing session. The end - start measure within an epoch can describe the relative information gains during each processing session from the item’s full presentation. The max represents the best performance achievable in a session, indicating the network’s current information processing capacity. Finally, if tmax is low, it suggests that the network’s maximum performance of the current session is hindered by subsequent timesteps, implying that additional training may be necessary. We took the output from the KNN classification in Python, described above, and designed a sequence of R scripts to measure, visualize and quantify the trends in these four measures (start, end - start, max, tmax). R’s suite of visualization tools provided a convenient arena within which to view trends across epochs, and in these analysis scripts we also built linear models to statistically test the significance of these trends. All of the scripts in our methods are available at GitHub here: https://github.com/JoyceJiang73/Learning-Curves/. The R scripts only require input of simulation CSV data that contain as fields: binary classification labels, time slice, epoch, and performance measure (e.g., KNN classification performance). We have reported the trained RNN model performance for the average of all pairwise simulations of sentence and gesture classification (epochs = 50) in S1 Text, including metrics such as validation accuracy, validation loss, test accuracy, test loss, recall, precision, and F1. These results confirm that almost all pairwise classifications have been sufficiently trained, indicating that the learning curves are capturing the learned experiences. For each of the four metrics, we conducted a series of significance tests using regression models to assess distinctness across data sources, reshuffled null data, classes, and epochs. Specifically: (1) A linear model was used to assess whether the learning curves of the two datasets differed significantly, (2) A linear model evaluated whether the learning curve for each training classification task significantly deviated from a superimposed (randomly reshuffled) null hypothesis across the four metrics, (3) Linear models were used to assess whether the learning curves for each dataset were distinct by class and across epochs, and (4) An ANOVA test was performed to determine whether class differences contributed more to developmental progression than epoch alone. All the significance tests (Adjusted R-squared) are reported in Table 2, and the corresponding visualizations for each metric are provided in their respective sections. https://doi.org/10.1371/journal.pone.0313772.t002 In Fig 6, we show the LSTM performance at the start of an item across epochs of training. In general, the sentence dataset shows low, near-chance performance, while gesture classifications are already well above chance performance. This chance-level performance for sentence items stays consistent across the entire training period, though the gestural dataset shows some improvement. For the gesture dataset, this suggests that the network has some information about a classification before much of a training item is even shown to the network. It would indicate that gestural data has spatial information in the point coordinates of the body and is exploited by the network at the very first time bin. With language, it takes time for word embedding vectors to be integrated in the network. To confirm these trends, we tested a linear model that predicted start performance by training data, showing that dataset accounted for about 98.14% (p < .0001) of the variance seen in Fig 6. The classification of the gesture data accounts for 95.51% (p < .0001) of variance internally to that dataset, whereas for sentence classifications accounts only for 10.28% (lower but also significant, p < .0001). The learning curves reveal that the first word of a language task has low diagnostic accuracy for a classification, but the spatial variance over gestural classifications is much more informative. The start performance for sentence null is the only metric that shows no significant difference between learning from ordered sentence classes and reshuffled data (among all significance tests; = 0.0001, p < .0879). This is expected, as all sentence binary classifications begin with a consistent initial performance of 0.50 (random guessing for binary classification), which remains unchanged even with randomly reshuffled sentence data. In contrast, the start performance significantly differs for ordered gesture classes versus reshuffled data because the various gesture pairs contain different levels of spatial information ( =0.0824, p < .0001), affecting their initial performance. Start metric over epochs, indicating that in the sentence task, the classification at the first time slice remains stable near change, whereas the LSTM’s embeddings for the gesture task are distinct across stimulus types, and also show slightly more improvement over training (while already being well above change relative to the sentence task). Start metric over epochs, indicating that in the sentence task, the classification at the first time slice remains stable near change, whereas the LSTM’s embeddings for the gesture task are distinct across stimulus types, and also show slightly more improvement over training (while already being well above change relative to the sentence task). https://doi.org/10.1371/journal.pcbi.1012286.g006 Curiously, if one only investigated maximum performance across training, these classification tasks could be regarded as relatively similar in their behavior. As shown in Fig 7, both sentence and gesture datasets yield a classification performance that is high, between 0.75 and 1.0 depending on the classification. In the gesture dataset, there are more “difficult” classifications, shown by outliers in max performance across training. This can be helpful in diagnosing representational challenges in the network’s training, marking what pairs of training items may be more difficult to distinguish than others. Again, as with the start measure, the max performance shows greater variance associated with classifications in the gesture case ( =0.8250, p < .0001) than the sentence case ( =0.2736, p < .0001). The difference between these two datasets is not as pronounced as in the start measure, as a linear model shows that only 2.11% (p < .0001) of the variance is associated with the dataset in a linear model predicting max performance observed within a trial. Despite the small value, it is nevertheless significant and driven by the relatively higher performance in the sentence task. The max performance also differs for both sentence ( =0.0042, p < .0001) and gesture tasks ( =0.0722, p < .0001) when the network learns from ordered datasets compared to randomly shuffled ones. The sentence conditions appear to pick up more spurious signals, ultimately approaching 1.00 as the epochs increase, while the gesture condition remains consistently unlearned across epochs. The learning pattern for tmax across epochs (see next section) provides additional insights into these trends. Both sentence and gesture tasks show an increase in max performance over training epochs. On average this asymptotes near perfect performance but can vary depending on binary classification. Both sentence and gesture tasks show an increase in max performance over training epochs. On average this asymptotes near perfect performance but can vary depending on binary classification. https://doi.org/10.1371/journal.pcbi.1012286.g007 We would expect that completed training in the LSTM should show that maximum performance should appear near the end of an item, as this would suggest that the network has extracted useful information in the whole presentation. Indeed, this is indicated by lower time-at-max values earlier in the training. During the first few epochs, the maximum value occurs proportionally earlier in a training item, similar to the example shown above in Fig 5. However in both sentence and gesture datasets, networks slowly extract features across the stimulus items for classification, as shown in Fig 8. The time gradually rises for all classifications, though it can also vary widely and tends to be more irregular in gesture. In a linear model predicting tmax from data source, sentence and gesture are only slightly different, with 0.34% (p < .0001) of the variance accounted for. Classifications in sentence and gesture both relate significantly in a linear model predicting time at max, with sentence classifications accounting for 9.51% (p < .0001) of the variance and gesture classifications 24.95% (p < .0001). Interpreting tmax alongside max, we confirm that the learning experience of a language task is more reliant on spurious signals for reshuffled data compared to ordered data, as max continuously increases to reach 1.00 while tmax remains sporadic. The gesture condition remains consistently unlearned across epochs for reshuffled data, as the max performance can occur at any point along the processing trajectory (any timestep). Both tmax values indicate that no meaningful delayed signals were developed when the network was trained on superimposed reshuffled data. The time at max differs significantly for reshuffled versus ordered data in both sentence ( =0.0120, p < .0001) and gesture ( =0.0058, p < .0001) conditions. Both sentence and gesture tasks show an increase in the proportion of time at which maximum performance is observed. Indicative of information gain, this rises in both – suggesting the LSTM comes to better integrate the full test items for its performance. However this is much more orderly in the sentence task than the gesture task. Both sentence and gesture tasks show an increase in the proportion of time at which maximum performance is observed. Indicative of information gain, this rises in both – suggesting the LSTM comes to better integrate the full test items for its performance. However this is much more orderly in the sentence task than the gesture task. https://doi.org/10.1371/journal.pcbi.1012286.g008 Finally, we wish to get a sense of how the LSTM models are extracting information within an information processing session. One way to do this is to discern how much higher its performance is at the end of a session compared to its performance initially. This is where we see the greatest difference between the datasets. As shown as Fig 9, sentence shows substantial information gain across stimulus items. By the end of training, the start and end of trials involves considerable difference, and the positive value of this difference shows that the network improves its performance significantly across presentations. With gesture, the situation is quite distinct. In most classifications, there is indeed a rise. But because gesture is informative even at the first time bin of an item, there isn’t much information gain remaining. Indeed even the highest instances of this value hover near 0.10. Therefore, while performance on gesture is higher at first, the network may find the task more difficult in integrating that spatial information over time to improve performance. As in the start measure, the difference between datasets is quite large. When data source is used to predict the end - start measure, 39.23% (p < .0001) of the observed variance in this measure can be associated with data source, with sentence associated with much higher information gain than gesture results. When looking at each data source separately, only 13.01% (p < .0001) of the measure is associated with the classification pairs in sentence. In the gesture dataset, this association is 38.08% (p < .0001), showing much higher contribution of the class differences in gesture. Both sentence’s (R2 =0.0027, p < .0001) and gesture’s (R2 =0.0340, p < .0001) end-start values differ significantly between ordered and reshuffled data. Consistent with max performance gradually reaching 1.00, the information gain of the sentence network increases incrementally from 0.00 to 0.50 when trained on reshuffled data. Combined with insights from tmax, we suggest that although learning may appear to occur even with reshuffled data, it is likely driven by spurious signals picked up during the processing session. Additionally, the end-start performance for gesture gradually improved from highly negative to slightly negative but was not able to reach 0.00 during the early epochs (e.g. epochs smaller than 50), which explains the flat max performance for the reshuffled gesture condition. The information gain from start to end of stimulus item shows a stark difference between datasets. The sentence task shows that the LSTM’s performance rises as it integrates data from start to end. With gestures, the story is more complicated, showing less improvement overall, and in one classification task, it is almost entirely unlearned. The information gain from start to end of stimulus item shows a stark difference between datasets. The sentence task shows that the LSTM’s performance rises as it integrates data from start to end. With gestures, the story is more complicated, showing less improvement overall, and in one classification task, it is almost entirely unlearned. https://doi.org/10.1371/journal.pcbi.1012286.g009 Recognizing the importance and challenges involved in systematically unpacking the internal representations of DNNs, this study introduced a multi-dimensional quantification and visualization approach, “learning curves,” which can capture two temporal dimensions of a model learning experience. First, it captures the “information processing trajectory,” how the network is doing as it processes test items. Second, it captures the “developmental trajectory,” describing how this processing is changing over training epochs. The former represents the influence of incoming signals on an agent’s decision-making, which is operationalized by the timestep within a single epoch of an RNN. The latter conceptualizes the gradual improvement in an agent’s decision-making abilities throughout its lifespan, operationalized by the iteration of epochs. The learning curve approach we illustrate in our two datasets shows that we can quantify and qualitatively investigate both of these dynamics within the same analysis, utilizing four descriptive metrics: start, end - start, max, tmax. Based on a series of significant tests (see Table 2), we first demonstrated that there is a data source difference between sentence and gesture classification in the overall learning patterns across all four metrics (all p’s < .0001). Additionally, the learning experiences of both datasets’ ordered classifications are distinct from their respective reshuffled null baselines, except for the start of the sentence classification. This is because both ordered (aggregated across all classes) and unordered binary sentence classifications will have an initial classification performance of 0.50 due to random guessing. This is corroborated by the significant test of the start metric for sentence class and gesture class: sentence pairwise classifications show very minor differences, with R-squared 0.10 (p < .0001), whereas gesture classes show much greater divergence, with R-squared above 0.90 (p < .0001), due to the initial spatial information contained in gesture coordinates, even at timestep 0, as mentioned in the previous results section. The significant difference between the reshuffled null baseline for ordered vs. unordered gesture classification is also due to different gesture class pairs having different prior spatial information, deviating from the 0.50 random guess scenario observed in sentence classification. The modality distinction is further evident in the end - start metric (where sentence classification shows a much larger information gain than gesture classification, with gesture’s information gain is more gradual). Finally, the ANOVA test capturing the R-squared difference between epoch × class versus epoch-only indicates significant distinctions between pairwise classes and not being distinguished by class (all p’s < .0001). This reveals that within the same multiclassification task, certain classes can be more difficult or easier to separate. Taken together, based on the analysis of these four measures derived from the learning curves across two distinct datasets, we highlight three insights gained from mapping these curves: non-monotonicity, pairwise comparisons, and domain distinctions. First, we observed a non-monotonic trend in the learning experiences of DNNs. In other words, learning does not always show an increasing monotonic improvement because networks sometimes reveal temporary decrements in their performance across training. This is a characteristic recognized in previous literature as a key advantage for their advancement in addressing the most challenging tasks. For example, in applying neural network models to language, the presence of non-monotonic learning was taken as evidence that networks “reorganize” their knowledge as they learn. This may mean networks are acquiring more efficient representations of a problem space and the drop in performance is indicative of a transition into that more efficient representation. Classically, in the case of models learning language, they show a decrement in performance when they “discover a rule” in grammars [68] Our results are a testament to the observation that both representational consolidation and “catastrophic forgetting” remain as important issues in DNN learning [69]. Specifically in our results, we found that RNNs exhibit different preferences for early versus late cues when addressing various sequential tasks. For instance, in the sentence task, we noted a more pronounced performance improvement occurring in the later stages of information processing (i.e., model development). In contrast, gesture learning tends to show quicker progress, with more variability across epochs and repetitions of simulations, suggesting that DNNs tend to rely on shortcuts, such as naive cues related to keypoint coordinates, for gesture classification, rather than focusing on high-level movement sequences. This shortcut-based “learning” also is evident in the higher initial performance (start) for gesture classification ( > 0.75). On the other hand, sentence classification begins at around 0.50 (the at-chance probability) and exhibits greater performance gains in later epochs, indicating that the models classify based on high-level semantic sequences. Additionally, although multiclassification tends to exhibit collective model performance, our between-class pairwise comparisons reveal the presence of outliers within multiclassification. For example, “joy/fear”, “surprise/fear”, and “sadness/fear” demonstrate higher information gains across epochs compared to other emotion pairs, suggesting that these classes are further apart from each other. The “body/head” classification appears to experience learning challenges, possibly because these two movements have difficulty being completely separated, as the head’s movement in naturalistic data may inevitably coincide with that of the body due to joint coordination. The combination of learning curve mapping and instance measures thus serves as an effective approach for “auditing” representations in multiclassification problems. The proposed pipeline improves model explainability beyond holistic evaluation of classification performance and ad-hoc attention visualization by unpacking pairwise class learning patterns to reveal any pairs that are unsuccessful in being discriminated or involve delayed knowledge gain. This granular examination allows modelers to better investigate a model’s appropriateness for the underlying task, as well as the properties of the processed input signals, reaffirming the value of our learning curve conceptualization. With these findings, it is tempting to infer that there are domain distinctions among different modality classification tasks. Gesture learning, for instance, may rely on autocorrelated signals (as body movements result from the coordinated contraction and relaxation of muscles), potentially emphasizing spatial semantics as early cues 3, while language learning relies on higher degrees of surprisal, irregularities and arbitrariness, as suggested by previous literature (see [43–46]). These domain distinctions are valuable to cognitive science in understanding how humans process and distinguish between various modes of communication, shedding light on the neural mechanisms underlying the flexibility and adaptability of the human mind when processing different forms of information and communication modalities. Though it is intuitive and tempting to explain these domain distinctions here, we cannot yet assert that these trends would hold for all sentence or gesture (keypoint) classification tasks, only the ones we investigate here. However the learning curve results would seem to align with an intuition of how linguistic symbols would be sequentially integrated into a neural network in contrast to the highly auto-correlated spatial information contained in gesture performance. Still, we cannot infer a broad generalization about “language vs. gesture” and only leave it as a potential path for future investigation. Indeed, this may be an additional benefit of a method like the one we present here. Learning curves could finely ascertain these domain distinctions, and expanding the set of data to test may permit generalization in future work. This may have theoretical implications itself. The learning curve analysis may provide information about the distinct sources of information from varied modalities. When neural models (or human brains, presumably) integrate distinctive sources of information, they may strengthen understanding of complex multimodal data by leveraging their unique information-processing and developmental benefits. This study provides conceptual and operational illustrations of applying learning curve methods to RNNs, though both the illustration and the proposed method come with certain considerations. First, we selected two distinct datasets to demonstrate how learning curves offer insights into different data and modality learning experiences. While illustrating significant domain distinctions, our study is limited in making broad generalizations between “language vs. behavioral.” Future cognitive and behavioral research interested in such conceptual generalization can apply our proposed method to various distinct language tasks (e.g., language translation) and behavioral datasets (e.g., facial expression detection) to achieve broader generalizations across different modalities. Additionally, as highlighted at the beginning, our method could have implications for multimodal DNN modeling, given that our simulations utilize datasets with distinct modalities. For the current scope, we focus on how our proposed method facilitates the understanding of distinguishable modalities as an initial step, while future work can explore adopting this method for multimodal DNNs and datasets (e.g., audiovisual emotion classification) to assess how multimodal learning differs from unimodal learning. From a methodological standpoint, as detailed in the model explainability technique comparison (Table 1), our proposed method is currently limited to temporal tasks and DNN architectures, specifically RNNs. However, it has the potential to be relatively easily adapted to various tasks, modalities, multiclassification, and different performance measures, beyond the binary classification used in this study. We have provided illustrations of the multiclassification learning curve based on sentence classification in S2 Text. Additionally, because our method generates measures at every timestep and epoch, it may be more computationally expensive compared to other explainability methods. However, this arises from the tradeoff between obtaining a more granular measure across all training steps versus only obtaining snapshot visualizations of a single neural network layer or attribution scores for all input features. As documented in the DNN explainability comparison (Table 1), each technique has its strengths and considerations, and researchers can exercise discretion in choosing the appropriate method. The learning curve method excels in providing a more systematic understanding of the model’s learning process across models and modalities. There is a long tradition in cognitive science and computational neuroscience of examining the internal representations of models [9–12]. One reason for this is to determine if a model’s features or processes reflect processes of the human mind. Such models can be informative for inferring properties of human mental processing, and so have direct theoretical implications. For example, Elman [13] and others [14] showed that RNNs can learn patterns sufficiently complex to resemble human grammar. By examining the internal activations of these recurrent networks, they showed that these systems are driven by graded, statistical features. Words are not discrete “symbols” but scalar vectors conditioned by linguistic context in time [15]. This was taken to challenge theories that see language as a purely abstract and symbolic recursive process [16]. Inspired by prior work, the learning curve method we proposed uses temporal mapping (information processing and developmental trajectories) to help modelers more comprehensively understand the underlying learning and decision-making processes of a complex model architecture without delving into the intricacies of interpreting its internal representations directly [23, 38]. This kind of systematic and quantitative approach has recently gained popularity in both computational cognitive science and deep learning communities [5, 38, 63–65], as it facilitates multidimensional comparisons across models and modalities, which were previously seen as challenging for DNN-like models. The current study illustrates multiple techniques for analyzing model learning experiences and highlights three insights across different communication modalities based on these analyses. Future studies can utilize this learning curve mapping approach to enhance model interpretability studies by evaluating a model’s appropriateness for the task at hand, examining the properties of the underlying input signals, and assessing the model’s alignment (or lack thereof) with human learning experiences, which is also a critical consideration for computational cognitive science and neuroscience research. Sentence and gesture RNN-LSTM model performance. (PDF) Table A. Average Performance metrics for sentence classification across all simulation runs (epochs = 50). (PDF) Table B. Average Performance metrics for gesture classification across all simulation runs (epochs = 50). (PDF) Table C. Average performance metrics for 10 examples of gesture classification across all simulation runs (epochs = 100). https://doi.org/10.1371/journal.pcbi.1012286.s001 (PDF) Illustrations of multiclassification results for sentence classification (epochs = 20). https://doi.org/10.1371/journal.pcbi.1012286.s002 (PDF) Illustrations of sentence and gesture simulations across 20 epochs. https://doi.org/10.1371/journal.pcbi.1012286.s003 (PDF) Illustrations of gesture simulations across 100 epochs. https://doi.org/10.1371/journal.pcbi.1012286.s004 (PDF) We thank the organizers, panelists, and audience for allowing us to present the initial version of our work at the 53rd Annual Meeting of the Society for Computation in Psychology (SCiP) and for sponsoring the registration fee. We are also grateful to the reviewers from the 45th Annual Meeting of the Cognitive Science Society for their comprehensive review and invaluable feedback. Additionally, we thank Hongjing Lu, Jungseock Joo, and Elisa Kreiss for their insightful feedback on the theoretical framework and methodologies.
--------------------------------------------------