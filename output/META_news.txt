List of news related to Meta stock price META:

Title: 10 Ways to Enrich the Trumps and the MAGA Movement
URL: https://www.motherjones.com/politics/2025/05/donald-trump-qatar-747-bezos-amazon-facebook-cbs-crypto-golf/
Time Published: 2025-05-22T16:34:00Z
Full Content:
Russ Choma, Dan Friedman, and Tim Murphy23 hours ago Illustrations by James Clapham Illustrations by James Clapham Eight years ago, if you wanted to line the first family’s pockets, your choices—while ethically unprecedented—were limited. There was only so much you could spend on hotel rooms or Mar-a-Lago fundraisers. Fortunately for foreign governments, megacorporations, and anyone else with an agenda, the Trumps have diversified. “It’s a whole different scale than it was in 2017,” observes Noah Bookbinder, president of Citizens for Responsibility and Ethics in Washington, who says President Donald Trump and his underlings “seem to believe that the American people don’t care. They’re not making much of an effort to hide any of this stuff.” A post shared by Mother Jones (@motherjonesmag) The options for paying tribute to the president, his kin, and the MAGA movement are now legion. There are even exciting new opportunities to protect your business by reaching a “settlement” with the leader of the free world. Or you can just hand over a 747 for him to use as Air Force One, as Qatar did. Best of all, these creative funding methods might not even be against the law—especially for Trump. (Thanks, SCOTUS!) So here’s your guide to participating in the brave new Trumpworld of executive enrichment. As the “Bitcoin orange” Trump-branded watches ($799) indicate, the 47th commander in chief is the “Crypto President.” Last year, Trump rolled out his own crypto firm, called World Liberty Financial. After Trump won, crypto investor Justin Sun bought $75 million worth of WLF tokens, with more than $50 million of that potentially going to the Trumps, according to Bloomberg. Shortly after Trump took office, the Securities and Exchange Commission—which had accused Sun of fraud in a federal complaint—agreed to pause its lawsuit while the parties pursued a “potential resolution.” That was one of more than a dozen lawsuits and investigations targeting crypto firms that the SEC reportedly backed off from after Trump took office. Foreign governments can get in on the action, too. A state-backed United Arab Emirates firm, for instance, recently agreed to use a Trump-affiliated digital coin in a multibillion-dollar deal—a move that, according to the New York Times, could hand the Trump family hundreds of millions of dollars. Savvy investors can also purchase $TRUMP, a meme coin the president launched just before his inauguration. A billion $TRUMP coins will ultimately be made available, though a company affiliated with the Trumps will own many of them—so if demand causes the price to rise, the Trumps theoretically get wealthier. The coin’s value rocketed past $70 before inauguration, then immediately crashed. But in April, it ticked up to more than $15—after the coin’s official website offered dinner with Trump at his DC-area golf club and a “VIP White House Tour” to the top $TRUMP holders. (The White House tour offer quickly disappeared following an uproar.) This week, Sun identified himself as the top $TRUMP investor. He declared his excitement to attend the president’s “Gala Dinner as his TOP fan” and “discuss the future of our industry.” Sun now holds $18.6 million worth of $TRUMP, according to CNBC. If you can’t match that, don’t worry: $MELANIA coins go for less than a buck. This is a simple one—you don’t even have to invest in $TRUMP coins. Business leaders can now simply pay $5 million for one-on-one meetings with Trump at Mar-a-Lago, Wired reported. If that’s too rich for your blood, there’s a cheaper option—donors can pony up $1 million each for a small-group “candlelight dinner” with the president. “This exorbitant level of payment for presidential access raises serious concerns about the possibility of corruption by candlelight,” Jon Golinger, a transparency advocate with Public Citizen, told USA Today. For those who recall the five-alarm scandal generated in the ’90s when the Clinton administration rewarded major campaign donors with overnight stays in the Lincoln Bedroom—well, clearly, that was a more innocent time. Trump’s ethical outrages are so numerous and brazen that these meals have barely registered a blip. The dinners are organized by MAGA Inc., a super-PAC that backed Trump’s 2024 campaign—and that also has spent tens of millions of dollars to cover Trump’s personal legal expenses. A “source with direct knowledge of the dinners,” however, told Wired that “it’s all going to” Trump’s future presidential library. Then again, the $400 million Qatari jet supposedly will, too. Trump has turned his vendettas against law firms that cross him into executive orders that threaten their ability to operate. When firms have fought back, judges have blocked Trump’s “unprecedented attack” on the rule of law. But some of the country’s wealthiest firms have made a different choice—accepting humiliating “settlements” with the president and promising tens of millions of dollars in free legal work for causes he supports. Paul Weiss was the first to capitulate, agreeing to provide $40 million in pro bono work in support of what Trump described as “the Administration’s initiatives.” From there, the price skyrocketed. Skadden inked a preemptive deal for at least $100 million in free legal work. Then five more firms cut deals for $100 million to $125 million apiece. Trump hopes to “use these very prestigious firms to help us out” on projects ranging from his trade war to protecting the coal industry. “Big Law continues to bend the knee to President Trump because they know they were wrong, and he looks forward to putting their pro bono legal concessions toward implementing his America First agenda,” Trump’s press secretary, Karoline Leavitt, has said. For years, Trump has filed lawsuits over news coverage he dislikes. Many of these cases have been thrown out of court. But after the 2024 election, some of the world’s most powerful media companies decided to fork over millions of dollars rather than fight back. It began in December, when Disney agreed to give $15 million to Trump’s presidential library foundation to settle a dubious defamation claim against ABC News. Then Meta, Facebook’s parent company, agreed to pay its own $25 million library tribute to settle a lawsuit Trump brought after the platform banned him following the January 6 insurrection. “It looks like a bribe and a signal to every company that corruption is the name of the game,” Sen. Elizabeth Warren tweeted. X—now controlled by Elon Musk—soon followed, settling a similar case for $10 million. Paramount executives reportedly believe that settling a Trump lawsuit against CBS, which it owns, would ease the path to a lucrative merger with Skydance. Paramount, which so far hasn’t surrendered, called the case an “affront to the First Amendment…without basis in law or fact” in a March court filing. Trump is also leveraging his control of the Federal Communication Commission to demand “the maximum fines and punishment” for CBS. “They should lose their license!” he declared. In a letter to Paramount’s chairperson Monday, Warren and other lawmakers charged that the company appears to be trying to settle the lawsuit and “moderating the content of its programs” in an effort to win FCC approval for the merger. “If Paramount officials make these concessions,” the lawmakers wrote, it might amount to an illegal “quid pro quo.” When the Washington Post spiked an already written Kamala Harris endorsement last year, its owner, Jeff Bezos, wanted to make one thing clear: There was “no quid pro quo.” Yes, he acknowledged, the head of Blue Origin—a Bezos company that competes for billions in government contracts—met with Trump the day the paper killed the editorial. But that was just an unfortunate coincidence. “When it comes to the appearance of conflict, I am not an ideal owner of the Post,” Bezos conceded. “Every day, somewhere, some Amazon executive or Blue Origin executive, or someone from the other philanthropies and companies I own or invest in is meeting with government officials.” Indeed. In December, after Melania Trump had pitched a documentary about herself to Amazon’s film studio, Bezos had dinner with the incoming first couple, according to the Wall Street Journal. Amazon later agreed to pay a whopping $40 million to license the film; 70 percent of that goes directly to Melania. An Amazon spokesperson said the dinner “was separate from any discussions” about the film. “We licensed the upcoming Melania Trump documentary film and series for one reason and one reason only—because we think customers are going to love it,” the spokesperson said. Trump’s son-in-law Jared Kusher advised his father on Middle Eastern matters during the president’s first term, with questionable results. But that work likely helped Kushner win hundreds of millions of dollars from Saudi Arabia, Qatar, and a United Arab Emirates fund for a private equity firm he launched after leaving the White House. Kushner’s company, Affinity Partners, also landed lucrative development deals with the governments of Serbia and Albania. He’s building a luxury Trump- branded property in a former Yugoslav Ministry of Defense building, empty since NATO bombed it in 1999. And in Albania, Kushner plans to construct a luxury resort on an island once used as a military outpost. Those deals were reportedly brokered by Richard Grenell, who forged ties with both governments while serving as Trump’s special envoy in the Balkans. Both countries want US help joining the European Union despite long-standing corruption challenges, among other issues. Grenell is now Trump’s envoy for special missions, a global portfolio that could make him a valuable friend for any enterprising foreign government. When he took office in 2017, Trump refused to relinquish ownership of his business empire, which offered obvious opportunities for those with means to enrich him. That hasn’t changed. He still owns nine hotels, 18 golf courses, four commercial real estate properties, six estates, and a real estate brokerage. In his first term, these properties took in tens of millions in revenue—including a $15.8 million condo sale to a woman linked to Chinese military intelligence and a $13.5 million mansion sold to an Indonesian politician and businessman (who is developing two Trump golf resorts). Trump no longer owns the Washington, DC, hotel that served as a clubhouse for Republican lobbyists and foreign leaders in his first term. But Trump’s other hotels—like one in New York that saw a huge revenue boost in 2018 hosting the entourage of Saudi Crown Prince Mohammed bin Salman—are still going strong. The Trump Org says it plans to open 10 new properties, including a hotel in the UAE’s Dubai and a golf resort in Oman. The White House claims the Trump family’s businesses don’t create conflicts of interest because “the president’s assets are in a trust managed by his children.” But that trust isn’t blind, so the president can still know who’s helping him—and his kids—get richer. Sure, you could buy a club membership and some Trump-branded golf accessories. But if you really want to help out the president, bring him a whole tournament. That’s what Turkish Airlines—in which the Turkish government controls a 49 percent stake—did when it hosted a tournament at Trump’s course outside DC during his first year in office. After the January 6 coup attempt, the PGA distanced itself from Trump. But the Saudi sovereign wealth fund–owned league LIV Golf rushed to embrace Trump, hosting numerous tournaments at his courses around the United States. The latest took place in April at Trump’s National Doral Golf Club in Miami—just as Trump’s tariffs were crashing the stock market. Trump claims the tournaments are “peanuts” to him financially, but industry experts say each one is a multimillion-dollar deal for his businesses. In a deposition in 2023, Trump bragged that he could sell his Turnberry course in Scotland to LIV Golf for “a fortune,” though he declined to say whether such an offer had actually been made. Trump isn’t the first president to accept corporate contributions for his inauguration, but the scale is unprecedented. Meta and Amazon weren’t shy about their huge donations of $1 million each—the public obsequiousness was the point. Pilgrim’s Pride, owned by a subsidiary of Brazilian meat company JBS, gave $5 million. In April, JBS—which has a history of scandals and legal problems—won SEC approval to be listed on the New York Stock Exchange, a longstanding company goal that the agency had previously blocked. A spokesperson for JBS told Forbes that the inaugural donation was not related approval of the stock listing. Syngenta—an agribusiness giant that is owned by a Chinese government–controlled firm and is currently fighting a Federal Trade Commission lawsuit—kicked in $250,000. In February, the SEC dropped a lawsuit against crypto platform Coinbase, which gave $1 million to Trump’s inauguration. Coinbase, Pilgrim’s Pride, and the SEC didn’t respond to questions from Mother Jones; Amazon noted it also gave to Joe Biden’s inauguration, and Syngenta said it works “with both sides of the aisle.” The money helped Trump—who reportedly kept close tabs on who paid up—raise more than $200 million for lavish inaugural events. Any surplus can be transferred to a political committee or his eventual presidential library. Inaugural committees also offer opportunities for presidential enrichment. In 2022, for example, Trump’s previous inaugural committee paid $750,000 to settle a lawsuit accusing it of overpaying Trump properties that hosted events. While serving as a Trump Org executive, the president’s eldest son has also launched a series of anti-woke ventures. He’s a partner at 1789 Capital, which invests in conservative companies, and he’s slated to join the board of online firearms retailer GrabAGun. Don Jr. doesn’t have an official White House job, but he’s touted his influence over administration personnel decisions and MAGA politics. He recently traveled, twice, to Serbia—where Kushner and the Trump Org are developing a hotel—and lent his support to the country’s embattled president. For moguls who aren’t ready to do a deal with Don Jr., there’s now a more direct way to send him and his business partners money—an exclusive, invite-only DC club called “Executive Branch” that reportedly costs more than a half-million dollars to join. The plan, according to Politico, is “to ensure the C-suite crowd can mingle with Trump advisers and cabinet members without the prying eyes of the press and wanna-be insiders.” Arthur Schwartz, an adviser to Don Jr., downplayed his influence in the White House but declined to respond in greater detail when asked whether his activities create ethical problems. “Write your ridiculous story. Literally no one cares,” Schwartz said via text. “We don’t actually give a fuck.” Subscribe to the Mother Jones Daily to have our top stories delivered directly to your inbox. By signing up, you agree to our privacy policy and terms of use, and to receive messages from Mother Jones and our partners. “Lying.” “Disgusting.” “Scum.” “Slime.” “Corrupt.” “Enemy of the people.” Donald Trump has always made clear what he thinks of journalists. And it’s plain now that his administration intends to do everything it can to stop journalists from reporting things it doesn’t like—which is most things that are true. We’ll say it loud and clear: At Mother Jones, no one gets to tell us what to publish or not publish, because no one owns our fiercely independent newsroom. But that also means we need to directly raise the resources it takes to keep our journalism alive. There’s only one way for that to happen, and it’s readers like you stepping up. Please do your part and help us reach our $150,000 membership goal by May 31. “Lying.” “Disgusting.” “Scum.” “Slime.” “Corrupt.” “Enemy of the people.” Donald Trump has always made clear what he thinks of journalists. And it’s plain now that his administration intends to do everything it can to stop journalists from reporting things it doesn’t like—which is most things that are true. We’ll say it loud and clear: At Mother Jones, no one gets to tell us what to publish or not publish, because no one owns our fiercely independent newsroom. But that also means we need to directly raise the resources it takes to keep our journalism alive. There’s only one way for that to happen, and it’s readers like you stepping up. Please do your part and help us reach our $150,000 membership goal by May 31. Abby Vesoulis Julia Lurie Julianne McShane and Henry Carnell Sarah Szilagy Isabela Dias and Noah Lanard Pema Levy Kiera Butler Julianne McShane and Dan Friedman Bob Berwyn Dharna Noor Dan Falk Reveal Subscribe to the Mother Jones Daily to have our top stories delivered directly to your inbox. By signing up, you agree to our privacy policy and terms of use, and to receive messages from Mother Jones and our partners. Save big on a full year of investigations, ideas, and insights. Help Mother Jones' reporters dig deep with a tax-deductible donation. Inexpensive, too! Subscribe today and get a full year of Mother Jones for just $19.95. Award-winning photojournalism. Stunning video. Fearless conversations. Subscribe to the Mother Jones Daily to have our top stories delivered directly to your inbox. By signing up, you agree to our privacy policy and terms of use, and to receive messages from Mother Jones and our partners. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Privacy Manager Copyright © 2025 Mother Jones and the Center for Investigative Reporting. All Rights Reserved.Terms of Service Privacy Policy Can you pitch in a few bucks to help fund Mother Jones' investigative journalism? We're a nonprofit (so it's tax-deductible), and reader support makes up about two-thirds of our budget. We noticed you have an ad blocker on. Can you pitch in a few bucks to help fund Mother Jones' investigative journalism? Sign up for the free Mother Jones Daily newsletter and follow the news that matters.
--------------------------------------------------

Title: Cost-effective solutions for high-throughput enzymatic DNA methylation sequencing
URL: https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1011667
Time Published: 2025-05-22T14:00:00Z
Full Content:
Characterizing DNA methylation patterns is important for addressing key questions in evolutionary biology, development, geroscience, and medical genomics. While costs are decreasing, whole-genome DNA methylation profiling remains prohibitively expensive for most population-scale studies, creating a need for cost-effective, reduced representation approaches (i.e., assays that rely on microarrays, enzyme digests, or sequence capture to target a subset of the genome). Most common whole genome and reduced representation techniques rely on bisulfite conversion, which can damage DNA resulting in DNA loss and sequencing biases. Enzymatic methyl sequencing (EM-seq) was recently proposed to overcome these issues, but thorough benchmarking of EM-seq combined with cost-effective, reduced representation strategies is currently lacking. To address this gap, we optimized the Targeted Methylation Sequencing protocol (TMS)—which profiles ~4 million CpG sites—for miniaturization, flexibility, and multispecies use at a cost of ~USD 80. First, we tested modifications to increase throughput and reduce cost, including increasing multiplexing, decreasing DNA input, and using enzymatic rather than mechanical fragmentation to prepare DNA. Second, we compared our optimized TMS protocol to commonly used techniques, specifically the Infinium MethylationEPIC BeadChip (n = 55 paired samples) and whole genome bisulfite sequencing (n = 6 paired samples). In both cases, we found strong agreement between technologies (R2 = 0.97 and 0.99, respectively). Third, we tested the optimized TMS protocol in three non-human primate species (rhesus macaques, geladas, and capuchins). We captured a high percentage (mean = 77.1%) of targeted CpG sites and produced methylation level estimates that agreed with those generated from reduced representation bisulfite sequencing (R2 = 0.98). Finally, we confirmed that estimates of 1) epigenetic age and 2) tissue-specific DNA methylation patterns are strongly recapitulated using data generated from TMS versus other technologies. Altogether, our optimized TMS protocol will enable cost-effective, population-scale studies of genome-wide DNA methylation levels across human and non-human primate species. DNA methylation profiling is important for understanding key questions in biology, but current techniques can be expensive and have technical limitations. Enzymatic methyl sequencing (EM-seq) was proposed as a potential solution, but thorough testing is still needed. In this study, we optimized a new method (Targeted Methylation Sequencing or TMS) to make it more cost-effective, flexible, and applicable to multiple species. We tested modifications to increase sample multiplexing, reduce DNA input, and use enzymatic fragmentation. We compared our optimized TMS protocol to common methylation profiling techniques and found strong agreement in DNA methylation levels. We also successfully applied the optimized TMS protocol to three non-human primate species. Finally, we show that common analyses of DNA methylation data produce similar results using TMS data versus data from other technologies. Together, we hope this work will enable cost-effective, population-scale DNA methylation profiling across human and non-human species. Citation: Longtin A, Watowich MM, Sadoughi B, Petersen RM, Brosnan SF, Buetow K, et al. (2025) Cost-effective solutions for high-throughput enzymatic DNA methylation sequencing. PLoS Genet 21(5): e1011667. https://doi.org/10.1371/journal.pgen.1011667 Editor: Duncan Sproul, The University of Edinburgh MRC Human Genetics Unit, UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND Received: September 13, 2024; Accepted: March 27, 2025; Published: May 22, 2025 Copyright: © 2025 Longtin et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability: All NHP data generated as part of this study has been deposited in NCBI’s Sequence Read Archive under accession number PRJNA1156067. The human genomic data generated as part of this study comes from the Tsimane Health and Life History Project (THLHP) and the Orang Asli Health and Lifeways Project (OA HeLP). Both THLHP and OA HeLP’s highest priority is the minimization of risk to study participants. Both projects adhere to the “CARE Principles for Indigenous Data Governance” (Collective Benefit, Authority to Control, Responsibility, and Ethics) and are also committed to the “FAIR Guiding Principles for scientific data management and stewardship” (Findable, Accessible, Interoperable, Reusable). To adhere to these principles while minimizing risks, genomic data from both projects are available via restricted access. These requests can be made via email following the instructions provided here: https://tsimane.anth.ucsb.edu/data.html and here: https://www.orangaslihealth.org/data.html. In both cases, requests for de-identified genomic data will take the form of an application that details the exact uses of the data and the research questions to be addressed, procedures that will be employed for data security and privacy, potential benefits to the study communities, and procedures for assessing and minimizing stigmatizing interpretations of the research results. Both projects are committed to open science and the leadership is available to assist interested investigators in preparing data access requests. All scripts used to perform the analyses described here are available at “https://github.com/alongtin15/TMS-Cost-effective-solutions-for-high-throughput-enzymatic-DNA-methylation-sequencing.” Funding: This work was supported by the National Institute of General Medical Sciences (R35-GM147267 to AJL), National Institute on Aging (R01AG054442 to HK, MDG, and BCT, R61AG078529 to ADM, R01AG060931 and R00AG051764 to NSM, R56AG071023 and R01AG084706 to JPH, DP5OD029586, R01AG088657 and R01AG083736 to AGB), National Institute of Mental Health (R01MH118203 and R01MH096875 to MLP), the National Cancer Institute (P30CA068485 to AGB), the National Science Foundation (BCS-2142090 to AJL, BCS-2010309, BCS-1848900, BCS-2013888 and BCS-1723237 to NSM), the Canadian Institute for Advanced Research (Azrieli Global Scholars Program to AJL), the French National Research Agency under the Investments for the Future (Investissements d’Avenir) programme (ANR-17-EURE-0010 to JS), the Kinship Foundation, (Searle Scholars Program to AJL), the Pew Charitable Trusts (Pew Biomedical Scholars Program to AJL and Pew-Stewart Scholar for Cancer Research Program to AGB), the Burroughs Wellcome Fund (Career Award for Medical Scientists to AGB), the Hevolution Foundation (Hevolution/AFAR New Investigator Award in Aging Biology and Geroscience Research to AGB), the Natural Science and Engineering Research council of Canada (RGPIN-2017-03782 to ADM), and the Canada Research Chairs program (950-231257 to ADM). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. The views expressed are those of the authors and do not necessarily reflect the views of the funders. Competing interests: The authors have declared that no competing interests exist. Understanding variation in DNA methylation levels across tissues, the lifespan, disease states, and populations is important for addressing key questions in biology. DNA methylation—the covalent addition of methyl groups to cytosines—is a semi-malleable and environmentally-responsive epigenetic modification involved in gene regulation in many species, including our own [1]. Because DNA methylation moderates gene expression throughout the life course, it is critical for processes such as development [2–4], cell programming [5], tissue specificity [6], aging [7–11], and disease progression [12–14]. For example, changes in DNA methylation are considered a “hallmark” of the aging process, with most studies reporting age-associated gains in methylation in hypomethylated regions (e.g., promoters and transcribed regions) and age-associated losses in methylation in hypermethylated regions (e.g., heterochromatic regions, Polycomb-repressed regions) [15–17]. These age-related patterns are so consistent that DNA methylation variation have been used to construct molecular clocks that reliably predict chronological age [18,19]. Further, because DNA methylation is known to respond to environmental inputs, it has been implicated as a mechanism through which diverse environmental exposures can impact long-term physiology and health (e.g., famine [20–24], psychosocial stress [25–29], or infection [30–33]). To profile genome-wide DNA methylation at scale, most studies rely on reduced representation methods: human studies have largely favored microarrays, while non-human studies have favored reduced representation bisulfite sequencing (RRBS) due to the lack of species-specific microarrays (though recent work has led to the development of the Infinium Mouse DNA Methylation BeadChip as well as the Mammalian Methylation Array) [34–36]. Both RRBS and microarrays quantify DNA methylation at a subset (1–5%) of CpGs in the genome, and thus provide a cost-effective strategy relative to genome-wide assays (e.g., whole genome bisulfite sequencing (WGBS)). For example, the Infinium MethylationEPIC v2.0 BeadChip, or EPIC array, covers ~ 930K CpG sites including functional elements identified by the ENCODE project [37], DNase hypersensitive sites, and putatively important sites for human disease and development [38,39]. In contrast, RRBS fragments DNA using the Msp1 enzyme that cuts DNA at CCGG motifs, which following size selection, enriches for 1–5% of the genome with high CpG content such as CpG islands and gene bodies [34,40]. Importantly, both microarrays and RRBS rely on sodium bisulfite, which converts unmethylated cytosines to thymine while leaving methylated cytosines protected from conversion. This chemical reaction requires high pHs and temperatures, which can cause unwanted DNA fragmentation and damage, especially to unmethylated cytosines [41]. Ultimately, such damage can create difficulties during library preparation as well as biases in the downstream data [41–43]. Enzymatic methyl sequencing (EM-seq) offers a useful alternative to bisulfite sequencing with several key benefits: EM-seq relies on enzymatic rather than chemical conversion of unmethylated cytosines to thymine, resulting in substantially less DNA damage [42]. As a result, whole genome EM-seq has been shown to recover more CpGs sites, have lower duplication rates, have better between-replicate correlations, and require less DNA input than WGBS [42]. However, existing EM-seq benchmarked protocols rely on whole genome rather than reduced representation strategies, hindering their adoption especially for population-scale studies. To address this gap, Twist Biosciences recently created a hybrid capture panel that targets ~4 million CpG sites in the human genome and is compatible with EM-seq. The Twist methylation capture reaction uses ~ 550k probes to target functionally relevant CpG sites (e.g., those in enhancers, gene bodies, and near transcription start sites) and to cover ~95% of CpG sites included on the widely used EPIC array [44–47]. Off the shelf, this protocol is similar or lower in cost to existing reduced representation approaches. However, we note that total cost for any sequencing-based approach will depend on the desired coverage (i.e., the average number of reads that cover each CpG site); best practices for average per CpG coverage are still debated, but most studies recommend at least 20x [48–51]. Increased coverage will increase the precision of DNAm estimates, and thus to some degree the desired coverage depends on the anticipated effect size. Here, we aimed to develop and benchmark an optimized and further cost-reduced version of the targeted methylation sequencing (TMS) approach suitable for population-scale studies, including both human and non-human primate (NHP) studies (Fig 1A). To do so, we built upon the off the shelf TMS protocol (Fig 1B), which recommends 8 plexing of samples per capture reaction and 200 ng of DNA input, and tested four multiplexing strategies (12, 24, 48, and 96 plex, using 200 ng of sample input; Fig 1C). We also tested five DNA input amounts (25, 50, 100, 200, and 400ng, using the 12-plex strategy) and other minor protocol modifications such as varying the annealing temperature during hybrid capture and varying the method used for DNA fragmentation (Fig 1C). Following optimization, we assessed: 1) the robustness of our protocol through a direct comparison with the EPIC array and WGBS; 2) the extension of optimized TMS for use in NHP species; and 3) the ability of our protocol to recapitulate biological results (epigenetic age estimates and identification of tissue-specific patterns) obtained from data generated using other technologies (see Table 1 for sample sizes and sample information; Fig 1C). Overall, we found that we were able to miniaturize and optimize the TMS protocol to ~USD 80 per sample, while maintaining data quality and comparability to existing methods. In total, our protocol provides coverage of approximately four times as many CpG sites relative to the EPIC array at one fourth the cost—a ~ 16-fold gain in the data-to-price ratio (S1 Table). [A] To optimize the TMS protocol, we used samples from three human and three NHP populations: the Tsimane of Bolivia, a Vanderbilt University Medical Center cohort, the Orang Asli of Malaysia, rhesus macaques from Cayo Santiago in Puerto Rico, tufted capuchins from captive sites throughout the United States, and gelada monkeys from Ethiopia. Created using BioRender. [B] The TMS protocol begins with DNA fragmentation and adapter ligation. Next, two enzymes, TET2 and APOBEC, are used to oxidize and deaminate the DNA. TET2 recognizes methyl groups attached to cytosines and converts them to Ca/g. APOBEC follows TET2 and converts the unmethylated cytosines to uracils. Following PCR amplification (which converts uracils to thymines), hybrid capture is used to enrich for targeted regions of the genome. Samples are then assayed via high throughput sequencing. Created using Microsoft Powerpoint. [C] Overview of experiments and analyses. The samples used for each set of experiments are noted by a population-specific icon. Icons from Biorender, OpenClipArt, and Microsoft Powerpoint. [A] To optimize the TMS protocol, we used samples from three human and three NHP populations: the Tsimane of Bolivia, a Vanderbilt University Medical Center cohort, the Orang Asli of Malaysia, rhesus macaques from Cayo Santiago in Puerto Rico, tufted capuchins from captive sites throughout the United States, and gelada monkeys from Ethiopia. Created using BioRender. [B] The TMS protocol begins with DNA fragmentation and adapter ligation. Next, two enzymes, TET2 and APOBEC, are used to oxidize and deaminate the DNA. TET2 recognizes methyl groups attached to cytosines and converts them to Ca/g. APOBEC follows TET2 and converts the unmethylated cytosines to uracils. Following PCR amplification (which converts uracils to thymines), hybrid capture is used to enrich for targeted regions of the genome. Samples are then assayed via high throughput sequencing. Created using Microsoft Powerpoint. [C] Overview of experiments and analyses. The samples used for each set of experiments are noted by a population-specific icon. Icons from Biorender, OpenClipArt, and Microsoft Powerpoint. https://doi.org/10.1371/journal.pgen.1011667.g001 https://doi.org/10.1371/journal.pgen.1011667.t001 Using DNA from a human population in Bolivia (Tsimane, see [52]), we tested four multiplexing strategies (12, 24, 48, and 96 plex, using 200ng of DNA sample input) and five DNA input amounts (25, 50, 100, 200, and 400ng, using the 12-plex strategy). Raw quality control metrics such as percent CHH methylation (a proxy for the rate at which unmethylated cytosines are converted to thymine) and mapping efficiency (percent of reads uniquely mapped to the genome) were high for all samples. Mapping efficiency was consistent across plexing strategies (average mapping efficiency: 12-plex = 71.9%, 24-plex = 72.9%, 48-plex = 72.5%, and 96-plex = 73.5%; ANOVA: F-value = 0.843, p-value = 0.472; Fig 2A) but affected by input amount, with higher DNA input having greater mapping efficiency (ANOVA: F-value = 13.57, p-value < 0.001, Fig 2B and S3 Table). CHH methylation was consistently well below 1%, indicative of high conversion rate across all plexing and input strategies (range = 0.1-0.27%; S1 Fig, and S4 and S5 Tables) [53]. [A] High (>70%) mean mapping efficiency across plexing strategies. Each point represents a sample within a plexing strategy and the y-axis represents the percent of reads uniquely mapped per sample. [B] Mapping efficiency increases as input amount increases. Each point represents a 12-plex pool made with varying DNA input amounts per sample, the y-axis represents the percent of reads uniquely mapped per sample. [C] Distribution of median DNA methylation levels for CpG sites located within different chromHMM genomic annotations; annotations from NIH Roadmap Epigenomics and data from the 96-plex, 200 ng input from experiment 1. [D] The total number of CpG sites falling within different chromHMM genomic annotations (using data from the 96-plex, 200 ng input from experiment 1). [E] Percent of reads that are not within the Twist probe set (i.e., off-target reads) following protocol modifications to annealing temperature and methylation enhancer (ME) volume. For each set of protocol conditions, the x-axis represents the percent of mapped reads that do not overlap with the Twist probe set. [F] Percent of Twist probes that are represented within each dataset following protocol modifications to adjust the annealing temperature and ME volume. For each set of protocol conditions, the x-axis represents the percentage of Twist probes that were represented by at least 1 read. [A] High (>70%) mean mapping efficiency across plexing strategies. Each point represents a sample within a plexing strategy and the y-axis represents the percent of reads uniquely mapped per sample. [B] Mapping efficiency increases as input amount increases. Each point represents a 12-plex pool made with varying DNA input amounts per sample, the y-axis represents the percent of reads uniquely mapped per sample. [C] Distribution of median DNA methylation levels for CpG sites located within different chromHMM genomic annotations; annotations from NIH Roadmap Epigenomics and data from the 96-plex, 200 ng input from experiment 1. [D] The total number of CpG sites falling within different chromHMM genomic annotations (using data from the 96-plex, 200 ng input from experiment 1). [E] Percent of reads that are not within the Twist probe set (i.e., off-target reads) following protocol modifications to annealing temperature and methylation enhancer (ME) volume. For each set of protocol conditions, the x-axis represents the percent of mapped reads that do not overlap with the Twist probe set. [F] Percent of Twist probes that are represented within each dataset following protocol modifications to adjust the annealing temperature and ME volume. For each set of protocol conditions, the x-axis represents the percentage of Twist probes that were represented by at least 1 read. https://doi.org/10.1371/journal.pgen.1011667.g002 After filtering for CpG sites with>5x coverage that were within the Twist probe set (+/- 200 bp) and that were covered in the majority of samples in a given experiment, we retained an average of 4,197,008 CpG sites (s.d. = 546,767) across plexing experiments and 4,051,941 CpG sites (s.d. = 93,106) across input experiments (S6 and S7 Tables). On average, this represented 96.42% and 92.19% coverage of the TMS probe set across the plexing and input experiments, respectively (S8 and S9 Tables). Across experiments, we found average coverage of targeted CpG sites to be far greater than our minimum coverage of 5x, ranging from 21-89x across datasets (S2 Fig and S2 Table). In addition to consistently recovering the expected set of CpGs, we also observed extremely repeatable methylation levels across the plexing and input experiments (all R2 > 0.99; S10 and S11 Tables). The CpGs covered by our experiments were distributed across diverse genomic annotations, and the median DNA methylation levels within a given annotation displayed expected patterns (Fig 2C and 2D) [54]. For example, we observed high levels of methylation in quiescent and heterochromatin regions and low levels of methylation in promoters and transcribed regions. In experiments 1 and 2, we used the recommended 65°C annealing temperature during the hybrid capture step—where prepared DNA is bound to the capture probe set to select CpG sites of interest—and the recommended 2uL of methylation enhancer, which increases the efficiency of this reaction. Here, we found that ~3/4 of all of our mapped reads were “on-target”, meaning that they overlapped with the designed probe set and represented successful hybrid capture (S8 and S9 Tables). This suggests that ~ ¼ of reads are “off target” and randomly distributed across the genome rather than within our regions of interest. We therefore performed a third experiment using Tsimane DNA to test two protocol modifications that might decrease the off-target proportion: we increased the annealing temperature (testing 65°C or 68°C) and we varied the amount of methylation enhancer (testing 0uL, 2uL, or 4uL). We note that similar previous work has reported on-target read percentages of 75–85% [55–57], suggesting the capture reaction will likely never be completely efficient. In experiment 3, we found that increasing the annealing temperature from 65°C to 68°C resulted in a lower proportion of off-target reads (ANOVA: F-value = 84.2, p-value < 0.0001; Figs 2E and S4, and S12 Table). Across samples annealed at 65°C, an average of 78.5% of reads were on-target, while this number rose to 84.2% at 68°C. However, this increase in capture efficiency came at a cost to the breadth of CpG sites covered: across samples annealed at 65°C, we observed coverage of on average 92.0% of the probe set, while this number fell to 72.2% for samples annealed at 68°C (Fig 2F, and S13 and S14 Tables). This suggests that higher annealing temperatures lead to greater but more specific binding during the hybrid capture step, and thus the increased capture efficiency comes at the expense of recovering all the expected CpG sites. In general, we did not find meaningful differences across methylation enhancer amounts and we therefore excluded this reagent from downstream experiments (Fig 2E and 2F). Given the loss of certain genomic regions at 68°C, downstream experiments focused on a 65°C annealing temperature. We next performed a fourth experiment focused on protocol optimization, in which we varied the strategies used to fragment genomic DNA prior to EM-seq library preparation: specifically, we tested mechanical fragmentation via Covaris sonication against enzymatic fragmentation with the NEBNext UltraShear reagent. Mechanical fragmentation is the current standard approach but is expensive, requires special equipment, and is labor intensive. Conversely, enzymatic fragmentation is cheaper, does not require special equipment, and is more compatible with automation. For experiments 3 and 4, we used the 96-plex strategy and 200 ng of sample input, since experiments 1 and 2 suggested that data quality does not suffer from higher plexing strategies. These experiments used DNA from a human population in Malaysia, the Orang Asli [58]. Enzymatic fragmentation resulted in a similar number of covered sites as was previously observed with mechanical fragmentation (n = 4,591,123 and 4,523,981 filtered CpG sites for the 10 and 20 minute protocols, respectively). Average site-specific methylation levels were also highly concordant between approaches (mechanical versus 10 min enzymatic: R2 = 0.9875; mechanical versus 20 min enzymatic: R2 = 0.9876; 10 min versus 20 min enzymatic: R2 = 0.9944; S5 Fig). This was also true when we focused on a subset of DNA samples processed using both methods (n = 3; mechanical versus 10 min enzymatic: average R2 = 0.971; mechanical versus 20 min enzymatic: average R2 = 0.971; 10 min versus 20 min enzymatic: average R2 = 0.987, S15 Table). From these experiments, we concluded that enzymatic fragmentation can be substituted into the protocol with no loss to data quality. We also used these data, which represent our “best” protocol (96-plex, 200ng input, 65°C annealing, no methylation enhancer, enzymatic fragmentation), to understand two critical aspects of experimental design—how many reads one would need to generate to achieve a given mean (or median) coverage per CpG site (S6 Fig) and how this mean coverage impacts power to detect differential methylation (S3 Fig). In general, we observe a ~ 1:1 relationship between the number of mapped, paired end reads (in millions) and mean coverage, such that 20 million mapped paired end (40 million total reads) translates to ~20x mean coverage (or ~14x median coverage) per CpG site. Using simulations [49,59] of datasets of different sizes (n = 100, 200, 400) and mean coverages (20x, 40x, and 80x), we found that increasing coverage can provide power gains for smaller sample sizes, but in larger datasets increasing coverage will matter less as power is derived from the overall sample size rather than gains in precision (S3 Fig). To ensure that TMS could perform comparably to the most popular current reduced-representation technology (the EPIC array), we generated paired data for 55 samples using both platforms (and following the 96-plexing, 200 ng input TMS protocol from experiment 1). After filtering, we analyzed 682,295 CpG sites common to both technologies, and found high concordance between per-site DNA methylation levels averaged across all individuals in the dataset (R2 = 0.97; Fig 3A). We also examined correlations between the two technologies when we subsetted to 1) variably methylated CpG sites (i.e., sites with methylation levels >10% and <90%; mean R2 = 0.83; Fig 3B); 2) CpG islands, shores, and shelves (R2 = 0.97, 0.97, 0.94, respectively); 3) hypomethylated (<50% average methylation; R2 = 0.89) regions; 4) hypermethylated (>50% average methylation; R2 = 0.70) regions; and 5) particular EPIC v2 array probe types (S7 and S8 Figs). Because methylation patterns are relatively consistent across individuals for many regions of the human genome, we also confirmed that these correlations were higher for EPIC-TMS data generated from the same sample compared to EPIC-TMS comparisons made between random pairs of samples (mean R2 for all sites: 0.95 versus 0.93 for random sample pairs, mean R2 for variable sites only: 0.83 versus 0.75 for random sample pairs; S9 Fig). [A] Correlation in DNA methylation levels for EPIC array versus TMS (R2 = 0.97). Each point represents the DNA methylation level of a given CpG averaged across 6 samples measured using the EPIC array (x-axis) and 96-plex, 200 ng input TMS (y-axis). The R2 value was generated using linear modeling. Sites were filtered to>5X coverage in >75% of samples within each technology. [B] Histogram of R2 values calculated for each individual sample (i.e., comparing per CpG DNA methylation levels measured on both technologies for a given sample). R2 values are provided when all CpG sites common to both technologies are included, as well as when only variably methylated CpG sites are included. [C] Correlation in DNA methylation levels for WGBS versus TMS (R2 = 0.9871). Each point represents the DNA methylation level of a given CpG averaged across 6 samples measured using WGBS (x-axis) and 96-plex, 200 ng input TMS (y-axis). The R2 value was generated using linear modeling. Sites were filtered to>5X coverage in >75% of samples within each technology. [D] Density plot of the average DNA methylation levels detected for common sites between the three technologies (713,282 sites). Notably, the EPIC array is biased against DNA methylation levels of 100%, as previously observed [51] and explained by the equation used to calculate beta values. [A] Correlation in DNA methylation levels for EPIC array versus TMS (R2 = 0.97). Each point represents the DNA methylation level of a given CpG averaged across 6 samples measured using the EPIC array (x-axis) and 96-plex, 200 ng input TMS (y-axis). The R2 value was generated using linear modeling. Sites were filtered to>5X coverage in >75% of samples within each technology. [B] Histogram of R2 values calculated for each individual sample (i.e., comparing per CpG DNA methylation levels measured on both technologies for a given sample). R2 values are provided when all CpG sites common to both technologies are included, as well as when only variably methylated CpG sites are included. [C] Correlation in DNA methylation levels for WGBS versus TMS (R2 = 0.9871). Each point represents the DNA methylation level of a given CpG averaged across 6 samples measured using WGBS (x-axis) and 96-plex, 200 ng input TMS (y-axis). The R2 value was generated using linear modeling. Sites were filtered to>5X coverage in >75% of samples within each technology. [D] Density plot of the average DNA methylation levels detected for common sites between the three technologies (713,282 sites). Notably, the EPIC array is biased against DNA methylation levels of 100%, as previously observed [51] and explained by the equation used to calculate beta values. https://doi.org/10.1371/journal.pgen.1011667.g003 Of note, the analyses described above reconfirmed a known bias in the EPIC array data [51,60], which does not allow for methylation levels of 100%. This is because EPIC-derived DNA methylation levels are represented as beta values, calculated as the ratio of the intensity of the methylated bead type to the total locus intensity plus an offset value. Due to the addition of the offset value, beta values of 1 are mathematically impossible. As a result, the correlation between average TMS- and EPIC-measured DNA methylation levels is slightly off the x = y line (Fig 3A) and correlations are much lower than the genome-wide average for hypo- as well as hyper-methylated regions. For further validation, we also generated WGBS data for 6 Tsimane samples included in experiment 3 (96-plexing, 200 ng input, 65°C annealing temperature, no ME, mechanical fragmentation). After filtering and merging with the TMS data, we retained 3,078,771 CpG sites covered by both the TMS and WGBS approaches. For these sites, the average methylation levels observed across technologies was highly correlated (R2: 0.9871; Fig 3C). We also found that the genome-wide distribution of DNA methylation levels derived from WGBS was more similar to TMS than to the EPIC array, specifically in that it included many sites with average methylation levels of 100% or close to 100%, as expected (Figs 3D and S10). To enable epigenomic analyses in our close primate relatives, we also tested whether TMS (96-plex, 200ng input protocol from experiment 1) could be effectively applied to three NHP species: tufted capuchins (Sapajus apella; n = 28 samples from blood), rhesus macaques (Macaca mulatta; n = 96 samples from 6 tissues (see S11 Fig and S16 Table)), and geladas (Theropithecus gelada; n = 68 samples from blood). While the probe set is designed from the human genome, NHP species share high levels of sequence homology with humans, especially in coding regions and regions near genes [61], leading us to hypothesize that a majority of CpG sites would be recovered. We mapped the Twist probe sequences to each of the NHP genomes to confirm this intuition, and from this analysis expected to capture 3.0-4.8 million CpG sites across the three species (Fig 4B). Importantly, for the rhesus macaque samples, we also generated paired RRBS data and compared our TMS results to a technology that does not rely on hybrid capture. [A] Optimized TMS in NHPs results in high mapping efficiencies despite the use of human-specific probes. Here, each of the species are mapped to their respective reference genome. We hypothesize that low mapping efficiency in certain rhesus macaque samples is due to variation in sample quality. [B] Number of expected and observed CpG sites covered in each NHP genome. Expected sites were derived from mapping the Twist probes to each NHP genome, while observed sites represent those detected with a coverage > 5X in >75% of samples. [C] Principal components analysis of TMS-derived DNA methylation levels for rhesus macaque samples spanning six distinct tissues. [D] Similar per CpG DNA methylation level estimates using RRBS (x-axis) and optimized TMS (y-axis) (R2 = 0.97). [E] Density plot of linear model R2 values obtained from comparing data generated via optimized TMS and RRBS for the same rhesus macaque samples. R2 values are provided when all CpG sites common to both technologies are included, as well as when only variably methylated (methylation > 10% and methylation < 90%) CpG sites are included. [F] Density curves of the average genome-wide DNA methylation level estimates for each NHP species. Curves show the expected bimodal distribution in which many of the CpG sites in the genome are either hypomethylated or hypermethylated. [A] Optimized TMS in NHPs results in high mapping efficiencies despite the use of human-specific probes. Here, each of the species are mapped to their respective reference genome. We hypothesize that low mapping efficiency in certain rhesus macaque samples is due to variation in sample quality. [B] Number of expected and observed CpG sites covered in each NHP genome. Expected sites were derived from mapping the Twist probes to each NHP genome, while observed sites represent those detected with a coverage > 5X in >75% of samples. [C] Principal components analysis of TMS-derived DNA methylation levels for rhesus macaque samples spanning six distinct tissues. [D] Similar per CpG DNA methylation level estimates using RRBS (x-axis) and optimized TMS (y-axis) (R2 = 0.97). [E] Density plot of linear model R2 values obtained from comparing data generated via optimized TMS and RRBS for the same rhesus macaque samples. R2 values are provided when all CpG sites common to both technologies are included, as well as when only variably methylated (methylation > 10% and methylation < 90%) CpG sites are included. [F] Density curves of the average genome-wide DNA methylation level estimates for each NHP species. Curves show the expected bimodal distribution in which many of the CpG sites in the genome are either hypomethylated or hypermethylated. https://doi.org/10.1371/journal.pgen.1011667.g004 When examining initial quality control metrics, we found that all three NHP species had high mapping efficiencies (average = 81.96% for capuchins, 82.62% for geladas, and 81.35% for macaques; Fig 4A). Further, the average CHH methylation levels were all extremely low (<1%), again suggesting high conversion rates (S12 Fig). Following filtering, we recovered ~ ½ to ¾ of expected CpG sites in the NHP datasets (3,343,133 in capuchin, 5,387,280 in gelada, and 5,486,073 in macaque). The number of sites recovered scales generally with divergence time (capuchins share a common ancestor with humans 35–45 million years ago, geladas and rhesus macaques share a common ancestor with humans 23–28 million years ago) [62]. In all species, we were able to reliably measure more sites than would be typical of RRBS (see below), and we note that some of the between-species variation in performance could be explained by heterogenous read depth (S2 Table) as well as reference assembly quality. In particular, the quality of the rhesus macaque genome is much higher than the gelada or capuchin (using CNEr in R and the N50() and N90() commands [63]): mmul_10 N50 = 153,388,924, N90 = 79,627,064; tgel1 N50 = 147,341,205, N90 = 77,542,005; cimit N50 = 5,274,112, N90 = 1,283,179. When examining average DNA methylation levels across species, we found that, as expected, all exhibited bimodal genome-wide profiles similar to humans (Fig 4F). Further, because the rhesus macaque samples were derived from 6 different tissue types (S11 Fig and S16 Table), we also confirmed that samples displayed expected tissue-specific epigenetic patterns. Specifically, we demonstrated that a Principal Components Analysis (PCA) was able to reliably separate samples by tissue type (Fig 4C), as has been observed in previous studies using both bisulfite sequencing and the EPIC array [64–66]. Studies of NHP species have historically relied on RRBS because of the species-specificity of microarray technologies and the cost barrier of WGBS [49,67,68]. To test how our optimized TMS protocol compares to RRBS, we generated paired data for all 96 rhesus macaque samples. After filtering both datasets to 721,766 common CpG sites, we found a high concordance of the average DNA methylation levels estimated by both technologies (R2 = 0.97; Figs 4D and S13). This remained true when we subsetted specifically to 92,692 variably methylated CpG sites (i.e., sites with average DNA methylation levels >0.1 and <0.9; R2 = 0.5945; Fig 4E). Thus far we have compared DNA methylation levels measured with TMS versus other technologies; if these measurements are robust across platforms, then power to detect biological patterns should also be similar. We thus asked whether data generated from paired samples, but using different technologies, could recapitulate 1) epigenetic age predictions using DNA methylation-based clock algorithms [7,69–73] and 2) tissue-dependent methylation signatures when comparing diverse organ systems. For analysis #1, we used the 55 VUMC cohort samples with paired EPIC and TMS data (focusing on 682,295 CpG sites passing filters and common to both technologies). For analysis #2, we used the 96 rhesus macaque samples with paired RRBS and TMS data (focusing on 391,758 CpG sites passing filters and common to both technologies). For analysis #1, we observed a high correlation between epigenetic age estimates derived from TMS and EPIC data (Fig 5A; average R2 = 0.91). This agreement was high across a variety of existing epigenetic clock algorithms. For analysis #2, we found that effect size estimates of tissue dependency (for example, from linear models comparing DNA methylation levels in liver to all other tissues) were very similar genome-wide when applied to TMS versus RRBS data (Fig 5B and 5C). Additionally, we confirmed that sites identified as significantly associated (FDR < 5%) with a given tissue in RRBS versus TMS data overlapped more than expected by chance (Fig 5D). Together, these results support the ability of TMS data to uncover biological patterns in similar ways as revealed by other technologies. [A] Pearson’s correlation coefficient comparing epigenetic age predictions for five PC-based epigenetic clocks run on TMS versus EPIC v2 array data from the VUMC cohort (n paired samples = 55). All correlations were significant following multiple hypothesis testing (FDR < 5%). [B] Correlation between standardized effect sizes, estimating liver-specific effects, using RRBS versus TMS data (n paired rhesus macaque samples = 96). To derive effect size estimates, models were run comparing the liver to all other tissues. Each point represents the effect size for a given CpG site common to both datasets. [C] Pearson’s correlation coefficient comparing effect sizes for estimates of tissue-specific effects using TMS versus RRBS data (n paired rhesus macaque samples per tissue = 96). Separate models were run for each tissue, comparing the focal tissue on the x-axis to all other other tissues to identify tissue-specific effects. All correlations were significant following multiple hypothesis testing (FDR < 5%). [D] Degree of enrichment (represented as an log2 odds ratio from a Fisher’s Exact test), between CpG sites identified as tissue-specific in TMS versus RRBS data using matched samples. Dashed line represents no enrichment and error bars represent confidence intervals. [A] Pearson’s correlation coefficient comparing epigenetic age predictions for five PC-based epigenetic clocks run on TMS versus EPIC v2 array data from the VUMC cohort (n paired samples = 55). All correlations were significant following multiple hypothesis testing (FDR < 5%). [B] Correlation between standardized effect sizes, estimating liver-specific effects, using RRBS versus TMS data (n paired rhesus macaque samples = 96). To derive effect size estimates, models were run comparing the liver to all other tissues. Each point represents the effect size for a given CpG site common to both datasets. [C] Pearson’s correlation coefficient comparing effect sizes for estimates of tissue-specific effects using TMS versus RRBS data (n paired rhesus macaque samples per tissue = 96). Separate models were run for each tissue, comparing the focal tissue on the x-axis to all other other tissues to identify tissue-specific effects. All correlations were significant following multiple hypothesis testing (FDR < 5%). [D] Degree of enrichment (represented as an log2 odds ratio from a Fisher’s Exact test), between CpG sites identified as tissue-specific in TMS versus RRBS data using matched samples. Dashed line represents no enrichment and error bars represent confidence intervals. https://doi.org/10.1371/journal.pgen.1011667.g005 We developed and benchmarked a multiplexed, cost-effective version of the TMS protocol and applied it to human populations from the US, Bolivia, and Malaysia as well as multiple NHP species. We recommend an optimal protocol for future work (96-plex, 200ng input, 65°C annealing, no methylation enhancer, enzymatic fragmentation), but found that data quality remained high across plexing strategies, input amounts, and protocol modifications. Importantly, the 96-plex version of the TMS protocol—including sequencing to achieve ~ 25x coverage per CpG site on the Illumina NovaSeq X—can currently be performed for ~USD 80 per sample (with roughly half being reagents and labor, and the other half being sequencing on the NovaSeq X platform; S1 Table). Relative to the commonly used EPIC array for human studies, this represents massive savings enabling larger-scale, population-based studies. We recognize that the total cost of TMS will vary by the amount of sequencing performed, and we provide simulations (and modifiable code) based on real TMS coverage distributions for users to explore the impact of coverage on power for a given study design. For example, we find that with a sample size of n = 100, moderate differences in methylation (e.g., 20%) can be identified with high power at relatively low read depths (e.g., 20x), while detecting small differences would require higher read depths. However, the relative impact of coverage on power diminishes at higher sample sizes. Researchers will thus need to tailor their sequencing plan based on both their expected effect size and the number of samples in hand (S3 Fig). We found high concordance between TMS-derived DNA methylation levels and those derived from other commonly used methods—namely the EPIC array, WGBS, and RRBS. WGBS is the gold standard for comprehensive DNA methylation measurement, but is prohibitively expensive for most studies given the breadth of sequencing (to cover the whole genome) and the necessity for deep sequencing (to achieve high levels of precision) [50]. RRBS has filled in as a more cost-effective alternative, but due to the stochastic nature of the Msp1 digestion followed by size selection, not all CpG sites are reliably covered across individuals and missing data can impede downstream analyses (S14 Fig). We note that variation in coverage (and thus precision) across CpG sites will be an issue, to some degree, for any sequencing-based technology. As a result of these challenges, microarray-based methods (such as the EPIC array) have become the most commonly used approach in human genomics. Consequently, many popular bioinformatics pipelines and specialized algorithms for DNA methylation data (e.g., epigenetic clocks or cell type deconvolution [74,75]) are currently keyed to microarrays. While DNA methylation levels derived from TMS are strongly correlated with the EPIC array, it is important to keep in mind that: 1) a small subset of sites are not covered by both technologies, and 2) because microarrays output beta values (equivalent to methylated signal/(total signal + an offset)), the relationship between TMS- and EPIC-derived values cannot be exactly 1:1. We caution that care will thus be needed when applying existing microarray-based algorithms to TMS data, though our initial attempts at doing so with epigenetic clock algorithms do seem to perform well. The study of DNA methylation in NHP species is deeply important to our understanding of gene regulatory evolution [76–78], comparative aging [67,68,79,80], and environmental impacts on phenotype [68,81]. For example, both captive and field-based NHP studies have strongly contributed to our understanding of how social and ecological inputs influence fitness-related traits through changes in DNA methylation [82,83]. These studies have sometimes relied on microarrays (e.g., the Mammalian Methylation Array [36,84–86] or the application of human arrays to NHP species [87–89]). However, given the high costs of arrays, a large proportion of previous work has relied on RRBS [68,80,81,83]. Although RRBS is easily adapted for non-human species, TMS can work with smaller amounts of input DNA than bisulfite-based protocols [42], which can be critical for studies of wild or endangered species. While TMS uses capture probes designed from the human genome, NHPs share high levels of sequence similarity, which we show is sufficient to reliably capture 2–3 million CpG. Though not all ~ 4 million CpG sites are captured, TMS still represents a more consistent and cost-effective approach relative to the alternatives. Notably, we find that TMS is effective in both catarrhine (monkeys of Africa and Asia) and platyrrhine (monkeys of Central and South America) species, suggesting it may be effective in other members of these clades for capturing conserved regions. One potential issue that requires further study is that the probes (which are designed from human genetic variation) do not specifically avoid or take into account within-species polymorphisms. To show that TMS data could detect expected biological patterns, in ways that are comparable to existing technologies, we performed the same analyses of epigenetic age estimation and tissue-specificity in matched TMS, EPIC, and RRBS data, respectively. From these analyses we found that the epigenetic ages estimated with TMS versus EPIC data were highly correlated as were genome-wide estimates of tissue specific patterns from TMS versus RRBS data. The potential portability of epigenetic clock algorithms is particularly encouraging, as this approach is becoming increasingly popular for measuring biological age [90–92], and will be exciting to pair with cost-effective methods going forward. Together, our optimized TMS protocol has the potential to add value and enable larger-scale studies in the many fields that query DNA methylation patterns, such as genetic medicine, developmental biology, evolutionary biology, anthropology, public health, geroscience, and more. For the Tsimane participants, informed consent was collected at three levels: by the individual (formal written consent), by the community, and by the Tsimane Gran Consejo (Tsimane governing body). All study protocols, including the generation of DNA methylation data, were approved by the Institutional Review Boards of the University of New Mexico (#07–157), the University of California Santa Barbara (#3-21-0652), and Universidad Mayor San Simon, Cochabomba. For the Orang Asli participants, informed consent was also collected at multiple levels: first by first describing the project to the community as a whole and seeking the permission of community leaders, and subsequently through individual-specific review of the protocol and formal written consent. The study protocol, including the generation of DNA methylation data, was approved by Vanderbilt University (IRB #212175) as well as the Malaysian Medical Research Ethics Council. For the rhesus macaque samples, the study protocol was approved by the Institutional Animal Care and Use Committee through the University of Puerto Rico’s Caribbean Primate Research Center (IACUC #A400117). For the gelada samples, the study protocol was approved by the Institutional Animal Care and Use Committees at the University of Washington (protocol 4416-01) and Arizona State University (20–1754 R) along with approval from the Ethiopian Wildlife and Conservation Agency. For the tufted capuchin samples, the study protocol was approved by the Institutional Animal Care and Use Committee at the Georgia State University (protocol A20018). Data generation drew on previously collected samples from multiple human and non human primate populations. A brief description of each population is provided below. The Tsimane are an Indigenous horticulturalists population spread across >90 villages in the Bolivian lowlands and totaling approximately 17,000 people [52]. We extracted DNA from 192 venous whole blood (WB) samples collected between the years of 2010–2021 as part of the Tsimane Health and Life History Project (THLHP). The THLHP has continuously collected demographic, behavioral, environmental, and health data along with the provision of medical services for over two decades [93]. Samples were frozen in liquid nitrogen, transferred on dry ice to Arizona State University, and stored at -80°C prior to analysis. The sample set for this project included 103 females and 89 males, with a mean age of 54.3 years old (range 18.0–83.6 years old) (see Table 1). Genomic DNA was extracted using the Zymo Quick-DNA 96 kit (Zymo Research #D3012) following the manufacturer’s instructions. The Orang Asli consist of ~19 ethnolinguistic groups and a total population of ~210,000 people [58]. They traditionally subsist on a mixture of hunting, gathering, fishing, small-scale farming, and trade of forest products [94,95]. We extracted DNA from 88 white blood cell (WBC) samples that were collected in 2023 as part of the Orang Asli Health and Lifeways Project (OA HeLP) [58]. Samples included in data generation were derived from venous blood draws followed by washing with QIAGEN PureGene red blood cell lysis. Samples were stored in liquid nitrogen upon collection, and at -80C for longer term storage. The Orang Asli sample included 46 females and 42 males, with a mean age of 35.3 years old (range 18–78 years old; Table 1). Genomic DNA was extracted using the Zymo Quick-DNA/RNA MagBead kit (Zymo Research #R2131) following the manufacturer’s instructions. We were granted access to de-identified EPIC array data (Infinium MethylationEPIC v2.0 Kit) and TMS data from 55 paired human whole blood samples. These samples were sourced from a healthy cohort recruited through the Vanderbilt University Medical Center (VUMC) in Nashville, TN USA. Due to IRB restrictions, demographic data or other metadata were not available for these samples. We obtained extracted DNA from rhesus macaque tissue samples (n = 96) collected by the Cayo Biobank Research Unit in partnership with the University of Puerto Rico’s Caribbean Primate Research Center (CPRC) [96–100]. Beginning in 2016, samples were collected from individuals living on the island of Cayo Santiago, an NIH-managed free-range colony of provisioned rhesus macaques. Specifically, as part of an ongoing population management plan designed by CPRC, select individuals were culled and tissues from all major organ systems were systematically harvested, stored in a fixative buffer, and frozen at -80C. This data set consists of samples from six different tissue types: adrenal, heart, kidney, lung, liver, and spleen, with 16 samples from each type and samples coming from 23 unique individuals (S3 Table). This dataset includes samples from 11 females and 12 males, ages 3.2 to 19.9 years old (mean 10.6 years old), collected from 2016–2019 (Tables 1 and S3). Genomic DNA was extracted using the Zymo Quick-DNA/RNA MagBead kit (Zymo Research #R2131) following the provided manufacturer’s protocols. We extracted DNA from whole blood from 68 geladas; 21 were female and 47 were male and all were considered adult (i.e., over 4 years old, the minimum average age of reproductive maturation in this species [101]) (Table 1). Gelada samples were collected as part of the Simien Mountains Gelada Research Project (SMGRP) which, since 2017, has carried out annual capture-and-release campaigns to collect morphometric data and whole blood samples from wild Ethiopian geladas [102]. Samples were stored in liquid nitrogen upon collection, and at -80C for longer term storage. Genomic DNA was extracted using the Qiagen DNeasy Blood & Tissue kits (Qiagen #69581) following the provided protocols. Blood was collected from individuals in the captive tufted capuchin monkey colony at Georgia State University in January 2023. Of the 28 capuchins, 19 were female and 9 were male with an average age of 19.4 years old (range 9–41 years old; Table 1). A trained veterinarian anesthetized the monkeys using 13 mg/kg Ketamine, delivered intramuscularly. Whole blood samples were collected during the monkeys’ annual physicals, stored at 4°C upon collection, and shipped to Arizona State University where they were flash frozen into 0.5mL aliquots and stored at -80°C until used for analysis. DNA was extracted using the Qiagen DNeasy Blood & Tissue kits (Qiagen #69581) following the manufacturer’s protocols. We used the Qubit dsDNA assay to determine the quantity of all extracted DNA. DNA libraries were normalized and prepared using the NEBNext Enzymatic Methyl-seq kit (P/N: E7120L) following a modified version of the manufacturer’s protocol that included 9 cycles of PCR for the final library amplification followed by a 0.65X bead cleanup. To prepare for the hybrid capture reaction, the total DNA input requirement (2000ng in this case) was divided by the total number of samples being pooled (12, 24, 48, or 96 as will be discussed below). In the 96-plex experiment, for example, 84ng of DNA from each sample was pooled totaling 8ug, and ¼ of the volume was used for the hybrid reaction and captured using the Human Methylome panel from Twist Biosciences following the manufacturer’s instructions (P/N: 105521). The final post-capture PCR reaction was split into 2 reactions per pool and cleaned with a 1X bead cleanup and then combined. Pool quality was assessed post-hybridization using the Agilent Bioanalyzer and quantified using a qPCR-based method with the KAPA Library Quantification Kit (P/N: KK4873) and the QuantStudio 12K instrument. Prepared library pools were sequenced on the NovaSeq 6000 at the Vanderbilt Technologies for Advanced Genomics (VANTAGE) Core. We used 150 bp paired-end sequencing and generally targeted 30-50M paired-end reads per sample. Real Time Analysis Software (RTA) and NovaSeq Control Software (NCS) (1.8.0; Illumina) were used for base calling. MultiQC (v1.7; Illumina) was used for data quality assessments. For each sample, we applied the Illumina DRAGEN Methylation Pipeline v4.1.23 using the custom bed file from Twist Biosciences. The deliverables from DRAGEN consist of FASTQs, bams, cytosine reports (which include counts of methylated and unmethylated reads per CpG site), and methyl and mapping metric reports. To determine whether TMS can be effectively multiplexed beyond the recommended 8-plex, we used 96 Tsimane samples to test four different multiplexing strategies during capture: 12-, 24-, 48-, and 96-plex. To test whether TMS is robust to DNA input amounts, we tested five input amounts: specifically, 25, 50, 100, 200, and 400 ng of sample were used as input into the EM-seq library prep. Here, we kept the plexing strategy constant (12-plex) and used three Tsimane samples, each represented three times within each pool and included three replicates of a control sample (HG01879 from the 1000 Genomes Project) [103]. To optimize the capture efficiency of Twist target sites, we tested the use of two different annealing temperatures–65° and 68° C–along with the use of a methylation enhancer (ME)– produced by Twist Biosciences (Catalog #103557) consisting of Tris EDTA buffer to block the binding of off-target probes thereby improving on-target capture efficiency. The specific combinations we explored were: testing a 65°C annealing temperature with 0uL (n = 192), 2uL (n = 96), and 4uL (n = 96) of ME and testing a 68°C annealing temperature with 0uL (n = 96) and 2uL (n = 192) of ME. These experiments were conducted with 96-plexed Tsimane samples (n = 192), and using 200 ng of sample input. Next, we tested the use of an enzymatic fragmentation method to replace the Covaris (LE220) mechanical fragmentation approach. Mechanical fragmentation is known to decrease library quality through damage to DNA; however, enzymatic fragmentation is not currently recommended by the TMS protocol. To compare these approaches, we performed the optimized TMS with enzymatic fragmentation using 4uL of NEBNext UltraShear (NEB #M7634S/L) for 10 or 20 minutes. This experiment was conducted using 96-plexed samples from the Orang Asli (n = 88) and using 200 ng of sample input. To evaluate the efficacy of optimized TMS on three NHP species—macaques, geladas, and capuchins—we applied the 96-plex protocol design from experiment 1 with 200 ng input. To compare rhesus macaque TMS to RRBS, we generated RRBS libraries using 150 ng of DNA input in combination with 1ng of lambda phage DNA and 1uL of Msp1—a digestive enzyme which cuts CCGG nucleotide motifs. Next, using NEBNext Ultra II reagents, we performed end repair and adapter ligation to the DNA fragments produced by Msp1 digestion. We then performed bisulfite conversion on the fragments using the EZ-96 DNA Methylation Lightning MagPrep kit (Zymo Research #D5046) following the manufacturer directions. The samples were then PCR amplified for 16 cycles with unique dual indexed sequencing primers. We selected for 180–2000 bp fragments and sequenced on an Illumina NovaSeq S2 flow cell with 2x51bp sequencing [80,104]. For experiments 1, 2, 7, and 8, we used a custom bioinformatics pipeline to process all FASTQ files into counts of methylated versus unmethylated cytosines at each CpG site. For experiments 3, 4, 5, and 6, we used Illumina’s Dynamic Read Analysis for GENomics (DRAGEN) pipeline [105] to process all FASTQ files into counts of methylated versus unmethylated cytosines at each CpG site. Importantly, both our custom pipeline and DRAGEN follow the same general steps and rely on the Bismark suite [106], making them highly comparable. We also processed 7 samples from experiment 4 using both methods to empirically confirm that our custom pipeline and the Illumina DRAGEN pipeline produced near identical results (S15 Fig). For our custom pipeline, we first trimmed adapters using Trimmomatic (version 0.39) [107] and TrimGalore (version 0.6.6) [108] for human and NHP samples, respectively. Following trimming, we used Bismark (version 0.24.0) [106] to map reads to each species’ respective genomes (hg38 for human, mmul10 for rhesus macaque, cimit for capuchin, and tgel1 for gelada). We retained only uniquely mapped reads and used the methylation extractor function within Bismark to extract counts of methylated versus unmethylated cytosines at each cytosine. These files were further filtered for CpG contexts only. For all samples, run through either the custom or DRAGEN pipeline, we extracted two measures of data quality that are automatically calculated by Bismark: the percent of reads that mapped uniquely to the reference genome and the average methylation percentage for cytosines in a CHH context. The latter value serves as a commonly used estimate of the efficiency with which a given protocol converts unmethylated cytosines to thymine, because cytosines located outside of CpG contexts are extremely unlikely to be methylated in the mammalian genome [109,110]. Estimates of CHH methylation were extracted from an automatically generated report file when using Bismark to align the trimmed FASTQ files to the reference genome. For experiments 1 and 2, we tested whether multiplexing strategy and input amount impacted mapping efficiency and percent CHH methylation using a one-way ANOVA test, followed by a pairwise t-test in the case of significance, with the ‘aov’ and ‘pairwise.t.test’ functions in the ‘stats’ R package [111]. For each study, we used the BSseq R package [112] to compile count matrices (derived from our custom pipeline or DRAGEN) across samples and to perform region, coverage, and missingness filtering. For experiments 1, 3, 4, 5, and 6 we used built-in functions in BSseq to filter for sites within the probes regions (+/- 200 bp) and for sites covered at>5X in >75% of samples. We made slight modifications to this filtering pipeline for other experiments. For experiment 2, where n = 3 for each input amount, we relaxed our missingness filter to sites with at least one read observed in at least ⅔ samples. For experiment 7, which focused on NHP genomes for which the probe set coordinates (which are provided in hg38) are irrelevant, we did not perform region filtering. The number of sites analyzed for each experiment (reported in the main text and in S4 Fig) therefore varies slightly depending on sample size, sequencing coverage, and other factors that impact which CpG sites passed our filters. To confirm the fidelity of optimized TMS, we also checked whether CpGs captured by the protocol were distributed as expected throughout different genomic regions (e.g., promoters, enhancers) and that the average methylation levels in different genomic regions were as expected. To do so, we annotated each CpG site by whether it fell into a gene body, promoter, or non-genic region, and by chromatin state. We used hg38 gene body coordinates from Ensembl’s ‘biomaRt’ package in R, and we defined promoter regions as the 2000 bp region upstream of TSSs. We annotated CpGs as falling in chromatin states as defined by hg38 ChromHMM annotations from NIH’s Roadmap Epigenomics Project [54]. We then counted the number of CpG sites that fell in each region (Fig 2C) and evaluated the median methylation across samples (Fig 2D). A subset of our experiments sought to understand and optimize two measures of efficiency of the hybrid capture step: 1) how many of the expected CpG sites from the probe set passed our filtering parameters and were thus analyzable and 2) how many of the reads we generated for a given sample were on-target and putatively captured by the probe set, rather than representing off-target randomly sequenced DNA fragments that do not contribute to analyzable data as they are often sparsely shared between samples. For #1, we used the bedtools (version 2.28.0) [113] intersect command to determine the proportion of CpG sites that are within +/- 200 bp with at least 1 probe [using a bed file available on the Twist Biosciences website (https://www.twistbioscience.com/resources/data-files/twist-human-methylome-panel-target-bed-file)]. For #2, we used the bedtools function bamtobed to convert the mapped reads for each sample into a bed file; because we used a paired end sequencing strategy, each bed coordinate included a fragment start position from R1 and a fragment end position from R2. We then used the bedtools intersect command to determine the proportion of mapped read pairs that are within 200 bp of at least 1 Twist probe. To understand what level of coverage is necessary to detect particular effect sizes in different sample sizes, we conducted a power analysis using simulated data based on the true coverage distribution of 1,000 sites in our TMS dataset, drawing with replacement to simulate sample sizes of n = 100, 200 and 400 (following the methods in [49,59]). We then assigned each sample a binary predictor variable (0 or 1), estimated methylation level differences between groups for a given effect size, and simulated the number of methylated counts per sample we would observe under this scenario by sampling from a binomial distribution (given the number of total counts and the probability of a count being methylated). We simulated data for effect sizes ranging from a 0–20% difference in methylation levels between groups and calculated power as the proportion of sites in which the predictor variable had a significant effect on methylation at a nominal p-value threshold of 0.001. We ran this analysis 3 times, first using the true mean coverage of our dataset (~20x), then again simulating coverage of 40x and 80x by multiplying the total counts of each site by 2 and 4, respectively. We used our filtered BSSeq object from experiment 5 to compare to data from the EPIC array generated for 55 paired human samples (average number of CpG sites measured with EPIC = 936,280; average call rate = 0.999). We downloaded the EPIC CpG coordinates from the Illumina website and merged with the TMS CpG locations, resulting in a shared dataset of 682,295 CpG sites passing filters and common to both technologies. We then performed two analyses to understand consistency. First, we calculated the average per-site methylation level across all samples included in the TMS or EPIC array datasets, respectively. We then ran a linear model testing the relationship between the two sets of average methylation levels using the ‘lm’ function in the ‘stats’ package in R. Second, we used the ‘lm’ function to estimate the R2 value comparing per-site methylation levels for estimates derived from each technology for a given individual (i.e., not averaged across the dataset). This resulted in a distribution of 55 R2 values. Because all humans share canonical methylation patterns, we also compared this distribution to a distribution of 55 R2 values derived from the same analysis after sample identity was permuted. We used the ‘t.test’ function in the ‘stats’ package in R to ask whether these distributions were significantly different. We used a very similar strategy to compare ~ 30x WGBS data generated for six paired Tsimane samples with TMS data generated from experiment 1 (96-plex, 200 ng input). First, we performed low level processing of the WGBS data using Illumina’s DRAGEN pipeline and merged this with our filtered TMS data, resulting in 3,078,771 CpG sites common to both datasets. We calculated the average methylation level across samples reported for each site and technology and ran a linear model using the ‘lm’ function in the ‘stats’ package in R to calculate the R2 value. We did not compare individual-based R2 values to permuted values for this experiment, given the small number of individuals. To estimate the number of CpG sites that we expected to recover when applying the human probe set to each NHP species, we converted the probe bed file to a FASTA file using the bedtools command ‘getfasta’ [113] and the hg38 reference genome. We then used Bismark to map the FASTA file to each non-human primate’s respective genome. From the mapped bam file, we used the ‘bamToBed’ function in bedtools to extract coordinates for the mapped probes and to add a + /- 200 bp offset. Finally, we applied the ‘getfasta’ function in bedtools to extract the sequence for the mapped regions (plus the 200 bp buffer) from the non-human primate genome and to count the number of CpG sites in this region set. Similar to the comparisons between TMS and the EPIC array, we used paired RRBS data for the 96 rhesus macaque samples to directly compare methylation data generated using TMS versus RRBS. To do so, we processed the RRBS data using the same custom pipeline and filtering parameters described for TMS data, with the only modification being that we used the ‘—rrbs’ parameter in TrimGalore to remove unmethylated cytosines artificially introduced during library preparation from the 3’ end of fragments. We merged the filtered TMS and RRBS datasets, resulting in 721,766 CpG sites common to both technologies. As described for the TMS-EPIC array comparison, we then 1) calculated the average per-site methylation level across all samples included in each dataset and compared these vectors using linear models and 2) estimated the R2 value for methylation level estimates derived from each technology for a given individual, and used a t-test to compare this distribution to a distribution for the same analysis where sample identity was permuted (S16 Fig). First, we compared epigenetic age predictions from paired samples that were sequenced on different platforms. We estimated epigenetic age from the PC-based versions of five well-established epigenetic clocks, including the Horvath multi-tissue clock, Hannum blood clock, PhenoAge clock, and telomere length clock. The PC-based versions of these clocks have much higher reliability and less susceptibility to technical noise than the original CpG-site level clocks [73]. We estimated epigenetic age from these clocks using the PC-Clocks R package [114] and calculated the Pearson’s correlation coefficient for estimates from samples generated with TMS versus the EPIC array. Second, we compared tissue-specific effect size estimates between samples generated with RRBS and TMS. Specifically, we asked whether tissue type significantly (FDR < 5%) predicted DNA methylation among the multi-tissue macaque data for each technology, using beta binomial models implemented in the R package ‘aod’. We performed these analyses iteratively to compare a given tissue to all other tissues (for example, comparing liver versus all other tissues to estimate liver-specific effects). We limited this analysis to variably methylated CpG sites (median methylation <90% or >10%). https://doi.org/10.1371/journal.pgen.1011667.s001 (XLSX) https://doi.org/10.1371/journal.pgen.1011667.s002 (XLSX) P-values generated from pairwise t-tests comparing the percentage of reads that were uniquely mapped to the human genome from sequencing data generated using the TMS protocol with varying amounts of input DNA. * represents a significant (p < 0.05) difference in mapping efficiency between conditions. https://doi.org/10.1371/journal.pgen.1011667.s003 (XLSX) P-values generated from pairwise t-tests comparing the percentage of cytosines in a CHH context marked as methylated (an estimate of conversion efficiency). * represents a significant (p < 0.05) difference in percent CHH methylation between conditions. https://doi.org/10.1371/journal.pgen.1011667.s004 (XLSX) P-values generated from pairwise t-tests comparing the percentage of cytosines in a CHH context marked as methylated (an estimate of conversion efficiency). * represents a significant (p < 0.05) difference in percent CHH methylation between conditions. https://doi.org/10.1371/journal.pgen.1011667.s005 (XLSX) Sites are filtered for those within the Twist probe set and covered at>5x coverage in >75% of samples. The average number of reads is provided as the total, such that the number of paired end reads would be ½ the reported value. 200ng of DNA was used for each sample for all plexing experiments. https://doi.org/10.1371/journal.pgen.1011667.s006 (XLSX) Sites are filtered for those within the Twist probe set and covered at>5x coverage in >75% of samples. The average number of reads is provided as the total, such that the number of paired end reads would be ½ the reported value. All input experiments were pooled using the 12-plex strategy. https://doi.org/10.1371/journal.pgen.1011667.s007 (XLSX) The percentage of Twist target probes (n = 551,803) covered by at least one read for each plexing strategy. https://doi.org/10.1371/journal.pgen.1011667.s008 (XLSX) The percentage of Twist target probes (n = 551,803) covered by at least one read for each input amount. https://doi.org/10.1371/journal.pgen.1011667.s009 (XLSX) R2 values generated using linear modeling to compare average site-level methylation between plexing experiments. Average site-level methylation was calculated by averaging the percent methylation for each site across all samples within a given plexing strategy and comparing these with average site-level methylation within shared sites in an alternate plexing strategy. All sites were filtered for>5X coverage in >75% of samples. https://doi.org/10.1371/journal.pgen.1011667.s010 (XLSX) R2 values generated using linear modeling to compare average site-level methylation between input amount experiments. Average site-level methylation was calculated by averaging the percent methylation for each site across all samples within a given input amount experiment and comparing with average site-level methylation within shared sites in an alternate input amount experiment. All sites were filtered for>5X coverage in >75% of samples. https://doi.org/10.1371/journal.pgen.1011667.s011 (XLSX) P-values generated from pairwise t-tests comparing the percentage of the total reads that were not associated with a Twist target probe (within +/- 200 bp) for each capture efficiency experiment. 65C/68C refers to annealing temperature and 0uL/2uL/4uL ME refers to volume of methylation enhancer. * represents a significant (p < 0.05) difference in the percent of probes captured between conditions. https://doi.org/10.1371/journal.pgen.1011667.s012 (XLSX) P-values generated from pairwise t-tests comparing the percentage of Twist target probes covered by at least one read for each capture efficiency experiment. 65C/68C refers to annealing temperature and 0uL/2uL/4uL ME refers to volume of methylation enhancer. * represents a significant (p < 0.05) difference in the percent of probes captured between conditions. https://doi.org/10.1371/journal.pgen.1011667.s013 (XLSX) Sites are filtered for those which are covered at>5x coverage in >75% of samples. https://doi.org/10.1371/journal.pgen.1011667.s014 (XLSX) R2 values generated using linear modeling to compare site-specific methylation for 3 samples, each processed with 3 different fragmentation methods- mechanical, enzymatic for 10 minutes, and enzymatic for 20 minutes. All sites were filtered for>5X coverage in >75% of samples. https://doi.org/10.1371/journal.pgen.1011667.s015 (XLSX) Age, sex, and tissue types for each individual in the rhesus macaque multi-tissue dataset, used to assess the function of TMS in a NHP with a direct comparison to RRBS data generated from these same samples (see also S8 Fig). https://doi.org/10.1371/journal.pgen.1011667.s016 (XLSX) https://doi.org/10.1371/journal.pgen.1011667.s017 (DOCX) Percentage of cytosines in a CHH context marked as methylated (an estimate of conversion efficiency) for varying (A) plexing strategies, and (B) input amounts. The dashed line refers to 1% CHH methylation and the solid line refers to 5% CHH methylation, a common cut off indicative of high levels of unmethylated cytosine conversion. https://doi.org/10.1371/journal.pgen.1011667.s018 (TIFF) (A) Average coverage per CpG site passing filters in a given experiment. Prior to calculations, CpG sites were filtered to include only sites within 200 bp of target probes and those with>5X coverage in more than 75% of samples. (B) Average read depth, in terms of paired-end reads, generated per sample in each experiment. https://doi.org/10.1371/journal.pgen.1011667.s019 (TIFF) Power analyses conducted on data for 1,000 simulated CpG sites (per sample size, effect size, and coverage combination) using the coverage distributions of observed, 96-plex TMS data. Lines represent the power to detect a 0–20% difference in methylation between two groups at a nominal p-value threshold < 0.001. Colors represent different levels of mean coverage per site (20x, 40x, and 80x) and facets represent sample sizes of n = 100, n = 200, and n = 400. https://doi.org/10.1371/journal.pgen.1011667.s020 (TIFF) Number of CpG sites within 200 bp of target probes after filtering for>5X coverage in more than 75% of samples by experiment. Colors are representative of each experiment which are defined in Fig 1C. https://doi.org/10.1371/journal.pgen.1011667.s021 (TIFF) Site-level methylation averaged across 3 samples processed using mechanical fragmentation, enzymatic fragmentation for 10 minutes, and enzymatic fragmentation of 20 minutes. Each point represents a site measured across both fragmentation methods and R2 values were generated using linear modeling. https://doi.org/10.1371/journal.pgen.1011667.s022 (TIFF) We subset the mapped read files for each sample (n = 88) included in our enzymatic fragmentation experiment (experiment 4) to include a random subset of 25, 50, or 75% of the total reads. We calculated the average (A) and median (B) coverage for on-target sites (y-axis) and observed a linear relationship between coverage and the number of subset reads (x -axis), which is useful for estimating what sequencing depth per sample will be needed to obtain various degrees of coverage. https://doi.org/10.1371/journal.pgen.1011667.s023 (TIFF) DNA methylation levels (A: n = 115,982 matched CpG sites; B: n = 575,401 matched CpG sites) averaged across 55 VUMC samples processed using TMS and the EPIC v2 array. Each point represents a site measured across both processing methods and R2 values were generated using linear modeling. https://doi.org/10.1371/journal.pgen.1011667.s024 (TIFF) For samples processed using both TMS and the EPIC array (n = 55 VUMC samples), we assessed the correlation in site-level average methylation levels for (A) hypomethylated regions (<50% average methylation); (B) intermediately methylated regions (average methylation above 10% and below 90%); (C) hypermethylated regions (>50% methylation); (D) UCSC-annotated CpG islands; (E) UCSC-annotated CpG shores (i.e., regions within 2kb of the boundaries of a CpG island); and (F) UCSC-annotated CpG shelves (regions within 2kb and 4kb of the boundaries of a CpG island R2 values were calculated using linear modeling; sample sizes represent the number of CpG sites included in each panel. https://doi.org/10.1371/journal.pgen.1011667.s025 (TIFF) For samples processed using both TMS and the EPIC array, we assessed the correlation in site-level methylation for variable sites (methylation >0.1 and <0.9) and all sites after permuting sample ID randomly. R2 values were generated using linear modeling. https://doi.org/10.1371/journal.pgen.1011667.s026 (TIFF) (A) Density plot showing the average methylation of a site (i.e., across samples) for filtered (>5X coverage in >75% of sites) sites captured between the three technologies (726,597 EPIC Array sites; 4,990,351 TMS sites; and 5,000,659 WGBS sites). Sites were not matched between the three technologies. (B) Average coverage per site of sites captured by WGBS after filtering for>5X coverage in >75% of samples. Median average coverage is 24.0X. https://doi.org/10.1371/journal.pgen.1011667.s027 (TIFF) The majority of individuals had 4 + tissues represented in the dataset. https://doi.org/10.1371/journal.pgen.1011667.s028 (TIFF) Percentage of cytosines in a CHH context marked as methylated (an estimate of conversion efficiency) following optimized TMS using genomic DNA from capuchins, geladas, and macaques. The dashed line refers to 1% CHH methylation and the solid line refers to 5% CHH methylation, a common cut off indicative of high levels of cytosine conversion. https://doi.org/10.1371/journal.pgen.1011667.s029 (TIFF) Site-level DNA methylation estimates averaged across 96 rhesus macaque samples processed using TMS and RRBS. Each point represents a site measured across both fragmentation methods and R2 values were generated using linear modeling. RRBS enriches for CpG dense regions of the genome, which tend to be hypomethylated. https://doi.org/10.1371/journal.pgen.1011667.s030 (TIF) Sites filtered for>5X coverage in >75% of samples processed using a given technology. A greater number of sites are covered consistently across all 96 samples using TMS compared to RRBS. https://doi.org/10.1371/journal.pgen.1011667.s031 (TIFF) Each point represents the average methylation at a given site for 88 samples that were processed using both pipelines (R2 = 0.9972). https://doi.org/10.1371/journal.pgen.1011667.s032 (TIFF) For samples processed using both TMS and RRBS, we assessed the correlation in site-level methylation for all sites after permuting sample ID randomly and compared them to non-permuted, or matched, sample IDs. R2 values were generated using linear modeling. Using a t.test, we found a significant difference between the means of the two samples (t = 7.6796, p-value = 8.224 x 10–13, mean of matched samples: 0.8345, mean of permuted samples: 0.7508). https://doi.org/10.1371/journal.pgen.1011667.s033 (TIFF) First, we thank the Tsimane and Orang Asli study participants and communities for their involvement and support. We also thank all members of the research teams associated with the Tsimane Health and Life Histories Project, the Orang Asli Health and Lifeways Project, the Simien Mountains Gelada Research Project, and the Caribbean Primate Research Center Cayo Santiago Field Station. Second, we thank the members of the Lea Lab at Vanderbilt University, the SMack Lab at Arizona State University, and the Vanderbilt Technologies for Advanced Genomics (VANTAGE) team for their feedback, expertise, and support in completing this work. Third, we are grateful to the research infrastructure provided by Vanderbilt University’s Advanced Computing Center for Research and Education and Arizona State University’s Sol Computing clusters. Fourth, we thank Dr. Rex Howard and Dr. Michael Hart, the GSU veterinarians, for their assistance in collecting the capuchin blood samples. The Cayo Biobank Research Unit Scientific Stakeholders are: Susan Antón, Lauren Brent, James Higham, Melween Martínez, Amanda Melin, Michael Montague, Michael Platt, Jerome Sallet, and Noah Snyder-Mackler.
--------------------------------------------------

Title: Sam Altman's $6.4 billion bet on Jony Ive has Zuckerberg-like qualities
URL: https://www.cnbc.com/2025/05/22/sam-altmans-6point4-billion-bet-on-jony-ive-zuckerberg-like-qualities.html
Time Published: 2025-05-22T12:00:01Z
Description: OpenAI is making a big bet in purchasing Jony Ive's nascent startup, but it's a deal reminiscent of some made by Mark Zuckerberg.
--------------------------------------------------

Title: D&AD Awards 2025: who won what?
URL: https://www.creativeboom.com/news/dad-awards-2025-who-won-what/
Time Published: 2025-05-22T11:52:00Z
Full Content:
With the judging process changing dramatically this year, which agencies won big in the D&AD Awards 2025? With the judging process changing dramatically this year, which agencies won big in the D&AD Awards 2025? For many, it's the biggest and most nerve-wracking night in the creative calendar. D&AD held its 63rd awards ceremony last night (Thursday 22 May) at London's Southbank Centre and handed out a record-breaking 668 Pencils across all categories. Most significantly, three coveted Black Pencils—the highest D&AD accolade—were awarded to groundbreaking work that exemplified the year's themes of commercial impact and narrative excellence. With no quota system for Pencil awards, D&AD maintains its reputation for uncompromising standards. In some years, no Black Pencils are given out at all, making the awarding of three a significant achievement for these winners: Designing Paris 2024 by W Conran Design won in Graphic Design for its innovative approach to sports marketing. The judges highlighted how the design created a unifying yet distinctive feel that successfully merged heritage elements with modern sporting aesthetics. A$AP Rocky - Tailor Swif by Iconoclast LA won in Music Videos, exemplifying the year's emphasis on narrative storytelling. The winning work featured long-form storytelling where music was integral to the narrative structure rather than an afterthought, demonstrating how narrative arcs can transcend different mediums and artistic disciplines. Spreadbeats by FCB New York won in Digital Marketing for what the agency described as "Innovation through nostalgic technologies." This work aligned with a trend of brands cleverly inserting themselves into events, sports, and games through inventive approaches. This year's judging process marked a major evolution in D&AD's approach. Jurors emphasised the critical importance of commercial viability, seeking work that demonstrates tangible business impact and drives meaningful behavioural change rather than celebrating creativity for its own sake. In other words, if there was ever a danger that agencies would make work primarily to win awards rather than serve the client, that's now history. In 2025, you have to do both. "This year's awards celebrated the power of design not just as a form of art but as a catalyst for commercial success and behavioural change," noted Dara Lynch, CEO of D&AD. "The idea is that innovative ideas must possess both aesthetic appeal and tangible impact." The Black Pencil winners perfectly embodied this philosophy. For instance, W Conran Design's 'Designing Paris 2024' was recognised for illustrating how design thinking can transform an entire city's attitude and behaviour. Judges from the Graphic Design category praised it as "a breakthrough for sports marketing and traditional sports marketing aesthetics," noting its playful yet scalable approach that successfully blended heritage with contemporary sport. Alongside commercial effectiveness, jurors were looking for exceptional craft. From Radio and Audio to Film, their message was clear: in an increasingly automated creative landscape, looking good simply isn't enough. "The resurgence of craftsmanship stands as a reminder that in an era of automation, true excellence lies in the thoughtful execution of ideas," Dara explained. "It's not enough just to look good; true creative excellence must also leave a lasting impression." This was particularly evident in D&AD's new Creator Content category, where judges called for clearer definitions of craft excellence and higher standards in content creation. The winning entries all demonstrated work that creates meaningful impact and genuinely engages audiences. The 2025 D&AD Awards achieved unprecedented global reach, with entries submitted from 86 countries worldwide—the highest number in the awards' history. This international participation resulted in over 30,000 pieces of work from 11,689 total entries. "It's exciting to see so many brands and companies refreshing their identities," said D&AD trustee Lisa Smith, global executive creative director at Jones Knowles Ritchie. "Judging has been challenging. Too many entries follow the same established design codes and trends, making everything start to look and feel alike, regardless of category. The work that stood out was the kind that breaks away from the expected: inspiring, well-crafted, and truly fit for purpose." The awards ceremony also recognised outstanding agency performance across various specialisations. FCB New York claimed the prestigious title of Advertising Agency of the Year, while Serviceplan Design was named Design Agency of the Year for the first time. DIVISION continued their remarkable winning streak, earning Production Company of the Year for the fifth consecutive year. In the network categories, Serviceplan took home Independent Network of the Year, while FCB was also named Network of the Year. Apple received recognition as Client of the Year, acknowledging its commitment to creative excellence across its marketing initiatives. The 2025 D&AD President's Award went to Koichiro Tanaka, founder of interdisciplinary creative boutique Projector. Chosen by D&AD President Kwame Taylor-Hayford, Tanaka was recognised for his pioneering work at the intersection of storytelling, interactivity, and craft, which helped define a formative era in digital creativity. "I'm very honoured," Tanaka said. "To receive this award from D&AD, which has lived longer than I have, and to feel part of its long story is now something I live with. Being recognised with care and an open heart is a special thing to me." Kwame, who is co-founder of Kin, explained his choice: "Koichiro Tanaka's pioneering work at the intersection of storytelling, interactivity, and craft helped define a formative era in digital creativity. His career journey, bold ideas, and meticulous attention to detail have been a constant source of personal inspiration." Beyond the three Black Pencils, the ceremony awarded three White Pencils, 48 Yellow Pencils, 176 Graphite Pencils and 434 Wood Pencils. Additionally, four Future Impact Pencils were awarded, recognising work that demonstrates potential for significant positive change. The diverse range of winning work spanned 44 categories, from traditional advertising and design to emerging areas like Gaming & Virtual Worlds and the newly introduced Creator Content category. Craft categories collectively garnered 198 Pencils, underscoring the year's emphasis on exceptional execution. The complete list of Pencil-winning work and shortlisted entries are showcased on the D&AD website, while the official D&AD Rankings will be released alongside the digital D&AD Annual. These rankings provide definitive tables of the most successful companies, networks, countries and clients based on the awards results. Get the best of Creative Boom delivered to your inbox weekly Creative Boom empowers and uplifts the creative community. Since 2009, we've been delivering the best in creativity through news, inspiration, insights, and advice to help you stand out and succeed in a competitive industry. Creative Boom™ © 2025 Creative Boom Ltd. Registered in England and Wales #07437294.
--------------------------------------------------

Title: Ryan Reynolds-Linked MNTN Goes Public at $16 Per Share to Grow Connected TV Ads
URL: https://www.adweek.com/convergent-tv/ctv-ryan-reynolds-mntn-ipo-public-16-per-share/
Time Published: 2025-05-22T11:21:40Z
Full Content:
We deliver! Get curated industry news straight to your inbox. Subscribe to Adweek newsletters. Connected TV advertising firm MNTN is making its market debut on the New York Stock Exchange. The company announced a $16-per-share IPO on May 21, the high end of its expected range. The share price puts MNTN’s valuation at around $1.2 billion. Actor Ryan Reynolds has served as MNTN’s chief creative officer since the company bought his creative shop, Maximum Effort, in 2021. MNTN divested from Maximum Effort in March, but Reynolds stayed on as CCO. However, Reynolds didn’t travel to New York for the IPO this week, MNTN founder and chief executive officer Mark Douglas told ADWEEK. MNTN aims to capitalize on the demand the company has seen for performance-focused CTV ad placements by going public, Douglas said. MNTN’s advertisers are primarily small and medium-sized businesses, with 96% using the ad platform to experiment with TV advertising for the first time. According to bankers’ estimates, there was demand for 14 times the shares that MNTN had available to sell, Douglas said, which helped bolster the case for an IPO. “The market responded,” he said. “A lot of our customers depend on Google, Meta, and now MNTN to help drive the growth in their business. So we want to be transparent in terms of the strength of our business, [in terms of] scale and so forth, to those companies.” MNTN’s offering consists of 11.7 million shares in total, with 8.4 million from the company itself and another 3.3 million from existing stockholders. Stockholders also gave underwriters—which include Morgan Stanley, Citigroup, and Evercore—the option to buy an additional 1.8 million shares at the initial price within 30 days of the IPO. While MNTN didn’t make an overall profit in 2024, adjusted earnings before interest, taxes, depreciation, and amortization (also known ads EBITDA) were $39 million, Douglas noted. He expects that to increase in 2025. MNTN Is Selling Ryan Reynolds’ Agency Maximum Effort, SEC Filing Shows As it eyes the next phase of growth, MNTN plans to continue focusing on small and medium-sized businesses, Douglas said. The company will also invest in product innovation using artificial intelligence. “We like large brands also—we have some of those,” Douglas said about MNTN’s mix of clients. “But in the time it takes to get one of those, you can add 6,000 smaller brands on the platform.” Douglas sees MNTN’s role as a matchmaker rather than as an advertising company, connecting consumers with brands and products they might love through 30-second ads on popular streaming platforms like HBO Max, Bravo, and ESPN. “It’s a really impactful medium,” Douglas said. Douglas founded the company in 2009 as SteelHouse, rebranding to MNTN in 2021. Kathryn Lundstrom is Adweek's sustainability editor.
--------------------------------------------------

Title: 8 Best AI SEO Tools for 2025 (Tested Firsthand)
URL: https://www.semrush.com/blog/best-ai-seo-tools/
Time Published: 2025-05-22T10:43:00Z
Full Content:
As a content marketer, I regularly use AI SEO software to analyze data faster, optimize content, and achieve higher rankings. But with so many tools making bold claims, it's hard to separate the genuinely useful from the overhyped. That's why I’ve tested dozens of AI SEO tools myself and narrowed them down to the 8 that actually deliver results. Here's my detailed review of each. Spoiler alert: here are my favorite AI tools for SEO: 1. ContentShake AI for generating SEO-friendly content 2. Semrush Copilot for personalized SEO recommendations 3. Clearscope for SEO content optimization 4. SurferSEO for advanced SEO content creation 5. ChatGPT for brainstorming and data analysis 6. Copy.ai for automating your SEO workflows 7. SERP Gap Analyzer for finding keyword opportunities 8. Perplexity for conducting online content research Let’s look at each of them in detail. Semrush Content Toolkit is the smart writing tool we've developed at Semrush. It combines AI with our proprietary SEO data to help you create entire articles, find content ideas, and write in your unique brand voice. I recommend using this tool for creating SEO-friendly drafts, especially if you’re not an SEO expert. I love that it finds your target keywords and analyzes SERPs so that it can optimize your content for search intent—something SEO beginners usually struggle with. Let’s look at a sample workflow. First, enter a high-level content idea and get dozens of data-driven keyword and topic suggestions. Next, click ‘Start writing’ and choose whether you want to create an SEO content brief or generate a full blog post from scratch. This will open the setup window, where you can adjust your target keywords, content length, and brand voice. Check the ‘Add an extra SEO boost’ box for advanced SERP analysis and click ‘Create article’. In just a few minutes, your draft will be ready. You can then use the blog editor to add more original ideas and further enhance your content. Content Toolkit also includes an integrated AI chat, an AI image generator, and a content optimization tool to improve your copy. Pros Cons Very easy to use—ideal for beginners with little SEO experience Less suited to large teams and enterprise users Uses Semrush’s data to optimize AI-generated content Some stock images suggested by the tool may not always be relevant Covers the entire content creation cycle, from ideation to publishing Semrush Copilot is an AI-powered assistant that offers personalized recommendations based on your SEO performance. Copilot analyzes all of your Semrush data from tools like Site Audit, Backlink Gap, and Keyword Gap.. It then consolidates the issues and action items into tailored recommendation cards, related to your keyword rankings, domain authority, organic competitors, and more. I use it to regularly check SEO recommendations and spot potential issues, such as with technical SEO or backlinks. When I see a recommendation or alert, I head to each specific tool to investigate it further: Pros Cons Makes it easy to prioritize action items for SEO performance Exclusive to Semrush users: not compatible with other tools Helps navigate the many tools in Semrush’s suite While the reports provide a lot of useful data, some users may want more in-depth summaries Offers daily alerts to keep you posted about real-time concerns Converts data into actionable insights in order of priority Clearscope is an AI content platform that helps with keyword research, content optimization, and analytics. The content optimization feature offers somewhat similar benefits to other AI content tools. But the workflow is different. When you add an existing URL and your target keyword, the tool will analyze the published page for you. It will then provide suggestions for improvement, such as: The tool also helps with topic research and generates basic SEO reports. For example, it tracks your ranking positions and provides high-level content analytics metrics. Pros Cons Automates the process of building content briefs and optimizing content More expensive than most SEO content creation tools and has no free trial Keep tabs on published content to find opportunities to refresh content Doesn’t offer many content creation possibilities Provides content analytics, which simplifies performance analysis Content analytics and keyword research tools are somewhat basic, and the content grade simply checks if you’ve covered all related keywords SurferSEO offers a suite of SEO tools to discover keywords, find content ideas, create outlines, optimize content, and more. Many content teams use it for content optimization. Similar to Clearscope, it analyzes the organic results for your target keywords and makes content structure recommendations. Surfer also offers some useful features for search engine optimizers, including a topical map, an SERP analyzer, and a content audit tool. A word of caution: I’d be careful with metrics like optimal content length and the recommended number of headings. While they might be useful suggestions, you’ll know best if they make sense for your particular content piece. Finally, while you can generate full articles, they each cost an additional $19, with a maximum of 30 articles on the cheapest plan. Pros Cons Easy to use when working with content writers, as it helps to track keywords in your drafts Might not be the best choice for beginners and folks looking for more automation as it involves more steps and requires more SEO experience Sleek and modern UI Limited credits make it difficult to use freely The Google Docs extension makes it easy to work anywhere The Google Docs extension can get a bit buggy Lots of data-driven tools useful for SEOs and content marketers The Content Editor doesn’t let you adjust your targets 💡 Find out who comes out on top in our ContentShake AI vs SurferSEO comparison. In the screenshot above, ChatGPT claims it can do a variety of SEO tasks. But I would take this with a pinch of salt. ChatGPT, along with other AI chatbots like Gemini and Claude, is amazing for brainstorming and analysis. But it doesn’t have any real-time SEO data and is not adapted for long-form content creation. So, I’d only use it together with an actual SEO tool that has access to SERP data. That said, ChatGPT is useful for other SEO tasks such as: Let’s look at an example. I downloaded a list of keywords from Semrush, uploaded the CSV file to ChatGPT, and asked it to group my keywords by category: Here’s the result: This is just one of many examples of how ChatGPT can save you time with SEO tasks—as long as you already have the data you need. Pros Cons Powerful AI with integrated data analysis features Doesn't offer real-time SEO data and specialized SEO output ChatGPT's best model (ChatGPT 4-o) is free for all users The interface is not optimized for long-form content creation Beginner-friendly interface and affordable price point Prone to factual errors and inaccurate responses 💡 Check out how ChatGPT compares to ContentShake AI. Copy.ai is an AI-powered automation platform with content creation tools for sales and marketing needs. Like other AI writing tools, it was originally used primarily for content creation. But it has recently shifted its focus towards automation. Copy.ai's new standout feature is its ability to create multi-step workflows for SEO processes. You can use the library of workflow templates for different use cases, such as writing a blog post, an SEO brief, and so on. The tool also lets you design custom workflows to automate different parts of your SEO setup. Here’s what it looks like: Pros Cons The unique workflow creation feature works great for enterprise SEO teams and large agencies with lots of processes It’s somewhat hard to use and might take a while to figure out Offers a sleek and modern UI Doesn’t offer any SEO data Integrates with 2,000+ tools SERP Gap Analyzer helps you discover low-difficulty keywords for your content plan. You can use it to find relevant keywords with higher search volume and less competition. This is especially helpful if you’re in a saturated niche and have to compete with lots of high-authority websites. Just enter your site and a seed topic to run an analysis. The tool then analyzes the search results for multiple relevant keywords and displays the ranking difficulty for each keyword. You can access other insights such as global search volume, SERP weaknesses, and more. Pros Cons Solves one of the key SEO pains—finding high-potential content topics Doesn’t integrate with most other SEO tools, except Semrush Generates briefs, outlines, and other content Can be a bit confusing to figure out Documents SERP analysis in detail to guide SEO efforts Perplexity is a conversational AI tool that’s connected to the Internet. It lets you parse the web, find information quickly, and conduct deep web research. Wondering why I included this tool in a list of SEO software? Because effective SEO is closely connected to quality content. And quality content needs fresh information and useful examples. I often use Perplexity to find out in minutes what could otherwise take hours of scouring web sources. For example: imagine I need to find an example of a content marketing campaign for small businesses for an article I’m writing. Perplexity will use various online sources and instantly suggest interesting ideas: Pros Cons Conducts image and video search There’s a limited number of questions that you can ask in a day on the free plan Organizes research with collections of multiple threads Limited to research and doesn’t serve very well for content creation Performs focused research on specific sites, such as YouTube, Reddit, etc. Sometimes hallucinates AI SEO tools use artificial intelligence to automate and fast-track SEO processes, such as keyword research, SERP analysis, and content creation. They can help you make better data-based decisions, increase productivity, and save time on routine tasks. Earlier this year, I surveyed over 2,500 small businesses and asked them about their experience with AI. And guess what? 68% of companies report higher SEO and content marketing ROI when using AI. There has been a lot of talk lately about Google “penalizing” AI-generated content. Let’s get this sorted once and for all. Google doesn’t care if it’s AI copy or not. It cares only about your content being genuine, useful, and relevant. Here’s what else they say: “Poor quality content isn't a new challenge for Google Search to deal with. We've been tackling poor quality content created both by humans and automation for years.” We’ve further proven that AI-generated content ranks nearly as well as human-written copy—as long as it offers value and original insights. I wouldn’t recommend relying solely on AI to create content for SEO and content creation. Why? Because AI doesn’t have the unique topical expertise that you need to create compelling content. But using AI as a tool to create content under your supervision and with your input? Or leveraging it to generate content ideas, cluster keywords, and analyze SEO data? Why not! That’s a wrap on my weeks of testing and shortlisting my favorite AI SEO tools. The key takeaway? Choose a tool that suits your specific needs and context. For example: Remember: there’s a limit to what AI tools can do. Use them to get rid of routine tasks, and to support your work, not replace it, and you won’t be disappointed. Margarita Loktionova Keyword search volume is the average number of monthly searches for a search term in a particular location. Google Keyword Planner is a free tool that lets you research the queries people type into Google. Learn how to get backlinks by responding to media requests, creating link bait, finding broken links, & more.
--------------------------------------------------

Title: Did WhatsApp really need Meta?
URL: https://www.theverge.com/antitrust/671831/meta-whatsapp-founder-brian-acton-testimony-antitrust-trial-ftc
Time Published: 2025-05-21T18:41:02Z
Full Content:
Cofounder Brian Acton butted heads with Meta before his departure, but took the stand as part of its defense. Cofounder Brian Acton butted heads with Meta before his departure, but took the stand as part of its defense. by Lauren Feiner In its antitrust case against Meta, the US Federal Trade Commission is asking a judge to consider an alternate reality. In that world, the company never bought Instagram and WhatsApp. The two apps remained competitive with Facebook, developing features that competed for users’ attention. And that competition created a thriving ecosystem of social media apps where people can connect with their friends and family. Meta has spent the past several days — during which it’s begun lodging its case-in-chief in a Washington, DC, courthouse — building a counternarrative. In its telling of this alternate present, Instagram and WhatsApp are shadows of what they are in our world. They lacked the resources, expertise, and vision to become robust and valuable online platforms, let alone formidable competitors. And consumers are the ones who ultimately suffered. One of Meta’s key witnesses for this defense is WhatsApp cofounder Brian Acton, who was called on Tuesday to help make its case that WhatsApp users, just like Instagram ones, benefited from Meta’s acquisition. Acton was the second app founder to testify in the case, after Instagram cofounder Kevin Systrom delivered mostly blistering testimony against the company a few weeks ago. Acton’s time on the stand came off less acrimonious, though both Meta and the FTC scored some key points. Acton was a striking witness for Meta to call given his high-profile departure from the company in 2017. The cofounder left $800 million in unvested restricted stock units on the table after butting heads with top Meta executives over putting ads in WhatsApp (when an FTC attorney pointed out the stock would have been worth $4 billion today, he joked, “please don’t say that,” but reassured himself it would only be $2 billion after taxes). The next year, he publicly advocated for people to delete Facebook in the aftermath of the Cambridge Analytica data scandal. Acton reaffirmed that he had absolutely no interest in building a feed into WhatsApp But Acton backed up some important claims Meta has been making throughout the trial. Meta has repeatedly argued that WhatsApp was unlikely to compete with Facebook in the social networking space, so it wasn’t just trying to take out a potential rival. Acton reaffirmed that he and cofounder Jan Koum had absolutely no interest in building social features like a feed into WhatsApp, or changing the company into an ad-supported business — even if their pre-acquisition investors wished they would. While the FTC has argued that WhatsApp could have succeeded on its own or with a different parent company, Acton said he and Koum rebuffed other offers, and felt that Meta’s infrastructure helped it skip over substantial work it would have had to do otherwise. On cross-examination, however, the FTC got some important admissions from Acton. Using Meta’s infrastructure might have helped it skip some steps, but Acton testified that WhatsApp didn’t actually migrate to Meta’s data centers to ward off outages — and WhatsApp had been highly capable of finding technical support for the app already. Instead, he said, he and Koum wanted to make sure Meta would continue operating the app even after they left. Far from being a bare-bones messaging app without Meta’s help, Acton testified that WhatsApp had already added several features before the acquisition like group messaging, video and audio messaging, and location sharing — with plans to add even more. WhatsApp was already growing incredibly fast prior to the acquisition, doubling in size every 12 to 18 months — that growth rate stayed pretty consistent even after Meta bought it. Acton was confident that even without Meta, WhatsApp would have grown from the more than 400 million monthly active users prior to the deal to one billion in about 18 months. This echoed testimony from Systrom that Instagram would likely still be successful without the sale. Meta CEO Mark Zuckerberg testified earlier in trial that he was surprised at how little interest WhatsApp’s founders had in building something larger than a “lifestyle company.” But while Acton reaffirmed his disdain for an ad-supported model that could pump up revenue, he conceded that he agreed to sell to Meta without securing a firm commitment against deploying ads, and he understood Meta’s offer price was likely based partly on plans to do so. That seemed to support the idea that the founders could have been open to monetizing their product more than they let on — potentially growing it into a rival for Meta. WhatsApp might have been an even better product — one that flourished in more markets with stronger privacy protections — without Meta’s stewardship, the FTC suggested. In a November 2014 email, it pointed out, a WhatsApp employee told Acton and Koum that executives at its new owner had “some reservation” about promoting the app in countries where Facebook Messenger was already a leader. Meta successfully pressured WhatsApp to change its privacy policy and terms of service in 2016 so that Facebook could capitalize on user data for its ads product (unless WhatsApp users opted out). And it pushed for a business version of the app, something Acton said he was “adamantly against,” fearing it would dilute WhatsApp’s end-to-end encryption. After Acton left, the product launched. A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe. © 2025 Vox Media, LLC. All Rights Reserved
--------------------------------------------------

Title: Tariff Truce and Liquidity Fuel Bitcoin to $109,000 All-Time High — What’s Next for BTC?
URL: https://zycrypto.com/tariff-truce-and-liquidity-fuel-bitcoin-to-109000-all-time-high-whats-next-for-btc/
Time Published: 2025-05-21T17:45:31Z
Full Content:
Bitcoin price rallied to a new all-time high above $109,000 on May 21, propelled by a sudden thaw in global trade tensions and expectations of looser monetary policy. The cryptocurrency traded around $109,369 on late trading, extending a rally that lifted prices about 6% in the past week. This move follows a breakthrough above $100,000 earlier in May, as investors responded to signs that President Trump’s global tariff offensive might be easing. The immediate catalyst was a surprise 90-day “tariff truce” between the United States and China. On May 12, officials in Geneva announced a pause on new levies and sharp rollbacks of existing duties, slashing reciprocal tariffs by roughly 115 percentage points to about 10%. Global markets leaped on the news: stock futures and Asian equities jumped, and commodity prices rallied on hopes of a broad trade-war détente. Bitcoin investors also took notice. Analysts note that the tariff pause removed the risk of ‘sudden re‑escalation’, which had been weighing on risk appetite. In other words, as the trade conflict eased, risk assets like Bitcoin rallied without the overhang of fresh tariffs. Broader macroeconomic conditions have added fuel to the crypto rally. Central banks worldwide are signaling easier policy. In the U.S., futures markets are now priced at multiple rate cuts this year, and a weaker dollar has coincided with renewed growth in global money supply. Meanwhile, the European Central Bank has maintained an accommodative stance, and the Bank of Japan continues massive asset purchases, reinforcing the loose global liquidity backdrop. China’s central bank has also begun pumping cash into markets. Such ample liquidity typically lifts risk assets, and Bitcoin has benefited accordingly. At the same time, institutional investors have been pouring into crypto. Hedge funds and asset managers have funneled record sums into spot-Bitcoin ETFs and futures, betting on higher prices. Leading figures in the crypto industry have expressed firm bullish conviction. Former PayPal and Meta executive David Marcus tweeted, “the bull case for Bitcoin has never been stronger,” citing easier custody solutions at banks and competition from sovereign and corporate buyers. Changpeng Zhao, co-founder of Binance, echoed this optimism. Analysts even project Bitcoin could reach around $150,000 if regulatory conditions remain favorable. Looking ahead, analysts are divided on just how far Bitcoin can climb. Some have raised year-end targets above $130,000. Jamie Coutts of Real Vision warns that the relentless expansion of fiat money means Bitcoin “may push…above $132,000” by late 2025. Even major banks have revised their models: Standard Chartered’s crypto research team recently suggested that a $120,000 second-quarter price target might be “too low”. On the other hand, many caution that the rally could pause or correct if global liquidity peaks or regulators tighten. Bitcoin’s price historically follows four-year “halving” cycles tied to new coin issuance, and some forecasters see the current monetary easing lasting through 2025. Those forces could cool Bitcoin’s momentum if central banks eventually reverse course or inflation spikes. For now, however, the rare alignment of easing trade tensions and abundant liquidity, coupled with growing institutional adoption, has set the stage for one of crypto’s strongest rallies.
--------------------------------------------------

Title: Irish stock market closes at fresh all-time high
URL: https://www.irishtimes.com/business/markets/2025/05/21/irish-stock-market-closes-at-fresh-all-time-high/
Time Published: 2025-05-21T17:20:44Z
Full Content:
Despite retail stocks weighing down the market, European stocks were little changed on Wednesday. General stocks gains limited the losses as investors reacted to changes in US taxation policies. The pan-European STOXX 600 closed slightly lower, down 0.8 per cent, as Dublin saw a marginal day of trading. Dublin While fallers outnumbered risers, the Iseq All-Share index ended the session slightly up 25.31 or 0.22 per cent from its previous close, at 11,399.48, its highest ever close. There was a mixed performance in the banking sector. Bank of Ireland rose 0.89 per cent to €11.995, following on from a strong performance on Tuesday. Fellow banking stocks, Permanent TSB dropped 0.58 per cent and AIB Group fell 0.45 per cent. Defensive stocks also had a mixed day of trading. While Kerry Group added 0.67 per cent, rising to a share price of €97.35, fellow dairy giant Glanbia fell 0.24 per cent. Healthcare services group Uniphar saw a 1 per cent fall in its share price. Ryanair, which rose 5 per cent after publishing financial results on Monday, saw a third consecutive rise this week adding 0.63 per cent, reaching €23.85 per share. London The FTSE 100 was nearly flat with a 0.06 per cent gain, while the midcap FTSE 250 fell 0.7 per cent, as the inflation data triggered a slight wobble on the more domestically-focused index. Among blue-chips, sportswear retailer JD Sports’ shares were the worst hit, dropping 10.6 per cent after it warned that President Donald Trump’s tariffs may force the company to hike prices in the key market. SSE fell 2.4 per cent after the renewable energy generator cut its five-year investment plans by 15 per cent. Keeping losses in check, precious metal miners gained 3 per cent as gold prices rose for a third straight session and hit a one-week high. Fresnillo gained the most in the FTSE 100 with 4.2 per cent rise. Water utility Severn Trent shares gained 2.3 per cent after it projected a doubling of adjusted earnings per share between 2025 to 2028. Europe The European benchmark shed 0.8 per cent amid declines in luxury and retail stocks. JD Sports fell 10.6 per cent to the bottom of the STOXX 600 after posting a 2 per cent drop in first-quarter underlying sales and warned that higher prices in its key U.S. market could hit customer demand. LVMH, Hermes and Kering among others fell over 2 per cent after luxury group Chanel reported a 4.3 per cent drop in its comparable yearly sales. Tech shares helped to outweigh these losses, German chipmaker Infineon’s 2.3 per cent gain after it said it would work with Nvidia to develop chips for new power delivery systems inside artificial intelligence data centres provided a boost to the sector. The STOXX 600, however, has recovered from its April slump, and is trading less than 3 per cent away from its all-time highs. An index tracking defence stocks was up 0.5 per cent after Trump selected a design for the $175 billion Golden Dome missile defence shield on Tuesday. Morgan Stanley raised its view on the European banking sector to “attractive”, citing better earnings potential from continued yield steepening. The European banks index is among the top performing sectors this year. Swiss bank Julius Baer slid 4.9 per cent after reporting a 130 million Swiss franc ($156.4 million) charge from a credit portfolio review and replacing its chief risk officer. New York In a volatile session on Wall Street, the S & P500 had slipped with Treasury yields rising in reaction to the US President Donald Trump’s proposed tax-cut law during mid afternoon trading. Boosting the Nasdaq, Google-parent Alphabet recorded a jump, while Nvidia and Meta Platforms climbed saw stocks rise slightly. Nine of the 11 S&P sub-sectors traded lower, with Healthcare being the worst hit. UnitedHealth Group saw its shares fall, following a Guardian report said the healthcare conglomerate secretly paid nursing homes thousands in bonuses to help reduce hospital transfers for ailing residents. HSBC also downgraded the stock to “reduce” from “hold”. In earnings, retailer Target fell after slashing its annual forecast due to a pullback in discretionary spending. Wolfspeed had lost nearly 70 per cent during mid afternoon trading following a report that the semiconductor supplier was preparing to file for bankruptcy within weeks. Despite the losses, U.S. stocks have had a solid month so far. The S&P 500 has climbed more than 17 per cent from its April lows, when Trump’s reciprocal tariffs roiled global markets. On the back of bitcoin recording a fresh all-time high of $109,481.83 (€96,460.61) during the session, exchange operator Coinbase gained alongside crypto miners such as Riot Platforms. – Additional reporting by Reuters. Join The Irish Times on WhatsApp and stay up to date Sign up to the Business Today newsletter for the latest new and commentary in your inbox Listen to Inside Business podcast for a look at business and economics from an Irish perspective Get the latest business news and commentary from our expert business team in your inbox every weekday morning © 2025 The Irish Times DAC
--------------------------------------------------

Title: The Era of the Business Idiot
URL: https://www.wheresyoured.at/the-era-of-the-business-idiot/
Time Published: 2025-05-21T16:43:19Z
Full Content:
Fair warning: this is the longest thing I've written on this newsletter. I do apologize. Soundtrack: EL-P - $4 Vic Listen to my podcast Better Offline. We have merch. Last week, Bloomberg profiled Microsoft CEO Satya Nadella, revealing that he's either a liar or a specific kind of idiot. The article revealed that — assume we believe him, and this wasn’t merely a thinly-veiled advert for Microsoft’s AI tech — Copilot consumes Nadella’s life outside the office as well at work. None of these tasks are things that require you to use AI. You can read your messages on Outlook and Teams without having them summarized — and I’d argue that a well-written email is one that doesn’t require a summary. Podcasts are not there "to be chatted about" with an AI. Preparing for meetings isn't something that requires AI, nor is research, unless, of course, you don't really give a shit about the actual content of what you're reading or the message of what you're saying, just that you are "saying the right thing." To be clear, I am deeply unconvinced that Nadella actually runs his life in this way, but if he does, Microsoft’s board should fire him immediately. In any case, the article is rambling, cloying, and ignores Microsoft AI CEO Mustafa Suleyman's documented history of abusing his workers. Ten custom agents that do what? What do you mean by "other tasks"? Why are these questions never asked? Is it because the reporters know they won't get an answer? Is it because the reporters are too polite to ask more probing questions, knowing that these anecdotes are likely entirely made up as a means to promote a flagging AI ecosystem that cost billions to construct, but doesn’t really seem to do anything, and the reporter in question doesn’t want to force Satya to build a bigger house of cards than he needs to. Or is it because we, as a society, do not want to look too closely at the powerful? Is it because we've handed our economy to men that get paid $79 million a year to do a job they can't seem to describe, and even that, they would sooner offload to a bunch of unreliable AI models than actually do? We live in the era of the symbolic executive, when "being good at stuff" matters far less than the appearance of doing stuff, where "what's useful" is dictated not by outputs or metrics that one can measure but rather the vibes passed between managers and executives that have worked their entire careers to escape the world of work. Our economy is run by people that don't participate in it and our tech companies are directed by people that don't experience the problems they allege to solve for their customers, as the modern executive is no longer a person with demands or responsibilities beyond their allegiance to shareholder value. I, however, believe the problem runs a little deeper than the economy, which is a symptom of a bigger, virulent, and treatment-resistant plague that has infected the minds of those currently twigging at the levers of power — and really, the only levers that actually matter. The incentives behind effectively everything we do have been broken by decades of neoliberal thinking, where the idea of a company — an entity created to do a thing in exchange for money —has been drained of all meaning beyond the continued domination and extraction of everything around it, focusing heavily on short-term gains and growth at all costs. In doing so, the definition of a “good business” has changed from one that makes good products at a fair price to a sustainable and loyal market, to one that can display the most stock price growth from quarter to quarter. This is the Rot Economy, which is a useful description for how tech companies have voluntarily degraded their core products in order to placate shareholders, transforming useful — and sometimes beloved — services into a hollow shell of their former selves as a means of expressing growth. But it’s worth noting that this transformation isn’t constrained to the tech industry, nor was it a phenomena that occurred when the tech industry entered its current VC-fuelled, publicly-traded incarnation. In The Shareholder Supremacy, I drew a line from an early 20th-century court ruling, to former General Electric CEO Jack Welch, to the current tech industry, but there’s one figure I didn’t pay as much attention to, and I regrettably now have to do so. Famed Chicago School economist (and dweller of Hell) Milton Friedman once argued in his 1970 doctrine that those who didn’t focus on shareholder value were “unwitting pup­pets of the intellectual forces that have been undermining the basis of a free society these past decades," and that any social responsibility — say, treating workers well, doing anything other than focus on shareholder value — is tantamount to an executive taxing his shareholders by "spending their money" on their own personal beliefs. Friedman was a fundamentalist when it came to unrestricted, unfettered capitalism, and this zealotry surpassed any sense of basic human morality — if he had any — at times. For example, in his book, Capitalism and Friedman, he argued that companies should be allowed to discriminate on racial grounds because the owner might suffer should they be required to hire an equally or better-qualified Black person. Bear in mind, this was written at the height of the civil rights movement, just six years before the assassination of Martin Luther King, and when America was rapidly waking up to the evils of racism and segregation (a process, I add, is ongoing and sadly not complete). This is a direct quote: Friedman was grotesque. I am not religious, but I hope Hell exists if only for him. The broader point I’m trying to make is that neoliberalism is inherently selfish, believing that the free market should reign supreme, bereft of government intervention, regulation or interference, thinking that somehow these terms will enable "freedom" rather than a kind of market-dominated quasi-dictatorship where our entire lives are dominated by the whims of the affluent, and that there is no institution that can possibly push back against them. Friedman himself makes the facile argument that economic freedom — which, he says, is synonymous with unfettered capitalism — is a necessary condition of unfettered political freedom. Obviously, that’s bollocks, although it’s an argument that’s proven persuasive with a certain class of people that are either intellectually or morally hollow (or both). Neoliberalism also represents a kind of modern-day feudalism, dividing society based on whether someone is a shareholder or not, with the former taking precedence and the latter seen as irrelevant at best, or disposable at worst. It’s curious that Friedman saw economic freedom — a state that is non-interventionist in economic matters — as essential for political freedom, while also failing to see equality as the same. I realize this is all very big and clunky, but I want you to understand how these incentives have fundamentally changed everything, and why they are responsible for the rot we see in our society and our workplaces. When your only incentive is shareholder value, and you raise shareholder value as a platonic ideal, everything else is secondary, including the customer you are selling something to. Friedman himself makes a moral case for discrimination, because shareholder value — in his example, the store owner — matters more than racial equality at its most basic level. When you care only about shareholder value, the only job you have is to promote further exploitation and dominance — not to have happy customers, not to make your company "a good place to work," not to make a good product, not to make a difference or contribute to anything other than further growth. While this is, to anyone with a vapor of an intellectual or moral dimension, absolutely fucking stupid, it’s an idea that’s proven depressingly endemic among the managerial elite, in part because it has entered the culture, and because it is hammered across in MBA classes and corporate training seminars. In simpler terms, modern business theory trains executives not to be good at something, or to make a company based on their particular skills, but to "find a market opportunity" and exploit it. The Chief Executive — who makes over 300 times more than their average worker — is no longer a leadership position, but a kind of figurehead measured on their ability to continually grow the market capitalization of their company. It is a position inherently defined by its lack of labor, the amorphousness of its purpose and its lack of any clear responsibility. While CEOs do get fired when things go badly, it's often after a prolonged period of decline and stagnancy, and almost always comes with some sort of payoff — and when I say "badly," I mean that growth has slowed to the point that even firing masses of people doesn't make things better. We have, as a society, reframed all business leadership — which is increasingly broad, consisting of all management from the C-suite down — to be the equivalent of a mall cop, a person that exists to make sure people are working without having any real accountability for the work themselves, or to even understand the work itself. When the leader of a company doesn't participate in or respect the production of the goods that enriches them, it creates a culture that enables similarly vacuous leaders on all levels. Management as a concept no longer means doing "work," but establishing cultures of dominance and value extraction. A CEO isn't measured on happy customers or even how good their revenue is today, but how good revenue might be tomorrow and whether those customers are paying them more. A "manager," much like a CEO, is no longer a position with any real responsibility — they're there to make sure you're working, to know enough about your work that they can sort of tell you what to do, but somehow the job of "telling you what to do" doesn't come with it any actual work, and the instructions don’t need to be useful or even meaningful. Decades of direct erosion of the very concept of leadership means that the people running companies have been selected not based on their actual efficacy — especially as the position became defined by its lack of actual production — but on whether they resemble what a manager or executive is meant to look like based on the work that somebody else did. That’s how someone like David Zaslav, a lawyer by trade and arguably the worst CEO in the entertainment industry, managed to become the head of Warner Brothers (that, and kissing up to Jack Welch, who he called a “big brother” that “picked him up like a friend”). It’s how Carly Fiorina — an MBA by trade — went on to become the head of HP, only to drive the company into a ditch where it stopped innovating, and largely missed the biggest opportunities of the early Internet era. The three CEOs that followed (Mark Hurd (who was ousted after fudging expense reports to send money to a love interest and still got tens of millions of dollars in severance), Leo Apotheker (who the New York Times suggests may have been worse than Fiorina), and Meg Whitman (famous for being a terrible CEO at HP and co-founding doomed video startup Quibi) similarly came from a non-tech background, and similarly did a shitty job, in part because they didn’t understand the company or the products or the customers. Management has, over the course of the past few decades, eroded the very fabric of corporate America, and I'd argue it’s done the same in multiple other western economies, too. I’d also argue that this kind of dumb management thinking also infected the highest echelons of politics across the world, and especially in the UK, my country of birth and where I lived until 2011, delivering the same kind of disastrous effects but at a macro level, as they impacted not a single corporate entity but the very institutions of the state. I’m not naive. I don’t think that the average politician is a salt-of-the-earth type, someone who did a normal job and then decided to enter politics. Especially not in the UK, where the trappings of class permeate everything, and we’re yet to shake off the noxious influence of the aristocracy and constitutionally-mandated hereditary privilege. Our political elite often comes from one of two universities (Oxford and Cambridge, the alma mater of 20% of current UK Members of Parliament) and a handful of fee-paying schools (like Eton, which is a hellmouth for the worst people to ever exist, and educated 20 of the UK’s 55 prime ministers). The UK has never been an egalitarian society. And yet, things have changed markedly in the past few decades.The difference between now and then is that the silver-spooned elite was, whether because they believed it or because it was politically expedient, not totally contemptuous of those at the bottom of the economic ladder. I was born in the midst of the Thatcher government, and my formative years were spent as British society tried to restructure itself after her reforms. Thatcher, famously, was an acolyte of the Friedman school of thought, and spent her nearly twelve years in office dismantling the state and pushing the culture towards an American-style individualism, once famously quipping that there was “no such thing as society.” She didn’t understand how things worked, but was nonetheless completely convinced of the power of the market to handle what was the functions of the state — from housing to energy and water. The end result of this political and cultural shift was, in the long run, disastrous. The UK has the smallest houses in the OECD, the smallest housing stock of any developed country, and some of the worst affordability. The privatization of the UK’s water infrastructure meant that money that would previously go towards infrastructure upgrades was, instead, funnelled to shareholders in the form of dividends. As a result, Britain is literally unable to process human waste and is actively dumping millions of liters of human sewage into its waterways and coastline. When Britain privatized its energy companies, the new management sold or closed the vast majority of its gas storage infrastructure. As a result, when the Ukraine War sparked, and natural gas prices surged, Britain had some of the smallest reserves of any country in Europe, and was forced to buy gas at the market prices — which were several times higher than their pre-war levels. I’m no fan of Thatcher, and like Friedman, I too wish hell exists, if only for the both of them. I wrote the above to emphasize the consequences of this clueless managerial thinking on a macro level — where the impacts aren’t just declining tech products or white-collar layoffs, but rather the emergence of generational crises in housing, energy, and the environment. These crises were obvious consequences of decisions made by someone whose belief in the free market was almost absolute, and whose fundamentalist beliefs surpassed the actual informed understanding of those working in energy, or housing, or water. As the legendary advertiser Stanley Pollitt once said, “bullshit baffles brains.” The sweeping changes we’ve seen, both in our economy and in our society, has led to an unprecedented, gilded age of bullshit where nothing matters, and things — things of actual substance — matter nothing. We live in a symbolic economy where we apply for jobs, writing CVs and cover letters to resemble a certain kind of hire, with our resume read by someone who doesn't do or understand our job, but yet is responsible for determining whether we’re worthy of going to the next step of the hiring process. All this so that we might get an interview with a manager or executive who will decide whether they think we can do it. We are managed by people whose job is implicitly not to do work, but oversee it. We are, as children (and young adults), encouraged to aspire to become a manager or executive, to "own our own business," to "have people that work for us," and the terms of our society are, by default, that management is not a role you work at, so much as a position you hold — a figurehead that passes the buck and makes far more of them than you do. This problem, I believe, has poisoned the fabric of almost every part of modern business, elevating people that don't do work to oversee companies that make things they don't understand, creating substrates of management that do not do anything but create further distance from actually doing a job. While some of you might automatically think I'm talking about Graeber's concept of Bullshit Jobs, this is far, far bigger. The system as it stands selects people at all levels of management specifically because they resemble the kind of specious, work-averse dullard that runs seemingly every company — a person built to go from meeting to meeting with the vague consternation that suggests they're "busy." As a result, the higher you get up in an organization, the further you get from the customer, the problem you've solving, and any of the actual work, and the higher up you get, the more power you have to change the conditions of the business. On some level, modern corporate power structures are a giant game of telephone where vibes beget further vibes, where managers only kind-of-sort-of understand what's going on, and the more vague one's understanding is, the more likely you are to lean toward what's good, or easy, or makes you feel warm and fuzzy inside. The system selects for people comfortable in these roles, creating org charts full of people that become harder and harder to justify other than "they've been here a while." They do not do "work" on the "product," and their answer as to why would be "what, am I meant to go down on the line and use a machine?" or "am I meant to call a customer and make a sale?" and the answer is yes, you lazy fucking piece of shit, you should do that once in a while, or at the very least go and watch or listen to somebody else do so, and do so regularly. But that's not what a manager does, right? Management isn't work, it's about thinking really hard and telling people what to do. It's about making the calls. It's about "managing people," and that can mean just about anything, but often means "who do I take credit from or pass blame to," because modern management has been stripped of all meaning other than continually reinforcing power structures for the next manager up. This system creates products for these people, because these people are more often than not the ones in power — they are your boss, your boss' boss, and their boss too. Big companies build products sold by specious executives or managers to other specious executives, and thus the products themselves stop resembling things that solve problems so much as they resemble a solution. After all, the person buying it — at least at the scale of a public company — isn’t necessarily the recipient of the final product, so they too are trained (and selected) to make calls based on vibes. I believe the scale of this problem is society-wide, and it is, at its core, a destruction of what it means to be a leader, and a valorization of selfishness, isolationist thinking, turning labor into a faceless resource, which naturally leads to seeing customers in an equally faceless way, their problems generalized, their pain points viewed as parts of a powerpoint rather than anything that your company earnestly tries to solve or even really thinks about. And that assumes that said pain points are even considered to begin with, or not ignored in favor of a fictitious and purely hypothetical pain point. People — be they the ones you're paying or paying you — become numbers. We have created and elevated an entirely new class of person, the nebulous "manager," and told decades-worth of children that's what they should aspire to, that the next step from doing a job is for us to tell other people to do a job, until we're able to one day tell those people how to do their job, with each rung on the corporate ladder further distancing ourselves from anything that interacts with reality. The real breaking point is fairly simple: the higher up you go at a company, the further you are from problems or purpose. Everything is abstract — the people that work for you, the people you work for, and even the tasks you do. We train people — from a young age! — to generalize and distance oneself from actual tasks, to aspire to doing managerial work, because managers are well-paid and "know what's going on," even if they haven't actually known what was going on for years, if they ever did so. This phenomenon has led to the stigmatization of blue-collar work (and the subsequent evisceration of practical trade and technical education across most of the developed world) in favor of universities. Society respects an MBA more than a plumber, even though the latter benefits society more — though I concede that both roles involve, on some level, shit, with the plumber unblocking it and the MBA spewing it. I believe this process has created a symbolic society — one where people are elevated not by any actual ability to do something or knowledge they may have, but by their ability to make the right noises and look the right way to get ahead. The power structures of modern society are run by business idiots — people that have learned enough to impress the people above them, because the business idiots have had power for decades. They have bred out true meritocracy or achievement or value-creation in favor of symbolic growth and superficial intelligence, because real work is hard, and there are so many of them in power they've all found a way to work together. I need you to understand how widespread this problem is, because it is why everything feels fucking wrong. Think of the Business Idiot as a kind of con artist, except the con has become the standard way of doing business for an alarmingly large part of society. The Business Idiot is the manager that doesn't seem to do anything but keeps being promoted, and the chief executive officer of a public company that says boring, specious nonsense about AI. They're the tenured professor that you wish would die, the administrator whose only job appears to be opening and closing their laptop, the consultant that can come up with a million reasons to charge you more money yet not one metric to judge their success by, the marketing executive that's worked exactly three years at every major cloud player but does not appear to have done anything, and the investor that invests "based on founders," but really means "guys that look at sound exactly like them." These people are present throughout the private and public sector, and our governments too, and they paradoxically do nothing of substance, but somehow damage everything they touch. This isn’t to say our public and private sector is entirely useless — just that these people have poisoned so many parts of our power structures that avoiding them is impossible. Our economy is oriented around them — made easier and more illogical for their benefit — because their literal only goal in life has been to take and use power. The Business Idiot is also an authoritarian, and will do whatever they need to — including harming the institution they work for, or those closest to them, like their co-workers or their community — as a means of avoiding true accountability or responsibility. Decades of neoliberalism has incentivized their rise, because when you incentivize society to become management — to "manage or run a company" rather than do something for a reason or purpose — you are incentivizing a kind of corporate narcissism, one that bleeds into whatever field the person goes into, be it public or private. We go to college as a means of getting a job after college using the grades we got in college, rendering many students desperate to get the best grades they can versus "learn" anything, because our economy is riddled with power structures controlled by people that don't know stuff and find it offensive when you remind them. Our society is in the thrall of dumb management, and functions as such. Every government, the top quarter of every org chart, features little Neros who, instead of battling the fire engulfing Rome, are sat in their palaces strumming an off-key version of “Wonderwall” on the lyre and grumbling about how the firefighters need to work harder, and maybe we could replace them with an LLM and a smart sprinkler system. Every institution keeps its core constituents and labor forces at arms-length, and effectively anything built at scale quickly becomes distanced from both the customer and laborer. This disconnection — or alienation — sits at the center of almost every problem I've ever talked about. Why would companies push generative AI in seemingly every part of their service, even though customers don't like it and it doesn't really work? It's simple: they neither know nor care what the customer wants, barely know how their businesses function, barely know what their products do, and barely understand what their workers are doing, meaning that generative AI feels magical, because it does an impression of somebody doing a job, which is an accurate way of describing how most executives and middle managers operate. Let me get a little more specific. An IBM study based on conversations with 2,000 global CEOs recently found that only 25% of AI initiatives have delivered their expected ROI over the last few years, and, worse still, "64% of CEOs surveyed acknowledge that the risk of falling behind drives investment in some technologies before they have a clear understanding of the value they bring to the organization." 50% of respondents also found that "the pace of recent investments has left their organization with disconnected, piecemeal technology," almost as if they don't know what they're doing and are just putting AI in stuff for no reason. Johnson & Johnson recently decided to "shift from broad generative AI experimentation to a focused approach on high-value use cases" according to the Wall Street Journal, adding that "only 10 to 15% of use cases were driving about 80% of the value." Their last two CEOs (Alex Gorsky and Joaquin Duato) both have MBAs, with current CEO Duato's previous ten years at Johnson & Johnson being "some sort of Chairman or Vice President," and the previous two CEOs (Alex Gorsky and William Weldon) were both pharmaceutical sales and marketing people. Fun fact about Alex Gorsky! During his first tenure at Johnson & Johnson he led marketing of products that deliberately underplayed some drugs' side effects and paid off the largest nursing home pharmacy in America to sell more drugs to old people. The term "executive" loosely refers to a person who moves around numbers and hopes for the best. The modern executive does not "lead," but prod, their managers hall monitors for organizations run predominantly by people that, by design, are entirely removed from the business itself even in roles like marketing and sales, where CMOs and VPs bark orders without really participating in the process. We talk eagerly about how young people in entry level jobs should "earn their stripes" by doing "grunt work," and that too is the neoliberal poison in the veins of our society, because, by definition, your very first experience of the workforce is working hard enough so that you don't have to work as hard. And anyway, the same managerial types who bitch about the entitlement and unrealistic expectations of young people are the same ones that also eviscerated the bottom rung of the career ladder — typically by offshoring many of these roles, or consolidating them into the responsibilities of their increasingly burned-out senior workers — or see AI as a way to eliminate what they see as an optional cost center, and not the future of their workforce. Society berated people for "quiet quitting," a ghastly euphemism for “doing the job as specified in your employment contract,” in 2022 because journalism is enthralled by the management class, and because the management class has so thoroughly rewritten the concept of what "labor" means that people got called lazy for literally doing their jobs. The middle manager brain doesn't see a worker as somebody hired and paid for a job, but as an asset that must provide a return. As a result, if another asset comes along that could potentially provide a bigger return — like an offshore worker, or an AI agent — that middle manager won’t hesitate to drop them. Artificial intelligence is the ultimate panacea for the Business Idiot — a tool that gives an impression of productivity with far more production than the Business Idiot themselves. The Information reported recently that ServiceNow CEO Bill McDermott — the chief executive of a company with a market capitalization of over $200 billion, despite the fact that, like SalesForce, nobody really knows what it does — chose to push AI across his whole organization (both in product and in practice) based on the mental consideration I'd usually associate with a raven finding a shiny object: You'll notice that all of this is complete nonsense. What do you mean "efficiency"? What does that quote even mean? 110% of plan? What're you on about? Did you hit your head on something Bill? I'd wager Bill is concussion-free — and an example of a true Business Idiot — a person with incredible power and wealth that makes decisions not based on knowing stuff or caring about his customers, but on the latest shiny thing that makes him think "line go up." No, really, that's Bill McDermott's thing. Back in 2022, he said to Yahoo Finance the metaverse was "real" and that ServiceNow could help someone "create an e-mall in the metaverse" and have a futuristic store of some sort. One might wonder how ServiceNow provided that, and the answer is it didn't. I cannot find a single product that it’s offered that includes it. Bill, like any of these CEOs, doesn't really know stuff, or even do stuff, he just is. The corporate equivalent of a stain on a carpet that nobody really knows how it got there, but hasn’t been removed. The modern executive is symbolic, and the media has — due to the large amount of Business Idiots running these outlets and middle managers stuffed into the editorial class — been trained to never ask difficult questions, such as "what the fuck are you talking about, Bill?" or even the humble "what does that mean?" or "how would you do that?" or saying "I'm not sure I understand, would you mind explaining?" Perhaps the last part is the symptom of the overall problem. So many layers of editorial and managerial power are filled full of people that don't know anything, and there's never anyone crueler about ignorance than somebody that's ignorant themselves. Worse still, in many fields — journalism included — we are rarely rewarded for knowing things or being "right," but being right in the way that keeps the people with the keys from scraping them across our cars. We are, however, rewarded for saying the right thing at the right time, which more often than not means resembling our (white, male) superiors, speaking like our peers, and delivering results in the way that makes everybody feel happiest. A great example of our vibes-based society was back in October 2021, where a Washington Post article written by two Harvard professors rallied against remote work by citing a Microsoft-funded anti-remote study and quoting 130-year-old economist Alfred Marshall about how "workers gather in dense clusters," ignoring the fact that Marshall was so racist they've had to write papers about it, how excited he was about eugenics, or the fact he was writing about fucking factories. Remote work terrifies the Business Idiot, because it removes the performative layer that allowed them to stomp around and feel important, reducing their work to, well...work. Office culture is inherently heteronormative and white, and black women are less likely to be promoted by their managers, and continuing the existence of "The Office" is all about making sure The Business Idiot reigns supreme. Removing the ability for the managerial hall monitors to look at you and try and work out what you're doing without ever really helping is a big part of being a manager — and if you're a manager reading this and saying you don't do this, I challenge you to talk to another person that doesn't confirm your biases. The Business Idiot reigns supreme. Their existence holds up almost every public company, and remote work was the first time they willingly raised their heads. Google demanded employees return to the office in 2021 — but let one executive work remotely from New Zealand because absolutely none of the decisionmaking was done with people that actually do work. While we can (well, you can, I'm not interested) debate whether exclusively working remote is as productive, the Return To Office push was almost entirely done in two ways: The New York Times, The Washington Post, The Wall Street Journal, and many, many other outlets all fell for this crap because the Business Idiots have captured our media too, training even talented journalists to defer to power at every turn. When every power structure is stuffed full of do-nothing management types that have learned exactly as little as they need to as a means to get by, it's inevitable that journalism caters to them — specious, thoughtless reproductions of the powerful's ideas. Look at the coverage of AI, or the metaverse, or cryptocurrency, or Clubhouse. Look at how willingly reporters will accept narratives not based on practical experience or what the technology can do, but what the powerful (and the popular) are suddenly interested in. Every single tech bubble followed the same path, and that path was paved with flawed, deferential and specious journalism, from small blogs to the biggest mastheads. Look at how reporters talk to executives — not just the way they ask things (like Nilay Patel's 100+ word questions to Sundar Pichai in his abominable interview), but the things they accept them saying, and the willingness reporters have to just accept what they're told. Satya Nadella is the CEO of a company with a market capitalization of over $3 trillion. I have no idea how you, as a reporter, do not say "Satya, what the fuck? You're outsourcing most of your life to generative AI? That's insane!" or even "do you really do that?" and then asking further questions. But that would get you in trouble. The editorial class is the managerial class now, and has spent decades mentoring young reporters to not ask questions, to not push back, to believe that a big, strong, powerful company CEO would never mislead them. Kara Swisher's half-assed interviews are considered "daring" and "critical" because journalism has, at large, lost its teeth, breeding reporters rewarded for knowing a little bit about a few things and punishing those who ask too many questions or refuse to fall in line. The reason they don't want you to ask these questions is that the Business Idiot isn't big on answers. Editors that tell you not to push too hard are doing so because they know the executive likely won't have an answer. It isn't just about the PR person that trained them, but the fact that these men more often than not have only a glancing understanding of their underlying business. Yet in the same way that Business Idiots penetrated every other part of society, they eventually found their way to journalism. While we can (and should) scream at the disconnected idiots that ran Vice into the ground, the problem is everywhere, because the Business Idiots aren't just at the top, but infecting the power structures underlying every newsroom. While there are many really great editors, there are plenty more that barely understand the articles they edit, the stories they commission, or that make reporters pull punches for fear of advertiser blowback. That, and mentorship is dead across effectively all parts of society, meaning that most reporters (as with many jobs) learn by watching each other, which means they all make sure to not ask the rough questions, and not push too hard against the party/market/company messaging until everybody else does it. And under these conditions, Business Idiots thrive. The Business Idiot's reign is one of speciousness and shortcuts, of acquisition, of dominance and of theft. Mentoring people is something you do to pass on knowledge — it may make them grateful to you, but it ultimately, in the mind of a Business Idiot, creates a competitor or rival. Investing in talent, or worker conditions, or even really work itself would require you to know what you're talking about, or actually do work, which doesn't make sense when you're talking to a worker. They're the ones who're meant to work! You're there to manage them! Yet they keep talking back — asking questions about the work you want them to do, asking you to step in and help on something — and all of that's so annoying. Just know the stuff already! Get it done! I have to go to lunch and then go back out to another lunch! I believe this is the predominant mindset across most of the powerful, to the point that everything in the world is constructed to reaffirm their beliefs rather than follow any logical path. Our stock market is inherently illogical, driven not by whether a company is good or bad, but whether it can show growth, even if said growth is horrifically unprofitable, and I'd argue it's because the market has no idea how to make intelligent decisions, just complex ones that mean that you don't really need to understand the business so much as you understand the associated vibes of the industry. Friedman's influence and Reagan's policies have allowed our markets to be dominated by Business Idiocy, where a bad company can be a good stock because everybody (IE: other traders and the business press) likes how it looks, which allows the Business Idiots to continue making profit using illogical and partially-rigged market-making, with the business press helpfully pushing up their narratives. This also keeps regular people from accumulating too much wealth — if regular people could set the tone for the markets as "a company that makes something people like and people pay them for it and they make more money than they spend," that might make things a little too even. It doesn't matter that CoreWeave quite literally does not have enough money for its capital expenditures and lost over $300m in the last quarter because its year-over-year growth was 420%. It doesn't matter that it has October loan payments that will crush the life out of the company either. These narratives are fed to the media knowing that the media will print them, because thinking too hard about a stock would mean the Business Idiot had to think also, and that is not why they are in this business. The "AI trade" is the Business Idiot's nirvana — a fascination for a managerial class that long since gave up any kind of meaningful contribution to the bottom line, as moving away from the fundamental creation of value as a business naturally leads to the same kind of specious value that one finds from generative AI. I’m not even saying that there’s no returns, or that LLMs don’t do anything, or even that there’s no possible commercial use for generative AI. They just don’t do enough, almost by design, and we’re watching companies desperately try and contort them into something, anything that works, pretending so fucking hard they’ll stake their entire futures on the idea. Just fucking work, will you? Agentforce doesn’t make any money, it sucks, but god damn is Marc Benioff going to make you bear witness. Does it matter that Agentforce doesn't make Salesforce any money? No! Because Benioff and Salesforce have got rich selling to fellow Business Idiots who then shove Salesforce into their organization without thinking about who would use it or how they'd use it other than in the most general ways. Agentforce was — and is — a fundamentally boring and insane product, charging $2 a conversation for a chatbot that, to quote The Information, provides customers with "...incorrect answers — AI hallucinations — while testing how the software handles customer service queries." But this shit is catnip to the Business Idiot, because the Business Idiot ideally never has to deal with work, workers or customers. Generative AI doesn’t do enough to actually help us be better at our jobs, but it gives a good enough impression of something useful so that it can convince someone really stupid that doesn’t understand what you do that they don’t need you, sometimes. A generative output is a kind of generic, soulless version of production, one that resembles exactly how a know-nothing executive or manager would summarise your work. OpenAI's "Deep Research" wows professional Business Idiot Ezra Klein because he doesn't seem to realize that part of research is the research itself, not just the output, as you learn about stuff as you research a topic, allowing you to come to a conclusion. The concept of an "agent" is the erotic dream of the managerial sect — a worker that they can personally command to generate product that they can say is their own, all without ever having to know or do anything other than the bare minimum of keeping up appearances, which is the entirety of the Business Idiot's resume. And because the Business Idiot's career has been built on only knowing exactly enough to get by, they don't dig into Large Language Models any further than hammering away at ChatGPT and saying "we must put AI in everything now." Yet the real problem is that for every Business Idiot selling a product, there are many more that will buy it, which has worked in the past for Software as a Service (SaaS) companies that grew fat and happy hocking giant annual contracts and continual upsells, because CIOs and CTOs work for Business Idiot CEOs that demand that they "put AI in everything now," a nonsensical and desperate remit that's part growth-lust and part ignorance, borne of the fear that one gets when they're out of their depth. Look at every single institution installing some sort of ChatGPT integration, and then look for the Business Idiot. Perhaps it's Cal State University Chanceller Mildred Garcia, who claimed that giving everybody a ChatGPT subscription would "elevate...students' educational experience across all fields of study, empower [its] faculty's teaching and research, and help provide the highly educated workforce that will drive California's future AI-driven economy," a nonsensical series of words to justify a $16.9 million-a-year single-vendor no-bid contract to a product that is best known as either a shitty search engine or a way to cheat at college. In some ways, Sam Altman is the Business Idiot's antichrist, taking advantage of a society where the powerful rarely know much other than what they want to control or dominate. ChatGPT and other AI tools are, for the most part, sold based on what they might do in the future to people that will never really use them, and Altman has done well to manipulate, pester and terrify those in power with the idea that they might miss out on something. Does anyone know what it is? No, they don't, because the powerful are Business Idiots too, willing to accept anything that somebody brings along that makes them feel good, or bad in a way that they can make headlines with. In any case, Altman's whole Sloppenheimer motif has worked wonders on the Business Idiots in the markets and global governments that fear what artificial intelligence could do, even if they can't really define artificial intelligence, or what it could do, or what they're scared of. The fear of China's "rise in AI" is one partially based on sinophobia, and partially based on the fact that China has their own Business Idiots willing to shove hundreds of millions of dollars into data centers. Generative AI has created a reckoning between the Business Idiot and the rest of society, its forced adoption and proliferation providing a meager return for the massive investment of capital and the revulsion it causes in many people, not just in the Business Idiot's excitement in replacing them, but how wrong the Business Idiot is. While there are many people that dick around with ChatGPT, years since it launched we still can't find a clean way to say what it does or why it matters other than the fact that everybody agreed it did. The media, now piloted by Business Idiots, has found itself declawed, its reporters unprepared, unwilling and unsupported, the backbone torn out of most newsrooms for fear that being too critical is somehow "not being objective," despite the fact that what you choose to cover objectively is still subjective. Reporters still, to this day, as these companies burn billions of dollars to make an industry the size of the free-to-play gaming industry, refuse to say things that bluntly because "the cost of inference is coming down" and "these companies have some of the smartest people in the world." They ignore the truth as it sits in front of them — that the combined annual recurring revenue of The Information's comprehensive database of every generative AI company is less than $10 billion, or $4 billion if you remove Anthropic and OpenAI. ChatGPT's popularity is the ultimate Business Idiot success story — the "fastest growing product in Silicon Valley history" that didn't grow because it was useful, or good, or able to do anything in particular, but because a media controlled by Business Idiots decided it was "the next big thing" and started talking about it nonstop since November 2022, guaranteeing that everybody would try it, even if even to this day the company can't really explain what it is you're meant to use it for. Much like the Business Idiot themselves, ChatGPT doesn't need to do anything specific. It just needs to make the right sounds at the right times to impress people that barely care what it does other than make them feel futuristic. Real people — regular people, not Business Idiots, not middle managers, not executive coaches, not MBAs, not CEOs — have seen this for what it was early and often, but real people are seldom the ones with the keys, and the media — even the people writing good stuff — regularly fails to directly and clearly say what's going on. The media is scared of doing the wrong thing — of "getting in trouble" with someone for "misquoting them" or "misreading what they said" — and in a society where in-depth knowledge is subordinate to knowing enough catchphrases, the fight often doesn't feel worth it even with an editor's blessing. I also want to be clear that this goes far beyond money. Editors aren't just scared of advertisers being upset. They know that if narratives have to shift toward more critical, thoughtful coverage, they too will have to be more thoughtful and knowledgeable, which is rough when you are a Business Idiot and got there by editing the right people in a way that didn't help them in the slightest. Nothing about what I'm saying should suggest the Business Idiot is weak. In fact, Business Idiots are fully in control — we have too many managers, and our most powerful positions are valorized for not knowing stuff, for having a general view that can "take the big picture," not realizing that a big picture is usually made up of lots of little brush strokes. Yet there are, eventually, consequences for everything being controlled by Business Idiots. Our current society — an unfair, unjust one dominated by half-broken tech products that make their owners billions — is the real punishment wrought by growth, a brain drain in corporate society, one that leads it to doing illogical things and somehow making money. It doesn't make any fucking sense that generative AI got this big. The returns aren't there, the outcomes aren't there, and any sensible society would've put a gun to a ChatGPT and aggressively pulled the trigger. Instead it's the symbolic future of capitalism — one that celebrates mediocrity and costs billions of dollars, every human work it can consume, and the destruction of our planet, all because everybody has kind of agreed that this is what they're all doing, with nobody able to give a convincing explanation of what that even is. Generative AI is revolting both in how overstated its abilities are and in how it continually tests how low a standard someone will take for a product, both in its outputs and in the desperate companies trying to integrate it into everything, and its proliferation throughout society and organizations is already fundamentally harmful. We’re not just drowning in a sea of slop — we’re in a constant state of corporate AI beta tests, new “features” sprouting out of our products like new limbs that sometimes function normally but often attempt to strangle us. It’s unclear if companies forcing these products on us have contempt for us or simply don’t know what good looks like. Or perhaps it's both, with the Business Idiot resenting us for not scarfing down whatever they serve us, as that's what's worked before. They don't really understand their customers — they understand what a customer pays for and how a purchase is made, you know, like the leaders of banks and asset managers during the subprime mortgage crisis didn't really think about whether people could pay those mortgages, just that they needed a lot of them to put in a CDO. The Business Idiot's economy is one built for other Business Idiots. They can only make things that sell to companies that must always be in flux — which is the preferred environment of the Business Idiot, because if they're not perpetually starting new initiatives and jumping on new "innovations," they'd actually have to interact with the underlying production of the company. Does the software work? Sometimes! Do successful companies exist that sell like this? Sure! But look at today's software and tell me with a straight face that things feel good to use. And something like generative AI was inevitable: an industry claiming to change the world that never really does so, full of businesses that don’t function as businesses, full of flimflam and half-truths used to impress people who will likely never interact with it, or do so in only a passing way. By chasing out the people that actually build things in favour of the people that sell them, our economy is built on production puppetry — just like generative AI, and especially like ChatGPT. These people are antithetical to what’s good in the world, and their power deprives us of happiness, the ability to thrive, and honestly any true innovation. The Business Idiot thrives on alienation — on distancing themselves from the customer and the thing they consume, and in many ways from society itself. Mark Zuckerberg wants us to have fake friends, Sam Altman wants us to have fake colleagues, and an increasingly loud group of executives salivate at the idea of replacing us with a fake version of us that will make a shittier version of what we make for a customer that said executive doesn’t fucking care about. They’re building products for other people that don’t interact with the real world. We are no longer their customers, and so, we’re worth even less than before — which, as is the case in a world dominated by shareholder supremacy, not all that much. They do not exist to make us better — the Business Idiot doesn’t really care about the real world, or what you do, or who you are, or anything other than your contribution to their power and wealth. This is why so many squealing little middle managers look up to the Musks and Altmans of the world, because they see in them the same kind of specious corporate authoritarian, someone above work, and thinking, and knowledge. One of the most remarkable things about the Business Idiot is their near-invulnerability. Modern management is resource control, shifting blame away from the manager (who should hold responsibility. After all, if they don’t, why do they have that job?) onto the laborer, knowing that the organization and the media will back it up. While you may think I’m making a generalization, the 2021-2023 anti-remote work push in the media was grotesque proof of where the media’s true allegiance lies — the media happily manufactured consent for return-to-office mandates from large companies by framing remote work as some sort of destructive force, doing all they can to disguise how modern management has no fucking idea how the workplace actually works. These articles were effectively fan fiction for managers and bosses demanding we return to the office — ridiculous statements about how remote work “failed young people” (it didn’t) or how employees needed remote work more than their employers because “the chitchat, lunches and happy hours” are so important. Had any of those reporters spoken to an actual worker, they’d say that they value more time with their families, rather than the grind of a daily commute softened with the promise of an occasional company pizza party — which usually happens outside of the typical working hours, anyway. These articles rarely (if ever) cared about whether remote work was more productive, or that the disconnect appeared to be between managers and workers. It was, from the very beginning, about crushing the life out of a movement that gave workers more flexibility and mobility while suppressing managers’ ability to hide how little work they did. I give credit to CNBC in 2023 for saying the quiet part out loud — that “...the biggest disadvantage of remote work that employers cite is how difficult it is to observe and monitor employees” — because when you can’t do that, you have to (eugh!) actually know what they’re doing and understand their work. Yet higher up the chain, the invulnerability continues. CEOs may get fired — and more are getting fired than ever, although sadly not the ones we want — but always receive some sort of golden parachute payoff at the end before walking into another role at another organization doing exactly the same level of nothing. Yet before that happens, A CEO is allowed to pull basically every lever before they take a single ounce of accountability — laying people off, pay freezes, moving from salaried to contracted workers, closing down sites, cutting certain products, or even spending more fucking money. If you or I misallocated billions of dollars on stupid ideas we’d be fired. CEOs, somehow, get paid more. Let me give you an example. Microsoft CEO Satya Nadella said that the “ultimate computer…is the mixed reality world” and that Microsoft would be “inventing new computers and new computing” in 2016, pushing his senior executives to tell reporters that Hololens was Microsoft’s next wave of computing in 2017, selling hundreds of millions of dollars’ worth of headsets to the military in 2019, then debuting HoloLens 2 at BUILD 2019 only for the on-stage demo to break in realtime, calling for a referendum on capitalism in 2020, then saying he couldn’t overstate the breakthrough of the metaverse in 2021. Let’s see what he said about it (props to Preston Gralla of ComputerWorld for finding this): As Gralla notes, Nadella said Microsoft would be, “…beefing up development in projects such as its Mixed Reality Tool Kit MRTK, the virtual reality workspace project AltspaceVR (which it had bought back in 2017), its HoloLens virtual reality headset, and its industrial metaverse unit, among others,” before firing all 100% members of its industrial Metaverse core team along with those behind MRTK and shutting down AltSpace VR (which it acquired in 2017) in 2023, before discontinuing HoloLens 2 entirely in 2024. Nadella was transparently copying Meta and Mark Zuckerberg’s ridiculous “metaverse” play, and absolutely nothing happened to him as a result. The media — outlets like The Verge and independents like Ben Thompson — happily boosted the metaverse idea when it was announced and conveniently forgot it the second that Microsoft and Meta wanted to talk about AI (no, really, both The Verge and Ben Thompson were ready and waiting) without a second’s consideration about what was previously said. A true Business Idiot never admits wrongdoing, and the more powerful the Business Idiot is, the more likely there are power structures that exist to avoid them having to do so. The media, captured by other Business Idiots, has become poisoned by power, deferring to its whims and ideals and treating CEOs with more respect, dignity and intelligence than anyone that works for them. When a big company decides they want to “do AI,” the natural reaction is to ask “how?” and write down the answer rather than think about whether it’s possible or whether the company might profit (say, by increasing their shareholder price) by having whatever they say printed ad verbatim. These people aren’t challenged by the media, or their employees, because their employees are vulnerable all the time, and often encouraged to buy into whatever bullshit is du jour like hostages being held by a terrorist group that eventually fall victim to Stockholm syndrome. They’re only challenged by shareholders, who are agnostic about idiocy because it’s not core to value in any meaningful sense, as we’ve seen with crypto, the metaverse and AI, and shareholders will tolerate infinite levels of idiocy if it boosts the value of their holdings. It goes further too. 2021 saw the largest amount of venture capital invested in the last decade, a record-breaking $643 billion, with a remarkable $329.5 billion of that invested in the US alone. Some of the biggest deals include Amazon reseller aggregator Thrasio, which raised $1 billion in October 2021 and filed for bankruptcy in February 2025, cloud security company Lacework, which raised $525 million in January 2021 then $1.3 billion in October 2021 and was rumoured to be up for sale to Wiz, only for the deal to collapse, and autonomous car company Cruise, which raised $2.75 billion 2021 and was killed off in December 2024. The people who lose their livelihoods — those who took stock in lieu of cash compensation, and those who end up getting laid off at the end — are always workers, while people like Lacework co-CEO Jay Parikh (who oversaw “reckless spending” and “management dysfunction” according to The Information) can walk into highly-paid positions at companies like Microsoft, as he did in October 2024 a few months after a fire sale to cybersecurity Fortinet for around $200 million according to analysts. It doesn’t matter if you’re wrong, or if you run your company badly, because the Business Idiot is infallible, and judged too by fellow disconnected Business Idiots. In a just society, nobody would want to touch any of the C-suite that oversaw a company that handed out Nintendo Switches to literally anyone who booked a meeting (as with Lacework). Instead, the stank remains on the employees alone. It’s so easy, and perhaps inevitable, to feel a sense of nihilism about it all. Nothing matters. It’s all symbolic. Our world is filled with companies run by people who don’t interact with the business, and that raise money from venture capitalists that neither run businesses nor really have any experience doing so. And despite the fact that these people exist several abstractions from reality, the things that they do and the decisions they make impact us all. And it’s hard to imagine how to fix it. We live in a system of iniquity, dominated by people that do not interact with the real world who have created an entire system run by their fellow Business Idiots. The Rot Economy’s growth-at-all-costs mania is a symptom of the grander problem of shareholder supremacy, and the single-minded economic focus on shareholder value inevitably ends at an economy run by and for Business Idiots. There is a line, and it ends here — with layoffs, the destruction of our planet and our economy and our society, and a rising tide of human misery that nobody really knows where it comes from, and so, we don’t know who to blame, and for what. If our economy actually worked as a true meritocracy — where we didn’t have companies run by people who don’t use their products or understand how they’re made, and who hire similarly-specious people — these people would collapse under the pressure of having to know their ass from their earhole. Yet none of this would be possible without the enabling layers, and those layers are teeming with both Business Idiots and those unfortunate enough to have learned from them. The tech media has enabled every single bubble, without exception, accepting every single narrative fed to them by VCs and startups, with even critical reporters still accepting the lunacy of a company like OpenAI just because everybody else does too. Let’s be honest, when you remove all the money, our current tech industry is a disgrace. Our economy is held up by NVIDIA, a company that makes most of its money selling GPUs to other companies primarily so that they can start losing the money selling software that might eventually make them money, just not today. NVIDIA is defined by massive peaks and valleys, as it jumps on trends and bandwagons at the right time, despite knowing that these bandwagons always come to an abrupt halt. The other companies feature Tesla, a meme stock car company with a deteriorating brand and a chief executive famous for his divorces from both reality and multiple women along with a flagrant racism that may cost the company its life. A company that we are watching die in real time, with a stagnant line-up and actual fucking competition from companies that are spending big on innovation. In Europe and elsewhere, BYD is eating Tesla’s lunch, offering better products for half the price — and far less stigma. And this is just the first big Chinese automotive brand to go global. Others — like Chery — are enjoying rapid growth outside of China, because these cars are actually quite good and affordable, even when you factor in the impact of things like tariffs. Hey, remember when Tesla fired all the people in its charging network — despite that being one of the most profitable and valuable parts of the business? And then hired them back because it turns out they were actually useful? This is a good example of managerial alienation — decisions made by non-workers who don’t understand their customers, their businesses, or the work their employees do. And let’s not forget about the Cybertruck, a monstrosity both in how it looks and how it’s sold, and that’s illegal in the majority of developed countries because it is a death-trap for drivers and pedestrians alike. Oh, and that nobody actually wants, with Tesla sitting on a quarter’s worth of inventory that it can’t sell. Elsewhere is Meta, a collapsing social network with 99% of its revenue based on advertising to an increasingly-aged population and a monopoly so flagrantly abusive in its contempt for its customers that it’s at times difficult to call Instagram or Facebook social networks. Mark Zuckerberg had to admit to the Senate Judiciary Committee that people don’t use Facebook as a social network anymore. The reason why is because the platform is so fucking rotten, run by a company alienated from its user base, its decrepit product actively hostile to anybody trying to use it. And, more fundamentally, what’s the point of posting on Facebook if your friends won’t see it, because Meta’s algorithm decided it wouldn’t drive engagement? Meta is a monument to disconnection, a company that runs in counter to its own mission to connect people, run by Mark Zuckerberg, a man who hasn’t had a good idea since he stole it from the Winklevoss Brothers. The solution to all that ails him? Adding generative AI to every part of Meta, which…uh…was meant to do something other than burn $72 billion in capital expenditures in 2025, right? It isn’t clear what was meant to happen, but the Wall Street Journal reports that Meta’s AI chatbots are, and I quote, “empowered to engage in ‘romantic role-play’ that can turn explicit” — even with children. In a civil society, Zuckerberg would be ousted immediately for creating a pedophilic chatbot, — instead, four days after the story ran, everyone cheered Meta’s better-than-expected quarterly earnings. In Redmond, Microsoft sits atop multiple monopolies, using tariffs as a means to juice flailing Xbox revenue as it invests billions of dollars in OpenAI so that OpenAI can spend billions of dollars on cloud compute, losing billions more in the process, requiring Microsoft to invest further money to keep them alive. All because Microsoft wanted generative AI in Bing. What a fucking waste! While also raising the costs of its office suite — which it’s only able to hold a monopoly on because it acted so underhandedly in the 1990s. Amazon lumbers listlessly through life, its giant labor-abuse machine shipping things overnight at whatever cost necessary to crush the life out of any other source of commerce, its cloud services and storage arm, unsure who to copy next. Is it Microsoft? Is it Google? Who knows! But one analyst believes it’s making $5 billion in revenue from AI in 2025 — and spending $105 billion in capital expenditures. There are slot machines with a better ROI than this shit. Again, it’s a company that’s totally exploitative of its customers, no longer acting as a platform that helps people find the shit they need, but to direct them to the products that pay the most for prime advertising real-estate, no matter whether they are good or safe. Let’s be clear: Amazon’s recklessness will kill someone, if it hasn’t already. Then there’s the worst of them — Google. Most famous for its namesake, a search engine that it has juiced as hard as possible, and will continue to juice before the inevitable antitrust sentencing that would rob it of its power, along with the severance of its advertising monopoly. But don’t worry, Google also has a generative AI thing, for some reason, and no, you don’t have a choice about using it, because it’s now stapled onto Google Search and Google Assistant. At no point do any of these companies seem to be focused on making our lives better, or selling us any kind of real future. They exist to maintain the status quo, where cloud computing allows them to retain their various fiefdoms. They’re alienated from people. They’re alienated from workers. They’re alienated from their customers. They’re alienated from the world. They’re deeply antisocial and misanthropic — as demonstrated by Zuck’s moronic AI social network comments. And AI is a symptom of a reckoning of this stupidity and hubris. They cut, and cut, and stagnated. Their hope is a product that will be adopted by billions of imaginary customers and companies, and will allow them to cut further without becoming just a PO Box and a domain name. We have to recognize that what we’re seeing now with generative AI isn’t a fluke or a bug, but a feature of a system that’s rapacious and short-term by its very nature, and doesn’t define value as we do, because “value” gets defined by a faceless shareholder as “growth.” And this system can only exist with the contribution of the business idiot. These are the vanguard — the foot soldiers — of this system, and a key reason why everything is so terrible all the time, and why nothing seems to be getting better. Breaking from that status-quo would require a level of bravery that they don’t have — and perhaps isn’t possible in the current economic system. These people are powerful, and have big platforms. They’re people like Derek Thompson, famed co-author of the “abundance” agenda, who celebrates the idea of a fictitious version of ChatGPT that can entirely plan and execute a 5-year-old’s birthday party, or his co-author Ezra Klein, who, while recording a podcast where his researchers likely listened, talked proudly about replacing their work with OpenAI’s broken Deep Research product, because anything that can be outsourced must be, and all research is “looking at stuff that is relevant.” And really, that’s the most grotesque part about Business Idiots. They see every part of our lives as a series of inputs and outputs They boast about how many books they’ve read rather than the content of said books, about how many hours they work (even though they never, ever work that many), about high level they are in a video game they clearly don’t play, about the money they’ve raised and the scale they’ve raised it at, and about how expensive and fancy their kitchen gadgets are. Everything is dominance, acquisition, growth and possession over any lived experience, because their world is one where the journey doesn’t matter, because their journeys are riddled with privilege and the persecution of others in the pursuit of success. These people don’t want to automate work, they want to automate existence. They fantasize about hitting a button and something happening, because experiencing — living! — is beneath them, or at least your lives and your wants and your joy are. They don’t want to plan their kids’ birthday parties. They don’t want to research things. They don’t value culture or art or beauty. They want to skip to the end, hit fast-forward on anything, because human struggle is for the poor or unworthy. When you are steeped in privilege and/or have earned everything through a mixture of stolen labor and office pantomime, the idea of “effort” is always negative. The process of creation — or affection, of love, of kindness, of using time not just for an action or output — is disgusting to the Business Idiot, because those are times they could be focused on themselves, or some nebulous self-serving “vision” that is, when stripped back to its fundamental truth, is either moronic or malevolent. They don’t realise that you hire a worker for the worker’s work rather than just the work themselves, which is why they don’t see why it’s so insulting to outsource their interactions with human beings. You’ll notice these people never bring up examples of automating actual work — the mind-numbing grunt work that we all face in the workplace — because they neither know nor care what that is. Their “problems” are the things that frustrate them, like dealing with other people, or existing outside of the gilded circles of socialite fucks or plutocrats, or just things that are an inevitable facet of working life, like reading an email. Your son’s birthday party or a conflict with a friend can, indeed, be stressful, but these are not problems to be automated out. They are the struggles that make us human, the things that make us grow, the things that make us who we are, which isn’t a problem for anybody other than somebody who doesn’t believe they need to change in any way. It's both powerful and powerless at the same time — a nihilistic way of seeing our lives as a collection of events we accept or dismiss like a system prompt, the desperate pursuit of such efficient living that you barely feel a thing until you die. I’ve spent years writing about these people without giving them a name, because categorizing anything is difficult. I can’t tell you how long it took for me to synthesize the Rot Economy from the broader trends I saw in tech and elsewhere, how long it took for me to thread that particular needle, to identify the various threads that unified events that are otherwise separate and distinct. I am but one person. Everything you’ve read in my newsletter to this point has been something I’ve had to learn. Building an argument and turning it into words — often at the same time — that other people will read doesn’t come naturally to anyone. It’s something you have to deliberately work at. It’s imperfect. There are typos. These newsletters increase in length and breadth and have so many links, and I will never, ever change my process, because part of said process is learning, relearning, processing, getting pissed off, writing, rewriting, and so on and so forth. This process makes what I do possible, and the idea of having someone automate it disgusts me, not because I’m special or important, but because my work is not the result of me reading a bunch of links or writing a bunch of words. This piece is not just 13,000 words long — it’s the result of the 800,000 or more words I wrote before it, the hundreds of stories I’ve read in the past, the hours of conversations with friends and editors, years of accumulating knowledge and, yes, growing with the work itself. This is not something that you create through a summation of content vomited by an AI, but the chaotic histories of a human being mashed against the challenge of trying to process it. Anyone who believes otherwise is a fucking moron — or, better put, just another Business Idiot. Subscribe today. It's free. Please. Great! You’ve successfully signed up. Welcome back! You've successfully signed in. You've successfully subscribed to Ed Zitron's Where's Your Ed At. Your link has expired. Success! Check your email for magic link to sign-in. Success! Your billing info has been updated. Your billing was not updated.
--------------------------------------------------

Title: Is The Grow a Garden Beanstalk Better Than The Candy Blossom?
URL: https://www.droidgamers.com/guides/grow-a-garden-beanstalk/
Time Published: 2025-05-21T15:57:05Z
Full Content:
This Grow a Garden Beanstalks guide tells you if this prismatic plant is the next best Candy Blossom, including its buying price, stock rotation chances, profits, and more! Should you save your money, or splash some cash on this rare seed? Find out in this guide! Start playing Grow a Garden on Roblox. Find out how the Beanstalk fares against other seeds in our Tier List. Or, find out when to check for its stock in our Grow a Garden Stock Guide. The Beanstalk is the first Prismatic rarity seed sold by the merchant on the main island. This puts this plant on quite a pedestal to impress players and make them filthy rich, given it has a 0.0005% restock chance every 5 minutes, and costs an expensive 10,000,000$ to buy a single seed. This seed costs just as much as the event Candy Blossom, which many players (myself included) missed out on. So is the Beanstalk just as profitable? The Beanstalk grows tall and is multi-harvest, meaning you don’t lose the original plant when you collect it, unlike Bamboo, Mushrooms, etc. An unmutated, average-sized Beanstalk typically fetches for 15-30k cash. It also produces multiple growth points from one stem, and since it grows upwards, it tends not to take up too much garden space. This makes them an excellent crop to harvest and very profitable! To really get your money’s worth, I recommend not collecting any fully grown Beanstalks for a while until they’ve had the chance to develop mutations. In short, yes, the Beanstalk is likely just as profitable as the Candy Blossom, making this plant meta! The Grow a Garden Discord Server has a channel dedicated to stock rotations, so you won’t miss out next time the Beanstalk makes a shop appearance. Or, you could buy it for Robux, but where is the fun in that? Copyright 2025 Droid Gamers Inc. - All Rights Reserved. - Privacy Policy
--------------------------------------------------

Title: Sergey Brin points to where Google Glasses failed — and what Android XR gets right
URL: https://www.cnbc.com/2025/05/21/sergey-brin-google-glass-android-xr.html
Time Published: 2025-05-21T12:55:44Z
Description: Google co-founder Sergey Brin on Tuesday said he's learned from his "mistakes" since the failed Google Glass launch in 2013.
--------------------------------------------------

Title: Google unveils AI upgrades at I/O conference amid search challenges
URL: https://www.thehindubusinessline.com/info-tech/google-unveils-ai-upgrades-at-io-conference-amid-search-challenges/article69600304.ece
Time Published: 2025-05-21T04:46:46Z
Full Content:
+ 707.66 + 185.30 + 95.00 + 534.00 + 472.00 + 707.66 + 185.30 + 185.30 + 95.00 + 95.00 + 534.00 Get businessline apps on Connect with us TO ENJOY ADDITIONAL BENEFITS Connect With Us Get BusinessLine apps on FILE PHOTO: The logo for Google is seen at the Google Store Chelsea in Manhattan, New York City, U.S. | Photo Credit: ANDREW KELLY Alphabet's Google kicked off its annual developer conference on Tuesday with a flurry of announcements showcasing its huge investment in artificial intelligence, while seeking to fend off concerns over the future of its business. The I/O conference in Mountain View, California, has adopted a tone of increased urgency since the rise of generative AI posed a fresh threat to Google's longtime stronghold of organizing and retrieving information on the internet. In recent months, Google has become more aggressive in asserting it has caught up to competitors after appearing flat-footed upon the release of Microsoft-backed OpenAI's ChatGPT. Top executives including CEO Sundar Pichai have often cited the pole position of its Gemini class of AI models on public leaderboards, ahead of top models from competitors including OpenAI and Meta. Google and Xreal debut Aura AR Glasses to rival Meta’s Orion plan Pichai said at the conference: "Over and over, we've been able to deliver the best models at the most effective price point." The Gemini app now has more than 400 million monthly active users, Pichai said. The Alphabet chief also announced other advancements showcasing AI, including an update to the company's Google Meet video conferencing software that would translate meetings between English and Spanish in real time. And Pichai showcased Google Beam, hardware that makes video chats feel like they are taking place in person, in partnership with HP and based on Google's Project Starline. At the same time, with consumer use of AI chatbots maturing, investors will be tracking the degree to which Google is aggressive in disrupting its search advertising business line, which accounted for the majority of the company's $350 billion in 2024 revenue. Earlier this month, Alphabet stock lost $150 billion in market value in one day after an Apple executive testified during one of Google's antitrust cases that AI offerings had caused a decline in searches on Apple's Safari Web browser for the first time. In turn, some analysts reassessed how to measure Google's dominant search market share, which has for years hovered around 90% by traditional metrics. A Bernstein analyst note this month placed the figure at 65% to 70% when accounting for usage of AI chatbots. Wells Fargo analysts estimated that Google's market share could fall to less than 50% in five years. The analysts cited a behavioral shift drawing consumers toward AI chatbots where they once used traditional search engines. Google's market position could be further rocked by the outcome of legal challenges, in particular a pair of antitrust cases brought by the Justice Department, which is seeking the forced sale of parts of the tech giant including its Chrome browser. Investment in AI accounts for most of Alphabet's $75 billion in forecasted capital expenditures this year, a dramatic uptick from the $52.5 billion in 2024 spending that the company reported. In April, Pichai reiterated those spending plans despite market uncertainty around tariffs. Google has injected more AI into its core search engine over the past two years, primarily through AI Overviews, generative AI summaries that are increasingly appearing atop the traditional hyperlinks to relevant webpages, and AI Mode, an experimental version that leverages AI more intensively to answer complex queries. Tuesday's announcements will likely include further updates to search as well as Google's effort to deliver a "universal AI agent." At last year's conference, the company teased Project Astra, a prototype tool that can talk to users about anything captured on their smartphone camera in real time. The company began experimenting with inserting ads into AI Overviews last May, though it has avoided any radical changes that would rock the boat. Meanwhile, Google is expanding other revenue streams to monetize AI. Last week, the company told Reuters its Google One consumer subscription service had crossed 150 million subscribers helped by "millions" of customers who signed up for a $19.99 per month plan with access to AI capabilities unavailable for most free users. Published on May 21, 2025 Copyright© 2025, THG PUBLISHING PVT LTD. or its affiliated companies. All rights reserved. BACK TO TOP Comments have to be in English, and in full sentences. They cannot be abusive or personal. Please abide by our community guidelines for posting your comments. We have migrated to a new commenting platform. If you are already a registered user of TheHindu Businessline and logged in, you may continue to engage with our articles. If you do not have an account please register and login to post comments. Users can access their older comments by logging into their accounts on Vuukle. Terms & conditions | Institutional Subscriber
--------------------------------------------------

Title: Google pushes AI upgrades, subscriptions at annual I/O developer event amid search challenges
URL: https://economictimes.indiatimes.com/tech/artificial-intelligence/google-pushes-ai-upgrades-subscriptions-at-annual-i/o-developer-event-amid-search-challenges/articleshow/121304002.cms
Time Published: 2025-05-21T02:05:19Z
Full Content:
5 Stories 7 Stories 9 Stories 9 Stories 8 Stories 6 Stories RBI has cut repo rates twice in 3 months. But that isn’t enough to boost the economy! How NCLT became tribunal of hope reviving assets worth billions despite resource crunch Protean eGov of PAN fame crashes 20% as govt turns down bid Explainer: Why RBI took a U-turn on banks and NBFCs investing in AIFs Stock Radar: Breakout from inverse Head & Shoulder pattern on daily charts makes Supreme Industries an attractive buy These large-caps have ‘strong buy’ & ‘buy’ recos and an upside potential of more than 30% Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Slideshow Top Definitions Top Prime Articles Private Companies Top Story Listing Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Worry not. You’re just a step away. It seems like you're already an ETPrime member with Login using your ET Prime credentials to enjoy all member benefits Log out of your current logged-in account and log in again using your ET Prime credentials to enjoy all member benefits. Offer Exclusively For You Save up to Rs. 700/- ON ET PRIME MEMBERSHIP Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get Flat 40% Off Then ₹ 1749 for 1 year Offer Exclusively For You ET Prime at ₹ 49 for 1 month Then ₹ 1749 for 1 year Special Offer Get flat 40% off on ETPrime What’s Included with ETPrime Membership Trump temper on H-1B visas is forcing Indians to do these things to stay put in US What Adani’s US indictment means for India Inc’s overseas fundraising Why veterans like Reliance, L&T are on acquisition spree? Aswath Damodaran has an answer. Will China’s dollar bond sale in Saudi Arabia trump the US in financial world? Huawei launches its own OS to compete with Google and Apple. But can it win beyond China? The problem with lab grown diamonds Why a falling rupee is a better option for the economy A list of top 20 momentum stocks that have delivered massive returns in one year Investment Ideas Grow your wealth with stock ideas & sectoral trends. Stock Reports Plus Buy low & sell high with access to Stock Score, Upside potential & more. BigBull Porfolio Get to know where the market bulls are investing to identify the right stocks. Stock Analyzer Check the score based on the company's fundamentals, solvency, growth, risk & ownership to decide the right stocks. Market Mood Analyze the market sentiments & identify the trend reversal for strategic decisions. Stock Talk Live at 9 AM Daily Ask your stock queries & get assured replies by ET appointed, SEBI registered experts. ePaper - Print View Read the PDF version of ET newspaper. Download & access it offline anytime. ePaper - Digital View Read your daily newspaper in Digital View & get it delivered to your inbox everyday. Wealth Edition Manage your money efficiently with this weekly money management guide. TOI ePaper Read the PDF version of TOI newspaper. Download & access it offline anytime. Deep Explainers Explore the In-depth explanation of complex topics for everyday life decisions. Health+ Stories Get fitter with daily health insights committed to your well-being. Personal Finance+ Stories Manage your wealth better with in-depth insights & updates on finance. New York Times Exclusives Stay globally informed with exclusive story from New York Times. TimesPrime Subscription Access 20+ premium subscriptions like Spotify, Youtube & more. Docubay Subscription Stream new documentaries from all across the world every day. Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Stories you might be interested in
--------------------------------------------------

Title: David’s Bridal Unveils an Upscale Concept
URL: http://wwd.com/business-news/retail/davids-bridal-unveils-upscale-diamonds-pearls-concept-1237798368/
Time Published: 2025-05-20T20:14:19Z
Full Content:
Today's Digital Daily Today's Digital Daily The new format, called Diamond & Pearls, officially launches May 29 in Delray Beach, Fla. Senior Editor, Retail David’s Bridal Inc., seeking to lift its image and widen its appeal to a higher-income audience, has launched a new retail format called Diamonds & Pearls. “It’s a boutique concept, with a curated and tech-enhanced experience blending affordable luxury and couture with a refined, elevated environment,” said Elina Vilk, president and chief business officer of David’s Bridal, in an interview. “Today’s brides are looking for more than a dress. They are looking for an immersive, shopping experience.” With the opening of the 3,500-square-foot Diamonds & Pearls at 9097 West Atlantic Avenue, Suite 104 in Delray Beach, Fla., “We saw an opportunity to reimagine the bridal journey through innovation, curation and style,” Vilk said. The official opening is May 29, though there’s already been a few “soft opening” days to get out any kinks. Related Articles Fashion Features Brides Like '90s-style Wedding Gowns, Brown Flowers and Lace-like Nails, Pinterest Says Designer and Luxury Giambattista Valli Won a Bridal Award in Barcelona Compared with the 193 David’s Bridal stores around the country, and their no-frills, moderate price presentations, Diamonds & Pearls takes a contemporary and personalized approach for the brides, bridesmaids and special occasion categories contained in a smaller space. It still has some of the same selections as David’s Bridal, with approximately one-third of the stock keeping units typically found in a traditional David’s Bridal store, but is curated to the most trending styles and silhouettes. There are also some exclusive designer offerings, from Viola Chan and Marchesa, as well as David Bridal’s signature lines such as Galina Signature, Oleg Cassini and Melissa Sweet, among other labels. Chan will work with clients on a custom design, either in person or through virtual calls. In addition, appointments can be arranged with personal stylists, Champagne is served, walk-ins are welcomed as well and trunk shows are planned. You May Also Like Inside the shop, interactive digital touchscreens powered by Shopify POS provide clients access to David Bridal’s full inventory of dresses, accessories and shoes. “Internally, we are calling it the store of the future,” Vilk said. The future may bring additional Diamonds & Pearls stores, as the company is close to signing another lease on the Northeast Coast. But before green-lighting an expansion of Diamonds & Pearls, Vilk and her team will analyze the performance of the first store for the next four or five months. “We are ‘test and learn’ people,” Vilk said. “We want to see how this store goes, but we anticipate this will be a big part of our strategy next year.” There’s been “so much demand” for the kind of bridal shopping experience Diamonds & Pearls provides. Prices start at $400 to $500, average just over $2,000, and go as high as $6,000 or $7,000 for a custom couture gown, Vilk said. “We are way more affordable than many bridal boutiques,” she said, suggesting other boutiques typically average out with a $4,000 to $5,000 price point. The launch of Diamonds & Pearls reflects David’s Bridal’s rebuilding efforts after going in and out of bankruptcy two times in the last seven years, in 2018 and 2023, leading to the sale of the company to asset manager Cion Investment. (“Diamonds and Pearls” is also the name of a 1991 Prince album.) “We’re actually doing pretty well,” Vilk said when asked how the business is currently performing. “We are completely changing the strategy and driving innovation with concepts like Diamonds & Pearls, becoming more asset-light and taking a tech-forward approach.” Vilk said David’s Bridal is rebuilding its wedding planning service, started a retail media company in January, and is looking at new AI tools. “Gen Z brides are tech-forward. They don’t start their journey in the store anymore. She’ll browse online first, then see it on social media,” said Vilk, a former Hootsuite, Meta, PayPal and Visa executive. “And she wants affordable luxury.” Sign up for WWD news straight to your inbox every day Get all the top news stories and alerts straight to your inbox. Get all the top news stories and alerts straight to your inbox. Send us a tip using our anonymous form. WWD and Women's Wear Daily are part of Penske Media Corporation. © 2025 Fairchild Publishing, LLC. All Rights Reserved.
--------------------------------------------------

Title: Google takes aim at ChatGPT, Meta with flurry of AI products and upgrades
URL: https://nypost.com/2025/05/20/business/google-takes-aim-at-chatgpt-meta-with-ai-products-upgrades/
Time Published: 2025-05-20T18:29:19Z
Full Content:
Alphabet’s Google said Tuesday it would put artificial intelligence into the hands of more Web surfers while teasing a $249.99-a-month subscription for its AI power users, its latest effort to fend off growing competition from startups like OpenAI. Google unveiled the plans at its annual I/O conference in Mountain View, Calif., which has adopted a tone of increased urgency since the rise of generative AI challenged the tech company’s longtime stronghold of organizing and retrieving information on the internet. In recent months, Google has become more aggressive in asserting it has caught up to competitors after appearing flat-footed upon the release of Microsoft-backed OpenAI’s ChatGPT in 2022. Alphabet CEO Sundar Pichai said at the conference: “Over and over, we’ve been able to deliver the best models at the most effective price point.” Its AI assistant Gemini app now has more than 400 million monthly active users, Pichai said. In a major update, the company said consumers across the United States now can switch Google Search into “AI Mode.” Showcased in March as an experiment open to test users, the feature dispenses with the Web’s standard fare in favor of computer-generated answers for complicated queries. Google also announced an “AI Ultra Plan,” which for $249.99 monthly provides users with higher limits on AI and early access to experimental tools like Project Mariner, an internet browser extension that can automate keystrokes and mouse clicks, and Deep Think, a version of its top-shelf Gemini model that is more capable of reasoning through complex tasks. The price is comparable to $200 monthly plans from AI model developers OpenAI and Anthropic, underscoring how companies are exploring ways to pay for the exorbitant costs of AI development. Google’s new plan also includes 30 terabytes of cloud storage and an ad-free YouTube subscription. Google already offers other subscription options, including a $19.99-per-month service with access to some AI capabilities unavailable for most free users and cheaper plans with additional cloud storage. Last week, the company told Reuters it had signed up more than 150 million subscribers across those plans. Pichai told reporters that the rise of generative AI was not at the full expense of online search. This “feels very far from a zero-sum moment,” said Pichai. “The kind of use cases we are serving in search is dramatically expanding” because of AI. Alphabet shares closed 1.5% lower at $165.32 on Tuesday. Google made a return to a bumpy effort years ago around augmented-reality glasses, demonstrating frames with its new Android XR software. Since that time, rival Meta Platforms has brought its own glasses with AI to market. On stage, two Google officials had a conversation in different languages while the glasses typed up translations for them, viewed through the frames’ lenses. Gemini, meanwhile, answered queries about one of the wearer’s surroundings as she walked around Mountain View’s Shoreline Amphitheater. An XR headset being developed in partnership with Samsung will launch later this year, an official said. Google also announced new partnerships with glasses designers Warby Parker and Gentle Monster to develop headsets with Android XR. Earlier this month, Alphabet stock lost $150 billion in market value in one day after an Apple executive testified during one of Google’s antitrust cases that AI offerings had caused a decline in searches on Apple’s Safari Web browser for the first time. In turn, some analysts reassessed how to measure Google’s dominant search market share, with one estimate stating it could fall to less than 50% from around 90% in five years. The analysts cited a behavioral shift drawing consumers toward AI chatbots where they once used traditional search engines. However, Robby Stein, an executive on the search team, in an interview said allowing users to answer more complex questions through AI could enable “new opportunities to create hyper-relevant, useful advertising.” Ads make up the majority of Google’s revenue. Investment in AI accounts for most of Alphabet’s $75 billion in forecasted capital expenditures this year, a dramatic uptick from the $52.5 billion in 2024 spending that the company reported. Tuesday’s announcements included further updates to Google’s effort to deliver to users a “universal AI agent,” which can perform tasks on someone’s behalf without additional prompting. In a number of demos, Google drew on capabilities developed in a testing ground it has called Project Astra to show off what its latest AI could do. These included pointing a smartphone camera at a written invitation and having AI add the event to a user’s calendar. Another showed how Google’s AI could remind students of an upcoming exam and prompt them with an AI-generated quiz to study the material. Google also presented a new AI model called Veo 3 that generates video and audio to create more realistic film snippets for creators. Advertisement
--------------------------------------------------

Title: Don't call it a comeback: Why Goldman sees the downtrodden Magnificent 7 roaring back to beat the market
URL: https://www.businessinsider.com/stock-market-outlook-magnificent-7-tech-stocks-valuations-earnings-nvda-2025-5
Time Published: 2025-05-20T16:50:33Z
Full Content:
The stock market's tech titans stumbled in the first quarter, but 2025 will still be another winning year for the Magnificent Seven, Goldman Sachs said. In a recent note, chief equity strategist David Kostin projected that the mega-cap tech cohort will once again outperform the rest of the S&P 500 in 2025, extending a streak of stellar gains to a third straight year. "We continue to expect that superior earnings growth will drive the Magnificent 7 to outperform the S&P 493 in 2025, but by a smaller magnitude than in recent years," Kostin wrote. "The share price outperformance of the Magnificent 7 has been tied to their earnings growth outperformance." It's a take that might seem contradictory to what's transpired so far this year, as trade policy, AI disruptions, and antitrust moves have muddled the outlook for Big Tech. The group, which includes Nvidia, Tesla, Meta, Amazon, Microsoft, Alphabet, and Apple, is down 5% year-to-date, trailing the 4% gain achieved by the broader index. Some big banks, such as Morgan Stanley, have called on investors to reduce tech exposure. Ye, despite the slump so far, Kostin noted that mega-cap tech demonstrated earnings outperformance in the first quarter. In 2025, earnings-per-share growth has surged 28% for the Mag Seven, handily outpacing 9% for the S&P 493. "This magnitude of surprise was the largest since the 2Q 2021 reporting season when the Magnificent 7 beat earnings estimates by 27%," the bank wrote. "Partly as a result of strong 1Q results, consensus 2025 earnings estimates for the Magnificent 7 are roughly in line with where they began the year." What's more, the top tech stocks are now trading at a discounted valuation, resulting from narrowing earnings growth compared to previous years. "The relative valuation is the lowest it's been in the last two years," Kostin told Bloomberg TV. "From a starting point of entry, it's actually looking somewhat more attractive at these levels." Where Big Tech secrets go public — unfiltered in your inbox weekly. Sign up chevron down icon An icon in the shape of an angle pointing down. Jump to
--------------------------------------------------

Title: Tesla and SpaceX Reputations Plummet in Major Poll
URL: https://www.newsweek.com/tesla-spacex-elon-musk-popularity-2074639
Time Published: 2025-05-20T14:26:43Z
Description: Since Musk joined the Trump administration, Tesla has seen its sales plummet amid backlash over his political views.
--------------------------------------------------

Title: Google pushes AI upgrades, subscriptions at annual I/O developer event amid search challenges
URL: https://www.channelnewsasia.com/business/google-pushes-ai-upgrades-subscriptions-annual-io-developer-event-amid-search-challenges-5142071
Time Published: 2025-05-20T13:06:37Z
Full Content:
Business A Google logo is seen at a company research facility in Mountain View, California, U.S., May 13, 2025. REUTERS/Carlos Barria/File Photo MOUNTAIN VIEW, California :Alphabet's Google said on Tuesday it would put artificial intelligence into the hands of more Web surfers while teasing a $249.99-a-month subscription for its AI power users, its latest effort to fend off growing competition from startups like OpenAI. Google unveiled the plans amid a flurry of demos that included new smart glasses during its annual I/O conference in Mountain View, California, which has adopted a tone of increased urgency since the rise of generative AI challenged the tech company's longtime stronghold of organizing and retrieving information on the internet. In recent months, Google has become more aggressive in asserting it has caught up to competitors after appearing flat-footed upon the release of Microsoft-backed OpenAI's ChatGPT in 2022. On Tuesday, it further laid out a vision for Google Search that lets consumers ask virtually anything, from simple queries to complex research questions, from analyzing what a smartphone camera sees to fetching an event ticket to buy. Google likewise said it aims to build AI that is personal and proactive, whether phoning a store for users or sending students a practice test generated on the fly. Subscribe to our Chief Editor’s Week in Review Our chief editor shares analysis and picks of the week's biggest news every Saturday. This service is not intended for persons residing in the E.U. By clicking subscribe, I agree to receive news updates and promotional material from Mediacorp and Mediacorp’s partners. Loading CEO Sundar Pichai said at the conference that Alphabet would build such AI with the cost in mind as well. "Over and over, we've been able to deliver the best models at the most effective price point," he said.Google's AI assistant Gemini app now has more than 400 million monthly active users, Pichai said.In a major update, the company said consumers across the United States now can switch Google Search into “AI Mode.” Showcased in March as an experiment open to test users, the feature dispenses with the Web’s standard fare in favor of computer-generated answers for complicated queries.Google also announced an “AI Ultra Plan,” which for $249.99 monthly provides users with higher limits on AI and early access to experimental tools like Project Mariner, an internet browser extension that can automate keystrokes and mouse clicks, and Deep Think, a version of its top-shelf Gemini model that is more capable of reasoning through complicated tasks.The price is comparable to $200 monthly plans from AI model developers OpenAI and Anthropic, underscoring how companies are exploring ways to pay for the exorbitant price tag of AI development. Google's new plan also includes 30 terabytes of cloud storage and an ad-free YouTube subscription.Google already offers other subscription options, including a $19.99-per-month service with access to some AI capabilities unavailable for most free users and cheaper plans with additional cloud storage. Last week, the company told Reuters it had signed up more than 150 million subscribers across those plans.Pichai told reporters that the rise of generative AI was not at the full expense of online search.This “feels very far from a zero-sum moment,” said Pichai. “The kind of use cases we are serving in search is dramatically expanding” because of AI.Alphabet shares closed 1.5 per cent lower at $165.32 on Tuesday. RETURN TO GLASSESGoogle made a return to a bumpy effort years ago around smart glasses, demonstrating frames with its new Android XR software. Since its early efforts, rival Meta Platforms has brought its own glasses with AI to market.On stage, two Google officials had a conversation in different languages while the glasses typed up translations for them, viewed through the frames' lenses.Gemini, meanwhile, answered queries about one of the wearer's surroundings as she walked around Mountain View's Shoreline Amphitheater.An XR headset being developed in partnership with Samsung will launch later this year, an official said. Google also announced new partnerships with glasses designers Warby Parker and Gentle Monster to develop headsets with Android XR.SEARCH PRESSURESEarlier this month, Alphabet stock lost $150 billion in market value in one day after an Apple executive testified during one of Google's antitrust cases that AI offerings had caused a decline in searches on Apple's Safari Web browser for the first time.In turn, some analysts reassessed how to measure Google's dominant search market share, with one estimate stating it could fall to less than 50 per cent from around 90 per cent in five years. The analysts cited a behavioral shift drawing consumers toward AI chatbots where they once used traditional search engines.However, Robby Stein, an executive on the search team, in an interview said allowing users to answer more challenging questions through AI could enable "new opportunities to create hyper-relevant, useful advertising." Ads make up the majority of Google's revenue.Investment in AI accounts for most of Alphabet's $75 billion in forecasted capital expenditures this year, a dramatic uptick from the $52.5 billion in 2024 spending that the company reported.Tuesday's announcements included further updates to Google's work to deliver a "universal AI agent," which can perform tasks on someone’s behalf without additional prompting.In a number of demos, Google drew on capabilities developed in a testing ground it has called Project Astra to show off what its latest AI could do.These included pointing a smartphone camera at a written invitation and having AI add the event to a user’s calendar.Google also presented a new AI model called Veo 3 that generates video and audio to create more realistic film snippets for creators. CEO Sundar Pichai said at the conference that Alphabet would build such AI with the cost in mind as well. "Over and over, we've been able to deliver the best models at the most effective price point," he said. Google's AI assistant Gemini app now has more than 400 million monthly active users, Pichai said. In a major update, the company said consumers across the United States now can switch Google Search into “AI Mode.” Showcased in March as an experiment open to test users, the feature dispenses with the Web’s standard fare in favor of computer-generated answers for complicated queries. Google also announced an “AI Ultra Plan,” which for $249.99 monthly provides users with higher limits on AI and early access to experimental tools like Project Mariner, an internet browser extension that can automate keystrokes and mouse clicks, and Deep Think, a version of its top-shelf Gemini model that is more capable of reasoning through complicated tasks. The price is comparable to $200 monthly plans from AI model developers OpenAI and Anthropic, underscoring how companies are exploring ways to pay for the exorbitant price tag of AI development. Google's new plan also includes 30 terabytes of cloud storage and an ad-free YouTube subscription. Google already offers other subscription options, including a $19.99-per-month service with access to some AI capabilities unavailable for most free users and cheaper plans with additional cloud storage. Last week, the company told Reuters it had signed up more than 150 million subscribers across those plans. Pichai told reporters that the rise of generative AI was not at the full expense of online search. This “feels very far from a zero-sum moment,” said Pichai. “The kind of use cases we are serving in search is dramatically expanding” because of AI. Alphabet shares closed 1.5 per cent lower at $165.32 on Tuesday. RETURN TO GLASSES Google made a return to a bumpy effort years ago around smart glasses, demonstrating frames with its new Android XR software. Since its early efforts, rival Meta Platforms has brought its own glasses with AI to market. On stage, two Google officials had a conversation in different languages while the glasses typed up translations for them, viewed through the frames' lenses. Gemini, meanwhile, answered queries about one of the wearer's surroundings as she walked around Mountain View's Shoreline Amphitheater. An XR headset being developed in partnership with Samsung will launch later this year, an official said. Google also announced new partnerships with glasses designers Warby Parker and Gentle Monster to develop headsets with Android XR. SEARCH PRESSURES Earlier this month, Alphabet stock lost $150 billion in market value in one day after an Apple executive testified during one of Google's antitrust cases that AI offerings had caused a decline in searches on Apple's Safari Web browser for the first time. In turn, some analysts reassessed how to measure Google's dominant search market share, with one estimate stating it could fall to less than 50 per cent from around 90 per cent in five years. The analysts cited a behavioral shift drawing consumers toward AI chatbots where they once used traditional search engines. However, Robby Stein, an executive on the search team, in an interview said allowing users to answer more challenging questions through AI could enable "new opportunities to create hyper-relevant, useful advertising." Ads make up the majority of Google's revenue. Investment in AI accounts for most of Alphabet's $75 billion in forecasted capital expenditures this year, a dramatic uptick from the $52.5 billion in 2024 spending that the company reported. Tuesday's announcements included further updates to Google's work to deliver a "universal AI agent," which can perform tasks on someone’s behalf without additional prompting. In a number of demos, Google drew on capabilities developed in a testing ground it has called Project Astra to show off what its latest AI could do. These included pointing a smartphone camera at a written invitation and having AI add the event to a user’s calendar. Google also presented a new AI model called Veo 3 that generates video and audio to create more realistic film snippets for creators. Subscribe to our Chief Editor’s Week in Review Our chief editor shares analysis and picks of the week's biggest news every Saturday. Get our pick of top stories and thought-provoking articles in your inbox Stay updated with notifications for breaking news and our best stories Get WhatsApp alerts Join our channel for the top reads for the day on your preferred chat app
--------------------------------------------------

Title: How Much You’d Have If You Invested $1K in These Top 8 IPOs of the Past 2 Decades
URL: https://finance.yahoo.com/news/much-d-invested-1k-top-130146535.html
Time Published: 2025-05-20T13:01:46Z
Description: Initial public offerings, or IPOs, are when a company offers its stock shares to the general public for the first time. The process is also known as "going...
--------------------------------------------------

Title: U.S. stock market futures May 20: Dow, S&P, Nasdaq mixed as Moody’s downgrade, Fed speeches shake confidence; Home Depot defies tariffs, Walmart warns on prices, global banks cut rates
URL: https://economictimes.indiatimes.com/news/international/us/u-s-stock-market-futures-may-20-dow-sp-nasdaq-mixed-as-moodys-downgrade-fed-speeches-shake-confidence-home-depot-defies-tariffs-walmart-warns-on-prices-global-banks-cut-rates/articleshow/121292381.cms
Time Published: 2025-05-20T12:06:59Z
Full Content:
(Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. The BrahMos link that fired up this defence stock 45% in one month Explainer: Why RBI took a U-turn on banks and NBFCs investing in AIFs How Azerbaijan’s support for Pak could put USD780 million trade at risk How to effectively navigate the volatile world of small-cap investment Why Azim Premji is getting into aviation, a sector Warren Buffett shunned for long Stock Radar: HAL breaks out from 8-month consolidation on daily charts; check target price & stop loss for long positions BJP blasts Congress over Bengaluru floods Trump gets SC nod to deport 350,000 Venezuelans NJ US Attorney charges Dem Congresswoman over ICE incident UK, France, Canada warn Israel over Gaza war Sambhal authorities launch demolition drive to clear roads US carries out first voluntary deportation flight LIVE: Congress leaders address Samarpane Sankalpa Rally 'CCI promote fair competition & prevent competitive practices' Trump, Putin hold starkly different talks on Ukraine war ‘I’ll shove it up their a**’: Trump blasts rigged 2020 election Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Definitions Top Prime Articles Top Market Pages Top Story Listing Top Slideshow Private Companies Top Commodities Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: What to know about Netflix's downgrade & 'Sesame Street' addition
URL: https://finance.yahoo.com/video/know-netflixs-downgrade-sesame-street-211559446.html
Time Published: 2025-05-19T21:15:59Z
Description: Netflix (NFLX) is rebounding from earlier losses after being downgraded by JPMorgan analysts to Neutral from Overweight. Yahoo Finance Senior Reporter Allie ...
--------------------------------------------------

Title: GraniteShares Announces Change in ETF Lineup
URL: https://www.globenewswire.com/news-release/2025/05/19/3084412/0/en/GraniteShares-Announces-Change-in-ETF-Lineup.html
Time Published: 2025-05-19T20:11:00Z
Full Content:
May 19, 2025 16:11 ET | Source: GraniteShares GraniteShares NEW YORK, May 19, 2025 (GLOBE NEWSWIRE) -- GraniteShares announced today that it will close and liquidate the following ETF: On May 09, 2025, the board of GraniteShares ETF Trust approved the liquidation of the GraniteShares 1x Short AMD Daily ETF (the “ETF”). The last day of trading for the ETF on NASDAQ Stock Market will be June 20, 2025. The last day creation orders will be accepted for the ETF will be June 18, 2025. Investors may sell their shares of the ETF until market close on June 20, 2025. Shares of the ETF will no longer trade on NASDAQ Stock Market after market close on June 20, 2025, and will be subsequently delisted. The final distribution to shareholders of the ETF is expected to occur on or about June 23, 2025. When the ETF commences the liquidation of its portfolio, it may hold cash and securities that may not be consistent with the ETF’s investment objectives and strategies. At the time the liquidation of the ETF is complete, the ETF shares will be individually redeemed. For shareholders that still hold shares of the ETF as of June 20, 2025, shares will be automatically redeemed for cash at the net asset value as of close of business on that date, which will reflect the costs of closing the ETF. Shareholders will generally recognize a capital gain or loss on the redemptions. The ETF may or may not pay one or more dividends or other distributions prior to or along with the redemption payments. About GraniteShares GraniteShares is an independent ETF issuer headquartered in New York City. GraniteShares will continue to offer the following leveraged single stock ETFs: In addition, GraniteShares’ ETF suite includes the following ETFs: Contact Information:William Rhind, CEO GraniteShares Inc +1 646 876 5049 william.rhind@graniteshares.com Important Information Investors should consider the investment objectives, risks, charges and expenses of the GraniteShares funds (the “Funds”) carefully before investing. For a prospectus or summary prospectus with this and other information about the Funds, please call (844) 476 8747, or visit the website at www.graniteshares.com. Read the prospectus or summary prospectus carefully before investing. To obtain a prospectus for BAR, please visithttps://www.graniteshares.com/Documents/25/Prospectus-GraniteShares-Gold-Trust.pdfTo obtain a prospectus for PLTM, please visithttps://graniteshares.com/media/gwrbh3ah/pltm_prospectus.pdfTo obtain a prospectus for COMB, please visithttps://graniteshares.com/media/4crf2x4e/graniteshares-etf-trust-comb-summary-prospectus.pdf Except as described above regarding the liquidation of the ETFs, shares of the Funds may be sold during trading hours on the exchange through any brokerage account, shares are not individually redeemable, and shares may only be redeemed directly from a Fund by Authorized Participants. There can be no assurance that an active trading market for shares in a Fund will develop or be maintained. Shares may trade above or below NAV. Brokerage commissions will apply. Fund Risks Multiple funds have a limited operating history of less than a year and risks associated with a new fund. The Leveraged and Daily Inverse Funds are not suitable for all investors. The investment program of the funds is speculative, entails substantial risks and include asset classes and investment techniques not employed by most ETFs and mutual funds. Investments in the ETFs are not bank deposits and are not insured or guaranteed by the Federal Deposit Insurance Corporation or any other government agency. The Fund is designed to be utilized only by knowledgeable investors who understand the potential consequences of seeking daily leveraged (2X) or daily inverse (-1X and -2X) investment results, understand the risks associated with the use of leverage and are willing to monitor their portfolios frequently. For periods longer than a single day, the Fund will lose money if the Underlying Stock’s performance is flat, and it is possible that the Fund will lose money even if the Underlying Stock’s performance increases over a period longer than a single day. An investor could lose the full principal value of his/her investment within a single day. The funds do not directly invest in the underlying stock. The Funds seek daily inverse or leveraged investment results and are intended to be used as short-term trading vehicles. Each Fund with “Long” in its name attempts to provide daily investment results that correspond to the respective long leveraged multiple of the performance of an underlying stock (each a Leveraged Long Fund). Each Fund with “Short” in its name attempts to provide daily investment results that correspond to the inverse (or opposite) multiple of the performance of an underlying stock (each an Inverse Fund). Investors should note that the Long Leveraged Funds and the Daily Inverse Funds pursue daily leveraged investment objectives and daily inverse investment objectives (respectively), which means that the fund is riskier than alternatives that do not use leverage and inverse strategies because the fund magnifies the performance of their underlying security. The volatility of the underlying security may affect a Funds' return as much as, or more than, the return of the underlying security. For the Leveraged Long Funds because of daily rebalancing and the compounding of each day’s return over time, the return of the Fund for periods longer than a single day will be the result of each day’s returns compounded over the period, which will very likely differ from 200% of the return of the Underlying Stock over the same period. The Fund will lose money if the Underlying Stock’s performance is flat over time, and as a result of daily rebalancing, the Underlying Stock volatility and the effects of compounding, it is even possible that the Fund will lose money over time while the Underlying Stock’s performance increases over a period longer than a single day. For the Daily Inverse Funds because of daily rebalancing and the compounding of each day’s return over time, the return of the Fund for periods longer than a single day will be the result of each day’s returns compounded over the period, which will very likely differ from -100% and 200% of the return of the Underlying Stock over the same period. The Fund will lose money if the Underlying Stock’s performance is flat over time, and as a result of daily rebalancing, the Underlying Stock volatility and the effects of compounding, it is even possible that the Fund will lose money over time while the Underlying Stock’s performance decreases over a period longer than a single day. Shares are bought and sold at market price (not NAV) and are not individually redeemed from the ETF. There can be no guarantee that an active trading market for ETF shares will develop or be maintained, or that their listing will continue or remain unchanged. Buying or selling ETF shares on an exchange may require the payment of brokerage commissions and frequent trading may incur brokerage costs that detract significantly from investment returns. An investment in the Fund involves risk, including the possible loss of principal. The Fund is non-diversified and includes risks associated with the Fund concentrating its investments in a particular industry, sector, or geographic region which can result in increased volatility. The use of derivatives such as futures contracts and swaps are subject to market risks that may cause their price to fluctuate over time. Risks of the Fund include Effects of Compounding and Market Volatility Risk, Inverse Risk, Market Risk, Counterparty Risk, Rebalancing Risk, Intra-Day Investment Risk, Daily Index Correlation Risk, Other Investment Companies (including ETFs) Risk, and risks specific to the securities of the Underlying Stock and the sector in which it operates. These and other risks can be found in the prospectus. Investing in physical commodities, including through commodity-linked derivative instruments such as Commodity Futures, Commodity Swaps, as well as other commodity-linked instruments, is speculative and can be extremely volatile and may not be suitable for all investors. Market prices of commodities may fluctuate rapidly based on numerous factors, including: changes in supply and demand relationships (whether actual, perceived, anticipated, unanticipated or unrealized); weather; agriculture; trade; domestic and foreign political and economic events and policies; diseases; pestilence; technological developments; currency exchange rate fluctuations; and monetary and other governmental policies, action and inaction. A liquid secondary market may not exist for the types of commodity-linked derivative instruments the Fund buys, which may make it difficult for the Fund to sell them at an acceptable price. The Fund is new with no operating history. As a result, there can be no assurance that the Fund will grow to or maintain an economically viable size, in which case it could ultimately liquidate. Derivatives may be more sensitive to changes in market conditions and may amplify risks and losses. This information is not an offer to sell or a solicitation of an offer to buy shares of any Funds to any person in any jurisdiction in which an offer, solicitation, purchase or sale would be unlawful under the securities laws of such jurisdiction. Please consult your tax advisor about the tax consequences of an investment in Fund shares, including the possible application of foreign, state, and local tax laws. You could lose money by investing in the ETFs. There can be no assurance that the investment objective of the Funds will be achieved. None of the Funds should be relied upon as a complete investment program. The Fund is distributed by ALPS Distributors, Inc, which is not affiliated with GraniteShares or any of its affiliates ©2025 GraniteShares Inc. All rights reserved. GraniteShares, GraniteShares Trusts, and the GraniteShares logo are registered and unregistered trademarks of GraniteShares Inc., in the United States and elsewhere. All other marks are the property of their respective owners NEW YORK, May 21, 2025 (GLOBE NEWSWIRE) -- GraniteShares, an ETF issuer specializing in high conviction ETFs, announced that it is launching two ETFs to add to its existing YieldBOOST lineup - the... NEW YORK, May 05, 2025 (GLOBE NEWSWIRE) -- As announced on March 05, 2025 the investment strategy for the GraniteShares 1x Short COIN Daily ETF (ticker: CONI) will be amended and result in a new...
--------------------------------------------------

Title: AI and Work (Some Predictions)
URL: https://calnewport.com/ai-and-work-some-predictions/
Time Published: 2025-05-19T17:41:52Z
Full Content:
One of the main topics of this newsletter is the quest to cultivate sustainable and meaningful work in a digital age. Given this objective, it’s hard to avoid confronting the furiously disruptive potentials of AI. I’ve been spending a lot time in recent years, in my roles as a digital theorist and technology journalist, researching and writing about this topic, so it occurred to me that it might be useful to capture in one place all of my current thoughts about the intersection of AI and work. The obvious caveat applies: these predictions will shift — perhaps even substantially — as this inherently unpredictable sector continues to evolve. But here’s my current best stab at what’s going on now, what’s coming soon, and what’s likely just hype. Let’s get to it… When generative AI made its show-stopping debut a few years ago, the smart money was on text production becoming the first killer app. For example, business users, it was thought, would soon outsource much of the tedious communication that makes up their day — meeting summaries, email, reports — into AI tools. A fair amount of this is happening, especially when it comes to lengthy utilitarian communication where the quality doesn’t matter much. I recently attended a men’s retreat, for example, and it was clear that the organizer had used ChatGPT to create the final email summarizing the weekend schedule. And why not? It got the job done and saved some time. It’s becoming increasingly clear, however, that for most people the act of writing in their daily lives isn’t a major problem that needs to be solved, which is capping the predicted ubiquity of this use case. (A survey of internet users found that only around 5.4% had used ChatGPT to help write emails and letters. And this includes the many who maybe experimented with this capability once or twice before moving on.) The application that has instead leaped ahead to become the most exciting and popular use of these tools is smart search. If you have a question, instead of turning to Google you can query a new version of ChatGPT or Claude. These models can search the web to gather information, but unlike a traditional search engine, they can also process the information they find and summarize for you only what you care about. Want the information presented in a particular format, like a spreadsheet or a chart? A high-end model like GPT-4o can do this for you as well, saving even more extra steps. Smart search has become the first killer app of the generative AI era because, like any good killer app, it takes an activity most people already do all the time — typing search queries into web sites — and provides a substantially, almost magically better experience. This feels similar to electronic spreadsheets conquering paper ledger books or email immediately replacing voice mail and fax. I would estimate that around 90% of the examples I see online right now from people exclaiming over the potential of AI are people conducting smart searches. This behavioral shift is appearing in the data. A recent survey conducted by Future found that 27% of US-based respondents had used AI tools such as ChatGPT instead of a traditional search engine. From an economic perspective, this shift matters. Earlier this month, the stock price for Alphabet, the parent company for Google, fell after an Apple executive revealed that Google searches through the Safari web browser had decreased over the previous two months, likely due to the increased use of AI tools. Keep in mind, web search is a massive business, with Google earning over $175 billion from search ads in 2023 alone. In my opinion, becoming the new Google Search is likely the best bet for a company like OpenAI to achieve profitability, even if it’s not as sexy as creating AGI or automating all of knowledge work (more on these applications later). The other major success story for generative AI at the moment is computer programming. Individuals with only rudimentary knowledge of programming languages can now produce usable prototypes of simple applications using tools like ChatGPT, and somewhat more advanced projects with AI-enhanced agent-style helpers like Roo Code. This can be really useful for quickly creating tools for personal use or seeking to create a proof-of-concept for a future product. The tech incubator Y Combinator, for example, made waves when they reported that a quarter of the start-ups in their Winter 2025 batch generated 95% or more of their product’s codebases using AI. How far can this automated coding take us? An academic computer scientist named Judah Diament recently went viral for noting that the ability for novice users to create simple applications isn’t new. There have been systems dedicated to this purpose for over four decades, from HyperCard to VisualBasic to Flash. As he elaborates: “And, of course, they all broke down when anything slightly complicated or unusual needs to be done (as required by every real, financially viable software product or service).” This observation created major backlash — as does most expressions of AI skepticism these days — but Diament isn’t wrong. Despite recent hyperbolic statements by tech leaders, many professional programmers aren’t particularly worried that their jobs can be replicated by language model queries, as so much of what they do is experience-based architecture design and debugging, which are unrelated skills for which we currently have no viable AI solution. Software developers do, however, use AI heavily: not to produce their code from scratch, but instead as helper utilities. Tools like GitHub’s Copilot are integrated directly into the environments in which these developers already work, and make it much simpler to look up obscure library or AI calls, or spit out tedious boilerplate code. The productivity gains here are notable. Programming without help from AI is rapidly becoming increasingly rare. Language model-based AI systems can respond to prompts in pretty amazing ways. But if we focus only on outputs, we underestimate another major source of these models’ value: their ability to understand human language. This so-called natural language processing ability is poised to transform how we use software. There is a push at the moment, for example, led by Microsoft and its Copilot product (not to be confused with GitHub Copilot), to use AI models to provide natural language interfaces to popular software. Instead of learning complicated sequences of clicks and settings to accomplish a task in these programs, you’ll be able to simply ask for what you need; e.g., “Hey Copilot, can you remove all rows from this spreadsheet where the dollar amount in column C is less than $10 dollars then sort everything that remains by the names in Column A? Also, the font is too small, make it somewhat larger.” Enabling novice users to access to expert-level features in existing software will aggregate into huge productivity gains. As a bonus, the models required to understand these commands don’t have to be nearly as massive and complicated as the current cutting-edge models that the big AI companies use to show off their technology. Indeed, they might be small enough to run locally on devices, making them vastly cheaper and more efficient to operate. Don’t sleep on this use case. Like smart search, it’s also not as sexy as AGI or full automation, but I’m increasingly convinced that within the next half-decade or so, informally-articulated commands are going to emerge as one of the dominate interfaces to the world of computation. One of the more attention-catching storylines surrounding AI at the moment is the imminent arrival of so-called agents which will automate more and more of our daily work, especially in the knowledge sectors once believed to be immune from machine encroachment. Recent reports imply that agents are a major part of OpenAI’s revenue strategy for the near future. The company imagines business customers paying up to $20,000 a month for access to specialized bots that can perform key professional tasks. It’s the projection of this trend that led Elon Musk to recently quip: “If you want to do a job that’s kinda like a hobby, you can do a job. But otherwise, AI and the robots will provide any goods and services that you want.” But progress in creating these agents has recently slowed. To understand why requires a brief snapshot of the current state of generative AI technology… Not long ago, there was a belief in so-called scaling laws that argued, roughly speaking, that as you continued to increase the size of language models, their abilities would continue to rapidly increase. For a while this proved true: GPT-2 was much better than the original GPT, GPT-3 was much better than GPT-2, and GPT-4 was a big improvement on GPT-3. The hope was that by continuing to scale these models, you’d eventually get to a system so smart and capable that it would achieve something like AGI, and could be used as the foundation for software agents to automate basically any conceivable task. More recently, however, these scaling laws have begun to falter. Companies continue to invest massive amounts of capital in building bigger models, trained on ever-more GPUs crunching ever-larger data sets, but the performance of these models stopped leaping forward as much as they had in the past. This is why the long-anticipated GPT-5 has not yet been released, and why, just last week, Meta announced they were delaying the release of their newest, biggest model, as its capabilities were deemed insufficiently better than its predecessor. In response to the collapse of the scaling laws, the industry has increasingly turned its attention in another direction: tuning existing models using reinforcement learning. Say, for example, you want to make a model that is particularly good at math. You pay a bunch of math PhDs $100 an hour to come up with a lot of math problems with step-by-step solutions. You then take an existing model, like GPT-4, and feed it these problems one-by-one, using reinforcement learning techniques to tell it exactly where it’s getting certain steps in its answers right or wrong. Over time, this tuned model will get better at solving this specific type of problem. This technique is why OpenAI is now releasing multiple, confusingly-named models, each seemingly optimized for different specialties. These are the result of distinct tunings. They would have preferred, of course, to simply produce a GPT-5 model that could do well on all of these tasks, but that hasn’t worked out as they hoped. This tuning approach will continue to develop interesting tools, but it will be much more piecemeal and hit-or-miss than what was anticipated when we still believed in scaling laws. Part of the difficulty is that this approach depends on finding the right data for each task you want to tackle. Certain problems, like math, computer programming, and logical reasoning, are well-suited for tuning as they can be described by pairs of prompts and correct answers. But this is not the case for many other business activities, which can be esoteric and bespoke to a given context. This means many useful activities will remain un-automatable by language model agents into the foreseeable future. I once said that the real Turing Test for our current age is an AI system that can successfully empty my email inbox, a goal that requires the mastery of any number of complicated tasks. Unfortunately for all of us, this is not a test we’re poised to see passed any time soon. The Free Press recently published an article titled “AI Will Change What it Means to Be Human. Are We Ready?”. It summarized a common sentiment that has been feverishly promoted by Silicon Valley in recent years: that AI is on the cusp of changing everything in unfathomably disruptive ways. As the article argues: OpenAI CEO Sam Altman asserted in a recent talk that GPT-5 will be smarter than all of us. Anthropic CEO Dario Amodei described the powerful AI systems to come as “a country of geniuses in a data center.” These are not radical predictions. They are nearly here. But here’s the thing: these are radical predictions. Many companies tried to build the equivalent of the proposed GPT-5 and found that continuing to scale up the size of their models isn’t yielding the desired results. As described above, they’re left tuning the models they already have for specific tasks that are well-described by synthetic data sets. This can produce cool demos and products, but it’s not a route to a singular “genius” system that’s smarter than humans in some general sense. Indeed, if you look closer at the rhetoric of the AI prophets in recent months, you’ll see a creeping awareness that, in a post-scaling law world, they no longer have a convincing story for how their predictions will manifest. A recent Nick Bostrom video, for example, which (true to character) predicts Superintelligence might happen in less than two years (!), adds the caveat that this outcome will require key “unlocks” from the industry, which is code for we don’t know how to build systems that achieve this goal, but, hey, maybe someone will figure it out! (The AI centrist Gary Marcus subsequently mocked Bostrom by tweeting: “for all we know, we could be just one unlock and 3-6 weeks away from levitation, interstellar travel, immortality, or room temperature superconductors, or perhaps even all four!”) Similarly, if you look closer at AI 2027, the splashy new doomsday manifesto which argues that AI might eliminate humanity as early as 2030, you won’t find a specific account of what type of system might be capable of such feats of tyrannical brilliance. The authors instead sidestep the issue by claiming that within the next year or so, the language models we’re tuning to solve computer programming tasks will somehow come up with, on their own, code that implements breakthrough new AI technology that mere humans cannot understand. This is an incredible claim. (What sort of synthetic data set do they imagine being able to train a language model to crack the secrets of human-level intelligence?) It’s the technological equivalent of looking at the Wright Brother’s Flyer in 1903 and thinking, “well, if they could figure this out so quickly, we should have space travel cracked by the end of the decade.” The current energized narratives around AGI and Superintelligence seem to be fueled by a convergence of three factors: (1) the fact that scaling laws did apply for the first few generations of language models, making it easy and logical to imagine them continuing to apply up the exponential curve of capabilities in the years ahead; (2) demos of models tuned to do well on specific written tests, which we tend to intuitively associate with intelligence; and (3) tech leaders pounding furiously on the drums of sensationalism, knowing they’re rarely held to account on their predictions. But here’s the reality: We are not currently on a trajectory to genius systems. We might figure this out in the future, but the “unlocks” required will be sufficiently numerous and slow to master that we’ll likely have plenty of clear signals and warning along the way. So, we’re not out of the woods on these issues, but at the same time, humanity is not going to be eliminated by the machines in 2030 either. In the meantime, the breakthroughs that are happening, especially in the world of work, should be both exciting and worrisome enough on their own for now. Let’s grapple with those first. #### For more of my thoughts on AI, check out my New Yorker archive and my podcast (in recent months, I often discuss AI in the third act of the show). For more on my thoughts on technology and work more generally, check out my recent books on the topic: Slow Productivity, A World Without Email, and Deep Work. I agree with natural language commands to be an awesome use-case that people are sleeping on! I have two examples of messing around with ai where I realized if they just got a little bit better at this, it would change everything. First, ai can already identify information in photos. I uploaded a picture of myself to ChatGPT and it started describing how I look. That was a neat trick. But once you can upload screenshots and it can read, transcribe, or act on every piece of information in the photo? Hey here is a screenshot of my spreadsheet, tell me how to fix it. Hey here is a screenshot of a web design that I like, please convert it into code for me. Second, using Cursor.ai IDE, I can ask the agents how to use the program and where things are in the program and of course it can write code, save files, delete files, make directories, prompt commands, and be a search engine all in one. I didn’t know where some of the stuff in the program was and it just told me where it was. I didn’t know how everything worked in the program and it just told me how it works. And then of course it also codes and makes commands for everything I ask. So imagine a future where in every single program you use on the computer, you can quickly screenshot it and ask how to do something or how to fix something and it tells you exactly how to do it, or just does it for you. Now I feel like all the talk about real agents is kind of hyped up? Like just because my IDE can do IDE tasks for me people seem to think agents are going to do my taxes for me and answer all my emails for me automatically. But I think that’s kind of just a dream. “Agent” seems to just be the new term for a long complicated process that had been automated. But I bet instead of the “agent” figuring all that out for itself, it’s going to happen the old fashioned way, where a bunch of engineers work very hard to automate a task. You know, like we’ve been doing for decades already. The photo analysis example is a great one. I think of that as part of the general idea of a super search, where now, the sources for its answer isn’t just the web, but also other things, like photos, or files on your local machine, etc. Sort of like a Google for everything which is, it seems to me, likely where the bulk of the money is actually waiting in the AI race. I agree that superintelligence in 2030 is very unlikely, and it’s good to communicate that. But I think superintelligence in 2040 is totally in the territory of “maybe, maybe not, who knows”, and by not emphasizing or mentioning that, you’re burying the lede. Anything that humans, and groups of humans, and societies of humans, can do, can also be done by code running on chips. After all, the human brain is a machine that runs an algorithm. No one knows the algorithm, but as soon as someone figures it out, they could run it on chips, like any other algorithm. Then we’d get AIs that can autonomously found and run billion-dollar companies, that can invent science and technology and language from scratch, and all the other crazy sci-fi-sounding stuff. And there could be hundreds of millions of such AIs, thinking much faster than humans, etc. Why do I say that “superintelligence by 2040” is in “maybe, maybe not, who knows” territory? Well, LLMs didn’t exist as recently as 2018. The entirety of deep learning was a backwater as recently as 2012. You can say that LLMs are not on the path to superintelligence, and as it happens, I’ll agree with you. But 15 years is evidently more than enough time for the AI research community to have pivoted to something entirely different and poured billions of dollars, and tens of thousands of person-years, into polishing and optimizing it. As an analogy, suppose a giant fleet of alien ships were definitely due to arrive on Earth at some point, but we didn’t know when, and 15-40 years seemed like a reasonable guess. But some people were going around saying the aliens would definitely arrive in <5 years. Then it would be good to correct those people! But it would be bad to imply that therefore we shouldn’t be thinking about and preparing for the alien invasion right now. The death of Moore’s Law (measuring transistors) was the birth of LLM scaling / virtual quantity (measuring virtual parameters). The death of LLM scaling / virtual quantity will be virtual quality. The manifestation of the thing that is manifesting has been going on for 70+ years and there have been dozens of barriers that were said to be impassible, but the manifestation has always found a new thing. Now that the manifestation has manifested the ability to create PhD’s out of thin air to work for its manifestation, now is when you are going to doubt it ?? Humanity is a small thing, and the physical universe is a shallow one. Now some wave from a bigger pond is washing into ours. Sadhguru has a nice take on this current affair https://youtu.be/uGwKU1jaDsY?si=J29PtlB8BuB8iTnx&t=2525 What I am most anxious to see play out is how the next generation of youth engage with Ai tools. The kids who are under 12 today, how will they perform in a world of automation having never known anything other than Ai ubiquity? As a parallel example, take social media. When Facebook launched, a generation of Xennials and first-wave Millennials initially used it as a tool to complement their pre-existing social lives (think wall posts and group events). In other words, when a generation of people with pre-established social skills adopt a new comms tech/device, they see it as complementary to their social world. They lived through the transition. It’s not until later that we see Gen Z struggling with social media, struggling to establish social skills through the devices and platforms. Why? Because Gen Z didn’t grow up in a world without them. They didn’t understand that those social apps were never meant to create social skills and connections, but only to complement the skills and connections you already had. So going back to Ai, many of the people adopting Ai tools and praising their capabilities–those people already had critical thinking skills and domain expertise in the first place! They are using it to complement their pre-existing work! What is completely unpredictable is how the untrained, unknowledgeable generation of youth coming up next will be able to perform in a world in which whatever skills and knowledge they manage to pick up in school are basically deemed low value because, well, “Ai can already do that”. What then for those kids? Comment Save my name, email, and website in this browser for the next time I comment. Cal launched the "Study Hacks" blog at calnewport.com in 2007, and has been regularly publishing essays here ever since. Over 2,000,000 people a year visit this site to read Cal's weekly posts about technology, productivity, and the quest to live and work deeply in an increasingly distracted world, while tens of thousands more subscribe to have these essays delivered directly to their inbox (see the sign-up form below). To read more, you can browse more than 15 years of past essays in the archive.In the fall of 2022, Cal launched a new portal, TheDeepLife.com, to serve as the online home for all other content relevant to the deep life movement he helped initiate. Here you can find all past episodes of Cal's popular podcast, Deep Questions, and explore an extensive library of original videos. This site is the online home for the computer science professor and bestselling author Cal Newport. Here you can learn more about Cal and both his general-audience and academic writing. You can also browse and subscribe to his long-running weekly essay series. For more on Cal's podcast, videos, and online courses, please visit his media portal, TheDeepLife.com Academic Communication[email protected] Media Inquires[email protected] All Other RequestsSee Contact Page
--------------------------------------------------

Title: Vulnerability Summary for the Week of May 12, 2025
URL: https://www.cisa.gov/news-events/bulletins/sb25-139
Time Published: 2025-05-19T16:04:02Z
Full Content:
An official website of the United States government Here’s how you know Official websites use .gov A .gov website belongs to an official government organization in the United States. Secure .gov websites use HTTPS A lock (LockA locked padlock) or https:// means you’ve safely connected to the .gov website. Share sensitive information only on official, secure websites. Free Cyber ServicesSecure by design Secure Our WorldShields UpReport A Cyber Issue Search Free Cyber ServicesSecure by design Secure Our WorldShields UpReport A Cyber Issue The CISA Vulnerability Bulletin provides a summary of new vulnerabilities that have been recorded in the past week. In some cases, the vulnerabilities in the bulletin may not yet have assigned CVSS scores. Vulnerabilities are based on the Common Vulnerabilities and Exposures (CVE) vulnerability naming standard and are organized according to severity, determined by the Common Vulnerability Scoring System (CVSS) standard. The division of high, medium, and low severities correspond to the following scores: Entries may include additional information provided by organizations and efforts sponsored by CISA. This information may include identifying information, values, definitions, and related links. Patch information is provided when available. Please note that some of the information in the bulletin is compiled from external, open-source reports and is not a direct result of CISA analysis. Back to top Back to top Back to top Back to top We recently updated our anonymous product survey; we’d welcome your feedback.
--------------------------------------------------

Title: Netflix stock downgraded on valuation concerns despite recent defensive strength
URL: https://finance.yahoo.com/news/netflix-stock-downgraded-on-valuation-concerns-despite-recent-defensive-strength-155033634.html
Time Published: 2025-05-19T15:50:33Z
Description: Netflix's high valuation is a key reason for the downgrade, according to JPMorgan.
--------------------------------------------------

Title: Big Tech's great flattening is happening because it's out of options
URL: https://www.businessinsider.com/big-tech-flattening-middle-managers-performance-efficiency-ai-2025-5
Time Published: 2025-05-19T12:24:11Z
Full Content:
Welcome back! In case you missed it, our new newsletter, Tech Memo, written by the great Alistair Barr, launched on Friday. Check out the first edition here. And if you aren't already, subscribe here. In today's big story, we're looking at Big Tech's obsession with cutting out middle managers and flattening their orgs. What's on deck Markets: When companies like Facebook and Zillow IPO, they turn to this man Tech: How one of the hottest coding startups almost died. Business: Gen Z is turning to blue-collar jobs. But first, no longer stuck in the middle. If this was forwarded to you, sign up here. Technology can quickly become outdated, but it's a job title in tech that's an endangered species: the middle manager. Big Tech is flattening its ranks to thin out layers of management in a bid to reduce bureaucracy, writes Emma Cosgrove, Tim Paradis, Eugene Kim, and Ashley Stewart. Middle managers have had to keep their heads on a swivel for a while. At the end of last year, BI's workplace expert Aki Ito detailed Corporate America falling out of love with the role. But the tech industry has taken the trend into overdrive, as is often the case. From Microsoft to Intel and Amazon, companies are shedding managers to make themselves as quick and lean as possible. The biggest immediate impact of flattening orgs is managers overseeing more workers. Some argue that will limit micromanagement. Others say you'll burn out the managers who are left behind. Big Tech is willing to take its chances, though. As Amazon CEO Andy Jassy said last fall: "I hate bureaucracy." "The goal again is to allow us to have higher ownership and to move more quickly," Jassy added. Big Tech's middle-management purge speaks to a larger trend: Let the stars shine and get rid of anyone else. Part of tech companies' efficiency push is to identify top performers and weed out underachievers. With that approach, you could argue there is less of a need for managers. No weak links in the chain means managers don't have to do as much hand-holding. Get out of the way and let your top performers do what they do best. This isn't a foolproof strategy, though. Someone being extremely capable at their job doesn't always correlate with them being an easy employee to manage. In fact, sometimes the opposite can be true. But what other options do these tech giants have? The pressure from startups like OpenAI and Anthropic is undeniable. Their smaller size also gives them a massive leg up to move quickly. And when it comes to AI, speed is the name of the game. Meanwhile, middle managers seem to only be slowing companies down. 1. Trump's "Big, beautiful bill" could cause some big chaos. Market pros say the president's tax bill would add $4 trillion to the US deficit, stoking mayhem in the bond market. That means another Trump vs. bond market showdown could be headed our way. 2. Bankers tell startups wanting to go public: "Go, go, go." Startups like Hinge Health put their IPO plans on hold when Trump introduced sweeping tariffs. Now that the stock market has recovered, bankers are telling companies to go public while they still can. 3. This "hick from Ohio" is a big deal for IPOs. Pat Healy could be the forefather of getting stock exchanges to compete for the right to get a company to list with them. From free Davos advertising to NFL star appearances, here's how Healy lands companies major marketing perks. 1. "Appstinence" is a virtue. Raised in the age of the smartphone, a growing cohort of people, mostly millennials and Gen Zers, are opting for dumb tech instead. As the evidence of our collective phone addiction adds up, even tech lovers are embracing the digital detox movement. 2. How Silicon Valley's favorite startup came back from the edge of disaster. StackBlitz was at death's door when Anthropic released its AI model Sonnet 3.5 in 2024. That led StackBlitz to create Bolt.new, a product that could write code based on prompts written in English — and the company's gold mine. BI's Alistair Barr has the full story. 3. Is AI coming for teachers? Duolingo CEO Luis von Ahn thinks so. On a recent podcast appearance, he told venture capitalist Sarah Guo that schools will still be necessary in an AI-driven future — but mostly just for childcare. He thinks AI will do the actual teaching. 1. Gen Z is dyeing white collars blue. The cost of college is skyrocketing, and the white-collar job market is unstable. That's led many young people to turn to trades instead, which can offer six-figure salaries and have a high demand for workers. 2. Selling a merger to Trump? MAGA-ify it. Cable giant Charter is merging with Cox, posing a bigger rival for Comcast. The merger still needs the green light from the Trump administration, and it seems like Charter is leaning into pro-American rhetoric to get it, BI's Peter Kafka writes. 3. LA investor Jessica Mah is in a legal battle with DGV investor Justin Caldbeck and two ex-employees. In a lawsuit, Mah has accused Caldbeck of sexually harassing her, which he denies. The lawsuits against Mah, meanwhile, accuse her of misusing company funds, harassment, and age discrimination, BI's Rob Price reports. Dairy Queen CEO explains what a job interview with Warren Buffett is like. WNBA season begins. Where Big Tech secrets go public — unfiltered in your inbox weekly. Sign up chevron down icon An icon in the shape of an angle pointing down. Jump to
--------------------------------------------------

Title: 'Magnificent Seven' Valuations Plunge to 7-Year Low, Investors Brace for Exit
URL: https://finance.yahoo.com/news/magnificent-seven-valuations-plunge-7-122331047.html
Time Published: 2025-05-19T12:23:31Z
Description: Elite Seven Face Valuation Collapse Not Seen Since 2018
--------------------------------------------------

Title: US-Sino AI Rivalry Enters ‘Competitive Coexistence’ After Tariff Truce
URL: https://www.forbes.com/sites/viviantoh/2025/05/19/us-sino-ai-rivalry-enters-competitive-coexistence-after-tariff-truce/
Time Published: 2025-05-19T12:14:56Z
Full Content:
ByVivian Toh ByVivian Toh, Contributor. The U.S. and China strike a 90-day tariff agreement. (Photo by Jonathan Raa/NurPhoto via Getty ... More Images) Wall Street and Shanghai markets may have cheered the U.S.-China tariff détente, but beneath the surface optimism lies a profound strategic recalibration of the world’s most consequential technological rivalry. The May 12 agreement, a 90-day suspension of escalating tariffs, has seen U.S. duties on Chinese imports plummet dramatically, from an aggressive 145% down to a more moderate 30%, while China reciprocated by reducing tariffs on American goods from 125% to 10%. This temporary peace, however, signals not an end to hostilities but a new phase defined by careful maneuvering and intense strategic rivalry, aptly termed "competitive coexistence." Influential Chinese commentator Ren Yi, widely known as Chairman Rabbit, encapsulated Beijing’s view by describing the tariff truce as a "beautiful counterattack." In a widely circulated WeChat post, Ren portrayed the agreement as a pragmatic victory against the Trump administration’s "impulsive paper-tiger tactics," underscoring China's long-term strategic acumen and disciplined approach in contrast to America's more reactive stance. His narrative resonated deeply with domestic Chinese audiences, reflecting Beijing’s confidence in its strategic positioning amid ongoing geopolitical tensions. Yet, beyond political narratives, global financial markets have demonstrated notable resilience. After the initial tariff announcement on April 9 triggered widespread anxiety, markets had already begun to recalibrate expectations by the time the two nations officially announced the truce. From April 9 to May 12, technology stocks notably benefited from investor confidence in the resilience and adaptability of major firms. Alibaba led the charge with a remarkable 25.5% rise in its share price, followed closely by Tencent with a 17.3% increase and Baidu’s 13.3% gain. U.S. tech giants also shared in the relief rally: Meta’s shares surged by 9.2% and Apple’s by 6%, although Alphabet’s stock experienced a modest decline, reflecting lingering investor concerns about its international regulatory exposure and vulnerabilities in key global markets. Share price of U.S. and China tech giants before and after the tariff truce. This market adjustment represents more than mere optimism — it highlights a deeper shift in investor psychology. Geopolitical tensions are no longer seen as isolated threats but strategic inflection points demanding corporate agility and adaptability. Investors increasingly expect firms to navigate geopolitical uncertainties effectively, turning potential disruptions into opportunities for strategic recalibration. Central to this recalibration is the intense race for artificial intelligence dominance. American companies like Meta and Microsoft are vigorously expanding AI capabilities through advanced cloud services, partnerships and product innovations. Meta's strong growth reflects increasing demand for its AI-powered ad tools and content creation services, which have significantly enhanced user targeting and engagement. Microsoft, meanwhile, continues to integrate OpenAI’s cutting-edge technologies into its Azure cloud platform, seeking to set global standards in enterprise AI solutions. Chinese tech giants are equally aggressive, accelerating development of self-contained AI ecosystems in response to tightening U.S. restrictions, including export controls on advanced AI chips and critical technologies. Tencent, for instance, reported robust financial results in Q1 2025, with revenues climbing 13% year-on-year to RMB 180 billion ($25.1 billion). This growth was fueled primarily by its Value-Added Services segment, including blockbuster games such as Honour of Kings and Delta Force, as well as AI-driven marketing enhancements on platforms like Weixin Search. Despite substantial investments in AI, Tencent managed an 18% rise in operating profits to RMB 69.3 billion ($9.7 billion), underlining its capacity to balance strategic investments with profitability. Meanwhile, China’s strategic pivot toward domestic technological innovation has been particularly evident in AI infrastructure development. Initiatives like Baidu’s PaddlePaddle and Alibaba’s ModelScope illustrate a determined push to build sovereign technological stacks capable of competing independently of foreign technologies. The broader geopolitical context, especially U.S. export controls and the rescinding of the Biden-era AI Diffusion Rule — replaced with targeted restrictions — has galvanized China’s resolve to innovate autonomously. The strategic realignment also underscores broader shifts in global supply chains, with U.S. companies actively diversifying manufacturing and production away from China to hedge against future geopolitical shocks. Apple epitomizes this approach through its accelerated expansion in alternative manufacturing hubs like India and Vietnam — a strategy known as “China+1.” However, despite these tactical shifts, China’s entrenched role in global manufacturing, bolstered by its extensive supplier networks, skilled workforce and manufacturing efficiencies, renders complete decoupling unrealistic and economically disruptive. Capital and talent flows have similarly been reshaped by geopolitical tensions. Chinese firms now face increasing obstacles when attempting to access foreign investment or public markets. Heightened scrutiny of researchers’ visas and the politicization of academic collaborations have further complicated talent acquisition. Nevertheless, both China and the U.S. continue to significantly ramp up domestic investments. The U.S. CHIPS and Science Act channels substantial federal funds into semiconductor and AI research, aiming to safeguard technological leadership. China mirrors these efforts with extensive initiatives bolstering technology zones, intensifying public-private partnerships and prioritizing STEM education nationwide. The resulting environment is one of strategic caution combined with sustained competitive aggression — a delicate equilibrium captured by the term "competitive coexistence." Companies must adeptly navigate a dual reality of collaboration and competition, reflecting a fragmented yet interconnected global technology landscape. In this new paradigm, the defining contest between the U.S. and China transcends tariff skirmishes. Instead, the battlefield extends to innovation, scalability and influence over international technological standards. This truce thus provides not only temporary economic relief but also a moment of clarity, spotlighting the relentless drive of both superpowers toward long-term technological supremacy. The markets, now more accustomed to such dynamics, perceive geopolitical frictions as persistent realities that demand continuous strategic foresight and adaptability.
--------------------------------------------------

Title: Why META Stock Is A Prime Choice In The AI Boom
URL: https://www.forbes.com/sites/greatspeculations/2025/05/19/why-meta-stock-is-a-prime-choice-in-the-ai-boom/
Time Published: 2025-05-19T11:48:29Z
Full Content:
ByTrefis Team ByTrefis Team, Contributor. CANADA - 2025/05/18: In this photo illustration, the Meta AI logo is seen displayed on a smartphone ... More screen. (Photo Illustration by Thomas Fuller/SOPA Images/LightRocket via Getty Images) Question: Why pay a premium of 35 times earnings for Microsoft stock when Meta stock is available at 25 times earnings? This seems particularly questionable when considering the following three key factors: Meta isn't exactly a safe haven investment. To illustrate this, consider how Meta's stock has reacted during past market shocks: it experienced a significant 77% decline during the 2022 inflation shock, compared to a peak-to-trough drop of only 25% for the S&P 500. Similarly, during the COVID-19 pandemic in 2020, Meta’s stock fell by 35%, while the S&P 500 saw a comparable decline of 34%. These instances suggest that Meta is not a particularly "safe" stock. Also, see Buy or Sell Meta Stock. Furthermore, it has already seen substantial gains this year, rising 7%, and a remarkable 32% in just one month, climbing from around $485 on April 21st to $640 currently. On the other hand, for a more balanced perspective, consider the Trefis High Quality strategy, which has outperformed the market with over 91% returns since its inception, as demonstrated in the HQ performance metrics. Confident in the long-term potential of the AI revolution? Meta Platforms presents a compelling investment opportunity due to its unique position. Unlike companies solely focused on AI development, Meta integrates AI across its massive and growing user base. The daily active people (DAP) across Meta's family of apps have grown by an impressive 16%, from 2.95 billion in 2022 to 3.43 billion currently. This vast engagement provides a distinct advantage in monetizing AI advancements through enhanced advertising, content recommendations, and overall user experience. A substantial portion of the company's revenue is directly generated from ad sales on its key platforms: Facebook, Instagram, Threads, and WhatsApp, making AI-driven improvements in ad targeting and delivery particularly impactful. Supported by significant investments in AI research and development, and leveraging this unparalleled global social media network, Meta is strategically positioned to capitalize on AI growth, regardless of the dominant technologies that emerge. Despite its compelling advantages, investing in Meta is not without risks. A key concern is the potential for earnings to fall short of expectations or for sales growth to decelerate from 22% in the past year to a possible 15% in the near future. This slowdown could be driven by broader economic headwinds, where businesses might reduce advertising expenditures to preserve capital, particularly in the event of slowing economic growth or a recession. Furthermore, unforeseen or unexpected shocks could negatively impact the stock’s performance. Consequently, investors should be prepared for the possibility of significant declines, potentially reaching 50% or more. Selling during such downturns could be detrimental to long-term returns. Instead of reacting impulsively, consider consulting a financial advisor with experience navigating multiple bear markets. They can provide insights into strategies like the Trefis HQ strategy and other prudent approaches to potentially capitalize on market downturns. Remember, substantial wealth can be generated in such environments by maintaining a rational perspective. Considering these factors, if you are a long-term investor with a 3-to-5-year horizon and a buy-and-hold strategy, Meta could present an interesting entry point at its current valuation, even after its recent price increase.
--------------------------------------------------

Title: Nvidia shares roar back as doubts about Big Tech fade
URL: https://financialpost.com/investing/nvidia-doubt-big-tech-fade
Time Published: 2025-05-19T10:00:32Z
Description: The chipmaker benefitted from confidence in Big Tech spending, easing trade tensions with China and new Middle East opportunities. Read on.
--------------------------------------------------

Title: Baxter & Bailey bring The Design Laundry to Barcelona
URL: https://www.creativeboom.com/insight/baxter-bailey-bring-the-design-laundry-to-barcelona/
Time Published: 2025-05-19T06:45:00Z
Full Content:
With humour, honesty and a few disaster stories, Baxter & Bailey's OFFF talk celebrated the messy side of creativity and why embracing mistakes might be the best design tool of all. With humour, honesty and a few disaster stories, Baxter & Bailey's OFFF talk celebrated the messy side of creativity and why embracing mistakes might be the best design tool of all. Baxter & Bailey could've taken the safe route for their Friday evening talk at OFFF. They could've showcased slick work for the Royal Parks, the Royal Mail, or the London Soundtrack Festival. Instead, they pulled back the curtain on what really goes on behind the pristine curtain of the design industry, revealing a tangle of mistakes, mishaps and moments of pure chaos. "We live in a world where everything we see is finished, polished and presented, which can be quite a daunting prospect for young designers entering the industry," they explained. "It's a nice reminder that even the most experienced designers mess up sometimes." With that, Matt and Dom launched into an hour of confession, comedy, and collective therapy. It was an airing of creative dirty laundry, shared in the spirit of learning and laughter. Baxter & Bailey started with their own biggest fumbles, warming the crowd up for what was to come. Dom kicked things off with a tale from a night train journey in Russia en route to a crucial pitch for a telecoms giant. Sharing a cabin with a snoring stranger called Constantin from Moscow, he awoke to find his roommate enjoying breakfast in nothing but his pants. In the panic to flee, Dom left his carefully crafted presentation boards on the train. "What I learnt from that mistake was: even if you wake up and the first thing you see is an old Russian man's crotch, keep cool, don't panic and remember what you're doing," Dom told the crowd. Matt followed with a story from a concept presentation a decade ago. Just a few minutes in, the client's top boss entered the boardroom, watched in silence… then stood up, gave a thumbs down, and stuck out his tongue with a rude noise of disapproval. "I still cringe every time I hear that story," said Dom. The pair soon revealed they weren't alone in their confessions. Before the talk, they'd floated the idea to British design legend Rian Hughes, who not only approved but offered his own disaster story. On screen, Rian recalled his first job interview at a Camden agency. Lost, rain-soaked, and 45 minutes late, he finally arrived with his portfolio strapped to a trolley. As he lifted it onto the table, he noticed something brown on his sleeve and realised he'd rolled the trolley through dog poo. One side of his portfolio was covered, and so was the director's mahogany table (not to mention the stench). "Despite this auspicious beginning, I got the job," said Rian. "But I don't recommend it as an interview technique – and the job was shit anyway. I lasted three months." With proof of concept secured, Baxter & Bailey launched The Design Laundry, a growing archive of creative calamities submitted by designers across the industry. The idea is that people share their worst moments, get them out in the open, and maybe learn something in the process. Here are just a few they aired at OFFF. Jamie Ellul, founder of Supple Studio, joined in from the remote countryside to share his cautionary tale. Early in his solo career, he landed a job designing a brochure for an accountancy firm. "I think it might have actually looked worse than it did in the beginning when I was finished," Jamie admitted. He sent the final artwork to print, then forwarded the printer's quote to the client before hopping on a train. Following weeks of radio silence, the client finally replied: 'Hi Jamie, I've been wondering how to respond to your email, having seen your note to the printer below on the email chain'. That note read: 'Please don't judge me—this is the shittest thing I've ever designed'. Cue laughter from the crowd – and a lesson in double-checking email chains. As Matt and Dom pointed out, email fails were the most common type of submission. We've all been there. Next up: Francis Jackson, formerly of MetaDesign and OPX and now a psychotherapist, shared her story in a letter. Years ago, she worked with The King's Trust on a beautifully designed book promoting the value of art in the built environment, Art in the Public Environment. The problem was that a glaring typo was revealed by none other than the King of England himself after hundreds of printed copies went out across the country. What the front cover actually said was: 'Art in the Pubic Realm'. When the King was handed a copy while visiting the offices, he swiftly returned it, eyebrows raised. I'm sure Francis was hoping for the ground to open up beneath her at that moment! Daljit Singh, co-founder of pioneering digital agency Digit, appeared onscreen in sunglasses, mid-holiday, to recount a transatlantic disaster. After landing in the UK from New York in 2007, Daljit got a message: Gene Simmons (yes, from KISS) wanted to meet him. Being a huge fan, he turned around, caught the next flight back, and headed straight to the hotel. But as he flew, doubt crept in, and he asked himself: could it be a prank? He'd played an elaborate hoax on his friend Simon not too long ago, so when told to ask for "Simon Smith" at reception (apparently Gene's alias), his suspicions grew. Still, he went upstairs, knocked on the door, and shouted, "Simon, come out, you massive tosser." None other than Gene Simmons opened the door. "You English guys have got such strange senses of humour," he said, before ushering Daljit in. The meeting lasted three awkward minutes. They didn't get the job. After airing these disasters, Matt and Dom returned to each story to highlight the learning points. Jamie reminded us to treat every job (and every email) with care. Francis' typo showed that even major mistakes fade with time. Daljit's story offered a reminder not to let doubt derail bold decisions. The laundry pile grew with more stories, laughs, and stains, and the pair could've gone all night. Luckily, this won't be the last spin cycle for The Design Laundry. Dom and Matt are bringing the idea to future events and inviting more submissions through their website. Because if creativity is a messy process, there's no shame in airing a little dirty laundry. Get the best of Creative Boom delivered to your inbox weekly Creative Boom empowers and uplifts the creative community. Since 2009, we've been delivering the best in creativity through news, inspiration, insights, and advice to help you stand out and succeed in a competitive industry. Creative Boom™ © 2025 Creative Boom Ltd. Registered in England and Wales #07437294.
--------------------------------------------------

Title: Big Tech Goes From Safest Bet to Biggest Question...
URL: https://finance.yahoo.com/news/big-tech-goes-stock-market-120007963.html
Time Published: 2025-05-18T22:07:36Z
Description: Big Tech Goes From Safest Bet to Biggest Question...

 
 
 
 (Top headline, 4th story, link)

 

 

 
 Related stories:Investors brace for reaction after MOODY'S strips USA of top credit rating...
Another Monday Jolt?
States Likely to Defy Downgrade...…
--------------------------------------------------

Title: Trump tariff uncertainty still makes earnings estimates and guidance useless
URL: https://finance.yahoo.com/news/trump-tariff-uncertainty-still-makes-earnings-estimates-and-guidance-useless-100023151.html
Time Published: 2025-05-18T10:00:23Z
Description: If investors are looking for thesis justification via earnings estimates and guidance, they shouldn't.
--------------------------------------------------