List of news related to Meta stock price META:

Title:  Is AI a bubble? - by Azeem Azhar and Nathan Warren
URL: https://www.exponentialview.co/p/is-ai-a-bubble
Time Published: 2025-09-21T05:47:01Z
Full Content:
A month ago, I set out to answer a deceptively simple question: Is AI a bubble? Since 2024, people have been asking me this as I’ve spoken at events around the world. Even as Wall Street bankers largely see this as an investment boom, more people are asking the question in meeting rooms and conference halls in Europe and the US. Some have made up their minds. Gary Marcus called it a “peak bubble.” The Atlantic warns that there is a “possibility that we’re currently experiencing an AI bubble, in which investor excitement has gotten too far ahead of the technology’s near-term productivity benefits. If that bubble bursts, it could put the dot-com crash to shame – and the tech giants and their Silicon Valley backers won’t be the only ones who suffer.” The Economist said that “the potential cost has risen alarmingly high.” The best way to understand a question like this is to create a framework, one that you can update as new evidence emerges. Putting this together has taken dozens of hours of data analysis, modeling and numerous conversations with investors and executives. This essay is that framework: five gauges to weigh genAI against history’s bubbles. I studied and lived through the dot-com bubble. I was an investor and felt its effects firsthand.1 Like many of you, I was also active during the Global Financial Crisis. For this reason, I’ve invested considerable thought and analysis into a robust framework to understand what’s going on. I’ll share it with you today. My thinking is grounded in the study of Carlota Perez, Bill Janeway and other giants in the field, as well as financial analyses of major banks and analysts. But what I haven’t seen anywhere else is a framework that translates those theories into a practical dashboard for AI today: a set of parameters you can read, compare across past bubbles and use to orient your decisions. We will make the underlying data and methodology available to members of Exponential View in the coming weeks. I usually reserve my deep dives for paying members of Exponential View. But this question is too important, and the public debate clouded by posturing, for me to keep it behind a paywall. So today’s piece is free for everyone to read. If you want to access this analysis as a PDF, you can download it here. If you are an investor or executive navigating these questions, I am opening a very limited number of consultation sessions to work directly on them. This is a bespoke opportunity to stress-test your strategy with the same analytical framework I’ve developed. You can apply here. Bubbles are among the oldest stories of capitalism. They’re parables of excess, belief and collapse. But bubbles are not just financial phenomena; they are cultural artifacts. They return again and again as morality tales about greed and folly. Tulip mania, often misremembered as a frenzy of bankrupt weavers and drowning merchants, was less disastrous than legend suggests. It was confined to wealthy merchants and left the Dutch economy largely unscathed. But the myth has endured, and that is the point: bubbles become stories we tell ourselves about the dangers of optimism. Some bubbles are financial: the South Sea frenzy of the 1720s, the roaring stock market of the 1920s, Japan’s real-estate boom in the 1980s and the housing crash of 2008.2 Some are technological. In the 1840s, railways were hailed as the veins of a new industrial body. They were. But a body needs only so many veins, and tracks were soon laid in places commerce could not sustain. Telecoms in the 1990s promised a wired utopia, only for 70 million miles of excess fiber to lie dark underground. The dot-com boom gave us visions of a new economy, much of which did eventually materialize, but not before valuations evaporated in 2000. The funny thing is that there doesn’t seem to be an academic consensus on what an investment bubble is. Nobel laureate in economics Eugene Fama has gone so far as to say they don’t exist. I want to go beyond the notion that we know a bubble when we see a bubble. There are two interrelated systems at work. The first, the most visible, is when stock markets become absurdly overvalued and collapse. The second is whether the quantity of productive capital (going into capital expenditure or venture capital) deployed collapses. The two are related, of course. A collapse in equity prices makes investment flows more expensive. And a secular decline in productive capital investment may be read by equity markets as a harbinger of a slowdown. But to frame this: we see a bubble as being a 50% drawdown from the peak equity value that is sustained for at least five years. In the case of the US housing bubble and the dot-com, that trough was roughly five years long. Full recovery to pre-bubble peaks took 10 years for US housing, and 15 for the dot-com. Alongside, we would expect a substantial decline in the rate of productive capital deployed, once again 50% from peak. For the purposes of this analysis, by “bubble” I’ll address both of those competing dimensions. Ultimately, it means a phase marked by a rapid escalation in prices and investment, where valuations drift materially away from the underlying prospects and realistic earnings power of the assets involved. Bubbles thrive on abundant capital and seductive narratives, and they tend to end in a sharp and sustained reversal that wipes out much of the paper wealth created on the way up. A “boom,” by contrast, can look very similar in its early stages with rising valuations and accelerating investment. But the crucial distinction is that, in a boom, fundamentals eventually catch up. The underlying cash flows, productivity gains, or genuine demand growth rise to meet the optimism. Booms can still overshoot, but they consolidate into durable industries and lasting economic value. Between the two lies a gray zone: periods of exuberance when it is genuinely hard to tell whether capital is building the foundations of a new economy or merely inflating prices that will not be sustained. It’s like being in the eye of a storm: you can feel the wind, rain and pressure, but you don’t yet know whether it will clear the air or flatten the house. This brings us to the present question: is AI another bubble? What unsettles many observers are the numbers. Since ChatGPT’s release in late 2022, hyperscalers have more than doubled their annual data center capex, betting heavily on the infrastructure needed to train and run ever-larger models. When Sam Altman floated a $7 trillion investment requirement last year, it sounded like hubris. Today, investors aren’t laughing; they’re wondering whether the scale of spending is sustainable. Bubbles are impossible to diagnose in real time. Only in retrospect do we know whether exuberance was justified or delusional. Rather than making a definitive call — which borders on unhelpful speculation — it’s better to benchmark today’s boom against history to spot where bubble-like dynamics may be creeping in. We’re thinking of this as flying a plane. Pilots don’t rely on a single instrument. They monitor a cluster of gauges to understand how the flight is going. We’ll use five of them here: Gauge 1 – Economic strain: Is investment now large enough to bend the economy? Gauge 2 – Industry strain: Are industry revenues commensurate with the deployed capex? Gauge 3 – Revenue growth: Is revenue rising/broadening fast enough to catch up? Gauge 4 – Valuation heat: How hot are valuations? Compared to history, are stocks excessively overpriced? Gauge 5 – Funding quality: What kind of money is funding this? Is it strong balance sheets, or fragile, flighty capital? In the rest of the essay, I look at each gauge to explain why it may be green, amber or red, and at the end I pull them together into a full dashboard view. The investment underway is vast, with Morgan Stanley expecting $3 trillion in AI infrastructure spend by 2029. But it has not yet reached the runaway extremes of history’s great blowouts. What makes this dimension tricky is dependence. In the US, more than a third of GDP growth can be traced to data center construction. This is not inherently bad, but it may be dangerous if the momentum falters. An economy leaning this heavily on one sector for growth can find the ground falling away faster than expected. The surge of capital expenditure (capex), poured into the physical infrastructure that AI demands, is an act of optimism. This is what capex is. Money spent today in the belief that it will become a funnel of revenue tomorrow. If it’s well-placed today, it will eventually lead to productivity gains and economic expansion. If you want to sell people hula hoops to promote their health and happiness, you need to buy hula hoops first. And somewhere down the line, there needs to be a factory making them. Funding that factory is capex. You deploy the capex to build a useful thing that you can sell. Capex investment is often, but not always, a precursor to a growing business. AI data centers have a similar quality, and something more. They are not just factories for a single product; they are infrastructure. Microsoft, OpenAI and the US government all see it this way. They see compute as a foundational utility of the 21st century, no less critical than highways, railways, power grids or telecom networks were in earlier eras. I made the same argument in my last book, so you can guess where I stand. Excluding the US, sovereign governments have made more than $1 trillion in commitments to AI as infrastructure by 2030. To build such infrastructure inevitably requires historic sums, on a par with the railway and electricity build-outs of the past. McKinsey projects that meeting AI-driven demand by 2030 will require around 156 GW of new electrical capacity — the size of Spain’s and Portugal’s grids combined. The firm estimates this will cost $5-7.9 trillion in capital spending. For context, this is about twenty times the cost of the Apollo missions. This would place AI data centers among the largest infrastructure build-outs in modern history. But useful though infrastructure is, especially when private capital gets involved, things can get divorced from reality. The financing structure matters as much as the technology itself. The railways in the US were largely privately funded and suffered from multiple investment bubbles. Electricity and road systems, by contrast, benefited from greater public investment and coordination, and were less prone to speculative excess. A boom becomes dangerous when the resources it demands start to bend the whole economy around it. Wages get sucked into one sector, supply chains reorient to serve it and capital markets grow dependent on it. The snapback is vicious when expectations break. One way to gauge the economic strain is to look at investment as a share of GDP, the portion of national output funneled into a single technological frontier. It is a crude but telling ratio. It shows how heavily the economy leans on one technological bet. In the case of AI infrastructure, most of that spending takes the form of capital expenditures: the servers, cooling systems, networking gear, shells of concrete and steel and the power infrastructure required to keep them running. By that measure, the railway bubbles were the heaviest. In the United States, railway spending peaked at around 4% of GDP in 1872, just before the first great crash. The telecom boom of the late 1990s, by contrast, topped out near 1% of GDP, a level that looks familiar today. The AI build-out sits in this middle zone. Around $370 billion is expected to flow into data centers globally in 2025, with perhaps 70% earmarked for the US,3 or roughly 0.9% of American GDP. Goldman Sachs projects spending will climb by another 17% in 2026. My own forecasts are in line with this view: annual capex of $800 billion by 2030, perhaps 60% in the US, which would bring the American share to 1.6% of 2025 GDP. The economic-strain gauge has three segments: green up to 1%, amber up to 2% and red above 2%. So today, genAI sits in the green zone – just. Of course, given stated commitments, it looks like it’ll head into amber soon. But there is a twist here that history’s bubbles did not face: rapid depreciation. Once laid, a railway track could last decades. The US was still running freight over 19th-century tracks well into the 20th. Telecom fiber rolled out in the 1990s still carries traffic 35 years later. GPUs, by contrast, age in dog years. Their useful life for frontier applications such as model training is perhaps three years, after which they are relegated to lower-intensity tasks. Roughly a third of hyperscaler capex is going into such short-lived assets. They remain, in theory, monetizable in years five and six. The rest goes into shells, power and cooling that last two or three decades. Adjusting for asset life makes the AI build-out look even more demanding: unlike railroads or fiber, the system must earn its keep in a handful of years, not generations. Whether this dynamic makes financial conditions worse or better is an open question. The pessimistic view is that a box of ripe bananas is worth less than the same value of Brazil nuts, because it must be consumed quickly. But the optimistic case is intriguing: shorter depreciation cycles may impose financial discipline on incoming investors. During the railway mania, decades-long asset lives masked the weakness of many business models; companies could stagger on for years before insolvency. In AI, the flaws may surface quickly, forcing either rapid adaptation or rapid failure. The strain is noticeable but not yet unbearable. Venture funding at the AI application layer, while noisy, remains modest compared with the telecom mania of the 1990s.4 That suggests there may still be running room before the cycle overheats. The trillions pouring into servers, cooling systems and power lines may be essential, but history suggests they are not where the enduring profits accumulate. In railways, many track companies went bust, but the logistics firms and commodity barons riding their lines prospered. In telecoms, it was not the builders of fiber who captured the upside but the platforms and service providers that used the bandwidth. The same logic may apply to AI: hyperscalers shoulder the capex burden, but the durable value may accrue to those who control the models, the data and the ecosystems built on top. This brings us to the second gauge on the dashboard: industry strain. Every boom needs to prove that the money poured into new equipment is starting to earn its keep. In any growth stage, it is unlikely that revenues will cover investment, but they should be non-zero. This gauge looks at the ratio of capex to revenues. We estimate that genAI revenues look on track to exceed $60 billion this year, from roughly nil five years ago.5 This number could well undercount the value being generated by genAI. Meta, for example, has suggested that the technology has increased conversions on its platforms by about 3–5%. Indirect effects like this may help explain why some analysts, such as Morgan Stanley, peg 2025 revenues far higher than our estimate, at $153 billion. The big hyperscalers, such as Google, Microsoft and Amazon, are without doubt moving into a phase of higher capex intensity. In 2021, before ChatGPT, hyperscalers invested about 44% of their operating cash flow in capex. By 2024, that had risen to 68%. In 2025, it will be higher still. But, as Pierre Ferragu of New Street Research points out, these firms can absorb this shift by replatforming, with structurally higher capital intensity driving growth and efficiency gains. This dynamic has been in place for a decade already: between 2015 and 2018, Microsoft Azure’s capex represented between 70% and 90% of revenues. It was an investment in the future. This makes for an interesting comparison to earlier boom cycles. The railroads are particularly pertinent. The railroads’ direct revenue contribution was tiny compared to the value the railroads created in the US economy. Railway bubbles were always tethered to the reality of cash flow. The bonds issued to finance new track and rolling stock had to be serviced out of passenger fares and freight revenues. Whenever capex outpaced earnings, the strain showed. The manias of 1873, 1883 and 1887 all followed the same pattern: a sharp decline in the ratio of annual revenues to capital spending, and in some cases, outright revenue contraction. At the height of the US railroad expansion in 1872, capex was around two times revenues. In the late-1990s telecom bubble, capex amounted to just under four times revenues. By contrast, today’s genAI boom runs on roughly $60 billion in revenues against about $370 billion in global data center capex – a capex-to-revenue ratio of six times, the most stretched of the three.6 On the industry-strain gauge, railways sat healthily in the green. GenAI is in amber, nearing red. It’s not quite a warning sign, not least because genAI has people clamoring for access to AI data centers. One report suggests that enterprise customers are committing to capacity before data centers are even built. What is driving that is usage, and with that comes astonishing revenue growth. The problem in the railroad and telecom booms was not sector strain per se, but that revenues ran out of momentum. Investment expects a return. After the railway bubble burst in 1873, revenue declined by 3% year over year. Telecoms did slightly better, declining 0.5%. Before the crashes, revenue growth was hardly explosive. Railways in 1873 expanded 22% – enough to double in three years. Telecom in the late 1990s managed only 16%, a doubling time of just over four years. By contrast, genAI revenues are still accelerating. By our estimates, genAI revenues will grow about twofold this year.7 And this is likely a conservative forecast. Citi estimates that model makers’ revenue will grow 483% in 2025. OpenAI forecasts annualized growth of about 73% to 2030, while analysts like Morgan Stanley estimate this market could be as large as $1 trillion by 2028, equivalent to compound growth of ~122% a year over the period. Our revenue growth gauge is an exponential one. It measures revenue doubling time in years; that is, at current growth rates, how long does it take for sector revenue to double? Here, genAI sits squarely in the green, doubling every year. Much of this spending flows down to the hyperscalers and neoclouds that run the infrastructure. Hyperscalers make tons of money, so they can afford to invest, trimming a few points off earnings if they think it positions them well for the future. Oracle projects that its cloud business could generate around $380 billion in cumulative revenues by 2030. In my conversations with large companies, I get the strong sense that they can’t get enough of this technology right now. And this likely supports the strong growth rates. IBM’s CEO survey shows that genAI is already expanding IT budgets, with 62% of respondents indicating they will increase their AI investments in 2025. Demand is so high that new data centers are at full utilization as soon as they come online, with Amazon CEO Andy Jassy noting that “as fast as we put the capacity in, it’s being consumed” and Sam Altman remarking that a lack of compute was affecting model upgrades. Jensen Huang expected this would happen early in 2024, predicting that demand for both Hopper and Blackwell chips would outstrip supply well into next year. Some 9% of US firms have one useful genAI use case. Back-of-the-napkin math suggests that 9% will become 75% within five years, and the number of use cases will rise from one to hundreds.8 That on its own points to a thousandfold increase in something. I say *something* because the current billing-by-token model may change.9 Even if it doesn’t, we’ll see dramatic price declines on per-token pricing, likely by orders of magnitude. The underlying point is that we are still at the foothills of enterprise use. For now, firms can barely secure enough tokens to meet their needs. The consumer side tells a parallel story. US consumers already spend about $1.4 trillion a year online. This could plausibly double to $3 trillion by 2030 if it grows at 15–17% a year (it has grown at more than 14% a year since 2013). Against this backdrop, a genAI app sector rising from today’s $10 billion to $500 billion within five years looks less far-fetched. Exponential growth rates of 300-500% are already visible in mid-sized startups and the large model providers (see last week’s essay on OpenAI’s valuation), suggesting that even a small reallocation of consumer digital spending could drive revenues into the hundreds of billions. Taken together, these signals point to an industry still in strong ascent, unlike the relatively meager revenue growth that preceded the railroad and telecom busts. If genAI revenues were to grow at even half the pace of last year, then, on my conservative forecast, they would reach $100 billion by 2026, covering about 25% of that year’s capex. If economic and industry strain show the weight of the boom and revenue growth its trajectory, valuation heat is the mood of the market. This is often where bubbles reveal themselves most clearly: how exuberantly investors are pricing the sector, regardless of fundamentals. As Carlota Perez has argued for decades, financial markets tend to overshoot in the early “installation phase” of each technological revolution, pouring in capital far beyond what near-term revenues justify. The frenzy looks irrational in the moment, but it is the mechanism by which society lays down the new infrastructure. The challenge is whether the frenzy can evolve into the “deployment phase,” when the infrastructure becomes universal and delivers real productivity gains. The dot-com bubble is the archetype. Companies with no profits at all floated at triple-digit multiples, some raising more on IPO day than they would ever earn in revenues. Boo.com raised $135 million from investors, including LVMH’s Bernard Arnault, and pitched itself as the “Amazon of fashion.” The site launched in 18 countries with localized language and shipping, with a virtual avatar, “Miss Boo,” who gave shopping tips on 3D models. It spent $25 million on advertising before launch, ballooned staff from 40 to 400, and opened plush offices across New York and Europe. The JavaScript- and Flash-heavy website was nearly unusable on dial-up connections, incompatible with Mac computers and buggy. One in four attempted purchases failed. Despite the extravagance – lavish parties, Concorde flights and celebrity stylists for Miss Boo – polls showed that just 13% of internet users knew the brand existed. Within 18 months, the money was gone. Boo.com collapsed in May 2000.10 What’s going on in genAI does not compare to this. The key measure here is the price/earnings ratio (P/E), a shorthand for how many years of current profits an investor is effectively paying for. If a company has $1 in annual earnings and trades at a P/E of 20, buyers are paying $20 for each dollar of current profit, assuming growth will make that worthwhile. A high P/E means companies are betting on rapid future growth, but too high, for too long, investors might be buying into a fantasy. This was the case in the dot-com era. At the peak, the Nasdaq traded at a P/E of about 72. One detailed study estimated that internet stocks alone carried an implied P/E of 605. In other words, investors were willing to pay for more than six centuries of current earnings. The issue wasn’t that demand disappeared – Amazon’s revenues grew from $2.76 billion in 2000 to $3.12 billion in 2001 – but that no company could grow fast enough to justify those sky-high expectations. The fundamentals improved, but expectations collapsed. Today, the picture is much calmer. The Nasdaq P/E is about 32, half that of the dot-com era. The broader tech market is higher than the long-run average, but nowhere near dot-com territory. Compared to the railroad bubbles, which had an estimated P/E of around 20, it is high. But the railroads ended badly, not because valuations were extreme in themselves, but because revenue growth faltered. By contrast, genAI revenues are still accelerating rapidly. Another indicator of valuation excess that many investors pay attention to is the so-called “Buffett Indicator”, which measures how far ahead of GDP equity market valuations have risen. There is no doubt that this indicator is peaky, well above a two-standard-deviation anomaly that often presages market corrections. We are more sanguine, for the moment. History rhymes; it doesn’t repeat. The Buffett indicator relies on GDP, and GDP is famously bad at capturing technological productivity gains. In addition, a large portion of big tech revenues comes from outside the US, decoupling their earnings from domestic GDP. Finally, AI-based productivity may affect margins – keeping output (and GDP constant), while increasing corporate profits. So while we keep an eye on the Buffett indicator (and its distant cousin, the Shiller CAPE ratio), we feel the different dynamics of an accelerating digital economy are not well captured by them. So how hot is this gauge? My verdict is green. Prices haven’t yet broken free of gravity in the way dot-com valuations did. Funding quality is not a standard metric but a composite judgment. It asks who the money is coming from, how it is structured and whether the capital is willing to wait years for returns or rather chase quarterly pops. Low-quality capital, in short, is short-termist, undisciplined and debt-laden; it rushes and flees quickly. High-quality capital is more patient, better underwritten, and able to withstand volatility. Every bubble has its signature weakness, invariably rooted in how it was financed. Railways were fueled by speculative retail investors with little capital behind them. By the early 1870s, funded debt averaged 46% of total assets among American railroads; when overbuilding met a credit squeeze, financing evaporated. The Panic of 1873 sent rail lines into receivership. Dot-com firms, a century later, were a little sturdier. Venture capital was a boutique business in 1995, with only $5.3 billion deployed. By 2001, more than $237 billion had been poured into startups, often by new and inexperienced managers. The frenzy spilled into public markets: IPO volume between 1999 and 2000 ran six times above historical averages. Companies went public with little revenue. Two of my friends took theGlobe.com public in 1998 and enjoyed a 606% share-price pop the day they went public. Telecoms in the late 1990s leaned on mountains of cheap debt. US and European carriers doubled and quadrupled their leverage in just a few years. Deutsche Telekom and France Télécom together added $78 billion in net debt between 1998 and 2001. When revenues failed to keep pace, defaults rippled through the sector. In each case, the capital that fueled the boom proved ephemeral. But the degree of fragility differed. Railways and telecoms were most exposed to credit crunches, with debt ratios ballooning. Dot-coms were hostage to market mood, with equity values evaporating. On this front, today’s AI boom looks sturdier. Microsoft, Amazon, Alphabet, Meta and Nvidia are minting extraordinary cash flows, easily enough to bankroll their own build-out. For now. But investment needs are racing ahead. Morgan Stanley reckons total global data center capex will hit $2.9 trillion between 2025 and 2028. Hyperscalers can cover perhaps half of that from internal cash. The rest must come from private credit, securitized finance and new operators. Governments have also pledged $1.6 trillion in sovereign AI investments, and Gulf capital is seeking new opportunities. Here is where the risks creep in. Morgan Stanley itself points to a $1.5 trillion gap that will need to be plugged by debt markets and asset-backed securities. The sums are enormous: $800 billion from private credit, $150 billion in data center ABS,11 and hundreds of billions more in OEM loans and vendor financing.12 That $150 billion alone would triple the size of the data-center securitized markets almost overnight. And not every borrower looks like Microsoft. Consider CoreWeave, an Nvidia-backed upstart now going public with $8 billion of debt. It has already slipped into technical default on loans, missing covenants without being insolvent, and its revenues depend on just two customers. Its economics rest on renting GPUs that depreciate by 20–30% per year, more like a WeWork lease than a hyperscaler’s durable balance sheet. In other words, the foundation is stronger than in past bubbles, but the superstructure is starting to resemble the old pattern. Esoteric debt structures, concentrated counterparties and hardware that may not hold value are reappearing. If genAI revenues grow tenfold, creditors will be fine. If not, they may discover that a warehouse full of obsolete GPUs is a difficult thing to secure. For now, my gauge is greenish. We’re not yet in bubble territory, but if financing keeps creeping this way over the next year without revenue growth as robust as 2025’s, it might tick over to amber. On the basis of these gauges, genAI remains in a demand-led, capital-intensive boom rather than a bubble. But booms can sour quickly, and there are several pressure points worth watching: If investment climbs toward 2% of GDP, it could suggest the economy is overweighting AI relative to its productivity returns; equally, if one or more hyperscalers were to cut capex by more than a fifth over the next three to five years, that would mark a sharp turn in sentiment that could trigger a rapid decline among other players. A sustained fall in current enterprise and consumer spending levels would be another warning, especially if foreshadowed by a shrinking Nvidia order backlog. At the same time, the economics need to improve: revenues per dollar of capital should move toward the 0.5–1.0 range. Should the gap fail to close, it would imply that scale is not delivering the expected efficiencies. If valuations start approaching a P/E ratio of 50-60, it would look frothy, since a genuine growth phase should see earnings catching up with prices, not lagging further behind. If internal cash covers less than 25% of capex, data center investment stability comes under pressure. Right now, stability comes from hyperscalers’ strong cash flows. If they stop covering the majority of capex, then more debt and securitization will creep in. Not great given a GPU’s depreciation cycle. If internal funding slips below a quarter of new capex and reliance shifts toward debt and securitization, the sector’s dependence on short-lived GPUs and its exposure to higher interest rates could quickly become destabilizing. My current heuristic is that if two of the five gauges head into red, you’re in bubble territory. Time to sell up, buy the VIX and take some deep breaths. In the year prior to the Panic of 1873, the railroad’s economic strain turned red, accompanied by a decline in funding quality. Anemic revenue growth didn’t help. With the telecoms crash of 2001, revenue growth and funding quality blared red. In the dot-com era, it was industry investment strain and valuations. GenAI isn’t there yet. Racing fast, the engine is whining, but not overheating. How long would it take for two gauges to get into the red? I’ve toyed around with combinations, and most scary scenarios take a couple of years to play out. (And not all scenarios are scary.) That said, so many macro factors, from a recession in the US, to rising inflation, a challenging interest-rate environment, and domestic or international politics, could dampen spirits. While we might not be solidly in bubbleland, it would be hubristic to assume the AI investment cycle is immune to those exuberant dynamics. Onward. For now. Share I even wrote a blurb on the 1999 book The Internet Bubble. Edward Chancellor’s book on bubbles is a useful guide. Last year, SemiAnalysis forecast that around 70% of data center capacity build-out will happen in the US over the medium term. Adjusted for inflation, VC investment in the internet in 2000 was $85.9 billion; Dealroom.co reports $47 billion for the AI application layer last year – a difference of $38.9 billion. Revenues are compiled from primary corporate disclosures (SEC filings, earnings transcripts, investor presentations, press releases) and supplemented by analyst and consultancy estimates (McKinsey, Gartner, IDC, etc.) where company reporting is incomplete. We prioritize disclosed figures, adjust for potential double-counting in revenue-sharing agreements (e.g., OpenAI–Microsoft), and categorize revenue streams (subscription, API, infrastructure, licensing) to maintain comparability. Private-company revenues (e.g., Anthropic, Midjourney) rely on reported run-rates or credible press/VC sources, flagged as estimates. See previous footnote for genAI revenue model. Railroads calculated from FRED data and telecoms from OECD data on US leased line revenues. Our model shows 130% growth among AI companies year over year. Starting from about six million US firms, assume 9% (about 540,000) now have one useful genAI use case; project adoption to 75% (about 4.5 million) within five years, and average use cases per adopting firm to rise from about 1 to about 100 – yielding roughly 450 million use cases, an increase of about 830 times; illustrative, not a forecast. Most genAI services today are billed “by the token.” A token is a unit of text, usually about four characters or ¾ of a word. Customers are charged for tokens processed in prompts and responses (for example, a 1,000-word output might equal ~1,300 tokens). This model directly links cost to usage, but it may evolve as enterprises negotiate flat-rate contracts, consumption bundles, or outcome-based pricing. I spent an afternoon walking through Boo’s deserted offices after the collapse to see if any physical assets retained value. Asset-backed securities are bonds sold to investors, backed by cash flows from a pool of underlying assets. In this case, the assets are data center leases or infrastructure contracts, allowing operators to raise capital by securitizing predictable rental or service revenues. OEMs (original equipment manufacturers) such as Dell, Cisco or Nvidia often extend credit to customers or partner with financing arms to help them buy hardware. Vendor financing is the broader term for suppliers lending to buyers, typically repaid out of the revenues generated from using the purchased equipment. First of all, thanks for the reference to my work in your thoughtful, balanced and all but comprehensive analysis. There is one further aspect of these transformational technologies that are deployed as network-based services. This is the pricing dynamics when (a) the ability to deliver the first unit of service requires very large capital investment and (2) the marginal cost of delivering incremental units of services is very low. Then, marginal cost will be below average cost and – under competitive conditions – price will tend to be driven towards marginal cost. The result, of course, is negative cash flow for all competitors which, in turn, incentivizes some mix of regulatory intervention to protect the competitors from each other (ICC, PUCs) and evolution towards oligopoly/monopoly. I gather that inklings of these dynamics can be seen in the decline of cost/price per token. I am very interested in your take on this issue when it comes to roll out of the foundational models at increasing scale. -- Bill Janeway You certainly are right to mobilize demand elasticity as the counter MC<AC. But token cost is not the full cost of AI deployment. Given the probabilistic nature of the LLM token-prediction process, applications will vary by the intensity of the need for human supervision, debugging, etc. I don’t know how representative this RCT was, but it did strike me as suggesting a somewhat more complicated scenario: https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/ . No posts Ready for more?
--------------------------------------------------

Title: Meta (META)’s AI Glasses Could Hit Tipping Point by 2026, Says Citi
URL: https://finance.yahoo.com/news/meta-meta-ai-glasses-could-210657334.html
Time Published: 2025-09-20T21:06:57Z
Description: Meta Platforms, Inc. (NASDAQ:META) is one of the AI Stocks Analysts Are Tracking Closely. On September 18, Citi reiterated the stock as “Buy” Citi stating...
--------------------------------------------------

Title: The WSJ Got Quarterly Reporting Wrong: A Corporate Executive's Response
URL: https://www.philmckinney.com/the-wsj-got-quarterly-reporting-wrong-a-corporate-executives-response/
Time Published: 2025-09-20T17:28:12Z
Full Content:
Subscribe to Studio Notes and get Phil's FREE two-hour innovation masterclass - the exact framework used by Fortune 500 companies! Join thousands of innovation leaders getting weekly insights Why James Mackintosh's defense of quarterly reporting ignores what actually happens in corporate boardrooms. James Mackintosh's recent Wall Street Journal piece defending quarterly earnings reporting reads like a textbook example of why academic analysis often misses the mark when it comes to understanding corporate behavior. As someone who lived this reality for years as Chief Technology Officer at Hewlett-Packard—the world's largest technology company at the time by revenue with massive R&D investments—I can tell you the quarterly reporting system creates perverse incentives that systematically undermine long-term innovation. Mackintosh's central argument rests on flawed premises and cherry-picked data that ignore the lived experience of corporate executives who actually make investment decisions under quarterly pressure. His dismissal misses a critical opportunity to address one of America's most significant competitive disadvantages. Let me share what quarterly reporting actually looks like from the inside. During my tenure as CTO at HP, I experienced how quarterly pressure created systematic patterns across the industry where R&D would be paused, or delayed during quarter-end periods. This became a predictable cycle that repeated quarter after quarter. This behavior wasn’t unique to HP. It's the inevitable result of a system that rewards short-term financial engineering over patient capital investment. When your stock price can swing dramatically based on missing earnings by a few cents per share, the rational response is to optimize for quarterly performance, even at the expense of long-term competitiveness. The academic research Mackintosh cites, including MIT's Robert Pozen's study of UK reporting changes, fundamentally misunderstands how corporate behavior evolves. These researchers have never sat in a boardroom where directors debate whether to continue funding a multi-year innovation project that won't generate revenue for 36 months while facing pressure to meet next quarter's numbers. They've never experienced the intense scrutiny from analysts who penalize any hint of reduced short-term profitability. Through my relationship with Alex Mandl—my former boss at Teligent who led the board negotiations to take Dell private—I witnessed the most compelling modern example. Michael Dell and Silver Lake paid $24.9 billion for one thing—freedom from the quarterly earnings pressure that was killing Dell Technologies long-term potential. Dell explicitly stated the goal was "no more pulling R&D and growth investments to make in-quarter numbers." R&D spending increased from $1.1 billion to $4.4 billion, transforming Dell from declining PC manufacturer to enterprise solutions leader. The result: an estimated $70 billion return by 2023. This wasn't theoretical—it was a $25 billion bet that quarterly reporting pressure was destroying long-term value. And they were proven spectacularly right. Mackintosh's reliance on the UK's 2014 experience with optional quarterly reporting reveals a critical flaw in his reasoning. He argues that because UK companies didn't dramatically change their investment behavior after switching to semi-annual reporting, quarterly reporting must not be problematic. This ignores a fundamental truth: we've trained an entire generation of CEOs, CFOs, and board members to think in 90-day cycles. You don't undo decades of conditioning overnight. When business schools teach earnings management, when compensation systems reward quarterly performance, and when analysts' careers depend on predicting short-term results, making reporting optional doesn't suddenly transform corporate culture. The UK comparison is meaningless without addressing the broader ecosystem that reinforces short-term thinking. Mackintosh points to Big Tech's massive AI investments as proof that quarterly reporting doesn't hinder long-term thinking. This argument reveals a profound misunderstanding of competitive dynamics. Companies like Google, Microsoft, and Meta can hide enormous R&D expenditures within their massive profit margins. When you're generating 20-30% operating margins on hundreds of billions in revenue, you can afford to invest $50 billion in speculative technologies while still meeting quarterly expectations. But what about companies in lower-margin industries? What about manufacturing firms, healthcare companies, or emerging technology businesses that can't disguise innovation investments as easily? The current system creates a two-tier economy where only the most profitable companies can afford to think long-term, while everyone else gets trapped in quarterly optimization cycles. America's competitive advantage has always been rooted in our ability to make patient, long-term investments in breakthrough technologies. The semiconductor industry, the internet, biotechnology, and countless other innovations emerged from companies willing to invest for decades before seeing returns. Today's quarterly reporting regime systematically discourages this kind of "patient innovation." I've written extensively about the concept of the "50-year overnight success"—the reality that truly transformative innovations require sustained investment over multiple decades. Companies need the freedom to pursue breakthrough technologies without explaining every quarter why they're spending money on projects that won't generate revenue for years. Do I think switching from quarterly to semi-annual reporting solves the problem? No. Six months isn't meaningfully different from three months when it comes to long-term thinking. But we can make meaningful reforms that would help: First, eliminate forward-looking earnings guidance. This practice forces companies to make public commitments about future performance, creating enormous pressure to meet those predictions regardless of changing circumstances. Second, create accounting treatments that allow companies to separate long-term innovation investments from operational expenses, giving investors clearer visibility into both current performance and future potential. Third, develop new metrics and incentives that reward patient capital deployment and long-term value creation, not just quarterly financial performance. This isn't an abstract policy debate—it's about America's economic future. While we optimize for quarterly performance, competitors in China and other nations are making massive, sustained investments in critical technologies. Their state-directed approach has significant flaws, as Mackintosh correctly notes, but at least they're thinking in decade-long time horizons. We need American companies to have the same freedom to pursue patient innovation without facing quarterly punishment from investors and analysts who fundamentally misunderstand the innovation process. Academic researchers and financial journalists can theorize all they want about efficient markets and rational behavior. Those of us who've actually run large corporations know better. The quarterly reporting system is broken, and defending it based on theoretical models and incomplete data serves no one except those who profit from short-term volatility. Next time the Wall Street Journal wants to analyze corporate behavior, perhaps they should talk to someone who's actually lived it. The American economy deserves better than academic theory masquerading as practical wisdom. The author is the former Chief Technology Officer of Hewlett-Packard and writes about innovation and corporate strategy on Substack. Phil McKinney is an innovator, podcaster, author, and speaker. He is the retired CTO of HP. Phil's book, Beyond The Obvious, shares his expertise and lessons learned on innovation and creativity. Five questions reveal which 'impossible' ideas become billion-dollar breakthroughs—while experts dismiss them as fantasy. How deliberately slowing down in a world obsessed with speed transformed my approach to breakthrough innovation decisions The inside story of vision without execution: why being right about the future means nothing without the courage to act on breakthrough insights
--------------------------------------------------

Title: Analysts revamp Nvidia stock outlook on its investment in Intel
URL: https://www.thestreet.com/technology/analysts-revamp-nvidia-stock-outlook-on-its-investment-in-intel-
Time Published: 2025-09-20T17:07:00Z
Description: Analysts provided their opinion on Nvidia stock, following the company's $5 billion investment into Intel.
--------------------------------------------------

Title: One Big Beautiful Bubble: Oracle, Amazon, Microsoft, Google, Meta Platforms, Palantir et al in the danger zone?
URL: https://www.thehindubusinessline.com/portfolio/big-story/one-big-beautiful-bubble-oracle-amazon-microsoft-google-meta-platforms-palantir-et-al-in-the-danger-zone/article70070297.ece
Time Published: 2025-09-20T15:42:52Z
Full Content:
-387.73 -96.55 -56.00 + 825.00 + 258.00 -387.73 -96.55 -96.55 -56.00 -56.00 + 825.00 Get businessline apps on Connect with us TO ENJOY ADDITIONAL BENEFITS Connect With Us Get BusinessLine apps on If Nifty50’s market cap shoots up 10 per cent in a day, it would gain around $225 billion. Compare this to a single stock – Oracle Corporation adding $255 billion to its market cap within few hours on September 10! The message from investors is clear: The AI mania is running up and strong. The software giant grabbed headlines when its shares gained 36 per cent on September 10. With this massive move up, the stock added over $255 billion to its market cap – joining the exclusive club of stocks that have gained $250 billion or more in a single day. Other members of the coveted club are Nvidia, Apple and Microsoft. What triggered this euphoria in Oracle shares? The company had reported results for Q1 FY26 on September 9 after market close. However, it wasn’t earnings that pleased the street. While its net profit came in line with expectations, revenue reported a slight miss. What, in fact, surprised the street was the company’s grand plans for scaling up its cloud infrastructure business, a whopping 14 times by FY30 – from $10.2 billion in revenue in FY25 to $144 billion. In other words, this is a compounded growth rate of 70 per cent between FY25 and FY30. As the AI race continues full steam, it appears Oracle doesn’t want to be left behind. With large language models (LLMs) being built and upgraded left, right and centre, there is heavy demand for data centres that have the compute (processing power) to support this. This is exactly where Oracle and its cloud infrastructure business come in. The company offers cloud-based compute, storage and networking services to clients. Simply put, clients can make use of Oracle’s high-performance cloud servers to deploy applications based on AI. During the said quarter, the company signed multiple multi-year, multi-billion-dollar contracts (largely under the cloud infrastructure vertical), taking the RPO (remaining performance obligations) figure to $455 billion, which is all the more the main reason for the rally. Readers can understand RPO as order backlog for now. This is a rise of 359 per cent year on year and 230 per cent quarter on quarter. RPO is further expected to reach $500 billion in the next few months. For context, Oracle’s FY25 revenue was $57.4 billion. Though the company is making the right moves to make the most of AI-led demand, there are a few factors that are noteworthy here. First, RPOs represent contracted revenue that is yet to be recognised. Revenue can be recognised only if Oracle could discharge its contractual obligations when the time comes. Thus the $455-billion RPO is not an absolute guarantee that it would translate to future revenue. Contract cancellations and unfavourable modifications to the contract terms could result in material write-down of RPOs. Second, Wall Street Journal reported that Oracle has struck a $300-billion deal with OpenAI, likely spanning over five years from 2027. Now, looking at this in tandem with the $455-billion RPO figure, it leads one to believe that much of the future revenue is tied to one single client – giving room for concentration risk. What is even more concerning is the fact that OpenAI is a cash-burning start-up, run on venture capital, with about $10 billion in annual recurring revenue, per media reports. Third, while the company appears to have a robust road-map for the cloud infrastructure business, there isn’t relatively as much clarity as to the future of the rest of the businesses. Currently, the cloud infrastructure business accounts for just 17 per cent of overall revenue based on FY25. In the last five quarters, this vertical has clocked year-on-year constant currency revenue growth of 51 per cent on average. This, while the rest of the verticals have grown at a mere 2.5 per cent. While such concerns linger, the market’s reaction appears premature and running ahead of fundamentals. Even in the last five years, the company’s execution record has not been strong relative to its stock price performance. On an absolute basis, while earnings have grown 23 per cent on an absolute basis over that of FY20, stock price during the same period has risen close to 400 per cent. Even before the 36 per cent jump last week, the stock was trading at a trailing PE of 54x, which is not inexpensive. As of September 19, it trades at a PE of 69x. All things considered, this very simply reflects investors’ euphoric bet on anything linked to AI. But is it turning into a bubble, one that is reminiscent of the dotcom bust? Before addressing the pertinent question, investors first need to understand the sheer size of economic interest involved in the ongoing AI race. Here’s some crazy statistic. Since January 2020, the market cap of the S&P 500 index has risen $31 trillion. On another hand, the top 10 AI stocks, that are part of the index, have added $18 trillion to their combined market cap. These include Nvidia, Microsoft, Apple, Alphabet, Amazon, Meta, Tesla, Oracle, Broadcom and Palantir. This is saying, the worth of just 2 per cent of the index’s stocks have grown equivalent to 60 per cent of the index’s gains, while the rest 98 per cent got the short end of the stick (40 per cent). Also, currently, the market cap of these stocks put together is as large as 40 per cent of S&P 500’s market cap. Further, the current AI surge demands unprecedented capital expenditure, as companies compete to build the computational infrastructure. These include real estate for data centres, the cutting-edge GPUs and related cooling systems and network hardware. In the past few years, Amazon, Microsoft, Alphabet, Meta and Oracle (in descending order of quantum of capex; referred to as Big 5 in this article) have been large spenders on capex. Together, the Big 5 have invested an aggregate of $586 billion over the last three fiscals. They are expected to incur about $860 billion in the next two years. This is almost as big as how much the US spends on defence every year. Historically, the Big 5 have been asset-light businesses. However, with capex picking up lately, their combined fixed assets turnover ratio (only tangible assets considered) has gradually declined year on year to almost half of what it was 10 years ago — from 4.6 times to 2.2 times now. To put it in perspective, if one were to assign the multiple of 4.6x to the capex expected to be incurred in 2025 – about $400 billion (see chart), the expectation is that there should be an incremental revenue of $1,850 billion. However, the combined revenue of the Big 5 in the recently-concluded fiscal was only about $1,500 billion and it has taken decades since these companies were incorporated to reach such a scale. Even at the lower end of the multiple – at 2.2 times, incremental revenue required out of the capex would work out to $880 billion by end of FY25. For context, the combined revenue of the cloud verticals of the Big 5 is just about $250 billion today. Even if one were to assume that incremental revenue will kick in with a lag, the argument does come across as a hard sell, especially with these figures. Another statistic that brings out how big the AI capex has become for the economy is the one from the US’ Bureau of Economic Analysis. It measures the contribution of segments of the economy to the overall GDP every quarter. For example, if the GDP growth (real) rate during the quarter is 3 per cent and the contribution of personal consumption expenditure (PCE) reads 1 per cent, then it means that PCE has contributed to a third of incremental GDP. Similarly, for the recently-concluded quarter (Q2 2025), GDP growth was 3.3 per cent, but contribution of gross private domestic investment came in at -2.7 per cent. However, contribution of investment in information processing equipment and software (tech capex), which form part of gross private domestic investment, stood at 0.8 per cent. This shows that despite a slowdown in the overall private capex in the economy, tech capex has stood resilient. For lack of further detailed classification, information processing equipment and software have been considered here, which can be safely assumed to be substantially driven by the ongoing AI, data centre capex. If one were to construct a two-quarter moving average line of this contribution to GDP measure, in the recent quarter, the contribution of tech capex to GDP (1.1 per cent) has, in fact, surpassed the contribution of PCE to GDP (0.7 per cent) for the first time, in the quarters analysed since 2022. This is big, especially when you consider the fact that PCE accounts for about 70 per cent of US’ GDP. Considering the above pointers, it is quite clear that the AI mania is now just too big to fail. Whether it is the wealth effect and its consequent positive impact on economic growth or their hard capex driving a good chunk of the economy or the investor sentiment riding upon these companies, it is in the best interest of all stakeholders that expected revenues start flowing in. Looking ahead, AI is still an evolving technology and so, the picture looks murky today. One, there isn’t yet concrete evidence proving efficiency gains from use of AI would be attractive and cost effective enough to drive healthy adoption in enterprises. Two, while LLMs are being trained by spending on bleeding edge GPUs, there are noticeable positive developments with smaller models that wouldn’t require as much compute as LLMs, sometimes possible even without GPUs – raising the question as to what is the right level of capex. Three, today broadly AI is being used in applications such as summarisation, generative media, transcription and others – where the AI models do not appear to stand differentiated from one another. Sure, AI has found use-cases such as support services, pharmaceutical research and so on. However, the single most important application for AI in the next decade – which could be robotics or autonomous driving, for example, are not yet on the horizon when it comes to mass deployment. On considering the above, the valuations of top AI stocks today are not cheap, with all except Nvidia and Amazon trading at PE ratios above five-year averages. Palantir’s market cap is $433 billion, while its revenue is just $2.9 billion. Outside the top 10, stocks like AMD, ARM Holdings trade at PE ratios of 86x and 172x respectively. CoreWeave, a recently-listed cloud services company, is yet to turn profitable but is a $61-billion unit, with revenue and net loss of $1.9 billion and $863 million (FY24). In the unlisted space, AI start-ups are raking in capital from VC/PE funds at lofty valuations. Entities from within the AI value chain have become backers in some cases. Case in point, Microsoft’s, Amazon’s and ASML’s investment in OpenAI, Anthropic and Mistral respectively. All three are cash-burning start-ups valued at about $300 billion, $180 billion and $14 billion respectively (per known latest funding rounds). This exuberance does make one reminiscent of the infamous dotcom bubble of 2000 – when anything that had ‘Internet’ stamped on it was selling like hot cakes, even at unreasonable valuations. Right before the bubble burst in March 2000, the S&P 500 reached a PE ratio of 29.5x – never to see a rerating to such levels until April 2021, which too lasted only briefly. Today, its PE is reasonably close to such levels at 26x. However, the current market cap (of S&P 500) to GDP ratio stands at 2x, much worse than about 1.3x in March 2000. Even on a forward earnings basis, the index and almost all top AI stocks trade at PEG (price to earnings growth) ratios of over 2x, which appear unsustainable, all things considered. Though AI presents a structural opportunity to bet on, investors need to be extremely watchful of valuations as they can make all the difference. Investors will be better served waiting for now, in search of better entry points with higher margin of safety. As the AI craze continues to heat up, it is worth pondering on this quote from John Hussman, a fund manager who was prescient in his prediction of the dot com crash: Valuations often matter suddenly, and with a vengeance. This is a lesson best learnt before a crash rather than after one. In the interest of how much depends on AI investment and wealth effect, one will have to hope this time is truly different! Published on September 20, 2025 Copyright© 2025, THG PUBLISHING PVT LTD. or its affiliated companies. All rights reserved. BACK TO TOP Comments have to be in English, and in full sentences. They cannot be abusive or personal. Please abide by our community guidelines for posting your comments. We have migrated to a new commenting platform. If you are already a registered user of TheHindu Businessline and logged in, you may continue to engage with our articles. If you do not have an account please register and login to post comments. Users can access their older comments by logging into their accounts on Vuukle. Terms & conditions | Institutional Subscriber
--------------------------------------------------

Title: Wrong time for dollar diversification
URL: https://www.thehindubusinessline.com/portfolio/personal-finance/wrong-time-for-dollar-diversification/article70069319.ece
Time Published: 2025-09-20T15:26:49Z
Full Content:
-387.73 -96.55 -56.00 + 825.00 + 258.00 -387.73 -96.55 -96.55 -56.00 -56.00 + 825.00 Get businessline apps on Connect with us TO ENJOY ADDITIONAL BENEFITS Connect With Us Get BusinessLine apps on Folks on social media have a knack for latching on to the wrong investment ideas at wrong times. The advice doing the rounds that Indian investors ought to go big on US stocks to avoid ‘losing money’ from rupee depreciation, is the latest example. Their arguments are on the following lines. # If you’ve earned an 18 per cent CAGR (compound annual growth rate) from your domestic equity funds in the last five years, don’t be too happy. After adjusting for rupee depreciation, your returns are just 13 per cent. The rupee has been losing 3-4 per cent against the US dollar for the last many decades. The only way to shield your portfolio from this is to invest in US markets. # The AI revolution is sweeping the world. As AI replaces humans across industries, many Indian industries will be disrupted. Stay safe by investing in US Magnificent 7 companies. There’s a grain of truth in each of these statements; but it’s no more than a grain. If you’re thinking of making a wholesale switch from Indian stocks to the US ones, look before you leap. Investing a small portion of your portfolio in dollar-denominated assets to fund your overseas expenditure is a good idea. But this is a particularly bad time to replace your Indian equity allocations with US equities, hoping to earn a higher return or gain from dollar appreciation. Let’s first look at why Indian investors need to stick with rupee assets. Buy what you know: Globally, even professional and institutional investors have a home-country bias. That is, they invest the bulk of their portfolio in assets denominated in the currency of their home country. US investors stick by dollar assets and Chinese investors prefer yuan-denominated assets. This is quite simply because you get the best results from investing in what you understand. An Indian investor has a greater shot at understanding and tracking the Indian economy, consumers, companies and government policy, than the US economy, its consumers or government policy. When professional investors or institutions invest outside their home country, they think of this as a risk and hedge against it. USD weakening: It is also dangerous to expect the dollar to deliver 3-4 per cent gains against the rupee like clockwork. Yes, over the last three decades, the rupee has lost ground against the dollar at a 3-4 per cent annualised rate. This is because India has consistently run a current account deficit and needed foreign capital inflows to fund it. But there’s no guarantee that rupee depreciation will continue at this rate to perpetuity. If a shrinking current account deficit, a healthy economy, improving sovereign ratings or attractive stock valuations or bond yields woo back foreign investors to India, this can slow the pace of rupee depreciation. In fact, the dollar index is down 10 per cent since the beginning of 2025, on tariff turbulence and rate cut expectations. US policymakers are, today, keen to allow the dollar to weaken substantially, so that US exports can be made more competitive. Mirror your expenses: The basic purpose of financial planning is to fund your future expenses. If your future expenses such as buying a home or setting up a pension fund are going to be in rupees, your investments need to be rupee-denominated too. Yes, rupee depreciation can indirectly peg up your expenses if imported goods such as crude oil, gold, industrial metals or edible oils get costlier. But all this gets captured in domestic inflation numbers. As long as you plan your investments to beat inflation, the currency impact on your daily expenses is taken care of. You need dollar assets in your portfolio only to the extent of financial goals that will entail spending in dollars. If you plan to tour the US, enrol in a US master’s programme, pursue higher education for your child in American universities, that portion of your portfolio needs to be invested in dollar assets. Switching from Indian stocks to US stocks today, based on valuation concerns, is akin to jumping from the frying pan into the fire. Here’s why. * As the Indian market has enjoyed a breathless bull run since Covid, the US market has rallied right alongside it. In the five years to September 2025, while the Nifty50 has gained 120 per cent, Nasdaq100 Index is up 127 per cent and US S&P 500 112 per cent. Yes, Indian markets have turned expensive after this rally with the Nifty50 at a 22 PE and the Nifty500 at 25. But then so has the US market, which now appears over-valued by most metrics. The Buffett Indicator (market cap to GDP) for the US hovers at 217 per cent against the fair level of 100 per cent. S&P index’ trailing PE of 26 times is well above the long-term average of 18. The forward PE of 25 stacks up against the long-term average of 17. Therefore, if Indian stocks can get derated on earnings disappointments, so can US stocks. * After a post-Covid boom, the Indian economy has normalised to 6.5-7 per cent growth rate. The US economy is, however, on the verge of a sharp slowdown. Last week, US Fed members projected that GDP growth could slow down to 1.6 per cent in 2025 and 1.8 per cent in 2026 from 2.8 per cent in 2024. With government spending cuts, US jobs growth has fallen off a cliff in recent months with monthly non-farm payrolls averaging 29,000 in June-August 2025 against 82,000 same time last year. The impact of these job losses on consumer spending and the economy is yet to play out. It is hard to believe at this juncture that tariff flip-flops, job losses and the spending slowdown that follows will have no impact on US corporate earnings. Whether Fed rate cuts will make up for all this remains to be seen. * Unlike the Indian government, which has been cutting back on borrowings post-Covid, the US has been binge-borrowing, with its debt at $37 trillion and debt-GDP ratio at over 120 per cent now. US bond market yields have been highly volatile in the last one year and are at present above the levels at which they were when the Federal Reserve started cutting rates in September 2024. Rising bond yields ratchet up the cost of borrowings for the US government and companies, and can spook capital flows. Global central banks have been diversifying away from the dollar into gold holdings in recent times. * A popular argument for investing in US equities is that no other market offers the opportunity to own a piece of global hyper-scalers such as Microsoft, Alphabet, Meta, Nvidia and Amazon. This is true. The Magnificent Seven, thanks to their technological prowess and the global dominance of their platforms, have managed scorching growth rates in revenues, profits and cash flows over the past decade. In recent years, they have invested eye-watering sums in AI capex, building up hopes that rising AI adoption will drive the next leg of supernormal growth for these companies. It is thanks to these expectations that the market capitalisation of Magnificent 7 stocks has rocketed to $21 trillion, nearly five times India’s GDP. Just seven stocks now make up 34 per cent of US stock market capitalisation. Mag 7 stocks have belted out a 38 per cent CAGR in the last 10 years. But after a decade of scorching earnings growth and even more scorching stock price performance, some market veterans are beginning to question if the hyper-scaler story has been stretched too far. There are worries about whether AI monetisation will lag capex, and if Mag 7 companies will end up decimating both their cash flows and shareholder returns in their pursuit of GenAI. Comparisons are beginning to be made to the infamous 1999-2000 dotcom bubble (See Big Story). Therefore, Indian investors who need dollar exposures in their portfolio because of specific goals, should explore stocks outside the Mag 7 and gold at this juncture. Others should stick to Indian equities and bide their time on overseas diversification. Published on September 20, 2025 Copyright© 2025, THG PUBLISHING PVT LTD. or its affiliated companies. All rights reserved. BACK TO TOP Comments have to be in English, and in full sentences. They cannot be abusive or personal. Please abide by our community guidelines for posting your comments. We have migrated to a new commenting platform. If you are already a registered user of TheHindu Businessline and logged in, you may continue to engage with our articles. If you do not have an account please register and login to post comments. Users can access their older comments by logging into their accounts on Vuukle. Terms & conditions | Institutional Subscriber
--------------------------------------------------

Title: Stock Indexes Rally to New Record Highs on Fed Rate Cut Optimism
URL: https://www.barchart.com/story/news/34926620/stock-indexes-rally-to-new-record-highs-on-fed-rate-cut-optimism
Time Published: 2025-09-19T20:39:59Z
Description: The S&P 500 Index ($SPX ) (SPY ) on Friday closed up +0.49%, the Dow Jones Industrials Index ($DOWI ) (DIA ) closed up +0.37%, and the Nasdaq 100 Index...
--------------------------------------------------

Title: Oracle stock surges on potential $20B Meta AI cloud deal
URL: https://finance.yahoo.com/news/oracle-stock-surges-potential-20b-203030099.html
Time Published: 2025-09-19T20:30:30Z
Description: Investing.com -- Oracle Corporation (NYSE:ORCL) stock surged 4.4% to a session high heading into the close on Friday after Bloomberg reported the company is ...
--------------------------------------------------

Title: Oracle in Talks With Meta on $20 Billion AI Cloud Deal
URL: https://finance.yahoo.com/news/oracle-talks-meta-20-billion-201949767.html
Time Published: 2025-09-19T20:19:49Z
Description: Under the multiyear deal, Oracle would provide the social media giant with computing power for training and deploying artificial intelligence models...
--------------------------------------------------

Title: Meta Platforms Stock Options - a Follow-Up on Three Ways to Play META
URL: https://www.barchart.com/story/news/34923630/meta-platforms-stock-options-a-follow-up-on-three-ways-to-play-meta
Time Published: 2025-09-19T17:54:44Z
Description: Three weeks ago, we suggested three ways to play META stock by shorting out-of-the-money puts, buying long-dated in-the-money calls, and shorting near-term...
--------------------------------------------------

Title: Stock Buybacks Have Slowed. Here's Why It Matters That They Could Bounce Back.
URL: https://www.investopedia.com/stock-buybacks-have-slowed-here-s-why-it-matters-that-they-could-bounce-back-11812904
Time Published: 2025-09-19T15:24:46Z
Full Content:
A slump in stock buybacks could be in the rearview mirror. What that means for stocks is more complicated. S&P 500 buybacks in the second quarter fell 20% from record highs in the first quarter, but they are expected tick up in the current one. With economic policy uncertainty starting to dissipate and interest rates headed lower, repurchase activity from companies in the broad market index is expected to return to record levels, according to S&P Dow Jones Indices. Wall Street analysts, however, say buybacks may not be as helpful to EPS growth as they once were. Headline buyback figures are big, but their growth has stalled lately even for the biggest companies in the index. Companies like Apple (AAPL), Meta Platforms (META), Alphabet (GOOGL) and Nvidia (NVDA), which usually account for about 30% of annual buyback spending, posted no meaningful year-over-year growth in the second quarter, according to a recent Goldman Sachs report. A lack of buyback activity has resulted in a lower buyback yield—which measures total buybacks over a given period divided by market cap at the start of that period—for the the S&P 500 over the past 12 months. The figure recently touched 2%, the lowest level in 20 years, excluding recessions. (That is, however, partly due to big spending on artificial intelligence that has contributed to a decline in share repurchases.) Share prices have risen faster than earnings have, and payout ratios have fallen, leading to higher price-to-earnings multiples and lower buyback yields. A falling buyback yield means less of a boost to earnings per share: Buybacks result in lower shares outstanding, and buybacks from 2005 to 2019 boosted EPS growth by a median of 1.2 percentage points annually, according to Goldman. That tailwind has fallen off lately. The decline in the S&P's buyback yield is expected to level off as share repurchasing activity returns, per Goldman. The firm estimates share repurchases to total $1 trillion this year, 5% higher than in 2024. But in the meantime, companies that have sustained buyback activity—so-called buyback aristocrats, or companies that consistently repurchase their shares—could benefit from a rising scarcity premium, Goldman analysts including Ben Snider wrote. The median buyback aristocrat tends to have larger market-caps, higher buyback yields, lower valuations relative to the broader index, and generated a higher year-to-date return. The list of aristocrats includes Bank of America (BAC), JPMorgan Chase (JPM), Applied Materials (AMAT), eBay (EBAY), Ross Stores (ROST) and TJX Cos. (TJX), according to Goldman.
--------------------------------------------------

Title: Norges Bank adds multibagger Cartrade Tech, 2 more smallcaps in Rs 209 crore worth bulk deals
URL: https://economictimes.indiatimes.com/markets/stocks/news/norges-bank-adds-multibagger-cartrade-tech-2-more-smallcaps-in-rs-209-crore-worth-bulk-deals/articleshow/124001494.cms
Time Published: 2025-09-19T15:23:58Z
Full Content:
Norway’s sovereign wealth fund, via Norges Bank, purchased shares worth Rs 209 crore in Cartrade Tech, Aarti Pharmalabs, and Tanla Platforms through bulk deals, highlighting confidence in select smallcaps despite mixed stock performances. (What's moving Sensex and Nifty Track latest market news, stock tips, Budget 2025, Share Market on Budget 2025 and expert advice, on ETMarkets. Also, ETMarkets.com is now on Telegram. For fastest news alerts on financial markets, investment strategies and stocks alerts, subscribe to our Telegram feeds .) Subscribe to ET Prime and read the Economic Times ePaper Online.and Sensex Today. Top Trending Stocks: SBI Share Price, Axis Bank Share Price, HDFC Bank Share Price, Infosys Share Price, Wipro Share Price, NTPC Share Price (What's moving Sensex and Nifty Track latest market news, stock tips, Budget 2025, Share Market on Budget 2025 and expert advice, on ETMarkets. Also, ETMarkets.com is now on Telegram. For fastest news alerts on financial markets, investment strategies and stocks alerts, subscribe to our Telegram feeds .) Subscribe to ET Prime and read the Economic Times ePaper Online.and Sensex Today. Top Trending Stocks: SBI Share Price, Axis Bank Share Price, HDFC Bank Share Price, Infosys Share Price, Wipro Share Price, NTPC Share Price Trump’s H1-B shock is just the beginning GST 2.0 to redraw India’s trucking map; small fleet owners set to gain DMart bets against Q-comm tide. Bandhan MF bites it; should you? Investors abandon sector funds, rush to large-cap safety amid market volatility Stock Radar: Cup & Handle breakout! Bajaj Finance climbs to new highs in September. Can the momentum sustain? Small-cap stocks: Pick right one & manage risk for wealth-creation: 5 small-caps from different sectors with upside potential of over 25% Hot on Web In Case you missed it Top Searched Companies Top Calculators Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Worry not. You’re just a step away. It seems like you're already an ETPrime member with Login using your ET Prime credentials to enjoy all member benefits Log out of your current logged-in account and log in again using your ET Prime credentials to enjoy all member benefits. Big Price Drop! Flat 40% Off Offer Exclusively For You Save up to Rs. 700/- ON ET PRIME MEMBERSHIP Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get Flat 40% Off Then ₹ 1749 for 1 year Offer Exclusively For You ET Prime at ₹ 49 for 1 month Then ₹ 1749 for 1 year Special Offer Get flat 40% off on ETPrime What’s Included with ETPrime Membership Trump temper on H-1B visas is forcing Indians to do these things to stay put in US What Adani’s US indictment means for India Inc’s overseas fundraising Why veterans like Reliance, L&T are on acquisition spree? Aswath Damodaran has an answer. Will China’s dollar bond sale in Saudi Arabia trump the US in financial world? Huawei launches its own OS to compete with Google and Apple. But can it win beyond China? The problem with lab grown diamonds Why a falling rupee is a better option for the economy A list of top 20 momentum stocks that have delivered massive returns in one year Investment Ideas Grow your wealth with stock ideas & sectoral trends. Stock Reports Plus Buy low & sell high with access to Stock Score, Upside potential & more. BigBull Portfolio Get to know where the market bulls are investing to identify the right stocks. Stock Analyzer Check the score based on the company's fundamentals, solvency, growth, risk & ownership to decide the right stocks. Market Mood Analyze the market sentiments & identify the trend reversal for strategic decisions. Stock Talk Live at 9 AM Daily Ask your stock queries & get assured replies by ET appointed, SEBI registered experts. ePaper - Print View Read the PDF version of ET newspaper. Download & access it offline anytime. ePaper - Digital View Read your daily newspaper in Digital View & get it delivered to your inbox everyday. Wealth Edition Manage your money efficiently with this weekly money management guide. TOI ePaper Read the PDF version of TOI newspaper. Download & access it offline anytime. Deep Explainers Explore the In-depth explanation of complex topics for everyday life decisions. Health+ Stories Get fitter with daily health insights committed to your well-being. Personal Finance+ Stories Manage your wealth better with in-depth insights & updates on finance. New York Times Exclusives Stay globally informed with exclusive story from New York Times. TimesPrime Subscription Access 20+ premium subscriptions like Spotify, Uber One & more. Docubay Subscription Stream new documentaries from all across the world every day. Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Stories you might be interested in
--------------------------------------------------

Title: This Week in Games - Nintendo September Direct 2025
URL: https://www.animenewsnetwork.com/this-week-in-games/2025-09-19/nintendo-september-direct-2025/.228798
Time Published: 2025-09-19T15:00:00Z
Full Content:
Welcome back, folks! I hope everyone enjoyed my cameo appearance in This Week in Anime yesterday. I'd have made a cute segue for it in last week's column, but things were arranged a bit too late for that. It's always nice to collaborate with Steve, Coop, and Chris, especially when I can chat about topics I'm familiar with. I got to meet Chris in person for the first time a few weeks back; he's just as charming as his writing. We were also able to meet up with fellow This Week in Anime alums Nicky. Good times were had by all! Today, though, we're covering a ton of news—so let's begin. People sure love tempting the Persona 5 curse. You'd think after Star Ocean: anamnesis, Dragalia Lost and the Fullmetal Alchemist gacha, folks would have a bit of trepidation about collaborating with Persona 5. Oh well! Here comes Overwatch 2. Overwatch 2 has had a lot of collaborations with Japanese media lately, and they've been hit-or-miss. The One-Punch Man outfits didn't look great, and the Cowboy Bebop collab was all right (given some weird faces). This time, the suits look pretty good. Mercy as Panther/Ana actually looks pretty great, and I can't fault any of the other choices (D.Va as Queen/Makoto, Genji as Skull/Ryuji, Wuyang as Joker/Akira, and Lifeweaver as Fox/Yusuke). They've even got art from Shigenori Soejima; that's at least as much effort Fortnite would put into one of these. The collab started on September 16; at the time of writing, further details haven't been released. I snark, but the skins look legitimately cool. I might be a bit burnt out at the sight of Persona 5, and Overwatch has never really done anything for me. But kudos to Blizzard for finally getting one of these right. A major surprise dropped into our laps earlier this week with the reveal of two new Game Boy Advance titles added to Nintendo's NSO+ service—and both of them are third-party titles from Bandai Namco! While they were exclusives to Nintendo's old handheld, this opens a lot of fun doors for the NSO+ library. Provided, we might want to keep those reins in check: these likely came about because Nintendo and Bandai Namco have some very close business relations. Those old side-scrollers starring Nickelodeon characters might still be a bridge too far, more's the pity. First up is Mr. Driller, Bandai Namco's long-running puzzle series starring the eponymous miner. The Mr. Driller games are fun, hectic puzzlers: using your drill, you tunnel your way through variously-colored minerals, with entire blocks of like-colored minerals vanishing at once. But as you drill your way through, blocks that aren't affixed to larger blocks will fall behind you, forcing you to keep on digging to keep from getting caught in the cave-in. Complicating things is your oxygen meter: you lose air as you drill, and harder minerals require more effort (and thus more air), forcing you to be careful in choosing your path as you tunnel. It can be a long way between spare air tanks... Mr. Driller has never been a massive hit for Bandai Namco, but the games are fun—especially if you can take them on the go. Also fun is the lore behind the title: canonically, the titular Mr. Driller, Susumu Hori, is the son of Dig-Dug's hero, Taizo Hori. Taizo has even made cameo appearances in other Mr. Driller games! (The mom is Toby Masuyo, protagonist of Baraduke/Alien Sector. According to Namco x Capcom, Toby and Taizo had an acrimonious divorce.) The other game is another attention-nabber: Bandai Namco's beloved Klonoa: Empire of Dreams! I have a soft spot for the Klonoa titles: deceptively deep platformers with stunning presentation and music, and stories that rip your heart out. Empire of Dreams sees the Dream Traveler Klonoa having to clear his name of the crime of "dreaming of adventure." Of course, there's a reason why dreams have been outlawed, and Klonoa is likely to make us shed a few tears as he finds out the truth... Empire of Dreams continues the series' traditional mechanics: Klonoa can jump and flutter around a bit by flapping his cute, floppy ears. Klonoa also has his Wind Ring, which lets him inflate enemies. From there, Klonoa can carry them around, throw them at other enemies, or throw them after jumping to extend his jump. It's basic stuff on paper, but eventually expanded into surprisingly complicated platforming sections. The Klonoa titles might be cute, but 100% completion is a true challenge in those titles. My heart always weeps for Klonoa. We were fortunate enough a few years ago to get the Phantasy Reverie Series back in 2022, and I was luckier still to get to review it. Bandai Namco even teased a possible Klonoa 3 if the collection sold well enough... but it's been three years. At the very least, the Phantasy Reverie Series regularly goes on sale for very cheap; at around $10, these are some of the best platformers you can get your hands on. I'd at least like to hope that Empire of Dreams coming to NSO+ means that the other GBA Klonoa titles are on the table. Dream Champ Tournament is also cute, but Legendary Star Medal never came to the United States. We can only hope for more chances to hear Klonoa's adorable "WA-HOO!" Both of these titles go live on NSO+ this September 25. Last week, I mentioned how Nintendo and Pokémon have become a serious "bitch eating crackers" for the gaming industry and for self-described "Gamers." For folks unfamiliar with the phrase, a "bitch eating crackers" is when you have such dislike for someone that something as innocuous as them eating crackers at the other side of the room ticks you off. I maintain that. Mind you: More than a few decisions in the following Direct did turn me off... but also, I wasn't catastrophizing about this being the worst display of arrogance from a company that "needs to be humbled." Because there wasn't any need to. I worry if I'm the crazy one, but then I read past the headline of an article... With 2025 being the 40th anniversary of the Mario Brothers, Nintendo has arranged for several fun goodies: special art and light displays at the Nintendo Museum in Kyoto, sponsoring the annual Kyoto Marathon... fun stuff. There was also the news that the Mario Bros. sequel is being developed, and it's going with Super Mario Galaxy. (A lot of folks have pointed out that Old Spice jumped the gun a little and spoiled the reveal with some of their promotional stock that is in stores now. Whoops.) Weird that the movie is going with Galaxy, but considering that none of the other Mario games escalate on the level Galaxy does, I see where they're coming from. Doki Doki Panic has been buried; Super Mario Bros. 3 and Super Mario World only really introduce the Koopalings, which aren't enough of a draw; Super Mario 64 and Super Mario Sunshine don't really shake up the formula enough, and Illumination still thinks Chris Pratt is more charming than Charlie Day, so that's a no-go on Luigi's Mansion. No word yet on who's playing Princess Rosalina, but Rosalina's storybook (as in, an actual physical representation of it) is for sale on Nintendo's website and set to release this November 25. The real disappointment was the news that Super Mario Galaxy and its sequel are getting a remastered physical release—at full price, US$70. This is following the Super Mario Bros 3D All-Stars, which already included the first Super Mario Galaxy. Let me be clear, I think people have lost their damn minds when it comes to game price discourse. I saw someone comment that Digimon Story: Time Stranger "doesn't look AAA, so it shouldn't be priced like a AAA game," which is a great way of completely disregarding the amount of effort that went into actually producing the game. It is my genuine belief that Steam's firehouse sales have destroyed any semblance of value when it comes to a game; why buy a game at full price when you can get it for 20% off at the Summer Sale? And why pay that when you can get it for 30% off at the Winter Sale? And why pay that when you can get it for $5 at the next Spring Sale? I feel like the effort put into remaking a game top-to-bottom isn't invalidated just because the game was originally released on Game Boy or because you can't see the individual scales on Gomamon's Marching Fishes. That said, US$70 for both Super Mario Galaxy games is where I draw the line. Both of those titles were US$20 on the Wii U! We already had Super Mario Galaxy in the first 3D collection! This is what a piss-take looks like, if you ask me. But this got weirdly ignored following the further news. At any rate, it releases on October 2, with promotional Amiibo (Mario with a Luma, and Rosalina reading a book to several Luma) releasing on April 2. I rather like the Rosalina Luma a lot. I'd rather buy that than the US$70 two-pack. I regret to inform you all that the following news involves tennis. (Cue the clip of Rex Mohs screaming his soul out in disgust.) We're getting a new Mario Tennis game, titled Mario Tennis Fever. The titular gimmick is the fever rackets: tennis rackets that can build up energy over the course of a rally to activate special effects. There are over 30 Fever Rackets, with all sorts of effects like freezing the opponent's side of the court or summoning Mini Mushrooms. There is also a record-breaking amount of characters in Mario Tennis, over 30. There are also a variety of modes, like the standard Tournament mode and online Ranked modes, along with a mode for motion controls or randomized effects on the court ("Mix-It-Up" mode). The Adventure Mode is most intriguing. It's a story mode where the Mario characters are turned into babies and have to relearn tennis skills as they travel through a world. It's a good way to mask a tutorial mode, especially for people who aren't great at tennis simulators. Mario Tennis Fever releases on February 12, 2026, but, y'know, it's tennis without Ultra-Smashes. Super Mario Wonder is also getting a dedicated port on Nintendo Switch 2—but it's not just a graphical update. New features and content were teased, but we won't hear about that until later. The main feature the Direct focused on was the Bellabel Park update, which adds a variety of multiplayer options. A lot of these are actually quite fun as party games. Also, like, if we're getting toys of Nintendo characters, can we get more Yoshi toys? There's so much money left on the table from Nintendo not making a full stable of Yoshi toys. Speaking of the T. Yoshisaur Munchakoopas: A thing I appreciate about the Yoshi games is their use of different aesthetics. The original Yoshi's Island was ostensibly a Super Mario World sequel—but it abandoned the look of the original game in favor of a crayonesque design, along with its own unique platforming mechanics (i.e., carrying Baby Mario around). Yoshi's Story wasn't great, but its pop-up book aesthetic still looks pretty good. And so on with other games like Yoshi's Woolly World. So Nintendo's going all-in with Yoshi and the Mysterious Book. With the help of the encyclopedia, Mister E (get it?), Yoshi and his friends must fill out the missing pages. The whole thing is done as a side-scrolling puzzle game, figuring out the various abilities of the creatures Yoshi encounters while using the world around him to move around. The trailer showcased a puzzle where Yoshi had to spread dandelion seeds to erode a boulder that was in his way; that's some good lateral thinking. They'll have a real winner on their hands if they can properly iterate on that kind of stage construction. Also, I like the in-universe aesthetic. It's a bit Super Mario Wonder in its approach, but with undercranked character animations to sell the "illustrated" aesthetic. See, this is the thing I like seeing from Nintendo; it's disappointing that this kind of attention to detail doesn't get its due appreciation. That's not Bill Trinen! Nintendo had a minor once-over for Tomodachi Life: Living The Dream, going over the basics of character creation and care. But there wasn't much else, outside of the confirmation of a Spring 2026 release. The burning question on everyone's mind is whether Living The Dream will actually incorporate gay/lesbian couples. There was a kerfuffle over the original Tomodachi Life on 3DS, since this was impossible then (outside of a glitch). Games have come a long way in the years since, and even other Nintendo games have allowed for more gender-affirming options, such as referring to body types as "styles." And that's before you consider other titles like the Story of Seasons games, allowing for non-gender-locked relationships. It'll be a significant disappointment if Nintendo doesn't follow along. Here's a funny one. Square Enix originally broke away from Nintendo during development of Final Fantasy VII because Nintendo decided upon a cartridge-based format for the Nintendo 64, and Square Enix (then Squaresoft) had bigger goals in mind for their upcoming title. This was a particularly strong bit for them, so much so that they referenced the matter in their magazine ads. We wouldn't see new Final Fantasy titles on Nintendo consoles for years after that. Anyway, Final Fantasy VII Remake Integrade is coming to the Nintendo Switch this January 22. The first part of Square's ambitious three-part remake of the original Final Fantasy VII, the "Integrade" update includes new story beats and stages that center around Yuffie, explaining what she was doing before Cloud and company find her in the forest. There might be some graphical compromises, but I feel like those are small potatoes compared to being able to play a game of Final Fantasy VII Remake Integrade's scope on the go. Speaking of Amiibo! Nintendo has decided to introduce a Smash Bros-style mechanic to Kirby Air Riders wherein you can train Amiibo Figure characters. These characters will learn racing techniques as they play against you, up to and including developing a "personality" based around their experiences racing. So far, we only have Kirby on the Warp Star and Bandana Dee (no need for a middle name here) on the Winged Star. But also: the figures are swappable! You can unplug the riders from their machines, which will come in handy with upcoming Riders and Machines that are being teased. You can expect Meta Knight, King Dedede, and Gooey to come along with certain machines. I lack the experience with Kirby Air Riders to make any educated guess on those. Also, because Sakurai's 40-minute Direct focused entirely on Kirby Air Riders wasn't enough, there's going to be a second one in the near future. More updates on Hyrule Warriors: War of Imprisonment! The gameplay stuff sounds fun, but the most important bit is that it's Gameshare compatible: players will be able to play co-op across two consoles with just one copy of the game, including the original Switch model. It's a great move. I honestly wish more games offered this as a feature by default. Anyway, War of Imprisonment adds some new wrinkles to the Hyrule Warriors formula in the form of Tears of the Kingdom's Zonai tech. Using these ancient artifacts, players can mess with enemy groups or alter the battlefield. I appreciate the puzzle-based addition to the battles. I'm still not crazy for the post-Breath of the Wild Hyrule Warriors games because of the original Hyrule Warriors offering so much, but I can't knock them for actually trying to expand the formula. Few things disappoint me than how the gaming community has left Dragon Quest in an ivory tower. If everyone who namedropped Dragon Quest Monsters during the Palworld/Pokémon discourse had actually bought a copy of Dragon Quest Monsters: The Black Prince, Square Enix might have brought over more Dragon Quest Monsters games. And hey, all those arguments about Expedition 33 and Square Enix's turn-based games are incomplete without recognizing all of the work Square Enix has put into titles like Dragon Quest XI (like how the Switch adds a sprite-based mode for playing the game, a la the 3DS remakes). Dragon Quest isn't the end-all, be-all of Japanese RPGs, and there were plenty of other games that were made leading up to their original title on Famicom... but getting anyone to recognize Dragon Quest in America is an uphill battle. At any rate: Dragon Quest VII is getting a remake. And while it's not an HD-2D remake along the lines of the Erdrick Trilogy, it's nevertheless a top-to-bottom remake of the whole game. Er—another one, since we also had a comprehensive 3D remake of Dragon Quest VII back on the 3DS. Dragon Quest VII is a weird entry for the series; while tremendously overshadowed in the United States due to releasing alongside the graphical powerhouse that was Final Fantasy X, DQ7 also had a very long intro; famously, the PS1 original front-ended the game with a lot of puzzles, so much so that it could take you as long as five hours to encounter your first Slime in battle. Of course, there's a story reason: the protagonist accidentally petrifies his village, and he has to recover several stone tablets to break the curse. This whole quest has a very tragic ending, as many vignettes in Dragon Quest do, but it leads to a game that can have triple-digit length. Dragon Quest VII's deal is its class system: much like Final Fantasy V, characters can be given specific jobs for battle, with each victory leading towards improving their proficiency in that class. There are a lot of classes, many of which are unlocked as combinations of other classes. And that's before we get to the Monster classes, earned by fighting monsters. Yes, there's a Metal Slime class, and yes, it's a pain to unlock and master. Look forward to this one on February 5. Here's one that gets me: Nintendo is adding the Virtual Boy's library to the Nintendo Switch Online repertoire, along with other consoles like the Super Nintendo and the Nintendo 64. People have had words about this. For starters, the Virtual Boy was a failure of a console: its early attempts at 3D (plus everything being in shades of red) caused headaches, and the console wasn't very comfortable. All told, only 22 games were made for the console—14 of them are available on the NSO+Expansion service. This includes several... well, I can't call them "classics," but they're still noteworthy. Wario Land was the first game in the series, and had its first entry made on the Virtual Boy, making this the first time it's been ported, ever. There's also Jack Bros., a Shin Megami Tensei spin-off starring Jack Frost, Pyro Jack, and Jack Ripper. The point of contention is that the Virtual Boy games aren't available to play in 2D—they're exclusively designed for 3D play. So to play the Virtual Boy games, you'll need an accessory that effectively re-creates the original console that you'll slot your Switch or Switch 2 into. The accessory costs $100 USD, which I agree, is a lot. I understand why Nintendo made it; it's to keep the games in context and because they likely don't have a way to allow these games to be played in 2D without extensively reworking them—a tall order for 30-year-old games from a failed console. Nobody's playing a Virtual Boy game to play it in 2D anyhow, the way I see it. The bit where I scrunch my nose up is people doomsaying about the $100 add-on... when there's a $25 alternative that's also for sale. If you don't want to spend $100, you don't have to. Heck, I even appreciate that Nintendo found a new use for the Nintendo Labo components. But the discussion begins and ends with the $100 doohickey. I've seen other peers point out how Nintendo also offers Switch-compatible variants of older controllers, from the NES and N64 controllers to even the Sega Genesis/Mega Drive controller for use with the NSO titles; a quick look at Nintendo's store reveals that those are all in the ballpark of $65. My dudes, the cardboard option is cheaper than a N64 controller. There's so much to complain about. I don't understand why this is the straw that breaks the camel's back, nor do I understand why people can't see the cheap option that's literally right there. Speaking of old stuff, Koei Tecmo is bringing back another Fatal Frame title with a remake of Crimson Butterfly. Originally released in 2003(!), Crimson Butterfly tells the story of a young woman named Mio searching for her twin sister, Mayu, in an abandoned town. Along the way, Mio investigates an old ritual that the village was undergoing. Jumpscares and creepy Japanese ghosts ensue. The series' traditional Camera Obscura is still the central mechanic, a camera that can banish ghosts. This one's another comprehensive top-to-bottom remake; if it's half as good as the Mask of the Lunar Eclipse remake from 2023 (which I reviewed), it'll be a treat! Others and I would definitely prefer a new Fatal Frame proper, but this is a good-enough stopgap. Look forward to it in early 2026. I am disappointed that nobody is talking about the new Suika Game. Suika Game was a huge hit among streamers and players alike, and countless Suika Game clones have emerged in its wake, like IntiCreates' PuzzMix. Just this past summer, Shift-Up even had a Suika Game-esque mini-game for their Boom The Ghost event in Goddess of Victory: NIKKE! In upping the ante, Aladdin X has taken the game to the only place left to go: space. Suika Game Planet has you dropping fruit into an orb at the center of the screen. You can arrange the fruit all around the planet, but they can't break its surface (it's more of a bubble, so the edges have some give). Another twist is in the form of Super Evolution, which grants you bonuses depending on how many fruits of which type you've combined in a row. It's even compatible with Game Share on the Switch 2! I don't expect Suika Game to register on most people's radar, but it'll definitely be a fun hit once it lands this Winter. A major twist was delivered regarding Metroid Prime 4: Samus has a sick sci-fi bike! The reveal comes along with the twist that Metroid Prime 4 is now seemingly an open-world game, with Samus riding her new bike across massive vistas in search of new locales to explore. There's some consternation over this, given that Metroid has been at its best when it's nurturing a claustrophobic vibe. I'm with them. The trailer shows a sequence of Samus facing off against a pack of wolf-like creatures during a blizzard; it seems so pedestrian for Samus to be fighting space-wolves, given all of the other bizarre alien life-forms she's encountered. Regardless: Metroid Prime 4 is slated for a December 4 release, with Amiibo and an art book set to release shortly beforehand. Donkey Kong Bananza has some DLC out now. Most of us expected DLC related to the twist reveal in Bananza, but the DLC is only tangentially related: you actually get to explore D.K. Island! It even plays the song from Donkey Kong 64! There's also the new Emerald Rush mode, a rogue-lite mode where Donkey Kong has to smash enough emerald ore within a time frame. You'll find fossils and stuff that can unlock perks as you run around smashing things, and better performance unlocks new stages in lower layers. A unique twist is that the locations for all of the ores and fossils are a constant, so you'll be able to strategize and plan out your route. The news of Pokémon: Pokopia has some people unsettled, if only because your player character is a Ditto who has shapeshifted (badly) into a human. This has deep ramifications for the world of Pokémon that nobody wants to contend with. The idea itself is fun; however, Ditto will meet other Pokémon and learn moves from them that allow it to modify the environment. As the environment is made more comfortable, more Pokémon arrive, teaching Ditto more moves that allow it to create a whole village for the Pokémon to live in. The idea has been likened to Dragon Quest Builders, which I feel is apt (provided I haven't played Dragon Quest Builders yet). It's a cute idea, and I can see it leading into explaining how Pokémon in the Pokémon Mystery Dungeon games have places to live. Let's go into a lightning round for some other titles: Danganronpa 2 is getting an expanded port with Danganronpa 2x2, which not only includes the base Danganronpa 2 but also a new scenario with a new story. It'll be available next year, also on PS5, Xbox Series X|S, and Steam. Dynasty Warriors Origins lands on Switch 2 this January 22, with paid DLC announced. Monster Hunter Stories 3: Twisted Reflections has a March 16 release date. Resident Evil 9 arrives for the Switch 2 on February 27, wildly enough, with Resident Evil VII and Resident Evil: Village in tow. A new Fire Emblem is coming! Fire Emblem: Fortune's Weave takes the series to a Greco-Roman setting, where some would-be heroes duke it out in front of their regent in the hopes of having a wish granted. Some fight for revenge, others fight for someone else. Sothis from Fire Emblem: Three Houses is back, so maybe this takes place in the same world? Who knows. I have an issue with Fortune's Weave, though. Several of the lead characters have darker skin; the protagonist we see is ostensibly a Black young man. His design is otherwise okay. I love his hair (he doesn't have the Killmonger haircut!). But a problem he and the other darker-skinned characters have is that they all have an ashy skin color. It does them a disservice. Mika Pikazo took a lot of heat for the toothpaste hair in Fire Emblem Engage, but she did right by the dark-skinned cast like Timerra by giving them beautiful skin tones. Even in Fire Emblem: Three Houses, Claude's olive skintone belied his Mediterranean-inspired origins. So it's a disappointment that the ball was dropped with Fortune's Weave. Still, the cast seems interesting, the designs are nice, and I especially like the hairstyles. So I'm hopeful. Here's where everyone lost their marbles. Paid DLC has been announced for Pokémon Legends: Z-A, which releases this October 16. Now, this DLC includes new story content, as well as two new Mega Evolutions for Raichu (Raichu X and Raichu Y). The announcement of paid DLC before the game has launched has people outraged... as though this is the first time any game has been announced with paid DLC at launch. The trailer also reveals new Mega Evolutions for the Kalosian starters, Delphox, Chesknight, and Greninja. Notably, these three Pokémon didn't have Mega Evolutions in their original game, which, in the eyes of many, was an oversight. Legends Z-A corrects this... but also, the Mega Stones for these Mega Evolutions are barred behind timed ranked play over the course of three in-game seasons. There has been a lot of discourse about this being beyond the pale and a sign of Nintendo's modern-day "greed." Let's break this down for a minute. First off: the DLC. Where have people been for the past decade? Super Smash Bros. Ultimate had announced paid DLC before it launched, too! And the only carrot on that stick was knowing that the first character in the DLC list was the Piranha Plant! Nintendo has done this with all three Xenoblade Chronicles games on the Switch, going back to Xenoblade 2! Getting the Season passes was how you unlocked a stash of items and the post-game stories (which are effectively games unto themselves). This isn't new! This hasn't been new for years! You can't even say it's exclusive to Nintendo—Sonic Racing Crossworlds also has been promoting paid DLC for the game, and that game hasn't launched either! There's a lot to argue over paid DLC as a means of bilking people out of their money, but that discussion has long since passed us by. One of the funniest clips from my favorite Vtuber is her going through almost a minute's worth of prompts from all the cosmetic DLC she bought before playing the game. For better or worse, paid DLC is just a standard factor in games these days. And you're telling me it's a problem now? In a Pokémon game? I've seen people claim that this artificially inflates the price of Pokémon Legends: Z-A--again, where have people been? Super Smash Bros. Ultimate costs $120 with both season passes—and you're going to want those, if you want to use Kazuya Mishima or Steve! It disturbs me how much people make this their hill to die on after actual years of literally every other publisher and developer going through with it. Especially since this is now such an entrenched belief with people. It's a truism that Nintendo is especially greedy, patent-starved and disrespectful to its fans and you're the crazy one if you bring up receipts proving otherwise. These are stances I can understand coming from rage-bait content creators online, who have the ears of the entire fandom. I am puzzled that people who should know better fall into these talking points. It's not even that paid DLC isn't a bad thing—it's sure as hell not, especially not since so many games had actual content cut from the main game just to be turned into DLC. That people decided Pokémon was where things were a bridge too far is what blows my mind. I half-expect that if Nintendo ever introduces a gacha system in a Pokémon game, it will finally prompt the rest of the industry to abandon the practice. There's also consternation about Pokémon imported from Pokémon Home getting stuck in Legends Z-A if and when they're imported. Again, not fun! I don't like this! But I understand why it's happening: Legends Z-A has a unique battle system and Pokémon data likely has to be reformatted to accommodate it, and it'll likely be too much of a pain to reformat Legends Z-A data back out of Legends Z-A. This means that the unique Mega Evolutions in Legends Z-A are landlocked into the game... for now. Seriously, there's no way Mega Victreebell or Mega Hawlucha aren't going to be made available some other way. Pokémon Champions is still releasing soon, after all. Again, this is disappointing... but people know that you can't import Pokémon from the first two Generations into Gen 3 onward, right? Even now, there's no legal alternative to get Pokémon from Pokémon Gold out of those games. Sure, there were digital alternatives on the 3DS—but if you wanted to rescue your old Lugia from your old cartridge, there's no official means. I'll go a step further, because as much as I dislike this, I still don't think it's the worst. Monster Rancher 4 had an exclusive in-game monster based on Kasumi from Dead or Alive. The only way to get her was by scanning a copy of Dead or Alive: Hardcore for the PlayStation 2. So you had to spend $100 to buy two games, to get a special-edition Monster. And that's just Kasumi—I'm not counting White Hound, a unique variant of the default Tiger monsters. You could only get White Hound in Monster Rancher 4 by scanning a DVD copy of Princess Mononoke (or Red Hot Chili Peppers' By The Way album). That's another, what? $30 USD? And I haven't even gotten to all of the other exclusive monsters. And then there's Yokai Watch 2's Local Yokai, which you could only get by playing participating Yokai Watch arcade games in certain regions of Japan. That's right: you had to physically travel across the country to get all the cards with the QR codes for these promotional monsters. Or buy the cards second-hand, which still takes money. It's bizarre how much anger gets levied towards Pokémon because I can point to so many similar issues from other monster-taming RPGs. And I think this really highlights the hollowness of much of the discourse surrounding the titles. It doesn't even invalidate the criticisms, but it definitely puts Pokémon into context. But that's the context that usually gets lost since most people don't look into the genre past Pokémon. That'll do it for this week. It's been a bit since I had a column this long... I stepped back and took a screenshot of the whole column when I had finished writing up the Nintendo Direct breakdown, and it was already long enough to be a column unto itself. That's what they pay me for, babes...! I might actually be glibbed out this time. Be good to each other. I'll see you in seven. This Week in Games homepage / archives
--------------------------------------------------

Title: Stocks Supported by Prospects of Additional Fed Easing
URL: https://www.barchart.com/story/news/34919049/stocks-supported-by-prospects-of-additional-fed-easing
Time Published: 2025-09-19T14:01:40Z
Description: The S&P 500 Index ($SPX ) (SPY ) today is up +0.16%, the Dow Jones Industrials Index ($DOWI ) (DIA ) is up +0.04%, and the Nasdaq 100 Index ($IUXX ) (QQQ...
--------------------------------------------------

Title: Google Is Now Worth $3 Trillion. Should You Buy GOOGL Stock Here?
URL: https://www.barchart.com/story/news/34917158/google-is-now-worth-3-trillion-should-you-buy-googl-stock-here
Time Published: 2025-09-19T13:00:02Z
Description: With Google now part of the $3 trillion club, should investors pay attention to this tech giant?
--------------------------------------------------

Title: BofA’s Hartnett Says Magnificent 7 Stock Bubble Is Still Growing
URL: https://financialpost.com/pmn/business-pmn/bofas-hartnett-says-magnificent-7-stock-bubble-is-still-growing
Time Published: 2025-09-19T10:33:30Z
Description: The bubble that has formed in US Big Tech stocks over the past two years has further to run and investors should position for more gains, according to Bank of America Corp. strategists.
--------------------------------------------------

Title: BofA’s Hartnett Says Magnificent 7 Stock Bubble Is Still Growing
URL: https://finance.yahoo.com/news/bofa-hartnett-says-magnificent-7-100833297.html
Time Published: 2025-09-19T10:08:33Z
Description: A BofA team led by Michael Hartnett studied 10 equity bubbles since the start of the previous century, finding that these periods of extreme overvaluation...
--------------------------------------------------

Title: 2025.38: Meta, YouTube, and Tech Press Attention
URL: https://stratechery.com/2025/meta-youtube-and-tech-press-attention/
Time Published: 2025-09-19T10:00:00Z
Full Content:
Stratechery Plus Learn MoreMember Forum Stratechery Plus Learn MoreMember Forum Latest Podcast This Week in Stratechery Welcome back to This Week in Stratechery! As a reminder, each week, every Friday, we’re sending out this overview of content in the Stratechery bundle; highlighted links are free for everyone. Additionally, you have complete control over what we send to you. If you don’t want to receive This Week in Stratechery emails (there is no podcast), please uncheck the box in your delivery settings. On that note, here were a few of our favorites this week. This week’s Stratechery video is on iPhones 17 and the Sugar Water Trap. Get notified about new Articles Please verify your email address to proceed. Stratechery Plus Updates Stratechery Plus Podcasts Stratechery Plus Interviews The most popular and most important posts on Stratechery by year. Explore all free articles on Stratechery. Explore all posts on Stratechery. Stratechery Plus UpdateS Stratechery Plus Podcasts Stratechery Plus Interviews © Stratechery LLC 2025 | Terms of Service | Privacy Policy Proudly powered by WordPress. Hosted by Pressable.
--------------------------------------------------

Title: Are Magnificent 7 stocks overpriced? Here are alternatives.
URL: https://www.usatoday.com/story/money/2025/09/19/magnificent-seven-stocks-nvidia-microsoft-apple-sp500/86224670007/
Time Published: 2025-09-19T09:03:57Z
Full Content:
If you know anything about stocks, you’ve probably heard of the Magnificent Seven. Seven tech giants – Amazon, Apple, Alphabet (Google), Meta, Microsoft, Nvidia and Tesla – have driven massive gains on Wall Street over the past decade, collectively earning an eye-popping 698% between 2015 and 2024, according to The Motley Fool. The S&P 500 as a whole returned a comparatively modest 178% in those years. If you own shares of any broad index fund that tracks large American companies, you probably own shares of the Magnificent Seven. In 2015, the Magnificent Seven made up 12% of the total market value of the S&P 500. In 2025, the seven companies account for 34% of that value. Anyone who has owned Magnificent Seven stock over those years has reason to rejoice. They are the toast of Wall Street. “The Magnificent Seven are heroes,” said Jim Cramer, a CNBC stock-market pundit, in a Sept. 15 broadcast. “And I’m not going to tell you to sell heroes, unless something changes that makes them feel a lot less heroic.” Why would anyone consider selling a stake in the Magnificent Seven? Here are some reasons. First of all, market forecasts suggest the Magnificent Seven are overpriced. Economists measure the value of a stock by a formula called cyclically adjusted price-to-earnings ratio, or CAPE ratio. It measures a stock’s price against corporate earnings. It tells you, in effect, whether the stock is overvalued or undervalued. Right now, the CAPE ratio for the S&P 500 stands at 39.7. That means stock prices are very expensive, relative to earnings. Forecasters point to two prior moments when the CAPE Ratio ran high. One was in 1929. The other was in 1999. In the decades that followed those peaks, the stock market plummeted. Most of the Magnificent Seven stocks have higher price-to-earnings ratios than the S&P 500 as a whole, according to another Motley Fool analysis. That means Magnificent Seven stocks are historically overvalued. Based on that premise, Vanguard projects that U.S. growth stocks, a category the Magnificent Seven dominate, will rise by only 1.9% to 3.9% annually over the next decade. Despite the red flags, investors are still scooping up Magnificent Seven stocks. Nvidia stock is up 28% on the year, as of Sept. 18. Meta is up 31%. Alphabet is up 32%. “I own them. I love them. I’m going to keep holding them for the next 10 years. That’s what I do,” said David Gardner, co-founder of Motley Fool. Motley Fool includes two of the Seven, Alphabet and Amazon, on its current list of 10 Top Stocks to Buy and Hold. The Magnificent Seven have plenty of upsides. They are some of the most successful companies in history, lauded for technological innovation, global reach and brand recognition, strong revenue and earnings, and diverse operations that can adapt to changing market conditions. “Here you’re talking about concentration in seven of the most profitable, most diversified companies in the world,” said Jonathan Swanburg, a certified financial planner in Houston. Whatever your opinion about the Magnificent Seven, if you are an index-fund investor, you may own more of the stocks than you realize. “The first step would be to understand just how much exposure you have to them,” said Andrew Patterson, head of active research at Vanguard. The runaway success of the Seven has reshaped the stock market, with the Magnificent Seven at its core. If you have $1,000 invested in a typical S&P index fund, roughly $340 of that money is tied up in the Magnificent Seven. Nvidia, Microsoft and Apple, alone, make up more than 20% of the value of the typical S&P index fund, and nearly 20% of the typical “total stock market” index fund. As a rule, market concentration is considered a bad thing. Investors are urged to diversify: not to hold only stocks, and not to hold too much of any one stock. Because of the massive gains by Magnificent Seven stocks, many everyday investors now own more of those stocks – and more stocks in general – than they intended. An investor who started out with a 60-40 mix of stocks and bonds may now own a 70-30 mix: The stocks have outperformed the bonds. Whether you own too much Magnificent Seven stock, or too much stock in general, depends on your tolerance for risk and how far you are from retirement, among other factors, Patterson said. A potential solution is rebalancing: Invest in other asset classes. Maybe even sell some of your Magnificent Seven stock. To avoid market concentration and overpriced stocks, forecasters say, here are some other investments to consider:
--------------------------------------------------

Title: Elon Musk's $1 trillion pay deal is ambitious — but so was his last 'mammoth' one, tech guru says
URL: https://www.businessinsider.com/tesla-stock-elon-musk-schiffer-compensation-pay-ai-tech-bubble-2025-9
Time Published: 2025-09-19T09:00:02Z
Full Content:
Tesla's $1 trillion pay deal for Elon Musk has triggered the same skepticism that greeted his ambitious 2018 pay deal, tech investor Eric Schiffer has said. In a wide-ranging interview with Business Insider, the CEO of private-equity firm the Patriarch Organization said Musk's previous deal also tied a "mammoth" payout to extremely challenging goals. That agreement required Musk to grow Tesla's market value from around $60 billion to $650 billion, its 12-month revenue from below $20 billion to $175 billion, and its adjusted profits from under $1.5 billion to $14 billion to get the full compensation. Schiffer recalled the deal was labeled "insane" and "crazy," with commentators declaring there was "no way in hell he'll ever achieve that." "And you know what, he did," Schiffer added. Under the company's latest plan, Musk would have 10 years to raise Tesla's market value from around $1.3 trillion to $8.5 trillion, boost its adjusted profits to $400 billion, deliver 20 million Tesla vehicles and a million Bots, reach 10 million active full self-driving (FSD) subscriptions, and bring a million robotaxis into commercial operation. "If he were to accomplish that, investors would do backflips, and of course, he should be compensated," Schiffer said. Awarding $1 trillion worth of stock to a CEO who's built an $8.5 trillion company would be reasonable, he added. Schiffer told Business Insider that the AI frenzy may have shades of the dot-com bubble at the turn of this century, but he's not worried about a crash. Many dot-com players were loss-making and had valuations "astronomically out of whack," Schiffer said. By contrast, Meta, Alphabet, and the other tech titans leading the AI charge are "money gushers" showing no signs of slowing down. Schiffer gave the example of Oracle, which recently disclosed that its expected contract revenue more than quadrupled year on year to $455 billion last quarter, thanks to strong AI demand. The software giant's stock price surged as much as 43% in a single day, boosting its market value by nearly $300 billion at its peak, as investors banked on years of rapid revenue growth. AI companies are "priced close to perfection," but their valuations also reflect a "continued escalation of earning estimates that I don't think are going to slow down, frankly," Schiffer said. "I think there's more to come." Schiffer, who has 18-year-old twins, said young people face a "more difficult" backdrop than previous generations between political divisions, wars, an affordability crisis, and AI-fueled job losses. He advised them to figure out their strengths and apply them to activities that are "profoundly human," where people maintain an advantage over AI. He gave the examples of healthcare and enterprise sales, where trust and relationships are vital. Schiffer also recommended they "get fluent with AI — learn how to master it, write it, work it." He added that "people are not going away," and knowing how to work with AI will benefit many industries for years. Shiffer also offered some advice for wealthy parents on not raising entitled children. "Do not give your money to your kids," he said. "That is not the move — it will not serve them or help them create agency, develop their own future, build their own security, or make things happen on their own." For those children who receive large sums from their parents, Schiffer urged long-term responsibility over frivolous spending. "That's a great gift, something you want to steward and manage and help to grow," he said. Jump to
--------------------------------------------------

Title: Apple’s iPhone 17, Air go on sale to years of demand for new look
URL: https://www.thestar.com.my/tech/tech-news/2025/09/19/apples-iphone-17-air-go-on-sale-to-years-of-demand-for-new-look
Time Published: 2025-09-19T08:00:00Z
Full Content:
Friday, 19 Sep 2025 A customer looks at Apple's iPhone 17 series at the company's Sanlitun store during the first day of sales of the smartphones in Beijing, China, on Sept 19, 2025. — Bloomberg For the past four years, some Apple Inc customers have been waiting for a fresh design before buying a new iPhone. The time has come for those consumers, with the iPhone 17 Pro, Pro Max and iPhone Air going on sale Sept 19. In a first since 2020, Apple is rolling out several new iPhone designs. It’s also releasing into stores a new Apple Watch SE, Watch Series 11, Watch Ultra 3 and AirPods Pro 3. Early sales across Asia showed strong demand for the Pro models. In Hong Kong, a small crowd assembled at Apple’s flagship store to check out the new handsets, but stock for walk-in purchases was limited only to the novel iPhone Air. Anyone wanting to purchase an iPhone 17 or Pro edition was directed to order online, where Apple indicated wait times of roughly three weeks. Australia, New Zealand, mainland China and Singapore’s Apple stores showed similar shipping times, with the iPhone 17 Pro Max wait extending to as long as four weeks. Japan was the only major market where consumers could acquire any new iPhone model with next-day delivery, while in South Korea the 17 Pro was available within a week whereas the Pro Max was marked as unavailable until late October. The new Pro editions return the iPhone to an aluminum shell and have a redesigned back, while the iPhone Air is Apple’s first attempt at a radically different design with its thin shape as the main selling point. The stakes are high for Apple. The company faces high expectations from Wall Street for sales over the holiday period, despite global economic concerns and the threat of future price increases due to tariffs. The Cupertino, California-based electronics giant is also trying to convince consumers its mojo is back after its artificial intelligence strategy fell flat. The new models’ reception in China will be of great importance to the company. Sales in the region fell 6% in the weeks leading up to the new device family launch, marking a steeper-than-usual slump. After years of success in the region over the past decade, Apple now only holds 12% of the local market, trailing Oppo, Huawei Technologies Co, Xiaomi Corp and others. The launch of its latest major iteration of the iPhone – a product that debuted in 2007 – hits as Meta Platforms Inc and others are moving on to AI-powered wearables, including smart glasses with displays. The new models also land weeks after Alphabet Inc’s Google and Samsung Electronics Co rolled out upgrades to their flagship smartphones. Samsung, in particular, is a threat to the iPhone’s dominance. It launched a seventh-generation version of its book-style foldable phone, which appears to be a hit with consumers due to its more durable frame and thinner look. Apple won’t have a foldable phone until the end of next year, Bloomberg News has reported. The iPhone 17 Pro line is resonating most with buyers because it improves the features that people most want in a new phone: battery life, camera technology and durability. It’s also priced from US$1,099 (from RM5,499 in Malaysia), only US$100 more than the iPhone 16 Pro it replaces. "Prices are being really increased for the first time in several years, so if we see a typical replacement cycle with higher prices, plus some progress on AI, that may not be an exciting setup for the stock, but it’s a decent one,” said John Belton, portfolio manager at Gabelli Funds, which has US$33bil (RM138.92bil) in assets under management. The iPhone Air, on the other hand, may not drive sales despite the attention it will receive. The US$999 device focuses on a thin profile, but has drawbacks in battery life, audio and cameras. It’s also the same price as a 17 Pro when factoring in an optional magnetic battery attachment to increase endurance. Earlier this year, Samsung launched the Galaxy S25 Edge, using a similar concept to Apple’s iPhone Air. Thus far, it’s been a sales dud. – Bloomberg Thank you for your report! Copyright © 1995- Star Media Group Berhad [197101000523 (10894-D)] Best viewed on Chrome browsers. We would love to keep you posted on the latest promotion. Kindly fill the form below We hope you enjoy this feature!
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq hit records as rate-cut relief, Nvidia's Intel bet lift markets
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-hit-records-as-rate-cut-relief-nvidias-intel-bet-lift-markets-200041899.html
Time Published: 2025-09-18T20:00:41Z
Description: Markets are shaking off initial hesitation prompted by Powell's warning that there's no 'risk-free path' for policy.
--------------------------------------------------

Title: Can Broadcom Stock Hit $420 in 2025?
URL: https://www.barchart.com/story/news/34900478/can-broadcom-stock-hit-420-in-2025
Time Published: 2025-09-18T19:24:51Z
Description: As Broadcom capitalizes on AI-driven demand, can the company’s ASIC dominance truly propel its stock to Macquarie’s $420 target?
--------------------------------------------------

Title: Oracle's Larry Ellison made his $365 billion fortune by breaking every rule of wealth management
URL: https://www.cnbc.com/2025/09/18/larry-ellison-365-billion-fortune-wealth-management.html
Time Published: 2025-09-18T12:27:48Z
Description: Larry Ellison's fortune is built on mountains of leverage and risk, allowing him to borrow against his shares and raise cash without giving up control.
--------------------------------------------------

Title: Forbes Daily: Federal Reserve Signals ‘Lift-Off’ For Rate Cuts
URL: https://www.forbes.com/sites/daniellechemtob/2025/09/18/forbes-daily-federal-reserve-signals-lift-off-for-rate-cuts/
Time Published: 2025-09-18T12:08:04Z
Full Content:
ByDanielle Chemtob, Forbes Staff andForbes Daily, Forbes Staff. It finally happened. After months of pressure from President Donald Trump, the Federal Reserve eased interest rates Wednesday. The central bank slashed the federal funds rate by a quarter-point to between 4% and 4.25%, and said it expects two more cuts by the end of the year amid concerns about a weakening labor market. In a note, Fitch Ratings’ head of U.S. economic research called the decision “lift-off” for a “decisively aggressive cutting cycle.” For consumers, lower interest rates mean cheaper borrowing costs and a potential drop in mortgage rates. Crypto could even get a boost, as some Americans could opt for riskier assets to earn a greater yield. ABC pulled Jimmy Kimmel Live! from the air Wednesday due to the host’s comments about conservative activist Charlie Kirk. Shortly before the Disney-owned broadcaster made the announcement, media company Nexstar, which owns a number of local ABC affiliates, said it would preempt the late-night talk show. The decision comes as Nexstar seeks FCC approval for its $6.2 billion acquisition of TEGNA, which would give it the largest share of local TV stations in the U.S. After three failed efforts to take StubHub public, cofounder Eric Baker finally got his wish Wednesday as the online ticket marketplace began trading on the New York Stock Exchange. Based on its $23.50 per share listing price, the company is valued at $8.6 billion, giving Baker an estimated net worth of $800 million. Shares of Lyft reached a three-year high as the rideshare firm announced a partnership with Waymo that will bring the autonomous ride-hailing service to Nashville. Waymo is looking to grow the market for its robotaxi rides, and has received $12 billion in investments from parent company Alphabet and others. Meta’s share price rose in premarket trading this morning after it showed off a new pair of AI-powered smart glasses at the company's Meta Connect keynote Wednesday night. The new Meta Ray-Ban Display will be able to read messages, receive video calls and follow map directions on its right lens. The glasses will be available September 30 and cost $799. A new climate economy is advancing in spite of threats to stall its progress, fueled by record clean energy investment, China’s green-tech boom, and a worldwide surge in renewable power. The second Forbes Sustainability Leaders list honors 50 individuals setting the pace for a just, sustainable economy and defining what climate leadership looks like today. MORE: Among the list’s honorees is billionaire Wendy Schmidt: She and her husband, former Google CEO Eric Schmidt, have directed a significant portion of their $2 billion in lifetime giving toward environmental initiatives. The stakes for private science funding have never been higher, with Donald Trump back in the White House and a second wave of federal rollbacks targeting climate funding and scientific independence. A small Israeli startup is stress testing AI models for giants like OpenAI and Anthropic to evaluate their capacity for malicious hacking, as the industry grapples with AI misuse. On Wednesday, the startup, which is changing its name to Irregular, announced $80 million in new funding, valuing it at $450 million. As ICE builds up its arsenal of equipment like tactical gear, guns and surveillance kits, contract records show the agency spent $78,000 on a robot capable of opening doors, climbing stairs and firing off smoke bombs during house raids. The contracts, along with purchases of drones and AI-powered facial recognition technology, have raised concerns about ICE’s spending on surveillance and arms that could be used to target the immigrant population. The CDC’s immunization advisory committee—whose membership Health and Human Services Secretary Robert F. Kennedy Jr. purged in June and replaced with people more aligned with his anti-vaccine views—is slated to meet starting today. On the agenda, a topic crucial to the health of the country and its pharma industry: childhood vaccinations, as well as Covid-19 shots. Since his appointment, Kennedy has been pushing anti-vaccine policies at the federal level and this meeting could prove critical for U.S. immunization policy going forward. Already, he has announced plans to award a no-bid contract to Rensselaer Polytechnic Institute to “investigate” long-debunked links between vaccines and autism, and appointed committee members like Retsef Levi and Robert Malone, who have been outspoken opponents of Covid vaccines, which saved an estimated 14.4 million lives the first year after they were made available, and Catherine Stein, who has argued against vaccine mandates. All of this puts pharmaceutical companies into uncharted territory. Vaccines are a big business: Grand View Research estimates the global vaccine market reached $88 billion last year, with the U.S. accounting for the major share of it. They’re also a safe business: Many vaccines have been around for decades, and regulations around them have been both relatively certain and grounded in evidence. That may no longer be the case. On Friday, shares of the biggest Covid-19 vaccine makers fell dramatically when Trump Administration health officials indicated they were investigating unsubstantiated reports of child deaths from the vaccine. WHY IT MATTERS The United States’ childhood immunization program is widely recognized as one of the greatest achievements of public health. It eradicated measles in the U.S. by the year 2000, slashed the infection rate of hepatitis B by 99%, and turned whooping cough into a rarity. Any weakening of recommendations for childhood immunizations could lead to fewer shots, and even a 10% drop in childhood vaccinations could lead to millions of hospitalizations and tens of thousands of deaths among the nation’s children, according to a recent study in the Journal of the American Medical Association. MORE Trump Administration’s Attack On MRNA Vaccines Threatens American Biotech Dominance French luxury conglomerate Kering, parent of brands such as Gucci and Alexander McQueen, confirmed that customer data was stolen in an April cyberattack, potentially impacting millions. It’s the latest luxury company to fall victim to cybercriminals: 7.4 million: The number of unique email addresses hacker group Shiny Hunters, which claimed credit for the breach, told the BBC that it had Over $10,000, up to $86,000: The amount spent by Kering customers in sample records the BBC reviewed, which contained their names and phone numbers 16%: The sales decline Kering reported in the first half of 2025 Everyone faces hardship in life, but it’s important to find ways to stay true to who you are even amid the most difficult of times. Embrace the cycle of struggle and rebirth, and focus your identity on your strengths—rather than just what you do for work. Lastly, create a sense of urgency around your goals to ensure you don’t procrastinate. This week’s episode of a popular animated show was delayed after it faced scrutiny for satirizing conservative activist Charlie Kirk—weeks before his assassination. Which show is it? A. Futurama B. Family Guy C. South Park D. The Simpsons Check your answer. Thanks for reading! This edition of Forbes Daily was edited by Sarah Whitmire and Chris Dobstaff.
--------------------------------------------------

Title: Small-Cap Russell 2000 Shakes Off Four-Year Funk on Rate Cut
URL: https://www.thedailyupside.com/finance/markets/small-cap-russell-6000-shakes-off-yearslong-record-funk-on-rate-cut/
Time Published: 2025-09-18T10:30:00Z
Full Content:
The hope for the S&P 500’s small-cap cousin after the Fed’s rate cut tells an important story about the broader economy. Sean Craig sean.craig@thedailyupside.com Federal Reserve officials lowered the target range for their benchmark interest rate by 25 basis points to a range of 4% to 4.25% — with Chair Jerome Powell flagging weakening jobs data — and penciled in two more cuts by the end of 2025. Yawn. The only thing less surprising than that yesterday was the calendar showing the day was Wednesday. But as Powell emphasized the Fed’s jobs, jobs, jobs focus, a winner emerged on the trading floor: the small-cap Russell 2000, a laggard during this year’s market rally that is now on pace to finally break the nearly four-year drought since its last record closing high. The S&P 500, naturally, has been on a tear, setting five records this month alone. Some market watchers including Deutsche Bank, factoring in rate cuts as drivers of stock prices, think it can hit 7,000 by the end of the year. The hope for its small-cap cousin, meanwhile, is less ambitious but tells an important story about the broader economy. The Russell 2000, which tracks the performance of 2,000 smaller companies in the broader Russell 3000 Index, was underwater through the end of June, while the S&P 500 had recovered from the post-Liberation Day tariffs shock by mid-May. But since the end of July, the Russell 2000 has jumped 10%, double the S&P’s rise in the same time. Following the rate cut announcement on Wednesday, the small-cap index rose more than 2% and at one point advanced past its all-time closing high of 2,442.74. It ultimately pared gains and finished up 0.2%, but a new record high, topping the one set in November 2021, seems finally within reach. That’s because of the unique meaning of a rate cut for small-caps: Not Out of the Woods: Reduced interest costs, of course, aren’t the only high-level economic factor on the minds of executives and investors. One risk for small-cap firms is that their lower margins, higher debt loads and attenuated supply chains are all highly sensitive to tariffs, which Powell said Wednesday could lead to a “one-time shift” in prices or cause “more persistent” inflation. For the time being, Bank of America and UBS are among the Wall Street heavyweights that foresee a Russell 2000 rebound. But even those who expect the market rally to broaden and include small caps, like Goldman Sachs, have cautioned, “there is limited scope for small-caps and other ‘lower quality’ stocks to consistently outperform,” which is a hint that you should diversify your portfolio. Meta will likely soon face competition from Amazon, Alphabet, Snap and other tech firms with augmented-reality glasses in development. President Trump wants to end mandated quarterly financial reports for publicly traded companies. Odds are his dream comes true. In a note last week, JPMorgan’s Andrew Tyler wrote that macro conditions could turn a widely-expected rate cut into a “sell the news” event. Demand for copper, already one of the most commonly used metals in the world, has surged amid the AI computing boom. The return-to-office orders have taken on new weight amid a softening labor market and broader economic anxiety. More than 94 million pet-owning US households are expected to spend $157 billion on their little Lunas and Rexes this year. Morgan Stanley analysts think the US economy has been in a “rolling recession” since 2022 — and it may already be almost over. iPhone sales surged to $44.5 billion in the quarter ending in June, besting analysts’ $40 billion expectations and rising 14% year-over-year. Investor appetite for StubHub and Klarna is especially compelling at a time when consumer sentiment starting to slump. The latest jobs report appeared to confirm that America’s labour market is slowing, which may clinch the rate cut markets are salivating over. Small, mountainous Switzerland — a country that ranks 61st in the world by total area — is one of the world’s biggest investors in the US. Last year marked the first time since Bain & Co started tracking data in 2005 that the private equity industry shrunk. © 2025 The Daily Upside
--------------------------------------------------

Title: Apple’s higher prices give iPhone doubters reason for optimism
URL: https://economictimes.indiatimes.com/industry/cons-products/electronics/apples-higher-prices-give-iphone-doubters-reason-for-optimism/articleshow/123971755.cms
Time Published: 2025-09-18T09:47:43Z
Full Content:
Despite a lukewarm reception to the latest iPhone's incremental updates, Apple's stock is rebounding, fueled by optimism surrounding higher prices for premium models. Analysts predict a rise in average iPhone selling prices, potentially boosting revenue. While facing challenges in AI deployment and lagging behind tech peers in growth, early demand for the iPhone 17 hints at improved upgrade rates. (Catch all the Business News, Breaking News, Budget 2025 Events and Latest News Updates on The Economic Times.) Subscribe to The Economic Times Prime and read the ET ePaper online. (Catch all the Business News, Breaking News, Budget 2025 Events and Latest News Updates on The Economic Times.) Subscribe to The Economic Times Prime and read the ET ePaper online. Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Prime Articles Top Commodities Top Definitions Private Companies Top Story Listing Top Slideshow Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Worry not. You’re just a step away. It seems like you're already an ETPrime member with Login using your ET Prime credentials to enjoy all member benefits Log out of your current logged-in account and log in again using your ET Prime credentials to enjoy all member benefits. Big Price Drop! Flat 40% Off Offer Exclusively For You Save up to Rs. 700/- ON ET PRIME MEMBERSHIP Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get Flat 40% Off Then ₹ 1749 for 1 year Offer Exclusively For You ET Prime at ₹ 49 for 1 month Then ₹ 1749 for 1 year Special Offer Get flat 40% off on ETPrime What’s Included with ETPrime Membership Trump temper on H-1B visas is forcing Indians to do these things to stay put in US What Adani’s US indictment means for India Inc’s overseas fundraising Why veterans like Reliance, L&T are on acquisition spree? Aswath Damodaran has an answer. Will China’s dollar bond sale in Saudi Arabia trump the US in financial world? Huawei launches its own OS to compete with Google and Apple. But can it win beyond China? The problem with lab grown diamonds Why a falling rupee is a better option for the economy A list of top 20 momentum stocks that have delivered massive returns in one year Investment Ideas Grow your wealth with stock ideas & sectoral trends. Stock Reports Plus Buy low & sell high with access to Stock Score, Upside potential & more. BigBull Portfolio Get to know where the market bulls are investing to identify the right stocks. Stock Analyzer Check the score based on the company's fundamentals, solvency, growth, risk & ownership to decide the right stocks. Market Mood Analyze the market sentiments & identify the trend reversal for strategic decisions. Stock Talk Live at 9 AM Daily Ask your stock queries & get assured replies by ET appointed, SEBI registered experts. ePaper - Print View Read the PDF version of ET newspaper. Download & access it offline anytime. ePaper - Digital View Read your daily newspaper in Digital View & get it delivered to your inbox everyday. Wealth Edition Manage your money efficiently with this weekly money management guide. TOI ePaper Read the PDF version of TOI newspaper. Download & access it offline anytime. Deep Explainers Explore the In-depth explanation of complex topics for everyday life decisions. Health+ Stories Get fitter with daily health insights committed to your well-being. Personal Finance+ Stories Manage your wealth better with in-depth insights & updates on finance. New York Times Exclusives Stay globally informed with exclusive story from New York Times. TimesPrime Subscription Access 20+ premium subscriptions like Spotify, Uber One & more. Docubay Subscription Stream new documentaries from all across the world every day. Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Stories you might be interested in
--------------------------------------------------

Title: How Micron Stock Surges 2x To $300
URL: https://www.forbes.com/sites/greatspeculations/2025/09/18/how-micron-stock-surges-2x-to-300/
Time Published: 2025-09-18T09:00:27Z
Full Content:
ByTrefis Team, Contributor. Micron Technology stock (NASDAQ:MU) has increased approximately 80% from about $87 at the beginning of 2025 to nearly $157, reaching all-time highs in the process. So, what is driving this increase for Micron? It all revolves around the demand for AI infrastructure, which is boosting sales of high-bandwidth memory products, also referred to as HBM. Even after this substantial rally, Micron stock is valued at about 20× estimated earnings for 2025. Projected growth looks robust: expected revenues are set to rise by 48% this fiscal year, based on consensus estimates, while growth is anticipated at roughly 31% for FY’26. Given this strong growth paired with a reasonable valuation, could Micron stock potentially double to around $300? Below, we outline this scenario. Demand is being propelled by the swift adoption of generative AI, which necessitates high-performance memory to function at scale. While DRAM offers capacity, HBM supplies the bandwidth and low latency required for large language models. Micron serves as a primary supplier for Nvidia’s (NASDAQ:NVDA) Blackwell GB200 platform, alongside AMD’s Instinct MI350 series GPUs. The memory requirements for each AI system are also escalating: for example, Nvidia’s latest Blackwell systems feature 33% more memory per node compared to earlier-generation chips. Additionally, as AI models transition from being text-only to incorporating multimodal formats such as video and speech, memory intensity is on the rise. With AI expected to be broadly adopted throughout the economy, Micron is positioned for a prolonged growth trajectory. To illustrate the scale of the investments, Amazon (NASDAQ:AMZN), Alphabet (NASDAQ:GOOG), Microsoft (NASDAQ:MSFT), and Meta (NASDAQ:META) have indicated that they could collectively invest $364 billion in capital expenditures for their respective current fiscal years. However, supply may not keep pace easily. Manufacturing HBM is more intricate than standard DRAM, and supply levels are still constrained. The production of HBM is wafer-intensive — it requires approximately three times as many wafers as standard DRAM to create the same quantity of bits due to its lower bit density and intricate 3D stacking. This forms a natural bottleneck. Although Micron has been expanding its HBM production capacity, output for 2025 is already sold out, with robust demand anticipated in 2026. Micron aims to significantly increase its HBM market share, targeting about 20 to 25% of the HBM segment by the end of 2025. Beyond HBM, Micron remains the sole volume producer of low-power DRAM for data centers, providing an advantage as AI workloads increasingly emphasize efficiency. Looking forward, even after this increase, Micron does not appear to be overvalued. Its revenues surged by 58%, from $21 billion in 2023 to $34 billion in 2024, as the company rebounded from a downturn. Consensus forecasts indicate revenues climbing from roughly $25.1 billion in FY’24 to $37.1 billion in FY’25, and by about 31% to $48.7 billion in FY’26. HBM has been a significant factor in this growth, with HBM revenue achieving a $6 billion annualized run rate and a projected $10 billion run rate by 2026. Increased HBM production does place pressure on conventional DRAM supply, contributing to rising DRAM prices, which may also enhance revenues for products outside of HBM. If we project a slightly more conservative annual growth rate of 25% between FY’26 and FY’28, Micron could generate approximately $77 billion in revenues by FY’28. Micron's net margins were around 20% over the first nine months of this year. It is possible that this figure could grow further due to a higher proportion of higher-margin HBM chips and improved economies of scale. Assuming net margins rise to about 22% in FY’28, this would equate to net income of approximately $17 billion. With a forward P/E ratio of about 20x, this would imply a market cap of around $340 billion, suggesting the stock price could nearly double from current levels. That said, competition persists. SK Hynix continues to dominate the HBM market with an approximate 50% share and an advantage in production, particularly regarding HBM4. Micron is increasing HBM3E production and testing HBM4, but it is still striving to catch up. HBM4 is anticipated to power Nvidia’s upcoming Rubin architecture for AI accelerators. The memory market has also historically been subject to significant cyclical behavior, and Micron is no exception. DRAM and NAND remain vulnerable to fluctuations in supply and demand along with pricing volatility. At this time, it appears that HBM serves as a partially secular growth driver, with AI infrastructure investments, high profit margins, and strong order visibility providing some degree of protection. Nevertheless, HBM presently comprises less than 20% of total revenues, meaning the company is not entirely insulated from traditional market cycles. Furthermore, AI demand may normalize following years of heavy investment if shareholders start demanding superior returns and more efficient capital management. The Trefis High Quality (HQ) Portfolio, featuring a selection of 30 stocks, has a history of consistently outperforming its benchmark, which includes all three – S&P 500, Russell, and S&P midcap. What accounts for this? Collectively, HQ Portfolio stocks delivered superior returns with lower risk compared to the benchmark index; a smoother ride, as demonstrated in the HQ Portfolio performance metrics.
--------------------------------------------------

Title: Meta’s new Ray-Ban smart glasses have an integrated display and gesture-based controls
URL: https://siliconangle.com/2025/09/17/metas-new-ray-ban-smart-glasses-integrated-display-gesture-based-controls/
Time Published: 2025-09-18T02:55:24Z
Full Content:
UPDATED 22:55 EDT / SEPTEMBER 17 2025 by Mike Wheatley Meta Platforms Inc. is racing ahead of its competitors in the virtual reality and augmented reality worlds with the launch of a new and more sophisticated pair of Ray-Ban branded smart glasses. It debuted alongside a new technology called Hyperscape, which enables Meta’s VR headset Quest to replicate real-world spaces in a virtual environment. Meta founder and Chief Executive Mark Zuckerberg took to the stage late today at the company’s annual Meta Connect event, where he showcased the company’s next-generation smart glasses. They’re called the Meta Ray-Ban Display, and unlike the original Ray-Ban Meta specs, they come with a built-in display on the right lens. It’s visible only to the wearer, and meant to show various applications, notifications and directions. The smart glasses can be controlled by a special wristband called the Meta Neural Band, which picks up on the user’s gestures. There’s an integrated AI assistant that allows for voice controls so users can navigate through social media applications and browse the internet, and the specs also feature a built-in camera, speakers and microphone. Meta Ray-Ban Display is a notable upgrade that makes it possible for smart glasses to do pretty much any task that can be done on a smartphone, and if it can popularize the concept it could be a game-changer for the company. For years, Meta has been forced to reach its users through third-party devices such as Apple Inc.’s iPhones, Google LLC’s Android phones and Microsoft Corp.’s Windows computers, but with the Ray-Ban Display, it now owns the hardware as well as the software. Onstage, Zuckerberg spoke about all of the different things users will be able to do with Meta Ray-Ban Display that couldn’t be done with the company’s original smart glasses. The most important thing is the display, which allows users to view posts in their Facebook feed and messages on apps like WhatsApp and Instagram, as well as live translations. The Neural Band that ships with the device is similar to something like a Fitbit fitness device, and its purpose is to give users a way to navigate apps in Ray-Ban Display using small hand movements. It uses a technology known as electromyography, which picks up on the signals sent between the brain and the hand when performing gestures, Zuckerberg explained. He also mentioned that it’s water-resistant, just like the smart glasses, and has 18 hours of battery life. It should be noted that Meta Ray-Ban Display lacks some of the features seen on Meta’s prototype Orion smart glasses, which were shown off by Zuckerberg at Connect 2024. The Orion came with augmented reality lenses that could overlay graphics directly on the lens, and had an eye-tracking system to understand what people are looking at. Meta Ray-Ban Display, on the other hand, only has a small display integrated with what is otherwise just a standard lens. The good news is that consumers will be able to get their hands on Meta Ray-Ban Display soon. Zuckerberg said it’s going on sale in the U.S., Canada, the U.K., France, Italy, Spain, Austria, Belgium, Australia, Ireland, UAE, Germany, Sweden, Norway, Finland, Denmark, Switzerland and the Netherlands at the end of this month, with a price tag of $799. It’s also going to launch in Brazil and India “soon.” Those who don’t find the idea of an integrated display to their liking may want to check out the improved version of Meta’s displayless Ray-Ban Meta glasses. The company’s original smart glasses are getting a better battery that lasts up to eight hours, which is almost twice as long as before. There’s also a new “conversation focus” feature that amplifies the voice of whoever the wearer is speaking to, so they can hear them more clearly in noisy environments. It utilizes the specs’ integrated microphone to pick out the speaker’s voice, while artificial intelligence algorithms filter out the background noise, Zuckerberg said. In addition, Ray-Ban Meta is adding support for German and Portuguese language translation, but it will come at a higher cost, with the new version priced at $379, up from $299 for the older model. There’s a third pair of smart specs too. Meta said it has partnered with Oakley Inc. to create the Oakley Meta Vanguard (above) which is designed for “high-intensity sports.” It was first announced earlier this year, and features Oakley’s characteristic wraparound design and integrates fitness applications like Garmin and Strava, so wearers can ask Meta AI questions about their training performance. It has an IP67 rating that certifies its dust and water resistance and comes with a 12 megapixel, 122-degree camera that’s fitted on the nose bridge, enabling it to shoot videos at 3K resolution. It also supports new capture modes including slow-motion, time-lapse and hyperlapse, which will be available on the company’s other smart glasses too. Zuckerberg said Oakley Meta Vanguard has a battery life of about nine hours, and will cost $499 when it goes on sale next month. Moving away from specs to Quest, which is Meta’s flagship VR headset, there wasn’t any new hardware on show, but the older model is getting what looks to be a fascinating new capability in the shape of Hyperscape. Zuckerberg first spoke about Hyperscape at last year’s Connect, and it’s now being made available in early access. With Hyperscape, Quest users can scan whatever physical space they happen to be in and then ask the device to recreate that environment in VR. Though the capture process only takes a few minutes, during which users just walk around the room or space and make sure everything is videoed carefully, the actual creation process will take a “few hours,” Zuckerberg said. It uses technologies including Gaussian Splatting, cloud rendering and high-bandwidth streaming to recreate an immersive, photorealistic replica of any physical space, with the new Horizon Engine enabling better graphics and faster performance to generate more realistic virtual worlds than ever seen before. On stage at Connect, Zuckerberg demonstrated a few renderings of real-world locations, including Gordon Ramsey’s kitchen (above) at his home in Los Angeles, The Octagon at the UFC Apex in Las Vegas, Chance the Rapper’s House of Kicks, and a room that’s filled with the social media influencer Happy Kelli’s extensive Crocs shoe collection. While users will only be able to enjoy the spaces they create alone for the time being, Zuckerberg said the company is working on allowing users to invite others into their digital spaces via a private link, and hopes to add this capability in future. The Hyperscape is being rolled out gradually on Meta’s Quest 3 and Quest 3S headsets, and will be available to users aged 18 and older. Support our mission to keep content open and free by engaging with theCUBE community. Join theCUBE’s Alumni Trust Network, where technology leaders connect, share intelligence and create opportunities. Founded by tech visionaries John Furrier and Dave Vellante, SiliconANGLE Media has built a dynamic ecosystem of industry-leading digital media brands that reach 15+ million elite tech professionals. Our new proprietary theCUBE AI Video Cloud is breaking ground in audience interaction, leveraging theCUBEai.com neural network to help technology companies make data-driven decisions and stay at the forefront of industry conversations. AWS adds fully managed AI models: Qwen3 and DeepSeek-V3.1 Agentic security arrives: CrowdStrike leads Day 2 keynote at Fal.con 2025 Nvidia to invest $5B in Intel as part of new chip partnership CrowdStrike expands Falcon with next-gen identity security and AI-era data protection Researchers detail Delphi-2M, an AI model capable of predicting 1,000+ diseases OpenMind releases open-source OS for intelligent robots AWS adds fully managed AI models: Qwen3 and DeepSeek-V3.1 AI - BY ZEUS KERRAVALA . 3 MINS AGO Agentic security arrives: CrowdStrike leads Day 2 keynote at Fal.con 2025 SECURITY - BY VICTOR DABRINZE . 1 HOUR AGO Nvidia to invest $5B in Intel as part of new chip partnership AI - BY MARIA DEUTSCHER . 3 HOURS AGO CrowdStrike expands Falcon with next-gen identity security and AI-era data protection SECURITY - BY DUNCAN RILEY . 4 HOURS AGO Researchers detail Delphi-2M, an AI model capable of predicting 1,000+ diseases AI - BY KYT DOTSON . 5 HOURS AGO OpenMind releases open-source OS for intelligent robots AI - BY KYT DOTSON . 6 HOURS AGO
--------------------------------------------------

Title: Stock market today: Nasdaq, Dow, S&P 500 futures rise after Fed signals more cuts, Nvidia bets on Intel
URL: https://finance.yahoo.com/news/live/stock-market-today-nasdaq-dow-sp-500-futures-rise-after-fed-signals-more-cuts-nvidia-bets-on-intel-231448028.html
Time Published: 2025-09-17T23:14:48Z
Description: Markets are shaking off initial hesitation prompted by Powell's warning that there's no 'risk-free path' for policy.
--------------------------------------------------

Title: Stock market today: Nasdaq, Dow, S&P 500 rise after Fed signals more cuts, Nvidia bets on Intel
URL: https://finance.yahoo.com/news/live/stock-market-today-nasdaq-dow-sp-500-rise-after-fed-signals-more-cuts-nvidia-bets-on-intel-231448487.html
Time Published: 2025-09-17T23:14:48Z
Description: Markets are shaking off initial hesitation prompted by Powell's warning that there's no 'risk-free path' for policy.
--------------------------------------------------

Title: Bitcoin Has Taken Gold’s Role In Today’s World, Eric Trump Says
URL: http://www.newsbtc.com/news/bitcoin/bitcoin-has-taken-golds-role-in-todays-world-eric-trump-says/
Time Published: 2025-09-17T22:00:21Z
Full Content:
Strict editorial policy that focuses on accuracy, relevance, and impartiality Morbi pretium leo et nisl aliquam mollis. Quisque arcu lorem, ultricies quis pellentesque nec, ullamcorper eu odio. Eric Trump on Tuesday described Bitcoin as a “modern-day gold,” calling it a liquid store of value that can act as a hedge to real estate and other assets. According to reports, the remark came during a TV appearance on CNBC’s Squawk Box, tied to the launch of American Bitcoin, the mining and treasury firm he helped start. Based on public filings and company summaries, American Bitcoin has accumulated 2,443 BTC on its balance sheet. That stash has been valued in the low hundreds of millions of dollars at recent spot prices. The firm mixes large-scale mining with the goal of holding Bitcoin as a strategic reserve, which it says will help it grow both production and asset holdings over time. Eric Trump’s comments were direct. He told viewers that institutions are treating Bitcoin more like a store of value than a fringe idea, and he warned firms that resist blockchain adoption. The tone was strong at times, and the line about Bitcoin being a modern equivalent of gold was used to frame American Bitcoin’s role as both miner and holder. Eric Trump has said: bitcoin is modern-day gold — unusual_whales (@unusual_whales) September 16, 2025 American Bitcoin moved toward a public listing via an all-stock merger with Gryphon Digital Mining earlier this year, a deal that kept most of the original shareholders in control and positioned the new entity for a Nasdaq debut. Reports show that mining partner Hut 8 holds a large ownership stake, leaving the Trump family and other backers with a minority share. The listing brought fresh attention and capital to the firm as it began trading under the ticker ABTC. Market watchers say the firm’s public debut highlights two trends: mining companies are trying to grow by both producing and holding Bitcoin, and political ties are bringing more headlines to crypto firms. Some analysts point out that holding large amounts of Bitcoin on the balance sheet exposes a company to price swings, while supporters argue it aligns incentives between miners and investors. Based on coverage of the launch, investors have reacted with both enthusiasm and caution. Supporters praise the prospect of a US-based miner that aims to be transparent and aggressive about building a reserve. Critics point to governance questions, possible conflicts tied to high-profile backers, and the usual risks of a volatile asset being held on corporate balance sheets. Eric Trump’s remark that Bitcoin has taken gold’s role in today’s world reflects both his belief in its value and American Bitcoin’s strategy of mining and holding. Whether that view sticks will depend on how investors and institutions respond in the months ahead. Featured image from Meta, chart from TradingView They say journalists never truly clock out. But for Christian, that's not just a metaphor, it's a lifestyle. By day, he navigates the ever-shifting tides of the cryptocurrency market, wielding words like a seasoned editor and crafting articles that decipher the jargon for the masses. When the PC goes on hibernate mode, however, his pursuits take a more mechanical (and sometimes philosophical) turn. They say journalists never truly clock out. But for Christian, that's not just a metaphor, it's a lifestyle. By day, he navigates the ever-shifting tides of the cryptocurrency market, wielding words like a seasoned editor and crafting articles that decipher the jargon for the masses. When the PC goes on hibernate mode, however, his pursuits take a more mechanical (and sometimes philosophical) turn. Christian's journey with the written word began long before the age of Bitcoin. In the hallowed halls of academia, he honed his craft as a feature writer for his college paper. This early love for storytelling paved the way for a successful stint as an editor at a data engineering firm, where his first-month essay win funded a months-long supply of doggie and kitty treats – a testament to his dedication to his furry companions (more on that later). Christian then roamed the world of journalism, working at newspapers in Canada and even South Korea. He finally settled down at a local news giant in his hometown in the Philippines for a decade, becoming a total news junkie. But then, something new caught his eye: cryptocurrency. It was like a treasure hunt mixed with storytelling - right up his alley! So, he landed a killer gig at NewsBTC, where he's one of the go-to guys for all things crypto. He breaks down this confusing stuff into bite-sized pieces, making it easy for anyone to understand (he salutes his management team for teaching him this skill). Think Christian's all work and no play? Not a chance! When he's not at his computer, you'll find him indulging his passion for motorbikes. A true gearhead, Christian loves tinkering with his bike and savoring the joy of the open road on his 320-cc Yamaha R3. Once a speed demon who hit 120mph (a feat he vowed never to repeat), he now prefers leisurely rides along the coast, enjoying the wind in his thinning hair. Speaking of chill, Christian's got a crew of furry friends waiting for him at home. Two cats and a dog. He swears cats are way smarter than dogs (sorry, Grizzly), but he adores them all anyway. Apparently, watching his pets just chillin’ helps him analyze and write meticulously formatted articles even better. Here's the thing about this guy: He works a lot, but he keeps himself fueled by enough coffee to make it through the day - and some seriously delicious (Filipino) food. He says a delectable meal is the secret ingredient to a killer article. And after a long day of crypto crusading, he unwinds with some rum (mixed with milk) while watching slapstick movies. Looking ahead, Christian sees a bright future with NewsBTC. He says he sees himself privileged to be part of an awesome organization, sharing his expertise and passion with a community he values, and fellow editors - and bosses - he deeply respects. So, the next time you tread into the world of cryptocurrency, remember the man behind the words – the crypto crusader, the grease monkey, and the feline philosopher, all rolled into one. Disclaimer: The information found on NewsBTC is for educational purposes only. It does not represent the opinions of NewsBTC on whether to buy, sell or hold any investments and naturally investing carries risks. You are advised to conduct your own research before making any investment decisions. Use information provided on this website entirely at your own risk. Over the past week, the Bitcoin market experienced significant volatile price action, resulting in a net price loss of 0.07%.... In his latest update, CryptoWzrd observed that Litecoin closed the day on a bearish note, moving in line with Bitcoin’s... The price of Bitcoin has had a mixed performance over the past week, falling beneath the $115,000 mark at the... Strict editorial policy that focuses on accuracy, relevance, and impartiality Morbi pretium leo et nisl aliquam mollis. Quisque arcu lorem, ultricies quis pellentesque nec, ullamcorper eu odio. NewsBTC is a cryptocurrency news service that covers bitcoin news today, technical analysis & forecasts for bitcoin price and other altcoins. Here at NewsBTC, we are dedicated to enlightening everyone about bitcoin and other cryptocurrencies. We cover BTC news related to bitcoin exchanges, bitcoin mining and price forecasts for various cryptocurrencies. © 2025 NewsBTC. All Rights Reserved. © 2025 NewsBTC. All Rights Reserved.
--------------------------------------------------

Title: The Best Deals Today: Alienware Laptops, iPad Air, and More
URL: https://www.ign.com/articles/the-best-deals-today-september-17
Time Published: 2025-09-17T19:05:00Z
Description: As of Sept. 17, the Alienware 16X Aurora RTX 5070 gaming laptop is $1,599.99 ($400 off) at Dell. Plus, save on the 2025 iPad Air, Lenovo Legion laptops, and more.
--------------------------------------------------

Title: Analysts Are Pounding the Table on This Red-Hot AI Stock
URL: https://www.barchart.com/story/news/34868633/analysts-are-pounding-the-table-on-this-red-hot-ai-stock
Time Published: 2025-09-17T16:07:52Z
Description: This AI stock just got a major upgrade from major analyst firm amid growing investor confidence.
--------------------------------------------------

Title: Show HN: I did a 4 hour conversational audiobook on the history of data centers
URL: https://www.stepchange.show/p/data-centers-the-hidden-backbone
Time Published: 2025-09-17T15:41:04Z
Full Content:
Listen to the Stepchange Show on Apple Podcasts, Spotify, YouTube, or wherever you listen to podcasts. Welcome to the third episode of the Stepchange Show—where we explore the technologies and systems that have transformed human civilization. Every time you stream a movie, send a text message, scroll a feed, or chat with your favorite AI, you’re touching an invisible, physical empire. We call it the cloud, but it isn’t in the sky. It is astonishingly physical—alive in over 12,000 buildings around the world, consuming almost five percent of U.S. electricity, and running through cables laid across the ocean floor. This is the story of data centers. From the humming punch-card rooms of the 1930s to the Cold War projects that accidentally birthed the internet, and onward to the gigawatt-scale AI factories of today, data centers have quietly become the industrial engine of our era. Six companies—NVIDIA, Microsoft, Apple, Alphabet, Amazon, and Meta—now dominate global markets in part because they command this infrastructure, just as railroads, steel, and oil once defined the fortunes of the last century. Data centers are the machines behind the modern world. They shape commerce, media, communication, and now artificial intelligence, and raise profound questions about energy, water, and climate. Join us as we uncover the physical reality of our digital world. Four main parts: Part I — Early Mainframes and the Birth of the Internet - 00:00:00Part II — The Utility of the Cloud and the Hyperscalers - 01:16:13Part III — The COVID Acceleration + AI Frenzy - 02:32:48Part IV — Themes & Reflections - 03:40:10 Chapter times:00:00:00 - The Invisible Empire of Data Centers00:03:11 - IBM Punch Cards: The First Nerve Center00:17:12 - SAGE: Connecting the Mainframes00:27:27 - The APARNET: The Cold War Births the Internet00:49:15 - The .Com Boom: The World Gets Wired01:16:13 - AWS: The Utility of the Cloud01:36:25 - Google: Warehouse Scale Computers01:58:28 - Microsoft: From Software to Services02:18:19 - Facebook: Sharing the Blueprints02:32:48 - COVID: Five Years of Growth in 18 Months02:45:53 - AI: The Dawn of the AI Age, Powered by NVIDIA02:53:46 - Power & Water: The Gigawatt Problem03:10:48 - Chips & Data: The New Cold War03:19:18 - Climate: The Emissions of Data Centers03:33:43 - Present Day Scale: Compute, Storage, and Communication03:40:10 - Themes: Our Takeaways and Reflections Thank you for joining us for the third episode of Stepchange. Don’t forget to subscribe. Hosted by Ben Shwab Eidelson and Anay Shah. We would love to hear from you at hi@stepchange.show. Full sources list Key Books Where Wizards Stay Up Late: The Origins of the Internet — Katie Hafner & Matthew Lyon Tubes: A Journey to the Center of the Internet — Andrew Blum (Amazon) The Big Switch: Rewiring the World, from Edison to Google — Nicholas Carr The Greatest Capitalist Who Ever Lived: Tom Watson Jr. and the Epic Story of How IBM Created the Digital Age — Ralph Watson McElvenny & Marc Wortman The Datacenter as a Computer: Designing Warehouse-Scale Machines (3rd ed.) — Luiz Barroso, Urs Hölzle & Parthasarathy Ranganathan Data Centers: Edges of a Wired Nation — (catalog link: Lars Müller Publishers / “Cloud”) Interviews Nat Bullard Brian Janous Christian Belady Peter Gross Sean James Jon Koomey Byron Rakitzis Ben Gilbert This was autogenerated and edited with LLMs. Please be aware that there may be mistakes or typos. Ben Shwab Eidelson (00:00:00): All right, Anay. Well, kids are in school. Anay Shah (00:00:04): Kids are in school. We survived the summer. I love how back to school kind of feels like the beginning of a new year. Ben Shwab Eidelson (00:00:11): It does. I always thought this time of year was the actual new year. Anay Shah (00:00:16): I like that. Ben Shwab Eidelson (00:00:28): Welcome to the third episode of the Step Change Show. We're here to cover the stories of human progress. We want to understand the technologies, systems, and infrastructure that shape our world. I'm Ben Eidelson, a co-founder of Step Change Ventures, a fund that invests in the companies that are accelerating today's biggest step changes. I'm based up in Seattle, Washington. Anay Shah (00:00:46): And I'm Anay Shah, fellow co-founder of Step Change Ventures, based in Los Angeles, California. Today we're going to be telling the story of the hyperscale-up of a recent infrastructure: the story of data centers. Every time you stream a movie, send a text message, call a car to pick you up, or talk to what feels like a fully formed computational consciousness, you are touching an invisible physical empire. We call it the cloud, but it isn't in the sky. It lives somewhere very, very real. Ben Shwab Eidelson (00:01:20): It lives in nearly 12,000 buildings worldwide, consuming almost 5% of electricity in the U.S. It also lives inside these garden-hose-sized cables that are laid across the darkest parts of the ocean floor. Today we are going to tell the story of this invisible infrastructure—a story that begins in the humming and clattering punch card rooms of the early 1900s, winds through the Cold War projects that accidentally birthed the internet, and leads to the gigawatt-scale AI factories the size of Lower Manhattan that are being built today. Anay Shah (00:01:50): The U.S. stock market is worth around $60 trillion, made up of over 4,000 companies. But there are just six companies at the top—Nvidia, Microsoft, Apple, Alphabet, Amazon, and Meta—that make up 30% of that market. They are today's railroads, steel, and oil companies, building the modern industrial engine of our time. Ben Shwab Eidelson (00:02:12): This is the story of data centers. But before we do, just one quick note: We have all of the research links and notes for this episode up at StepChangeShow. And if you are listening to this and you think of a friend or colleague who might enjoy it, please send it over their way. All of this is a new endeavor for us, and we appreciate it getting into the hands—or rather ears—of folks who may dig it. All right, so data centers. Anay Shah (00:02:36): So Ben, I was recently reading an article saying these new technologies are saving work for everyone nowadays. Pretty soon we'll have nothing to do at all. Ben Shwab Eidelson (00:02:44): That's right. I think they were saying that we have some new electronic brain that's going to make most workers obsolete. Anay Shah (00:02:49): Right. But the CEO behind one of these technologies quickly countered, saying that it was a way to save time, not replace jobs. He said it was a small tool to help great minds benefit mankind. Ben Shwab Eidelson (00:03:00): Was this the CEO of Anthropic, OpenAI, or maybe Google? Anay Shah (00:03:04): You might think so, but no. This was Thomas J. Watson Sr., the original leader of IBM. Ben Shwab Eidelson (00:03:11): So, IBM, the computational powerhouse of the last century. Long before we had the Googles, Amazons, Metas, and Microsofts, there was Big Blue IBM. And long before IBM ever built an electronic computer, they were doing computation of a different sort. Anay Shah (00:03:32): And if you walked into a large company back in the 1930s, you might find yourself in one of their windowless rooms, humming and clattering like a small factory. There'd be long rows of metal cabinets, whirring gears, and clerks feeding stacks of stiff paper cards into these behemoth machines. Each card was a sliver of information: an employee's hours, an invoice, maybe a customer's address. But together, they formed the first centralized nerve center for corporate data. Ben Shwab Eidelson (00:04:05): And I think that's why we argue that those rooms, with those cards of information being processed on these devices that were doing computing, are the first real data centers. Anay Shah (00:04:17): That machine has quite an interesting history, born to solve a government problem: the problem of the U.S. census. Ben Shwab Eidelson (00:04:26): It's wild. The 1880 census data was collected in 1880, and it took seven years before it was tabulated. They realized it just was not working at human computation scale anymore. Anay Shah (00:04:37): And so one man, Herman Hollerith, had an invention to solve this. Ben Shwab Eidelson (00:04:42): He observed the way railroad conductors checked tickets, noting that these were papers with punched-out holes that ultimately stored and represented information. If you could build a machine that could count the number of punch holes, you could then do computation at scale without needing humans to count markdowns on a form. Anay Shah (00:05:05): That's right. These cards stored information, and then you could have machines compute off of them. The 1890 census was the machine's first big break. Even with a population 25% larger than the decade before, it was completed in two years instead of more than seven years, and it came $5 million under budget. That's not something a new technology could typically achieve for a government. So, fast forward to Watson. He sees the punch card machine and understands what it can do. By the mid-1920s, he's convinced. Ben Shwab Eidelson (00:05:37): So by 1924, he rebranded the company International Business Machines. Anay Shah (00:05:42): A much more fitting name than CTR. Ben Shwab Eidelson (00:05:47): Computing-Tabulating-Recording Company. Anay Shah (00:05:50): "There is no limit for this tabulating business," he told his executives in 1927. He doubled down, selling off all the other less promising lines of business and pouring the company's resources into the tabulating division. They designed a proprietary 80-column IBM card that only worked on IBM machines, which became the IBM card—a sort of early-days lock-in where customers who wanted to use the machines had to buy the cards. Ben Shwab Eidelson (00:06:22): Everyone obsesses over the Gillette razor blade business model, but the IBM card was not only how you did the computation, but it also stored information and became your record-keeping method inside businesses running on these IBM machines. Anay Shah (00:06:35): Once you put invoices, time cards, and all of that information onto the IBM card, you're not migrating it off. Ben Shwab Eidelson (00:06:41): It's on a piece of paper. Anay Shah (00:06:42): Yeah, that's right. So now you've produced a very steady stream of revenue alongside selling the tabulator machine. The timing for this was perfect. This is the late '20s, early '30s. The New Deal brought in a massive government focus around record-keeping. In 1935, the Social Security Administration signed a contract with IBM, requiring millions of their cards and machines to process benefits and print checks. One of their New York plants was soon printing 10 million cards per day. Ben Shwab Eidelson (00:07:14): It's amazing. Anay Shah (00:07:15): And within a couple decades, most large companies had punch card rooms. They had various machines they used to sort, tabulate, and store financial information, payroll information, employee information, customer information—everything you could think of. In order to manage that, they'd hire clerks and technical specialists to operate and maintain it. Ben Shwab Eidelson (00:07:36): It's easy to take for granted, but this was the first time a company, its accounting, and its invoicing could be calculated at this scale. Think about what was going on in this era: companies were starting to scale in new ways. More global trade was also starting to happen. Thomas Watson Sr. ended up very interested in diplomacy, traveling around the world trying to preach that commerce across borders would create peace. Unfortunately, this backfired. He did a lot of commerce with Germany in the lead-up to the Holocaust, and there are many stories around IBM's computational power being used to help Germans with record-keeping. Ultimately, this product was foundational to so much of the scale of this era, including the scale of war. Every time there was a war or a big government project, regardless of which country was fighting, they needed IBM machines to do calculations. Anay Shah (00:08:34): This was the way to process and store information. Ben Shwab Eidelson (00:08:38): But it was all still cards and mechanical switches, ultimately. Anay Shah (00:08:42): And rooms full of paper. Ben Shwab Eidelson (00:08:44): Rooms of paper. So everyone was running on punch cards. What was this thing? What was this room? What should we call it? Anay Shah (00:08:50): This is where we're going to put our stake in the ground and say the earliest data centers came out of the IBM punch card data center. The easiest way to understand it is: it's a physical space or a collection of facilities designed to house and operate an organization's data and computing infrastructure. You can think of it as primarily storing information, computing information, and over time, connecting and allowing for the communication of information. At its basic core, a data center in the evolution we're going to talk about from the mid-1900s to the present day is the physical space around storage, computing, and connectivity. Ben Shwab Eidelson (00:09:33): At this moment, IBM was already a growing company of import, right? Doing global international business. By 1945, they had around 25,000 employees and annual revenue of approximately $140 million. But something was about to change, and that was also coming out of the war. World War II was this big moment of government investment in R&D. One of the things they often found themselves doing was calculating artillery firing tables, which took into account the wind speed, weather, and all these different factors to figure out where they should shoot artillery. There were teams of human computers, clerks who were doing mechanical calculation to figure things out. But that was a bottleneck. So the army funded a team at the University of Pennsylvania to build the first real electronic computer, a machine that could crank through those calculations an order of magnitude faster. That became the Electronic Numerical Integrator and Computer, also known as ENIAC. Anay Shah (00:10:33): This was all about math. It was focused on calculating numbers and producing mathematical results that would help them, in this case, be more efficient in war planning. Ben Shwab Eidelson (00:10:44): Yeah, I mean, they didn't have the... Anay Shah (00:10:45): TI-83 or the games that go on it. Ben Shwab Eidelson (00:10:48): It, or the games that go on it. They didn't have Tetris. So they needed a calculator that could go faster than the people who were a bottleneck at this point. The ENIAC had no mechanical parts that slowed its operation. It took up nearly a 2,000-square-foot room, but it did operate over 1,000 times faster than any previous computational device. It could execute 5,000 additions per second. At this time, IBM, with its commercial product—the quickest punch card machine—could complete just four additions a second. So it wasn't an order of magnitude faster; it was three orders of magnitude faster. Anay Shah (00:11:25): Four additions per second to 5,000 additions per second. This seems like something mathematicians and CEOs privy to this would think, "Oh, I can see the future." But no. A distinguished Harvard mathematician dismissed the idea as foolishness, saying there would be no big market for computers. He believed that the country would need maybe half a dozen, mainly for military and scientific research. Our friend Thomas J. Watson Sr. said general-purpose computers had nothing whatsoever to do with IBM or IBM's mainline of equipment and profitability. Ben Shwab Eidelson (00:11:59): But Watson's stubbornness to stay wedded to the past was only matched and maybe outdone by his son's stubbornness to push the company forward. Anay Shah (00:12:09): Runs in the family, but takes a different shape. Ben Shwab Eidelson (00:12:12): This is a father-and-son story for the ages. Thomas Watson Jr. eventually fought in the war, came back, and was ready to take his seat as a leader inside IBM. He viewed electronics as the future of the company, and this became the intergenerational battle for the control of IBM. Thomas Watson Jr. saw that there was a future for the company in electronics and wanted to push it that way. Anay Shah (00:12:38): There's some interesting parent-child psychology where you can see him completely frustrated and angry that his father's not going to jump on what he thinks is very clearly the future. Ben Shwab Eidelson (00:12:50): IBM did do a wartime project with Harvard mathematician Howard Aiken that was a kind of hybrid electromechanical vacuum-tube machine. They followed that with their first machine that they really wanted to demonstrate to the public after the war. This was called the Selective Sequence Electronic Calculator (SSEC). This was in 1948. They showed that in a Madison Avenue showroom, right in the middle of Manhattan, so that people could walk by and see this thing computing. Anay Shah (00:13:20): And to see this thing, these machines were still the size of rooms. --- Ben Shwab Eidelson (00:13:25): And it was fast, not by today's standards, but by those days, it was very fast. It didn't have any memory in the sense that we were used to computers today having memory, but it had this punched paper tape as a form of storage. Despite Thomas Watson Sr. not liking this as the new business, he didn't mind the company getting some PR points for it. They'd send reporters down to watch the machine do computation. This is the moment when the press said, "Pretty soon no one's going to have a job to do as a result of this electronic brain." Anay Shah (00:13:54): The electronic brain that would displace workers. And so, in 1951, IBM's longtime customer, the U.S. Census Bureau, went with Univac instead of IBM tabulators for its next census. Thomas Watson Jr. recalls this moment, saying, "My God, Univac is smart enough to start taking all the civilian business away!" This is really what shakes IBM to once again reinvent itself. Ben Shwab Eidelson (00:14:22): You go back to 1880, 1890. This whole invention was for the census problem. And that problem gets harder: more people, more things you want to know about people, and more factors you want to track. And so, that is actually a perfect test bed for computation at this time. Here comes the ENIAC team, now commercialized with Univac, and that wakes IBM right up. Anay Shah (00:14:41): And this company's been growing, right? IBM, in 1940, was doing about $45 million in revenue with 12,000 employees. And a decade later, they're doing $250 million in revenue with 30,000 employees. And if you're betting the company on a new direction, things could go south for a lot of people. It's a big responsibility for Watson Jr. Ben Shwab Eidelson (00:15:02): And they pushed forward. And so, out of these early computer experiments, they finally built their first commercial product, which was the IBM 701 Electronic Data Processing Machine. This is the first real machine where they thought, "Hey, we're going to build more than one of these, and we're going to sell some of them. We're actually going to try and make this into a business, not just a research project." Anay Shah (00:15:20): And as we'll see with some of the companies closer to the present day, they had an installed base of customers; they had a sales machine. And so, they started this business line, and within five years, they had 85% of the computer market. Ben Shwab Eidelson (00:15:34): That's right. So, that team that invented the first vacuum tube computer in the ENIAC and then the Univac, they got outrun by the better commercial go-to-market sales machine that, if you remember, IBM was founded with. So, who was buying these things, and how many did they sell? Well, the 701 was still a pretty bespoke product. They sold 19. These were going to National Labs, the Weather Bureau, and a lot of aerospace firms that were doing a lot of calculations. And the whole business model was actually not around buying these machines; it was an extension of the tabulating business model that IBM had always had, where they wanted you to lease the machine, rent it, and buy punch cards. Anay Shah (00:16:08): We got hardware as a service. Ben Shwab Eidelson (00:16:10): Exactly. It was a beautiful business model. The era of the '50s for IBM was going from this research-centered product to what I'd consider their Model T. So, in this era, they built the first disk drive. They started using tape in addition to punch cards. They wrote Fortran, which is the first real kind of modern programming language where you could write words, and it would get translated into computer instructions by the end of the decade. In 1959, they launched the IBM 1401, which was available to thousands of companies. And they ended up selling around 12,000 of those machines. They exited the decade with a real... Anay Shah (00:16:45): computing business, and they're dominating the market. Ben Shwab Eidelson (00:16:48): So, that first machine shipped 19 systems. They had another machine that shipped 123. After that, they had the first transistorized Model T. The 1401 crossed over 10,000 units. From 1950 to 1962, IBM's revenue rose tenfold, from $260 million to $2.6 billion. And their headcount went from 30,000 employees to almost 130,000 employees. Woo! Anay Shah (00:17:12): And so, what else is happening right now? We're in the 1950s; World War II has ended. The Cold War is getting colder. And we find that the Soviet Union's long-range nuclear-armed bombers are able to cross the Arctic and reach American cities in a matter of hours. So, this makes the decision window to detect and intercept down to two minutes. Ben Shwab Eidelson (00:17:36): It's easy to forget, but the Cold War was primarily a technological arms race. Who's going to get there faster if something's going down? If the Soviet Union launches these bombers, how quickly can we respond, know about it, and then respond? And our Air Force and our defense systems were not designed for that speed. Anay Shah (00:17:53): Yeah, every threat forced a technology upgrade, forced our government to respond, to invest. We had a laboratory project at MIT that, in 1951, could process live radar data in real time, proving that automation could close this gap that we were seeing in national security. The challenge was: how do you take this radar research prototype and turn it into a 24/7 machine that can operate at government scale? And this is a moment that became the largest computing project to date. Ben Shwab Eidelson (00:18:23): Thomas Watson Jr. saw this, and he knew that they needed to win this contract. IBM actually had a policy of essentially making only 1% profit on any defense work, which they kept throughout this. But they said this would push them to the future. And so, the contract awarded in 1954 became one of the largest contracts ever, worth more than $500 million in 1950s dollars. Around $5.5 billion in today's dollars. And so, how did it work? What did it do at full deployment when they got there in the 1960s? It was a network that spanned 27 different centers, where each center had a pair of special IBM computers designed for this. In case one of them was being serviced or down, the other one would become primary. So they built redundancy into the network. And the scale was unprecedented. In total, it was 56 computers. These were acre-sized floors, with multi-megawatt power draw to these systems. And they were connected—connected over early modems. This was the moment that really drove the production of the modem, where you could actually have data sent over telephone lines. And so, they would lease special lines between these centers. Anay Shah (00:19:32): And these centers have characteristics that we're going to talk about more. You have redundancy built into this; you have building-level scale; you've got multiple facilities networked together. These end up being a lot of the same characteristics that drive data center growth and evolution decades later. Ben Shwab Eidelson (00:19:52): That's right. And I feel like there's the scale of it, but I think the connectivity of this system is the difference. You have storage; you have compute, which was happening; but now you had connection and communication, because you want this center over here to respond to information that was computed over there. And so, this moment in history—the SAGE system, I think—is the first real time that machines were communicating with each other. Pretty big moment. Anay Shah (00:20:17): So, once we're able to do this in government, it's going to catch the eyes of other industries. Ben Shwab Eidelson (00:20:23): The airlines had seen the SAGE project take off, and they were sitting there with a new problem on their hands. This was the '60s. Air flight was booming, and the way that reservations got processed was absolutely insane. You would call your travel agent; your travel agent would call the airline; and the airline would have to run clerks around, pulling cards out to book a seat. Supposedly it took up to 90 minutes. Anay Shah (00:20:49): Well, you had to figure out who had the middle seat in Row 17. Ben Shwab Eidelson (00:20:52): Who had the middle seat, exactly. Anay Shah (00:20:54): How were you going to figure that out without looking up the card? Ben Shwab Eidelson (00:20:56): As American Airlines scaled to a real operational scale, it started to break down. So, they reached out to IBM, and they kicked off this project that would be known as Sabre. It was essentially a commercial version of the SAGE system. Two IBM mainframes were purpose-built, and would be connected over phone lines. Those systems would house the source of truth around the reservations. And there would be terminals. These weren't terminals with screens; these were terminals with paper where you'd still have a travel agent, but they would be querying over the terminal what seats were available. And things would be booked automatically. So, it would go from 90 minutes to seconds at a terminal. And it removed the clerk in the middle. This set the stage for e-commerce. This was the first time, I think, you were buying something over a computer. Anay Shah (00:21:44): You had 90 minutes to do a reservation, now down to seconds. And this scaled American Airlines operations by the mid-'60s to be able to do a full 40,000 reservations per day. Ben Shwab Eidelson (00:21:55): Insane. This system continues to this day to be part of the backbone of airline booking. It's quite an amazing foundational moment where commerce is happening between data centers. And so, they were running airlines; they were doing these major government projects. Businesses were now buying the 1401 machine off the shelves. But there was a different problem. Entering the mid-'60s, all of these mainframes had their own accessories. There was no compatibility. So, in 1964, they launched the System/360. And that turned this chaotic, messy, ad hoc world of different mainframe models into a platform. It was essentially one architecture; you could choose how powerful you wanted... Anay Shah (00:22:38): it to be, buy your base package, and then feature it up based on what you need. Ben Shwab Eidelson (00:22:42): That's right. And so, for facility planners, they could now plan a room and a facility, and then be able to scale up the machine as needed. I believe roughly $5 billion of investment in R&D was spent in this era. Anay Shah (00:22:55): Another "bet the company" moment. Ben Shwab Eidelson (00:22:57): And it worked. They were shipping thousands per month of these devices in the '60s. And so, if you zoom out and look across the '50s and '60s, as we talked about earlier. From the '50s to '62, they scaled to $2.6 billion. By 1970, they were doing $7.5 billion in annual revenue and had a team of 270,000 employees. Anay Shah (00:23:19): In today's dollars, that's $62 billion in revenue. Ben Shwab Eidelson (00:23:23): And the market noticed. IBM was king at that moment. In 1970, IBM accounted for 6.8% of the total U.S. stock market. It almost feels like an unprecedented thing to say that today Nvidia is currently 7% of the U.S. stock market. Anay Shah (00:23:44): These mainframes became part of the zeitgeist. They became part of a cultural milieu because they were often nicknamed "the glass houses." You'd have the mainframes and their operators in an enclosed room. And as we mentioned before, you'd have windows and glass walls around it because you wanted to show off how automated your systems were, how advanced your company was. There was a recognition that these "glass houses"—these mainframes—were powering the billions of dollars of revenue from their enterprise customers. And so, the security, the operational control, the climate control, and cleanliness... It was all part of this cultural moment where mainframes and these early data centers became a meaningful part of how our society and economy ran. Ben Shwab Eidelson (00:24:34): If any company was using it effectively, they had more demand for use than they had supply for the computer. And so, oftentimes, people would be sitting there waiting for their information to come back, for their computation to run. Sometimes it would be a day or longer, so the utilization was pretty high. But there was this long interactive latency problem. And an invention that echoes through in multiple ways to this day changed the fundamental way that computing was thought of: from this single-threaded "I'm only working on one problem at once" to "No." The job is to make sure my hardware is as utilized as possible. As a machine, I don't care whether or not it's the same problem from the same person. I just need to be calculating. Anay Shah (00:25:20): I need to be operating all the time to utilize my capacity. It can be for this problem; it can be for that problem. You can slot them in and out; it doesn't matter. Ben Shwab Eidelson (00:25:29): That's right. And so, a team at MIT in the early '60s developed what was called CTSS, the Compatible Time-Sharing System, and turned it onto the campus in 1963. And so, students could sit there with electric typewriters and interact with this machine, multiple users at the same time, and feel like they had control over the computer. Anay Shah (00:25:52): It felt like it was your own. Ben Shwab Eidelson (00:25:53): And this was a huge moment. This was the first time there were logins, users, files, and instant feedback. And so, it was revelatory to be there in the mid-'60s, experiencing that after the idea of a computer was just this operated thing by someone else. Anay Shah (00:26:09): That's right. You had to queue up, get in line, hand over your data to the operators—the specialists—who would then go and do their thing, and you would sit and wait. Now you're interacting. It's you and the computer. Ben Shwab Eidelson (00:26:22): And the way this worked was that you didn't obviously have monopoly control over the computer. The computer was switching what problem it was working on, depending on what free cycles it had. The human perception is, "Oh, how can a computer do that?" Well, the computer is operating at a much faster frequency than we're able to realize. Even if it's something as simple as responding to a keystroke, there are plenty of milliseconds in between my keystroke and your keystroke for the computer to respond and then switch back to the other problem. And as we'll see later in this story, this notion of intelligently slicing up the hardware and utilizing it becomes the backbone of everything in how the cloud and data centers are architected today. And it started with this timesharing innovation. Well, we had these mainframes that had taken over the business world, and timesharing. So, multiple people could be on a campus connected to that mainframe, feeling like they had their own computer. They were still fully disconnected from anyone else in the world. They were still islands onto themselves. So yes, now on this island, multiple people could be there, but it's not like you could go visit the next-door island. What are we missing? Anay Shah (00:27:27): It's something we take quite for granted. Many people in different places, but working on the same system at the same time. Now remember, we are in the late 1950s, in the Cold War, and Eisenhower wanted to ensure that the U.S. would not be blindsided by a technological surprise. Ben Shwab Eidelson (00:27:47): There's this particular moment right when everyone looked up in the sky and for the first time, a man-made thing was floating in space, and that's Sputnik. The fact that the Russians beat us to the skies with Sputnik kicked off, I think, a level of panic from a geopolitical standpoint that's hard to connect to today. Anay Shah (00:28:07): And so, after the panic of Sputnik, Eisenhower created the Advanced Research Projects Agency, or ARPA. And in those years, ARPA was pouring money into space, into missiles, into computing. And we wanted to ensure, in this nuclear-armed world, that weapons could not destroy a centralized command system. And so, the network itself needed to be decentralized. This need to ensure reliability combined with a slightly more mundane problem that Bob Taylor faced when he was at his office in the Pentagon: he was working at three terminals. He had one machine connected to MIT, another connected to UC Berkeley, and a third for a different research system altogether. And he could talk to any of them, but never at the same time. So he was rolling his chair back and forth, and he's quoted saying, "We ought to find a way to connect all of these different machines." He told his boss. After a 20-minute meeting, Taylor walked out with a million-dollar budget and a simple mandate: to make it happen. Ben Shwab Eidelson (00:29:10): And we won't go into every moment and step because the ARPANET itself is quite a story. But I think a couple of innovations set the stage for the Internet in a really important way. One is the concept of packet switching. If you think about a phone line, AT&T built its whole business on this idea: you're going to connect a phone call between two people, and there's going to be a circuit that connects those two phones together. This is called circuit switching. But there's this crazy idea: if you want that resilient system so that if one node goes out, the next thing doesn't go out, you need to route things around in different ways and not be dependent on the one route you have. So, you need to have more of a spiderweb-like network to say, "Well, from here to here, there are actually five, or ideally 50 or 5,000, ways to get between those two points." And to do that, you need a flexible communication system. And so, the idea is to break up information into packets—little pieces of mail that would get sent from one node to another. Different packets could even take different routes, but on the other side, someone would reassemble them into the message. Anay Shah (00:30:15): It's the same singular message, broken up into many, many different pieces and routed through many, many different pathways. Ben Shwab Eidelson (00:30:24): It's funny, I hadn't thought about this until we connected to the timesharing story, but it's actually about better utilization of the network. In a circuit-switched mode, you have this direct line; you and I have a direct line together, and you have a direct line to someone else. That line is usually empty. It's pretty hard to fully utilize and build out the circuit-switched thing because the phone call ends, and you think, "Okay, now it's just sitting there unused." In packet switching, packets are finding their way through this wild world. So, a lot of skepticism obviously came from AT&T, which said, "If you want to have two computers talk to each other, we'll build the phone lines, and you can run them." But a crack team that had kind of spun out of MIT—this company, Bolt, Baranek and Newman—latched onto this packet switching model and ran forward. And the core thing that they needed to build was what was called the Interface Message Processor, or IMP. This was ultimately the router that would sit in front of the host machine. Universities like MIT and Berkeley would have these mainframes. But to connect those machines together, there would need to be these nodes that would sit in front of them and be able to build this network. And so, BBN used a Honeywell minicomputer that was fridge-sized, cost about $80,000, and built the logic around packet switching and rolled this out. There were a lot of engineering heroics that went into getting the first machine ready. But then it was time. Anay Shah (00:31:47): Almost 55 years ago, a small group of grad students gathered at UCLA to wait for the machine to be rolled off the truck, champagne in hand, and were able to celebrate the arrival of the first IMP. The second one was delivered to Stanford weeks later. And the prophetic first message, as the story goes, was meant to be "login," right? You now have users that can log in, but after inputting the first two letters, it crashed. And so, the first message was simply "lo," "lo." Ben Shwab Eidelson (00:32:19): So close to "lol." And, of course, what is a network with two connections? Pretty limited. But then they rolled out UC Santa Barbara and the University of Utah, and then they were just rolling out new IMPs to the large universities, month over month. Anay Shah (00:32:34): And these became nodes—almost a new node every month. And then every new node that came on the network amplified the value. Because now it's not just bidirectional communication; it's multilateral communication. Ben Shwab Eidelson (00:32:47): So, they were building all this logic to do the resilient routing, to do the discovery of devices, and everything that you needed to do that. And of course, the original pitch was, "Hey, we've funded as a government all these expensive computers at all these research universities. Let's drive utilization." Maybe one team has a special program for doing something; another team has a special program for doing something. Let's let the departments connect. Anay Shah (00:33:07): One unexpected application really bubbled to the top that the researchers discovered. Quick, asynchronous messages were often more valuable than logging into someone else's system to run the code. And in 1972, ARPA's Bob Kahn said, "Everyone really uses this thing for electronic mail." And the network's purpose had quietly shifted from sharing machines to connecting people in the form of email. --- Ben Shwab Eidelson (00:33:36): It's wild how as soon as you have enough connectivity, it's always the killer app. The killer app is always people communicating at a distance. Anay Shah (00:33:45): That's right. People wanting to do what we are biologically programmed to do, which is connect with other humans. And as sophisticated as these machines were, it was the simple act of asynchronous communication that became the killer app. Ben Shwab Eidelson (00:34:02): And more people wanted it. There were new ways to connect within your local network over these terminals, so then you could connect into the broader network. It was no longer just about wanting access to a terminal to access the machine on your campus; it was about wanting a terminal so you could email your friends at the other campus. Sounds like an early social network that we'll get to later. And so the expansion continued. By 1972, there were 29 nodes. By 1975, over 50. Anay Shah (00:34:27): And some of these connections weren't just from university to university. We were now leaping oceans. It was clumsy, but we were able to go over the Atlantic Ocean from a node in Virginia to the one in Cornwall, England. Ben Shwab Eidelson (00:34:41): There was some story where they came back from some conference in England and someone had left their electric shaver behind. Anay Shah (00:34:47): It was the middle of the night, so in England it was 3 o'clock in the morning. But he knew that his colleague was a workaholic, so he sent off a message at 3 o'clock in the morning, England time, to see if this person was awake. And he was. He saw that he was logged on and he's like, "Hey, did I leave my electric razor there?" Ben Shwab Eidelson (00:35:01): "Can you get my razor, please?" And it all worked. It was phenomenal. So, by the early 1980s, other research networks started to pop up in the US and elsewhere. People were like, "Oh, this ARPANET thing is pretty cool. We're going to build our own." There was DECnet, there was an IBM Net, there were various nets that companies and other research groups started to stand up. They had a new challenge: "We want to talk to everyone on any of these networks, and how are we going to do that?" And these IMPs were not designed for that. They had presumed there was only one network they were trying to build out. And so Vint Cerf and Bob Kahn, who were involved in ARPANET, figured out what they needed to build was a common language to figure out addressing: where to send a packet of mail. And then also, the control protocol for sending information, to make sure duplicate packets weren't sent. And so this became TCP/IP: TCP for the Transmission Control Protocol, and IP for the Internet Protocol. You can think of IP as solving the addressing problem. You might be familiar with your IP address, which then communicates through routers what you're looking for, and who you're trying to get the packet to. And then TCP is the reliable way that a packet gets acknowledged. "Hey, this little chunk of information has arrived." It's like certified mail. Anay Shah (00:36:16): Like signing for my package. I have an address, so the package knows where to go. And then I signed for it. Ben Shwab Eidelson (00:36:21): "And I don't need you to send me the package again, because I got it." And so this combination became the backbone for connecting all of these networks together, and ultimately the backbone for the next stage of not the ARPANET, but the Internet – a network of networks. In 1983, they took every ARPANET host and said they needed to move to TCP/IP. That switch, on January 1, 1983, became the framework for the modern-day internet: many networks, all speaking the same language, able to connect. Anay Shah (00:36:54): And that's where we have the advent of the .com, .edu, and .gov, because we now had standard protocols for multiple nets to communicate together. Ben Shwab Eidelson (00:37:06): 20.3.1.72 doesn't have the same ring as pets.com. Anay Shah (00:37:11): Pets.com, an important one, but we're not quite there yet. No, but by the mid-1980s, as Ben mentioned, there were multiple nets now connected. And ARPANET itself was showing its age. By 1990, it became apparent that the ARPANET needed to be decommissioned. But the work it had done, the ideas of decentralized networking, packet switching, and open protocols had firmly taken root. Research institutions and large organizations had firmly established how to exchange messages. Ben Shwab Eidelson (00:37:41): So they were able to send files back and forth, maybe send each other's research papers and data sets. But this was not a thing with a web browser yet. We needed a structure around documents. And so Tim Berners-Lee, a researcher at CERN in Switzerland, proposed the document structure that would become the application-layer infrastructure of the World Wide Web. Anay Shah (00:38:01): And so the network that started off as a Cold War research project in response to looking up at the sky and seeing the Russians launch Sputnik, had now become the public and commercial Internet – an open highway for data, and the foundation upon which we would build the modern data center economy. Ben Shwab Eidelson (00:38:18): So we now have this expanding network, but the computing on this network was still this terminal-based interaction between a terminal and a mainframe that then connected to the network, or a terminal and a minicomputer which connected to the network. And I think it's worth saying a bit more about minicomputers, which were both what became that IMP device and also a primary source of connection. These were not something that you'd buy for your house. They still were the cost of maybe a car when they first came out, but they were much more accessible than a big IBM mainframe, a room-sized computer. And so the company, Digital Equipment Corporation, or DEC, launched the first minicomputers in the mid-60s. A lot of what innovated on the minicomputer was software like Unix, C programming, and sockets, and all these foundational innovations that would later echo through to today. But there was room for a computer that a normal person could buy. Anay Shah (00:39:11): On one fateful day, January 1975, the cover of Popular Electronics magazine showcased the Altair 8800, a machine you could buy at home for $439, or $1,500 souped up, to run your own code. And there were a few hobbyists that went to the newsstand and picked up this magazine. Two in particular saw the cover and decided to do something about it. Ben Shwab Eidelson (00:39:42): And so Bill Gates and Paul Allen, famously, when they were at Lakeside High School in Seattle, they had access to a computer and a terminal, and they would spend hours and hours programming. But that's different than having one in your own house that you can play with. And so when they saw this come out, it was immediately clear that this was a moment when computing had gotten cheap enough that a normal person could afford their own personal computer, or PC. Anay Shah (00:40:06): And you could see that if the first one was released at $450, it was only a matter of time before that would become accessible to more and more people. Ben Shwab Eidelson (00:40:16): Microsoft formed immediately to sell a BASIC compiler for this device. The Apple II launched in 1977, which kicked off the PC wave for many folks, and it paired with VisiCalc in 1979. So, all of a sudden, you had a killer app – a spreadsheet – that wasn't just for home use, but now had a corporate function, driving the purchase of these PCs. And so someone could model a budget without needing to connect to the mainframe down the hall. Anay Shah (00:40:43): I can stay at my desk and run my finance operations. Ben Shwab Eidelson (00:40:47): That's right. And IBM, while not the first here, was actually pretty quick to realize there was a problem for them. Right. If everyone is doing all the computation on their desk, they're not going to need the mainframe down the hall anymore. The glass house had been shattered. So they kicked off a skunkworks project. It was actually pretty amazing. They isolated it outside of the New York region, down in Boca Raton. They said this team would use off-the-shelf parts to design an IBM PC. In 1981, they built and launched the IBM PC, and they licensed MS-DOS from Microsoft, which had been founded six years earlier, as the core operating system. They thought hardware was the business, but time would show them to be incorrect. A bunch of IBM-compatible devices, including Compaq and others, flooded the market through the 80s. So prices fell. Hardware became a commodity, and Microsoft, with DOS and then Windows, became the Wintel duopoly that ultimately took the mindshare and market from IBM. Anay Shah (00:42:00): That's amazing. You got Lotus 1-2-3 dominating spreadsheets. You've got WordPerfect. I remember using that for word processing. And then Microsoft comes along and says, "I'm going to bundle all this into Office" and really took the cake. So productivity was through the roof, and we were able to do more personally than we could ever do before. And the way we collaborated? It was that magic floppy disk. We were able to hand that back and forth and keep on rolling. Ben Shwab Eidelson (00:42:21): And that's great, right? Until it's not. Anay Shah (00:42:24): Until it's not. Ben Shwab Eidelson (00:42:25): You want to work with someone across a big building? You're going to run them a floppy disk? And what if they have a different version? It's a mess. I think the "personal" in personal computing started to become a hindrance here. So we needed to connect these devices as well. Enter the invention of Ethernet and IBM's Token Ring, which wired the floors and connected these PCs into a corporate network. And a new set of companies emerged. Novell, building NetWare, actually turned a server into a hub for shared disks and printers. You had a PC on your desk, and if there was a printer room and you wanted to print, you'd have a different computer. That was a specialized PC that we'd call a server, connected to that printer or to shared storage. Now we were able to access files and print over a network. A whole company, Novell – which at some point was actually the second-largest software manufacturer after Microsoft – Anay Shah (00:43:23): Wow. Ben Shwab Eidelson (00:43:23): was booming in this era. Microsoft, doing what they do very well, responded by building a network-connected operating system – eventually Windows NT and other services – so that you could do it all within Microsoft's ecosystem. Anay Shah (00:43:37): So throughout our conversations, a lot of the OGs in this industry pointed to this moment as the introduction of the client-server era. What does that mean? Ben Shwab Eidelson (00:43:46): It means an application was really split in two. You had your computer sitting there on your desk running as the client and accessing a central database or a server in the back room. A bunch of technologies came up to support this, from Unix servers to ERP systems to Windows NT. But now, all of a sudden, you would think about an application as networked from design, where you'd have local client software that could use the best capabilities of the local PC, connected to the server. Not across the world, but the server in the server room, on site. Anay Shah (00:44:20): What was that server down the hall or in the other room or across the world? Ben Shwab Eidelson (00:44:25): Most notably, this was the era when the architecture of the personal computer was adopted. This included the x86 (Intel-based) architecture or the Sun workstation architecture. Increasingly, those would become the servers that could run these applications. This was a notable shift, again, from the IBM mainframe era. It got to the point where someone selling your company an application would sell you an appliance, which was really a package of the software and the hardware together. So you ended up with a sprawl of different appliances that were being managed, all serving different functions, all written with their own bespoke software and operating systems. So, while the functionality was amazing, people became very inefficient. Anay Shah (00:45:10): Up until this point, there was a culture and an ethos among data processing professionals that put conservation as the highest ethical principle. To waste a CPU cycle or a byte of memory was embarrassing. Ben Shwab Eidelson (00:45:23): I think part of it was that they had gotten cheap enough; these servers, which would have been $10,000 a month to rent just 10 or 15 years prior, had now gotten cheap enough. And it wasn't unreasonable to have them sitting there idle at the same time. In a macro sense, this inefficiency later created the space and value for the evolution of time-sharing into virtualization, where we asked, "Why can't we run these applications on one box? Why do they all need their own box sitting there waiting for a command?" Anay Shah (00:45:49): That's right. If a box was not being maximally utilized by a single client, share the box. Ben Shwab Eidelson (00:45:56): We will do a whole episode on how that came to be. But this was still an era of deep innovation, particularly concerning redundancy. You had RAID arrays, which provided redundant storage, and backup tape libraries that were off-site, because you still had these massive points of failure. Your company server, which might be your system of record for your customers, was sitting there in the closet. I heard from someone who ran some of it at REI at one point that they had a flood near their headquarters, and they were like, "Well, the website's going to go down, we're going to lose all this business." So this model was not designed for scale and redundancy in a major way. So, to make this moment concrete – of just how computing power and connectivity was starting to shift – let's talk about a deeply innovative retail company that, no, is not Amazon. We're talking about the OGs of major retail and scale. It's Walmart. Anay Shah (00:46:50): That's right. So, you're at the register at your neighborhood Walmart location: you scan the toothpaste, the barcode beeps. Within seconds, a satellite dish behind the store sends that transaction to the sky and over to Bentonville, Arkansas, where they're hosting their mainframe computer to record the sale. A few minutes later, a massive data warehouse would update how many tubes of toothpaste you had just bought. Then, perhaps before the end of the day, Procter & Gamble's factory would receive an update that they needed to make more toothpaste. This was the cutting edge of retail in the late 80s, and Walmart made a massive decision to take this a step further, truly driving innovation across the retail industry. They invested $24 million to build their own private satellite network linking all Walmart stores to headquarters. This was fairly unprecedented at the time, right? Ben Shwab Eidelson (00:47:43): It was the largest private satellite network. Anay Shah (00:47:46): That had been built, and it created a unified, real-time machine. So any single event that happened within the Walmart ecosystem rode on this private network, enabling them to mine their data in a way that was unheard of before. By mining their sales data – which could now be collected in real time across all Walmart stores over their private satellite network – they discovered that when hurricanes approached, the sale of Pop-Tarts increased 7x over their normal rate. Having spent a summer working in Bentonville, Arkansas, I can tell you this is deep in their DNA. They are constantly looking at signals like this. Specifically, they discovered that it was the Strawberry Pop-Tart that was most in demand before a storm. This led to the legendary insight of meteorologists predicting a severe weather event, and Walmart stocking their affected stores with pallets of Strawberry Pop-Tarts. Ben Shwab Eidelson (00:48:43): It makes me wonder if the reverse is possible. It's like, instead of checking the weather, you go to Walmart to see if the Strawberry Pop-Tarts are there, because it's such a reliable system. Like, I know that they're watching the weather, but I mean, there's some crazy innovation here. Both the satellite link, and then Walmart, supposedly, was the owner of the first commercial 1-terabyte enterprise data warehouse they built with a company called Teradata. They were just maniacal about making sure all the sales and customer data flowed into one place. By 2001, just nine years later, that one-terabyte warehouse had grown to 70 terabytes. Anay Shah (00:49:14): Wow. Ben Shwab Eidelson (00:49:15): Right. So this was just an explosion of connectivity inside the enterprise, even while there was the personal computer. Some people had fun with their computers at home. I remember playing on an IBM XT as a kid, and some early programming and early games. It was not like most people had a real personal use case. We're still talking about a corporate-centered world. So we talked about the rise of PCs and the growing client-server LAN inside a company. We talked about these data centers; we talked about Walmart's satellite network. But what was happening now, going into the early 90s, with this whole ARPANET/NSFNET Internet thing? What was happening outside the office? Anay Shah (00:49:55): Yeah, and perhaps what was happening under the ground. Ben Shwab Eidelson (00:49:58): So the first network that was available to all these researchers was the evolution of this into the NSFNET. It became the de facto US internet backbone. Anay Shah (00:50:07): This network was connecting 2,000 computers in 1986 and expanded to over 2 million by 1993. Ben Shwab Eidelson (00:50:15): It was no longer just researchers who wanted to do stuff with it. So, the design and topology was a high-speed national backbone that had, at this point, gone to a T3 line (at 45 Mbps) that connected a small number of regional networks. So, kind of like a central hub-and-spoke model. These regional networks would then connect to universities, labs, and nonprofits. But this was not designed for commercial scale. Anay Shah (00:50:39): In fact, it was prohibited, according to their terms of use, to have commercial traffic running on that backbone. It was specifically designed for research, education, and government data. Ben Shwab Eidelson (00:50:50): So, I think some private commercial network could happen inside just a regional hub, but not across that whole big backbone. So you did not have the beginnings of what could be a commercial internet. If two regional networks – say, New York and Philadelphia – wanted to connect, they had to flow back up to that NSFNET backbone. There was no neutral point where they could connect and exchange. So this was the moment when commercial ISPs and telcos started to see, "Okay, this internet thing is interesting, this packet-switched model." People wanted to do new things with this. Maybe it was trying to get out of the lab into commercial use. How were we going to connect? What were we going to do? Anay Shah (00:51:29): In 1992, a group of network providers were sitting in Virginia, drinking a beer, and decided to connect their networks outside of Tysons Corner. This specific group of engineers was from Metropolitan Fiber Systems, the local telco. They chose Tysons Corner outside of Washington D.C. because there was a dense network of defense contractors and early providers, which were heavy users of the current internet. They famously set up in a repurposed parking garage to become the de facto on-ramp for new ISPs. When you think about an important hub, you wouldn't typically think of a parking garage, but it was the right place at the right time. It turned into what was called Metropolitan Area Exchange East – East Coast. So MAE-East is what formed. And if you connected into MAE-East, that meant you had the internet at your doorstep. Ben Shwab Eidelson (00:52:27): This became the hub for the internet. If someone sent an email from London to Paris, it most likely went through MAE-East. Anay Shah (00:52:35): Within a couple of years, roughly half the world's internet packets were flowing across the MAE-East parking garage. --- Ben Shwab Eidelson (00:52:43): And I think this is something we'll see again and again, which is that the Internet forms around hubs. It's not always obvious why that became the hub specifically, other than it did first. And there's just this gravitational pull of connectivity. This wasn't new to the Internet; this was something that we saw with telcos. There's this concept called carrier hotels. You're in a city like New York, and you had two different carriers that were trying to connect with each other, right? Think Sprint and AT&T. Instead of having to connect all throughout the city in multiple spots, they would all show up in a neutral zone called a carrier hotel and build their connectivity infrastructure there. Anay Shah (00:53:21): You'd be able to tap into each other's long-haul routes, their local fiber routes, without having to build their own intercity footprints individually. Ben Shwab Eidelson (00:53:31): And so May east was the first sort of flavor of this, where they would all come in and connect. Ultimately, a device — think a switch — that is connecting. Okay, you're coming in, plugging in your ISP traffic; here I'm plugging in mine. Now users across our ISPs can connect, and it's just one big switch room. The problem is, the Internet is scaling, and you don't necessarily want to all be bottlenecked on one switch. It's not the most secure thing. And if you're, I don't know, eventually building a video streaming service or you make a deal between two ISPs, you don't necessarily want everyone else to be in on that deal. And so you saw the evolution of this model to a different model that became known as MeetMe rooms. These are neutral physical rooms where an ISP or someone trying to hook into an ISP can provide their boxes and their connectivity, and then those two can connect directly. Anay Shah (00:54:20): So, a bring-your-own-box method. Instead of everyone connecting through the existing box, you BYO your box for the deal you want to do in the private MeetMe room. Ben Shwab Eidelson (00:54:30): That's right. Something sounds funny about BYO to the private MeetMe room, but we're talking about ISPs connecting for data. Anay Shah (00:54:38): The farthest thing from a non-platonic conversation as you could be. Ben Shwab Eidelson (00:54:44): And so, sure enough, this all worked. The NSF kind of officially sanctioned this method and designated these NAP points — Network Access Points. The first one, May east. They designated Sprint to run a NAP in New Jersey near the transatlantic cable landing points, one in Chicago, and one in San Francisco. Then they eventually added Mae west in San Jose. Anay Shah (00:55:06): And these MeetMe room models started to take off. The carrier hotels, you had One Wilshire, a large building on the west side of Los Angeles, which were law offices, and gave way to a single floor that could host hundreds of carrier routers and thousands of cross-connects, eventually making it one of the most valuable space per square foot on the entire West Coast. Ben Shwab Eidelson (00:55:29): It's so funny. It's like this ugly building or just relatively nondescript architecture, and you're like, "What's going on in there?" Law offices make sense. And now this is where the West Coast Internet is coming through. Anay Shah (00:55:40): That's right. And we'll talk a little bit later about undersea cables, but they come in and want to find their shortest path to One Wilshire. Ben Shwab Eidelson (00:55:48): And the business model was pretty genius for this. These telco hotels, these MeetMe rooms, ultimately provided power, cooling, and cross-connect, and they would charge rent. The dot-com boom to come would boost this model to new heights. Anay Shah (00:56:03): And it provided an elastic infrastructure that Internet companies in the dot-com boom and after could leverage, including, as we'll see soon, hyperscalers, where you could flexibly increase and decrease your capacity because they were specialized in providing all the necessary infrastructure to host the connectivity. Ben Shwab Eidelson (00:56:25): So all of this infrastructure being set up to commercialize the Internet, to provide a scalable backbone, to enable the private market of ISPs and telcos and others to invest in making the Internet faster. And we now have the World Wide Web. The NSF has actually funded a little project called Mosaic, which is the first user-friendly web browser. We have personal computing. Enter the mid-90s and the dot-com boom. Anay Shah (00:56:53): Let's boom! Ben Shwab Eidelson (00:56:54): The ISPs are ready to build the network. All we need are the users. Anay Shah (00:57:00): And boy, are they coming! In May of 1995, Bill Gates writes the famous Internet tidal wave memo: "The Internet is the single most important development to come along since the IBM PC." And it was that same year that Netscape went public. Why was Netscape so significant, Ben? Ben Shwab Eidelson (00:57:21): I think it's significant for two reasons. One is it kicked off the accessibility of the Internet. It turned the Internet from this network for researchers to share files to this browser that you could download, install, and access the web, opening all of that up. And then it was that from a product perspective. But that also captured the economic perspective and interest: "Is there a thing here? Is there a new industry, a boom that you can make your millions or billions off of?" Its journey from founding to IPO in such a short cycle kicked off a mania. Anay Shah (00:57:56): It kicked off another mania in the business world, which was: having barely any revenue or profits, you could hit a multi-billion dollar valuation, which is what they did. And the web exploded. You had 23,000 websites in 1995 to over 10 million by the year 2000. Global users climbed to over 350 million. Nasdaq tripled in two years. If you put a dot-com at the end of your name, just kind of like you put 'AI' at the end of your name today, you could raise millions on an idea and a slide deck. And in 1999 alone, you had more than 400 Internet companies going public, pulling in $40 billion. What a liquid IPO market that we could only dream of! Ben Shwab Eidelson (00:58:40): Today Microsoft hits all-time highs in the stock market. The energy was manic, right? Founders in their 20s would become paper millionaires overnight. Engineers were hopping jobs for stock options. This is the rise of the Aeron chair, the foosball tables, and the new economy where the rules of things like revenue no longer apply to business. Anay Shah (00:58:59): But it wasn't just applications; it was infrastructure as well. Carriers spent half a trillion dollars on fiber and wireless. You had these colocation companies expanding at breakneck speed. This company called Exodus Communications was the world's largest web hosting provider at the time, providing server colocation. Its revenue went from 12 million in 1997 to 250 million two years later. It peaked at a $32 billion market cap three years after that. It was laying the infrastructure in the ground and building the applications above it in a period of unprecedented growth. Ben Shwab Eidelson (00:59:37): And so to launch a startup, you had to build a site, a service, and a database. You needed money way ahead of time to buy the servers to stick in the colo box. Anay Shah (00:59:48): Wait, so you're saying, in order for me to launch a web business, I had to buy hardware? Ben Shwab Eidelson (00:59:53): Exactly. You had to take most of your venture capital dollars and spend it on servers even before you knew if anyone wanted to go to your website to begin with. Anay Shah (01:00:02): So you couldn't test your idea out, you couldn't A/B test, you couldn't do a landing page that drew in a... Ben Shwab Eidelson (01:00:07): waitlist, couldn't do any of that. Anay Shah (01:00:09): Wow, what a different world! Ben Shwab Eidelson (01:00:11): So all these new servers are trying to run on the new backbones that are being laid to power this boom. The traffic still at this point met at only a handful of public exchange points. Networks are plugged into these shared boxes at places like May east that we just talked about. That can only scale for so long. Anay Shah (01:00:30): You'd start to hit choke points, and May east, one of the earliest network access points, became one of those major choke points. Ben Shwab Eidelson (01:00:38): We needed different models for companies, ISPs, non-ISPs, to connect. You had DEC kicking off Palo Alto with the Palo Alto Internet Exchange, a non-telco neutral spot. Then you had the founding of Equinix where they took that model and scaled it with their first site in Ashburn, Virginia. Anay Shah (01:00:59): Ashburn is the Wall Street for data centers. In 1999, Equinix launches their first data center under this new model in Ashburn, Virginia, right next to May east, the choke point. But the original network access point isn't. Ben Shwab Eidelson (01:01:13): This, near D.C.? I've never been there. Have you been to Loudoun? Anay Shah (01:01:16): I grew up right outside of D.C. on the other side of the river. Loudoun county is on the Virginia side. It's rural farmland. It's past Dulles Airport. There's really nothing there. But it's proximal to a large East Coast population, the undersea cables and May East. And you actually had AOL choose Loudoun county in the mid-90s to set up a huge dial-up campus. So they laid fresh fiber and drew even more carriers into the region. And May east had grown so large it outgrew the parking garage it was in. So the Exchange relocated to Ashburn. So you've got this unassuming farmland outside of Dulles Airport. And it was really catalyzed by a couple things. Outside of AOL pioneering the new site, it was policy-led. So Loudoun county ruled that data centers could be treated like ordinary office parks. They eliminated all these special use hearings and provided incredible tax breaks over time to attract data centers into this network. Coupled with that, Dominion Energy, seeing what was coming ahead, offered some of the lowest industrial rates to string high-voltage lines to this empty land to bring power and fiber together to create the new data center model which Equinix pioneered in Ashburn and became the largest Internet hub on planet Earth. Ben Shwab Eidelson (01:02:42): So it's the combination of perhaps for a moment, cheap land (not so much anymore), fiber and connectivity, cheap and accessible power, and favorable policy. One interesting policy story that I heard on this: Apple was looking for where to put a new site in the late 2000s, 2009. Apple's obviously building more services, needing more storage, and building more data centers. So they run a process, and it turns out that North Carolina gives them a better deal. So Virginia fights back and in reaction passes major tax breaks to say that if you're building a data center, basically if you're building anything more than $150 million of investment, you're going to employ more than 50 people — no tax. Anay Shah (01:03:22): You, of course, are. If you're building a data center, of course. Ben Shwab Eidelson (01:03:24): Of course. Yeah, you can't build a data center for $150 million. Anay Shah (01:03:27): Local policy, as you mentioned, is so important. Federal policy actually played a big role here. In 1996, we passed the Telecommunications Act. And one of the main things it did was force the incumbent telcos — which were regional monopolies from the AT&T breakup — to lease their physical network (their copper pairs, their fiber) to their competitors. So prior to this, a data center was a private enterprise tied to a single carrier because the carriers only used their own fiber. Now it enabled carrier-neutral sites. So you could become a tenant of a data center and choose from multiple fibers provided into that building. And it opened up an explosion of choice for tenants and for this carrier-neutral model. Ben Shwab Eidelson (01:04:11): And so this is a flywheel of a deregulation environment to build this because once they have the tax incentives in, it becomes a major source of economic prosperity for the region. Now, we'll fast forward later on in the story to today, and maybe hitting some of the first real pushback. Anay Shah (01:04:27): And similar to Frankfurt, Amsterdam, London, Tokyo, the biggest hubs are where you can find cables and carriers and connect the most networks with the least amount of friction possible. And it's this flywheel that continues to make Ashburn the largest home of data centers in the U.S. And what a time! Ben Shwab Eidelson (01:04:47): In the first six months of 1995, Internet traffic was doubling every 100 days. The telcos were convinced that you couldn't overbuild. You just needed all the fiber you could put down. Anay Shah (01:04:56): WorldCom and these other telcos, they just poured billions into this. And the idea of overbuilding — not possible, particularly in this Internet boom, right? But by 2000, how much of that installed fiber was actually being used? Ben Shwab Eidelson (01:05:11): It was only about 3% — 3% was lit up. So they just laid down fiber, the shards of glass that were just sitting there empty. Anay Shah (01:05:19): So to give you a sense of the amount of fiber miles laid during this period, you could go around the circumference of the Earth 5,000 times. Ben Shwab Eidelson (01:05:27): Wow. So this was all over land, right? But how would you connect to Europe? Anay Shah (01:05:31): Ah, and that overbuild, you're right, was not just limited to land. Nearly all intercontinental Internet traffic rides on undersea cables, which I'd kind of heard of, but I didn't really have a full appreciation for the fact that bundles of glass threads are wound into a garden hose structure and laid down on the ocean floor all over the world. So imagine a planet stitched together with hair-thin strands of glass tucked into an armored hose and laid across the darkest parts of the ocean. That's the undersea cable system, and it's the real physical Internet that connects the continents. So 99% of international data still rides on these cables, not satellites. That's racing pulses of light through fibers thinner than the human hair. Ben Shwab Eidelson (01:06:23): The story arguably starts way back in the 1850s, when we had the first telegraph cables crossing the Atlantic. They brought those across steamships and landed the first link in 1866, able to send news for the first time across the ocean. Anay Shah (01:06:38): So instead of weeks, you could get the news in minutes. Ben Shwab Eidelson (01:06:41): Yeah, you're not sending the news via ship, you're sending it via electrons. Anay Shah (01:06:44): Amazing! Ben Shwab Eidelson (01:06:45): Fast forward to 1988, and we land the first transatlantic fiber-optic cable. It runs between the U.S., the U.K., and France, and it kicks off this new era of cross-continental capacity. By the late 90s, as this boom was happening, you can imagine the funding routes going into wiring all of this up. Anay Shah (01:07:07): You lay all this cable down with specialized cable ships that survey the seabed, unspool the cable, and then near shore they bury it underneath to protect it from anchors and storms, and then raise it up through some nondescript concrete box. This garden hose has optical repeaters that boost the light every 50-100 km, so you can sprint thousands of miles without fading. Then you get to land, and these concrete bunkers have the cable hop up into terrestrial fiber and then run straight to your One Wilshire or any of your nearby hubs that then connect you onward. --- Ben Shwab Eidelson (01:07:44): What a wild thing. You have a garden hose-shaped thing moving all this data. What do we actually mean? Well, modern cable, as an example, one laid in 2018, can move 250 terabits per second. To conceptualize that, you can send 6,000 HD movies in one second, or about 20% of global internet traffic can go in one garden hose. How is this working? Well, at the beginning of using fiber optics, you would shine a laser down and blink your zeros and ones. But we've moved from doing that with one wavelength to doing what's called wavelength division multiplexing. That's a fancy way of saying we use the rainbow; we're using multiple. Usually, it's around 80 to 120 different colors that can go down the same cable at the same time. The other thing is we've added more fibers. So instead of there being a pair of fibers, we now have up to 12 to 16 pairs of fibers. And then the last thing is called coherent optics. This is a way of modulating the amplitude of the light. So instead of it just being on and off, you can actually have multiple steps. All of this adds up to today, probably 250-300 terabits maximum capacity in one hose. There are hundreds of hoses around the world. And I think the other thing that's really cool about this is the thing that was laid was the glass. We keep increasing the capacity of the glass because the glass is the glass—resilient infrastructure. It's kind of like railroads. It's like we're still using the tracks. Anay Shah (01:09:07): From long ago, even if the engine gets upgraded. And just like we've been talking about with this network effect, when you bring more networks in, it increases the value and the speed. And so that's a big reason why Ashburn, as we talked about, hardened. Once hundreds of carriers and thousands of cross-connects land into a single place, moving it is impossible. And so if you need to reach Europe fast from the East Coast, you're going to colocate where the undersea cable is already coming. You've got New York, New Jersey, Virginia, tying to Cornwall, England, and to Marseille. You've got the Red Sea and the Mediterranean corridor connecting into Djibouti as a critical touch point. You've got Miami as a key touch point into Latin America. Japan, Singapore, Hong Kong, Taiwan are key corridors into Asia Pacific. Mombasa and Lagos now light up Africa's East Coast. Hundreds of cables have converged that have connected the entire world. And this then forms the network of data centers that we have built and will continue to build through this story. Ben Shwab Eidelson (01:10:16): And so the boom continues, right? Anay Shah (01:10:19): No, there was something like the bust to that boom. Ben Shwab Eidelson (01:10:23): That's right. And by 2001, everything collapsed. Advertising folded, startups folded. Exodus, that we talked about earlier, filed for bankruptcy, had nearly $6 billion in debt. PSINet, one of the largest ISPs, had already collapsed. So, was this the death of the internet and the death of the value of all of this fiber that was laid? Clearly not. It was the death of a particular moment and an overbuild and a bubble. But in fact, the actual infrastructure that was built out would prove immensely valuable as services that mattered matured and business models matured. In other words, I'd say the application layer of this era died, but the infrastructure lived on and would eventually thrive. Anay Shah (01:11:03): So the dot-com tide went out. But the overbuilt assets were exactly what we needed for the next chapter. And it included infrastructure that we can't live without. The carrier hotels, the fiber, the data centers, the glass in the ocean didn't disappear. It just changed owners, and there was a fire sale, right? Assets were being sold at a fraction of the cost. A few key actors survived, and a few new ones stepped in. Equinix survived the crash. They doubled down on the real asset that they had, which was interconnection. That neutral meet-me room turned into a marketplace where competitors paid you to be neighbors because the value was in the speed, the reliability, and the flexibility that these carrier hotels provided. Meanwhile, private equity swoops in, as they will in every bust cycle, and reframes the category. So, one in particular buys a couple dozen distressed facilities around the world, turns it into a vehicle called Digital Realty Trust, takes it public as the first pure-play data center REIT, and treats compute space like real estate. These weren't high-tech moonshot assets they were buying. They're bringing patient capital in to standardize the shell, finance it cheaply, and get long leases, which becomes a blueprint for the next two decades of data center buildouts. Ben Shwab Eidelson (01:12:30): In addition to the sobering economic environment of 2001 from the bust, there was also September 11. These financial institutions were still operating in this moment, having their key servers trading information connectivity with banks in their offices or right near their offices. So when 9/11 happened, the Verizon 140 West Street Central Office, one of the largest telecom hubs in the city, was blasted with debris, and dust flooded the equipment rooms. Tens of thousands of voice and data circuits were knocked offline immediately. Most of those circuits were powering exactly that: brokerages, market data providers, the trading floors, low-latency connectivity to the stock exchange and clearinghouses. So imagine the market is just disconnected now in a flash. And this cascaded. Engineers worked night and day to bring things back online. I was looking into Morgan Stanley's experience. They saw their whole trading system go down. They had a disaster recovery site in New Jersey, but they didn't have the same level of connectivity and data feeds. So it took them a few days. They ran new fiber through building basements, patched hubs into another telco hub that was still operating to restore capacity so they could trade when the stock exchange opened on September 17th. And I think coming out of this, there was a whole reshaping of the data center world to think about resiliency in a new way. I think it showed the physicality in the city of this connectivity right in this moment, where you could take for granted that you could take action on data over there, whether a trade or market data. Anay Shah (01:14:07): And it wasn't enough to have redundancy on another floor in the same neighborhood. Ben Shwab Eidelson (01:14:12): Right. Anay Shah (01:14:12): We had to start thinking about an infrastructure build-out in different locations with different networks, facilities, and routes to really build true resilience and switch over to the point where now, if a data center goes down, there's automatic rerouting, and we don't see those same blips, although it happens from time to time. Ben Shwab Eidelson (01:14:31): This early 2000s phase is a real maturing and growing up of the entire industry to realize that these servers and data centers are holding important financial data and need to be treated as such. But meanwhile, in consumer land, there's a glimmer of light, and it's a big one. This is the era that we move from that squeaky, squealy phone modem to broadband. Anay Shah (01:14:55): I remember touring for colleges, and some had Ethernet across the campus, and others didn't. Ben Shwab Eidelson (01:15:01): I feel like getting broadband to our house was a radically different experience. It was like a different internet. Anay Shah (01:15:08): The image didn't load from top to bottom. Ben Shwab Eidelson (01:15:10): Bottom, it would just fly through. And so this is when BitTorrent starts soaring. This is when Skype launches in 2003, and you can actually make VoIP calls. Anay Shah (01:15:18): Oh, Napster was possible. Ben Shwab Eidelson (01:15:20): World of Warcraft launches in 2004. I remember that, taking the college campus by storm. Early web video products started to come out. Anay Shah (01:15:28): By 2005, you had a billion people online, about 16% of the planet. Ben Shwab Eidelson (01:15:35): So all those folks prognosticating with excitement in the late '90s were not wrong. They were just off by five or six years. And so there's also the advent of CDNs. So Akamai's footprint exploded to provide more and more storage and replication and caching at the edge. Anay Shah (01:15:52): And so what exactly does this mean? Ben Shwab Eidelson (01:15:55): It means at the places where your ISP is connecting, if someone's downloading an image—let's say you load the New York Times on your computer, and someone else does on their computer down the street—you don't both need to go all the way back to the New York Times home server for access to that photo. It's now been cached on a nearby CDN that's directly hooked up to your ISP. Anay Shah (01:16:13): So in the early to mid-2000s, you have the consumer coming back. Applications are flourishing. The internet is becoming a part of the fabric of society. And there are a handful of companies that survived the bust and captured this moment unlike any other. They not only built incredibly large consumer and enterprise businesses, but they actually became critical infrastructure companies that helped build the modern data center world. That is what we're seeing booming today. The place we're going to start with is the best place to buy books: Amazon.com. So whatever book you wanted to... Ben Shwab Eidelson (01:16:50): Find, from A to Z. In July 1995, Amazon launches. By '97, they IPO, and by '98, they are no longer just a bookstore. They're on their path to becoming the Everything Store. Now, it was not initially Bezos's and Amazon's intention to become the infrastructure provider of the world. But this high-growth moment of the late '90s set the stage for what they would need to build, not just for themselves, but for everybody. In those early days, they were running expensive, quote, unquote, reliable servers from the likes of DEC. Extremely expensive products—high-margin servers. Now, the problem is Amazon was not a high-margin business. They're trying to go for scale. They're selling things at whatever the cost was to pass through. They are a retailer trying to be the lowest-margin retailer out there. The cheapest way to get your book delivered to your doorstep. Anay Shah (01:17:45): So, running a retail business, they're always... Ben Shwab Eidelson (01:17:48): Tight on cash, so spending it on servers stopped making sense. So by 2000, they were spending so much on infrastructure they were worried this was going to bankrupt them. And so they kicked off a big project to rewrite all of Amazon.com onto Linux and to run it on much cheaper HP servers. Anay Shah (01:18:05): And this is when Amazon was famously a huge monolithic codebase. Every new category they launched, they had to work across their entire codebase. And it became this hairball. Ben Shwab Eidelson (01:18:17): By, I think, around 2002, Bezos had had enough of that, and he issued the famous API mandate that internally every team had to expose functionality through hardened, documented service interfaces designed not just to be used by internal teams, but eventually potentially externalizable. Anay Shah (01:18:33): Classic Bezos. There are no exceptions. Every team must communicate through these interfaces. There were no backdoors, no direct threads, no direct linking. It didn't matter what that technology did. It would, without exception, be designed from the ground up to communicate externally to other teams. Ben Shwab Eidelson (01:18:52): And if you didn't do this, what would happen? Anay Shah (01:18:54): If you didn't do this, you were canned. So, in this moment of reboot, it seems like the question that the company's leaders are asking themselves is: How are we able to scale our business like a software business and not like a furniture business? What if compute could scale with demand? And if we can do this for ourselves, why not rent it to the rest of the world? They were on the precipice of not just a technical breakthrough, but a business model breakthrough. Because for decades, running an online business meant these multi-year leases, these expensive servers, and over-provisioning to handle peak demand. And Amazon would go on to flip this on its head and fundamentally change internet businesses by saying you can rent a server by the hour and pay for only what you use. Ben Shwab Eidelson (01:19:47): It is both deeply innovative at the time and also funny because we've had decades—a century—of doing this with our electricity bills in our own houses or water bills, right? This is utilities, utilities. You've always just paid for what you use. But what it continually enables is the driving down of cost and better utilization of centralized infrastructure. And Amazon had just lived through this painful period of having to rewrite their software and change their server architecture. And I think it was two things. One is: we never want to go through this again. A. B. No one should have to go through this again. And C, if we start to build the infrastructure for the world, that's going to accrue to our costs and our benefit. And there's going to be a flywheel here, just like any scale economies provider ever experiences. The bigger we get, the better. Anay Shah (01:20:39): That's right. And that utilization is such a key point. So, what happens in March of 2006? Ben Shwab Eidelson (01:20:44): Amazon launches the first real AWS service: S3, or Simple Storage Service. Anay Shah (01:20:51): What does this Simple Storage Service do for me as a small internet business? Ben Shwab Eidelson (01:20:56): It lets anyone put a blob of data on this non-physical disk and access it anywhere in the world. And that sounds simple in the name, but it was shockingly hard to put a blob—whether that was a megabyte, a gigabyte, or a terabyte of data—out there and have everyone around the world be able to access it quickly. Amazon abstracted everything away so that you could do that and just pay a monthly fee. You didn't have to build a server and plug in a hard disk and build another one that copied the data over and all these other things. It just gave you what you need as a developer. Anay Shah (01:21:32): So this enabled me to store information. Ben Shwab Eidelson (01:21:35): Correct. Anay Shah (01:21:35): A few months later, what did they do? Ben Shwab Eidelson (01:21:37): They launched the Elastic Compute Cloud, also known as EC2. Anay Shah (01:21:41): So I can store, and now you're telling me I can compute. How does this work? Ben Shwab Eidelson (01:21:45): You can compute. And so what EC2 essentially was the ability to spin up computers—servers, as you saw fit. Now, what they were actually letting you spin up is what's called a virtual machine, where you can say, "I want to run Linux," or "I want to run Windows," or "I want to run some SQL Server OS," and I would have this virtual machine where I could deploy that, run it, and run whatever code I need to. And if I need a second machine, I push a button and get a second machine. If I need a third machine, a third machine. And I pay by the hour, by the machine, only for what I'm using. Anay Shah (01:22:16): This is outstanding. So I'm building a business, and I think I'm going to grow fast, but I don't know what traffic I'm going to get next month or in six months. And so I just raised a bunch of VC money. Now you're telling me I don't have to buy these expensive HP and Sun servers, and as I grow, I can just rent more compute and rent more space? Ben Shwab Eidelson (01:22:37): Yeah, not just that, but I think the activation energy here was brought way down. Before there was the challenge of getting a server and getting space and all that. But in this model, now you just put down your corporate card, and you're off and running with the foundational building blocks that you need. Anay Shah (01:22:51): This is the Cambrian explosion for startups. Ben Shwab Eidelson (01:22:53): Key to this is the utilization point. So many startups had bought servers that then never hit full utilization, or we talked about all these appliances sitting in the backroom closet not hitting full utilization. What enabled Amazon to drive utilization? It was the fact that, yes, they gave you a quote-unquote "server" to run your operating system on. They did not give you a server; in reality, they gave you a virtual machine. Not a machine, a virtual machine. Well, what is a virtual machine? A virtual machine is the flavor of the concept of a machine. And this goes back to a company, VMware, that was founded in 1998 by Diane Green and Mendel Rosenblum. Seid and I got to take operating systems in college from Mendel. And then Diane, as we get to later in the story, was running Google Cloud around the time I was leaving Google. So, legends in this field and in this industry. What VMware did is they made it possible to take a normal computer—whether it was a server or a PC, for that matter—and run virtual machines on that computer. And why that's typically hard is a computer is usually made to run one operating system at a time, and that operating system is managing applications and making sure that the computer doesn't crash. Well, if all of a sudden you have multiple machines running on a computer at a time and one does something that you can consider kind of unsafe—right?—that would stall out the machine or do something that they weren't supposed to do, that could break the whole model. But VMware's first product enabled an unchanged Windows and Linux to run side by side on the same x86 box. This accelerated to even more interesting use cases. So you can actually hot-swap VMs on machines at the same time. Let me give you a concrete example here. Let's say you're playing a game on a PC. It was as though all of a sudden, in the snap of a half-second, that game moved to another machine mid-frame. So this was actually designed to be able to hot-swap a virtual machine from one server to the next. Anay Shah (01:24:55): So it didn't matter that you didn't actually have your own server, because your virtual machine could float around as needed. So let's say that you have a hard drive crash. Well, you could have a snapshot running in the background and you could flip over to that one in real time. And so this notion of a virtual machine becomes the backbone for both the utilization point, because one physical server can be used to actually host multiple virtual machines, and a lot of the redundancy and fallback designs. Ben Shwab Eidelson (01:25:24): And VMware wasn't the only one to do this. Eventually there was the open-source Xen project. That is what Amazon and AWS first used. This kind of thread becomes better and better over time as all the hyperscalers have figured out how to maximally virtualize everything that they do. Anay Shah (01:25:38): And so now, rather than me trying to run my pets.com and ensure that my product is getting to my customer and my website is doing everything... And then when it crashes, me having to stop everything, file the ticket, and pause business to fix the crash—that's just abstracted out to the specialists, to a business that is designed to solve this problem for me. And all I get then is continuous production, continuous service. Ben Shwab Eidelson (01:26:08): And it gets cheaper for you every year. Anay Shah (01:26:10): It gets cheaper. --- Ben Shwab Eidelson (01:26:11): The cost of S3 and EC2 has just gotten cheaper and cheaper and cheaper. It's an amazing business. And for many others, as we'll get into the story, building these data centers and the utility business is not their high-margin business. But for Amazon, this business has margins, which makes it a high-margin business compared to their retail business. Anay Shah (01:26:30): It's almost like if I were an airline spending all this time trying to figure out how to book a reservation, and then you gave me a program that could do it for me. I can now focus on serving the customer and serving more customers faster, and it explodes. Ben Shwab Eidelson (01:26:45): Right. If you look at S3 in 2007, there were 10 billion items stored in S3. By 2009, that was about 64 billion. And by last year, 400 trillion items were stored in S3. Anay Shah (01:26:58): That's a really big number. Ben Shwab Eidelson (01:27:00): It's a very big number. It's a lot of items. This powers the startup ecosystem and industry. Let's go back in time. So, what was driving the growth of all these numbers? Anay Shah (01:27:11): We're in Hackathon City. We're having happy hours with engineers and folks with ideas coming together. Now, if you have an idea, you can drop a credit card down, and you don't need to negotiate and buy hardware. No invoices, no contract, no sales calls. You're up and running within hours. AWS very brilliantly saw this as a pathway for short-term and long-term growth. In the short term, they can get a bunch of early-stage startups using their compute, and that's not going to amount to a lot of money. But some of them are going to grow, and they're going to be built on AWS. So they actually had a business model of giving out free credits at these happy hours, at these hackathons, to make AWS the default infrastructure platform to build a new company. Ben Shwab Eidelson (01:28:03): I think it feeds through from that go-to-market to their product design. Their product design was deeply unopinionated about what you were going to do. It was to make it easy for you to get a server to go do what you want to do with it. Here's storage, as simple as it can be. These are the simplest Lego blocks you can build on. And so simple, in fact, that people then built what feels like the same business on top of them. Ever heard of Dropbox? Dropbox is just an S3 application for this whole early period. Anay Shah (01:28:31): It's storage. Ben Shwab Eidelson (01:28:32): It's storage. Let's make it easy for syncing files from your computer to this new cloud thing, backing them up, syncing them, and sharing them with other people. Dropbox and S3 are intimately linked, and Dropbox is built to do exactly that. It wasn't until 2015 that they were like, "Okay, we should probably look at the cost of this." They eventually moved off of AWS to their own servers because all they are is the storage layer. So that's a rare case where it made sense. Anay Shah (01:28:59): That's all they are. They're still getting $11.99 a month for me. Ben Shwab Eidelson (01:29:01): That's right. Good lock-in. The Dropbox AWS story is a classic one, but there's probably no better partnership to exemplify this time than the one that Netflix had with AWS. Anay Shah (01:29:14): Their ability to scale with these companies is really something to behold. In 2008, Netflix was still primarily a DVD-by-mail company. For those of us that remember, it had launched a streaming service as a side feature. The leadership knew that there was something here. But in August of 2008, Netflix suffered a major database corruption in its primary data center. For three days, it disrupted their DVD shipping and their streaming ability. It stopped their business. This was a huge wake-up call. The recovery was very painful for Netflix. They realized that their on-prem, vertically scaled systems were too fragile, and they couldn't recover quickly enough from major failures. The leadership decided they needed architecture that was designed to be more fault-tolerant, elastic, and globally available, because they had aspirations of being able to stream their future business all over the world. They concluded that they needed to focus on their core business. Building this out in-house was slow and costly. So Netflix actually became AWS's first marquee, all-in, public reference of a customer that scales. This proved to be vital. By 2015, Netflix was delivering billions of hours of content annually, almost entirely over AWS and their own CDN. Ben Shwab Eidelson (01:30:33): They were running thousands of EC2 instances and had all of their videos, the canonical system of record, in S3 — like the actual videos that we are watching now. They did at some point realize that it was so important for them to own the latency and cost of that last mile of delivery—that edge. This is a perfect example of a use case for CDNs. They launched in 2012 what they call OpenConnect and the Open Connect appliances. This means that they would go into those "meet-me rooms" that we had talked about before and drop a Netflix peering box that would directly connect to your local ISP, and they would do this for free. Right? The ISP just says, "Hey, we have a lot of people trying to access Netflix. Let's make it better, faster, and cheaper." Anay Shah (01:31:17): It's a win-win. Ben Shwab Eidelson (01:31:18): It's a win-win. They cut out the CDN that they were using at the time. Netflix saves money. Customers are happier. Everyone gets their videos faster. This way, when someone on the ISP accesses the newest popular movie, it's already close to them. Amazon doesn't actually even — they barely get hit in that moment. It's obviously running what's called the control plane for Netflix. Anay Shah (01:31:38): Yeah, because if I have to wait three or four seconds for that preview to load, I might not watch that show. Ben Shwab Eidelson (01:31:43): You might churn. This is what powers streaming to this day. The same model is what enables others to enable streaming at scale. All of this — this whole Netflix case study — is the perfect flywheel and customer to show that AWS can scale with you, scale in a really challenging environment, be resilient, and power global reach. Netflix was not just a U.S. company at this point. Anay Shah (01:32:05): Yeah. For anyone that saw AWS and the cloud as a concept — as risky, unable to scale, or not enterprise-ready — Netflix helped debunk that for chief information officers around the world. Ben Shwab Eidelson (01:32:22): They would just do whatever it took. I can't remember the exact story — I think this is from the Acquired episode — where Amazon would allow you to ask them to roll in a big truck, slurp up all your data into hard drives, then they'd bring it to their data center to plug in and dump all your data into your AWS instance. So, they figured out what was needed to close the enterprise customers. It's just amazing that someone known for selling books was able to so quickly build the brand around how to do this new, private, secure utility thing at scale. Anay Shah (01:32:54): So AWS continued to rethink what data centers are used for and how to build them out to serve the customer. Rather than having one mega-facility per market, they introduced the concept of Availability Zones, which became clusters of independent data centers within a region — each on separate power lines, each on separate fiber paths, linked with millisecond private connections. This Availability Zone concept and clustering was yet another piece of the data center evolution that AWS contributed to the ecosystem. Ben Shwab Eidelson (01:33:29): I feel like this notion of U.S. East and U.S. West — this is how people think about their servers now — is this abstracted notion of an Availability Zone. Anay Shah (01:33:39): AWS continued to do this globally, and this actually became a blueprint for hyperscale. Ben Shwab Eidelson (01:33:44): To end the Amazon story here for now, I think it would not be wrong to credit them with really kicking off the utility-scale cloud story, right? And getting so many things right: giving developers the simple building blocks they could use; going after cost, reliability, and redundancy; making sure to build the right guardrails for virtualization so that they could actually do this efficiently and build for the long term, using the same APIs internally so that it created a flywheel for them to move faster and bring those cost benefits back to the core business. It propelled them so far ahead in this business; they are still the leader to this day in the core cloud infrastructure product. Anay Shah (01:34:28): As a result, as we've been talking about, the data center is simple at its core, right? It's a facility with storage, compute, and connectivity. What AWS does is take the data center and enable it to build an economy on top of it. Ben Shwab Eidelson (01:34:46): To frame it another way, it used to be that if you wanted lights in your house — if you wanted electricity in your house to power those lights — you would put a dynamo to burn coal underneath your house, right? And if you wanted a website on the internet, you would buy a server and plug it into your home ISP. That is insane to us today. "What do you mean? You're going to power my house with electricity from underneath my house, right?" Instead, I'm going to hook up this shared infrastructure with all of my neighbors, and we're going to centralize our demand and build the cheapest, biggest infrastructure we can to generate the power. That's the same thing happening here for the first time. This is the introduction of the concept of the cloud, right? People have had the internet where they're interacting with publicly visible websites. Private enterprises have servers, but the idea that you have your own storage over there in a Dropbox folder, or you as a developer could build whatever you wanted and access this floating blob of data — that's to you — invisible where specifically it is, but it's out there. That was, I think, the conceptual notion of the cloud. I've come to somewhat dislike the phrase because it feeds into the invisibility of the infrastructure, right? It says, "It is nowhere. Your photos are nowhere, your storage is nowhere." Anay Shah (01:36:07): That's so far from the truth. Ben Shwab Eidelson (01:36:09): It is somewhere. Anay Shah (01:36:10): It is in multiple locations. Ben Shwab Eidelson (01:36:12): In fact, the cloud is almost the design antithesis of the glass room. The glass room where you want to show off, "This is where it is. This is where the compute is happening." Anay Shah (01:36:22): It is only happening here; it is happening right now. Ben Shwab Eidelson (01:36:25): But it served a good purpose in explaining this notion of migrating to this unknown location, and has remained pretty sticky, I think. So, while Amazon was out there selling books, a little startup out of Stanford was helping you organize and access the information on the growing World Wide Web. Anay Shah (01:36:44): Google was founded in 1998, and shortly after, it was handling about 10,000 search queries per day. By the end of 2006, it was processing the same number of searches every second. Google acquired YouTube in 2006, a year after its founding, and at the time, it was already one of the fastest-growing websites in the world with 100 million video views per day. Ben Shwab Eidelson (01:37:07): I remember folks having their first webmail-based accounts, like a Hotmail account. They'd have 2 to 4 megabytes. Well, Google notoriously on April 1, 2004, announced and launched Gmail as an invite-only service that had a full gigabyte of storage. Anay Shah (01:37:22): I remember you could get access if you referred in. Ben Shwab Eidelson (01:37:25): If you referred in, or they started going on eBay for like $150, it was a hot ticket. By 2010, Gmail had over 150 million users. And of course, with more gigabytes of. Anay Shah (01:37:36): Storage available, it was amazing. It was from the very beginning the front door to the internet. But it was also going to need to be an infrastructure company. Ben Shwab Eidelson (01:37:46): So, you go back to 1999, a year into Google's life, and an engineer named Urs is being shown around as part of his recruitment by Larry Page, the CEO and founder. Urs says you couldn't really set foot in the first Google cage because it was so tiny. The cage was 7ft by 4ft with 30 PCs arranged on the shelves, providing the world with more Google than it could handle. Our direct neighbor was eBay. A bit further away was a giant cage housing Deck machines and AltaVista. All of this was hosted at Exodus in Santa Clara, one of those colo locations that we talked about earlier. Anay Shah (01:38:21): So, Google was competing with AltaVista and all those others at the time. It was powering part of Stanford Search. Ben Shwab Eidelson (01:38:28): That's right. Anay Shah (01:38:29): And running off of 30 PCs. Ben Shwab Eidelson (01:38:32): Running off of 30 PCs in this colo center, it cost Google about $1,400 per month per Mbps of data. So they had to purchase 2 Mbps. At the time, 1 Mbps was about a million queries per day. From the beginning, Google looked at its servers and infrastructure differently. It was never interested in taking the tried-and-true path of buying the Sun or even HP boxes. Anay Shah (01:38:56): One of the most memorable stories of this time that explains this is the infamous corkboard. So, in the early days, Google engineers literally mounted motherboards on corkboard. They had $15 box fans pushing air across. They had zip ties holding it together. In a traditional IT philosophy, this is heresy. But the idea was that if a part doesn't add reliability at fleet scale, strip it away. Ben Shwab Eidelson (01:39:24): This all set in motion an ethos of questioning the assumptions and, ultimately, deep, deep vertical integration of their infrastructure. Some might argue that that was Google's superpower, and still remains so to this day. Apple's piling all of that thinking into making the perfect iPhone; Google's doing it to their data centers. Anay Shah (01:39:43): So Google had a number of innovations that came from this early time. One was around how to do power backup. So, the standard was to spend for a facility-wide big battery. Well, by 2000, Google was questioning this model and seeing the waste in the large battery. So they actually put little batteries on each server. Again, they were accepting that one might go down, but that was okay as long as the whole fleet was reliable. The key to all of this working was moving reliability up the stack into the software layer. Google was not using off-the-shelf file systems and off-the-shelf software. Ben Shwab Eidelson (01:40:22): They built everything here themselves. They assumed that hard drives would break, so they built the Google File System. This was around 2001-2002. So, this is a distributed file system that would take files, chunk them up, and spread them across three different servers no matter what. So you could always have resiliency. Also, this would make accessing a search index much faster. In 2003, they built Borg, a cluster manager, which ultimately was all about managing where jobs were running on which servers and machines. And it really was an extension of the virtualization and predates Kubernetes, which we'll talk about a little bit later. So, these were all the software components, but they were also questioning many of the physical constraints that people had looked at with servers. Most notably, they did some of the first experiments in "hot aisle, cold aisle" airflow containment. Traditionally, data centers were just a bunch of hot computers in a room, and they would blast cold air into the room. So you'd have to keep the room as cold as possible, often uncomfortably cold. There was no real thought about getting the hot air out of the room. Over time, it became accepted wisdom that you should point servers in one direction, get the hot air out of the room, and encourage that to happen. Google realized that the more you took that to an extreme, the better. So they would heat up and isolate the hot aisle from the cold aisle because then you could actually extract the hot air more efficiently and not mix it back into the cold. So, what we mean by cold aisle and hot aisle is that cold air needs to blow across those exposed chips and motherboards that would extract the heat off of the server into the hot aisle, and then the hot air from the hot aisle would need to be extracted out of the room. Google became a leader in designing the best airflow there. Anay Shah (01:42:05): This is a concept that continued for years. You also think about the traditional data center and this idea of assuming something's going to fail and building around that. Whereas in a colo facility, or when you're renting out these cages, every server had to work because it was a different company utilizing it. Not only that, 20 years ago you had specialty cleaning crews that were moving around the data center, and they would sweep up the room and then analyze the contaminants they had just swept up to understand how to continue to optimize the cleanliness and space around it. The idea of laying in a corkboard and a zip tie around it, and assuming something would fail, really takes the data center concept that existed then and completely flips it on its head and, as we'll see, drives better performance. Ben Shwab Eidelson (01:42:59): All of this comes together when they build their own first scaled, homegrown data center. Anay Shah (01:43:05): I love this idea: Don't think of a data center the way we've been defining it, as a room packed with servers, power, and connectivity. Think of it as a warehouse-scale computer. Urs wanted us to think of it not as many machines, but as one machine. Ben Shwab Eidelson (01:43:22): So, you think about the components of a computer, right? The storage, the memory, the compute. Urs saw what they were building out over time in their data centers and realized that they were just building a large computer that happened to be the shape of the warehouse. Anay Shah (01:43:38): Yeah. If you accept this premise, then you stop trying to make each server perfect, but you start thinking about making the fleet reliable, and you design entirely differently. So, this is about 2004. You have a wild man named Chris Sacca. So, before he was a famed investor, he was apparently a young, sloppily dressed individual walking around rural Oregon looking for "shovel-ready" enterprise zones where he could find some tax breaks. He was walking around asking for such astronomical quantities of power that, allegedly, a nearby town suspected him as a terrorist and called the Department of Homeland Security. But this was just what he was looking for. The Dalles, Oregon, had a site for him: 30 acres next to a decommissioned aluminum smelter that once drew enough power to power the needs of a small city. Sacca was ecstatic. He said it was visionary. This little town with no tax revenues had figured out that if you want to transform an economy from manufacturing to information, you've got to pull fiber. So Google went on to build out The Dalles, Oregon, and bring all of their innovation to bear to increase the performance of this data center. So, what that meant was low-cost, steady hydropower from the Columbia River; the Bonneville Corridor, providing high-voltage transmission; a cool, dry climate that lets you run a cooling process most of the year that's very economical; long-haul fiber that traces the river's edge; and a town that was hungry for a new tenant. This all came together to launch Google's first warehouse-scale computer to drive down unit economics, improve the performance of a data center, and therefore the unit economics of the entire fleet. --- Ben Shwab Eidelson (01:45:35): And I would argue this is potentially the first built-from-the-ground-up, hyperscale data center. They're building to solve their problems, but their problems are at this point scaling so rapidly that they're thinking about how to optimize all these things, tuned with software, tuned with virtualization, and so this is the new blueprint for the future. That is still how we're generally designing data centers today. Anay Shah (01:46:01): They had a head start in thinking about infrastructure from the get-go, making it part of their DNA. It's not as if the other companies were standing still; they were building concurrently. This was a race, and they had to do it quietly. Google was the first to get operational in 2006. They quickly replicated this elsewhere in North Carolina and then in Finland, testing different climates to optimize. This included using seawater, hot aisle containment, and other ways to optimize cooling and performance. Ben Shwab Eidelson (01:46:32): Yeah, there were other areas they kept pushing on, like the power supply system. Normally, each server you think about plugging into the wall converts AC to DC and then steps down DC for all the various components. Google instead realized that if they could just have a higher voltage DC current coming into the rack, they could then convert once cleanly and do it as late as possible. With that higher voltage, you'd have less loss, and you'd get efficiency gains across the whole path. So, at the scale Google was building servers, they could optimize every single part of the stack. Anay Shah (01:47:07): They later announced this 48-volt DC in 2016, publicly revealing double-digit efficiency gains that the world could then incorporate into data center design. Now, we've talked quite a bit about performance and efficiency, and there's good reason for that. Many data center companies were hemorrhaging money on power. The impact on the bottom line was significant enough that it needed focus. We had the distinct pleasure of talking to one of the OGs in data centers, Christian Belady. He told us this fantastic story of bringing the metric for power efficiency to the industry. Ben Shwab Eidelson (01:47:55): It's called PUE, or Power Usage Effectiveness. This is a simple ratio of the total energy consumed by a data center, divided by the actual IT equipment you're trying to power. Right? So, it very easily spits out the overhead in running your data center. If it's two, that means twice as much energy is used for overhead than for the actual servers. And how did this metric come to be? Such a simple, beautiful metric. Anay Shah (01:48:21): Christian Belady was working at HP in the late 90s, and HP had a customer in Japan called NTT Docomo. Christian had been in the industry for so long that he had formulated enough standards through HP and the industry to come up with 10 best practices. He took these 10 best practices to Docomo, and they said, "Yes, this is great, we're going to implement all of this!" He replied, "Fantastic, come back in three months and we'll do a review." So he goes back to Japan, and they had printed out everything, with big stacks of paper on the desk. Everyone was in a suit and tie in a hot room when they told him, "Mr. Belady, we've done everything you said. Here are all of our reports, and nothing's changed. The servers are all still the same." Ben Shwab Eidelson (01:49:07): "It's even hotter in the hot aisle, and no one wants to go back there. So it doesn't seem like it's any better." Anay Shah (01:49:12): "That's right. We're still hot when we go in there. We don't think this is working. We're going to go back to the way we were doing it." Ben Shwab Eidelson (01:49:18): And it drives Christian nuts. The whole point was that you actually want the hot aisle to be hotter and the cool aisle to be cooler. That separation is what drives the efficiency. You don't want the air to mix. He knew in his bones, "No, you're running in a more efficient way. You have to see that." Anay Shah (01:49:35): NTT couldn't see the change or measure the efficiency gains of the hot aisle/cool aisle containment or the other recommendations that Christian was making. So he invented this incredibly elegant metric and kept it internal to HP. They continued to implement it in HP for six years until his good friend Chris Malone suggested he publish a paper and present it at Ken Brill's Uptime Institute in 2006. Ben Shwab Eidelson (01:50:02): At the Uptime Institute, they presented the paper and kicked off a new organization called The Green Grid to publish PUE and other metrics. The team included Christian Belady, Paul Perez, Bruce Shaw, and Larry Vertel. These folks later became CTOs of Dell and led big teams at AMD. They thought it was just going to be an internal "here's the thing we tried" kind of paper. It hit the industry like a storm because the PUE race was now on, and Google wanted to win. Many enterprises had PUE numbers around 2, meaning twice as much power went to lights, cooling, and everything else compared to the amount of power that actually ran the IT hardware. Google pushed it down to 1.1, meaning only 10% of the power was not directly used to power the computers. Anay Shah (01:50:50): Their internal teams took hold of this and ran with it. They used this rethinking of the data center from the ground up as one computer system, with software being the reliability layer, not the hardware. This, along with innovative geographic placement, drove performance and PUE. It's come to a point where it essentially can't be optimized past one. Ben Shwab Eidelson (01:51:16): Before we get too far ahead of ourselves, let's talk about what it is to go to one of these data center campuses. Anay Shah (01:51:23): You're driving up into vast open space where you suddenly see massive buildings growing out of the ground. From the sky, it maybe looks like a distribution center. You can see some steam flowing out from the building. But as you drive into this campus... Ben Shwab Eidelson (01:51:42): You start noticing: "Why would there be so much power infrastructure if this is just a warehouse?" Because you're seeing big battery packs, extra turbines, or generators. Anay Shah (01:51:52): The other thing you'll notice is that security is everywhere. You'll have to pass through a series of checkpoints as you get to the front desk. Maybe they're powering Netflix, but they're also powering Pentagon operations. They have an enterprise obligation, so security is paramount. So, once you're approved, you manage to get through multiple layers of gates. Ben Shwab Eidelson (01:52:14): And then there's a moment—the moment of big reveal—when you walk in, and it finally hits you: the rows and rows and rows of machines that just go on further than... Anay Shah (01:52:24): The eye can see. These facilities are bigger than a football field, and they're lined with racks of blinking lights and machines. You're hearing this constant whir, and you're noticing the temperature. When you walk in, it might be fairly comfortable. Ben Shwab Eidelson (01:52:42): I think 80 degrees is average in a Google data center today. What did it used to be like? Anay Shah (01:52:47): You had to keep these machines cool, and the best way to do that was to chill the room. So it was frigid; the entire room was blasting with AC. Ben Shwab Eidelson (01:52:57): Back in the 80s, you'd just wear a thick sweater all the time. Now they can comfortably hover around 80. That's because of the really good containment they did of hot and cold aisles. Now you've taken in the machines, adjusted to the noise, and they've given you earplugs to get comfortable. You look up and start to see the infrastructure coming in, powering these servers and connecting them. Google famously had bright-colored wires and marked where the plumbing was. But it wasn't always like this. If you go back to the 80s, I think it was quite different. Anay Shah (01:53:27): You're now building right on the floor. But one of the ways to cool the data center decades ago was to raise the platform. So you'd actually look down and see that you're walking on panels above a hidden space where cold air was being pushed through. Ben Shwab Eidelson (01:53:41): Per the trend of everything we've talked about, with Google driving the evolution, the question was: "What is the cheapest way to most efficiently do the thing we're doing?" And it stopped being those raised floors over time. Hopefully, that gives you a little taste of what it's like to walk into one of these. There is a nice Google podcast called 'Where the Internet Lives' that really gives a nice tour of exactly this. Anay Shah (01:54:00): It's a Google and Latitude Media production where you can actually hear the sounds as they record inside the data center. So the whir is very palpable. Ben Shwab Eidelson (01:54:12): Google's investments continue to grow. They continue to vertically integrate, deeper and deeper. They go to their own sites, start buying up their own fiber. They took advantage of some of that dark fiber that we mentioned earlier, snapping a lot of it up. All of this is a capacity strategy. Ultimately, they want to have the capacity redundancy to drive their own cost down. While, mind you, they're building the highest-margin, best business in the world in the form of search and ads. So that whole flywheel allowed them to continue to invest in this infrastructure. Google had explosive demand, as they were the web company here. They had explosive internal use driving the value of the vertical integration. But the whole industry saw what was going on with AWS after its launch. And Google was one of many companies that realized they had some assets to put toward that, too. So they put together a team that launched App Engine in 2008. It let you run apps on Google's infrastructure, but was very opinionated about the app. It only supported Python with a particular framework. So it was quite different than Amazon's basic building block EC2 approach. It wasn't until 2013 that Google launched a general-purpose virtual machine capability. As a result of this approach, as well as Google's lack of enterprise sales and marketing motion, it took quite a while for their cloud program to get running. Things started to shift in 2014-2015. Google went public about its containerization strategy. If you recall our discussion of virtualization: you don't actually need to package up the whole operating system. You don't really care that it's running Windows or Linux. You actually generally just want to run the application. Containers are really a way to do that at the application level. This continued even further more recently with the idea of serverless functions. Anay Shah (01:55:59): So Google in 2014-2015 took the world by storm by open-sourcing Kubernetes and the Google Kubernetes Engine. It very quickly became the default way for programmers to use containers and migrate away from virtual machines. Now, they still didn't have an enterprise sales and partnerships muscle. But Ben, you were there at this time, right? Ben Shwab Eidelson (01:56:18): That's right. That's when they brought in Diane Greene, who we previously talked about as the founder of VMware, to help build that muscle. By 2018, their cloud share was 7%, AWS was 34%, and Azure was 15%. So they were a distant third. Following through to 2019, a new leader came in who has been the leader since, and he has made big pushes, growing massively. Their cloud business is now a $13 billion business, growing 32% net-net. You look at Google's story, and it's interesting. They've always been technically ahead, especially internally. They've always had amazing dogfooders—meaning people inside using their infrastructure. The problem was they did not know how to sell to this customer. They were not really a developer platform, especially to the enterprise. This is a market Amazon had cracked, so they really had to learn new skills despite having the best tech in the entire ecosystem. The other story is that Google's vertical integration sometimes went too far. They figured out the best way to do this for themselves, but sometimes that led to blind spots in what someone not in the Google ecosystem might need. So, the idea that every developer was going to build a new Python framework that Google released was a little self-serving when it turned out engineers might just want maximum flexibility, which Amazon deeply understood. Anay Shah (01:57:33): And it all starts with giving the industry an entirely new way of thinking about it: Treat the data center as a single, evolving machine. Build the hardware to suit the software that will survive its own failures. Put the buildings in thermodynamically optimal and grid-optimal locations, and publish your math. Showcase your performance and your PUE so that everyone can have FOMO and try to play catch-up. So, Google was spending billions on land, buildings, and hardware, designing its own infrastructure and overturning the data center world by building software to bind it all. Which forced the entire industry to meet hyperscalers like Google on its own terms. Competition was heating up. It wasn't just Amazon and Google. There were even more household names quietly transforming themselves. Ben Shwab Eidelson (01:58:28): By the mid-2000s, Microsoft was unquestionably the software king. Windows was powering 90% of desktops. Office was licensed to print money. But things were changing. Broadband penetration was climbing. We started to see the emergence of connected phones—BBM and BlackBerrys and things like that at the time. And internet-native companies. They saw Google and Amazon building, climbing, and proving that you didn't need to buy software on a CD to run interesting computing applications. Remember, the cloud, as we've been talking about, isn't just about backend developers. Applications themselves were starting to move—not just their backend, but the front end to the browser as well. Anay Shah (01:59:10): Right, on the browser. You remember, Salesforce was leading the charge here. They launched with this idea of "no software, all in the browser." By 2005, they were doing nearly $200... Ben Shwab Eidelson (01:59:20): Million in revenue and capturing the mindshare, proving that this could be the future—not just of consumer applications, but also a business product like a CRM. Anay Shah (01:59:31): CRM enterprise B2B software. Ben Shwab Eidelson (01:59:34): So you're there, sitting at Microsoft. That is your greatest fear: that you're not going to be part of the application and operating system of the future. That is your whole business. At this point, it's not like Microsoft didn't run any web services. Ten years prior, Gates had sent his memo of the "Internet Tidal Wave." They had invested; they bought and scaled Hotmail; they had MSN; they had Xbox Live. But these were all self-contained services. They hadn't yet thought about how to expose an infrastructure to others to build on. They loved building operating systems that would ultimately run on others' infrastructure. They had built one of the best businesses in history doing this. They would write this code once, print as many CDs as they could, and it was this amazing, high-margin business. They didn't have to go buy other people's hardware. Anay Shah (02:00:16): They'd print the CDs, they'd print the money. This idea of infrastructure as a service was radical. Inside Redmond, Washington, the pivot for Microsoft was framed as nothing less than existential. Ben Shwab Eidelson (02:00:28): As is often the case in a company motion like this, it's about people and talent. So they bought a company, Groove Networks, that brought in the famous Ray Ozzie, who had previously led the development of Lotus Notes. They brought him into Microsoft and made him co-CTO of the company. This was so existentially important. Think about what Meta's doing today to staff up talent. It was *that* important at the time. The cloud was the existential thing. "We need the leader of the cloud, Ray Ozzie, to come in and lead the way through this charge." Anay Shah (02:00:58): "If we're going to bet everything we've built on this pivot, we're going to bring in the best talent." Ben Shwab Eidelson (02:01:03): So they bought the company, put them in charge, and this was a 5,000-word manifesto about what the future of Microsoft needed to be. It's worth reading the whole thing, and we'll link to it in the show notes. But Anay, would you read that key section that really stood out? Anay Shah (02:01:17): In Ray's memo, he says: "'Computing and communications technologies have dramatically and progressively improved to enable the viability of a services-based model. The ubiquity of broadband and wireless networking has changed the nature of how people interact, and they're increasingly drawn towards the simplicity of services and service-enabled software that just works.'" Ben Shwab Eidelson (02:01:40): He ends that section by saying: "'Businesses are increasingly considering what services-based economics of scale might do to help them reduce infrastructure costs, or deploy solutions as needed and on a subscription basis.'" Anay Shah (02:01:54): "'—just give people the end result that they want and demystify everything that's happening.'" Ben Shwab Eidelson (02:02:00): That's right. And he ends with: "We must respond quickly and decisively." For a company of Microsoft's scale and history, that was really trying to be a... Anay Shah (02:02:09): Wake-up call. And didn't Gates and Ballmer kind of give Ray Ozzie a blank slate, like, "You're the leader here. Help us understand where the future is"? Ben Shwab Eidelson (02:02:17): Not just that, they let him carve out and run this separate from the Microsoft Server and Tools business, which had built Windows Server and SQL Server—which would have been the obvious place to try these things. But they knew that this was bigger than an evolution from what they were doing in their previous software business. This was a transformation: a fundamentally different product, a different business model, and a different go-to-market motion. So when everything had to change like that, it was very hard for the leader of the incumbent thing to keep going. Let's make this concrete. What does this mean for a server or a data center? Microsoft had Windows Server and SQL Server. When we talked about those server rooms that a company would run, they would very often be running Microsoft software. This was the IT sale they had perfected by this point. You would sell the Windows Server license, it would be Active Directory, it would be running Exchange and Outlook for your email, and it would be running your calendar and all these things inside your company's network. It'd be running Windows on their PCs. Anay Shah (02:03:15): So it's this beautifully connected thing. And this is saying, "No, no, no, we've got to move that server from your bottom line CapEx as a company into OpEx in our data centers." This is a transformational move from what you're providing at the end of the day—from just software that's going to run on someone else's computer—to actual services. Ben Shwab Eidelson (02:03:32): Ray Ozzie goes for it. He staffs up a team they call Project Red Dog. They bring in Dave Cutler, who had previously led Windows NT and is a legendary programmer, to help lead the architecture of this new project. Anay Shah (02:03:46): So they're putting together this cloud strategy that is revamping Microsoft internally, and at the same time, ramping up direct competition to the front door of the Internet: Google. They evolved an old MSN Search product into what then became Live Search and eventually Bing, as we know it, in 2009. This was a moment that kind of rippled across the industry, especially for Google's dominance. Ben Shwab Eidelson (02:04:12): I was at Microsoft at this time. Anay Shah (02:04:13): Oh, yeah. Ben Shwab Eidelson (02:04:14): I joined Microsoft out of college in 2008 and was there in this moment: they had Windows Live Search. They briefly called it Live Search, then came this big rebrand to Bing. Microsoft was willing to spend a lot on this repositioning, on this marketing—so much so that they gave Yahoo a very sweet deal to power Yahoo's search and ads and gain some infrastructure market share. Anay Shah (02:04:35): This catapulted them into getting up to 20% share, which is no small feat in an exploding market that Google has been dominating. --- Ben Shwab Eidelson (02:04:45): It's important to remember that doing search well and search ads well is a really hard scale problem that essentially only one other company had cracked, and that was Google. To respond quickly with an active, updated index of the internet, and to do this auction for the right ad unit—all of these things that you have to build—is what propelled Google to build their vertically integrated servers. And Microsoft was now putting themselves in the position of having to solve those same really hard technical problems, in addition. Anay Shah (02:05:12): To those really hard technical problems, we saw with Google that in order to effectively run this search business, you have to have a lot of compute power behind you. You now have to get into the data center business and build out infrastructure that is your own to drive down the cost and drive up performance. We mentioned Christian Bellady previously. He was at HP in 2007, gets a recruiting call from Microsoft, and his initial reaction is, "What the hell's a mechanical engineer going to do at Microsoft?" Because Microsoft is known as the software company, and so he turns them down a couple times. After the third call, he decides to go up for an interview, enjoys the process, gets an offer, but doesn't think much of it. And I believe he's at his parents' house when he suddenly gets an email from Bill G. (Bill@microsoft.com) explaining why he should accept the job offer. And Bill essentially says, "Everything's going to the cloud. We're investing for the cloud business, and we need to build out this new infrastructure as core to the future of Microsoft." Ben Shwab Eidelson (02:06:24): This seems like a wacky idea to go there, but Bill G emailing me? Like, you're there with your parents, and you've got to give this a go. Let's pack our things, move to the Seattle area, and join Microsoft. And so at this point, he starts there, and Microsoft had just finished building their first real, more integrated data center build-out. This is in Quincy, Washington, in Eastern Washington, a 13-megawatt build. And he walks in there, and everyone's looking around, and the first thought they have is, "We're going to get fired because there's no way we're going to fill this thing with servers." There's just no way that we have the demand. Anay Shah (02:07:00): And this was their first massive-scale, purpose-built cloud data center campus. This was Microsoft entering the infrastructure business. Ben Shwab Eidelson (02:07:11): And similar to what we saw with Google at The Dalles, a lot of the same things that attracted them to Quincy were power from the same Columbia River. They're getting cheap 1.9-cent per kilowatt-hour power at the time to power this. And the climate was that dry, cool climate that allowed them to be really economical with using outside air cooling for much of the year. They had multiple fiber routes. It's in Washington, so they could connect their Redmond headquarters directly to Quincy. Anay Shah (02:07:34): The environment, the fiber, the power—it all comes together. And they, like Google, knew they had to work with the local community and the local government. And so they actually worked with the City of Quincy to build a Quincy water reuse system to treat and recirculate the cooling water to reduce the dependence on the local community. Ben Shwab Eidelson (02:07:53): They built Quincy. They have it up and running. He's worried they're not going to fill it, but they kept going. Azure ends up launching in 2008, which we'll talk about in a moment. But I think it's very easy for a big company that's bet a lot of resources building something internally to think that they have an idea of what demand is going to look like. But they overbuilt in Chicago. Then they had it mothballed, they tried to sell it, and then nine months later, they were so happy. They didn't sell it because they needed the space. Anay Shah (02:08:16): It was hilarious. They tried to sell it to the government. Ben Shwab Eidelson (02:08:18): So, what's going on there, and what's happening, is they're launching this project, 'Red Dog,' which would become known as Microsoft Azure. And it's hard to predict the scale of the utilization of infrastructure as you launch it. So let's go back to October 2008—the big reveal. This is Microsoft's big annual Professional Developers Conference, and Microsoft unveils not Microsoft Azure at the time, but Windows Azure. Anay Shah (02:08:46): And this wasn't just Windows running somewhere else. It's a platform for developers to build and host applications on Microsoft infrastructure, paying only for what they used. Rent some storage and compute, pay for what you use. Ben Shwab Eidelson (02:09:00): And the key thing that you mentioned is, it was designed for those Windows developers. So it used all the familiar tools: the .NET Framework and Visual Studio and all these things. And it made it easy for you to take your work that you'd done over there that would run on Windows Server right inside of your office, and take that same code and shift it to run on Microsoft servers. What a powerful integration and move! It kind of split the thread between AWS, which gave these really basic building blocks—S3 and EC2—of this unopinionated 'run your own operating system, bring your own full thing, and we'll just make it work' approach. Anay Shah (02:09:35): Right. They were agnostic to what you were doing on top of it and how you were doing it. Ben Shwab Eidelson (02:09:37): Exactly. To Google App Engine, which was overly specific and cute about it, in a way Microsoft was almost more Google's approach, but it was the same code and the same thing that a developer had already used. And so they had this massive developer ecosystem and integrators, and this whole kind of motion around it. Anay Shah (02:09:54): It wasn't just easy for the developer community; this is Microsoft that has an enterprise sales engine. They've got a go-to-market motion. They have the finance department that knows how the sales team is going to operate and how to budget for it. And so they're bringing this world-class distribution and, most importantly, the trust from this massive customer base to bear with Azure. And the adoption is extremely rapid because people are already embedded in the ecosystem, and here you are coming with an improved product, and the enterprise base adopts it very seamlessly. Yeah. Ben Shwab Eidelson (02:10:28): And it's not the story that Amazon has to make for the first time, which is, "Hey, come build your thing with us in the cloud," and introduce this. This is four or five years later, and it's sitting there like, "How do we think about the cloud?" We don't have an agreement with Amazon; that's not an existing vendor relationship. Whereas with Microsoft, you're like, "This is the agreement we have, this is the IT relationship we have, this is the full trust." If you're telling us this is how we can move to the cloud in a seamless, secure, safe way that's going to integrate with Active Directory and Outlook, and I just don't have to run the machines in my closet anymore? Sign me up. Ironically, I think some of the people they had the hardest time signing up were the engineers and product managers in Redmond, Washington. We were actually talking with Ben Gilbert of Acquired about this, and he at the time (this was 2012, 2013) was working on the web version of Microsoft Word. So Microsoft is finally going to compete head-to-head with Google Docs on product and ship a version of Word that you can use via your browser. And of course, the desire was for everyone to use Azure. Did they? Absolutely not. They said, "No. This is the custom server we need. We need this. Many of them can scale this big, and we had to do some custom JavaScript rendering, all this stuff." We want our own servers, probably still in Quincy, but we're not using the Azure APIs and layers. Anay Shah (02:11:43): So the own employees are demanding all this heterogeneity, resulting in a wide variety of server SKUs. Ben Shwab Eidelson (02:11:52): Amazon, for context, building out AWS, I think had something like 20 different types of machines that were running AWS. Google, in their whole build-out, standardized on this vertically integrated, commodity-built server design. And so they would just keep it down to a handful of machine types. So supposedly at Microsoft, there were dozens and dozens of different SKUs that Microsoft was maintaining. So instead of calling this a server farm, someone called it a server zoo. You just think about where they're coming from in the evolution of all these different web services and different teams building their own thing. It's a whole different world. And around this time, I think energy starts to become an interesting layer as well. Anay Shah (02:12:31): So similar to Christian getting recruited in 2007, thinking, 'What the hell am I going to do at Microsoft?' he then goes out to recruit an energy expert. Brian Janes. In 2011, Microsoft saw the need for an energy person to be in-house. Ben Shwab Eidelson (02:12:47): I'd say Microsoft saw the need, but I think Brian was confused. Anay Shah (02:12:51): Brian was like, 'I do energy, you don't need an energy person.' And his thought was, 'This is a dead-end job.' Being an energy person at a software company doesn't make sense because what they're trying to do is all about land, it's all about fiber. Energy's a distant third or lower down the list in the priority of how you think about scaling this type of infrastructure. Ben Shwab Eidelson (02:13:10): Yeah, I think at this time, for context, utilities were happy you'd show up with your 10 Quincy, 13-megawatt data center, and utilities are like, 'Sure, great, sign up!' Anay Shah (02:13:20): Yeah, generally power demand is relatively flat. There's excess power available, and they're happy to sign you up and increase their profits because it's not going to drive any new infrastructure needs on their end yet. Ben Shwab Eidelson (02:13:34): And how things change! This is 2011, 2012. Fast forward a little more than a decade to where we are now, and we'll get there. Anay Shah (02:13:43): But Brian took the job and ended up building a phenomenal team. And within a few years, his team was actually the decision-maker of where they were going to build out Microsoft infrastructure. And so it's a lead-in to us actually starting to care about where our energy is coming from and the whole complexity around where you're sourcing and doing these clean energy build-outs that Brian's team was leading. Ben Shwab Eidelson (02:14:03): The team thought it was a real kind of planning challenge, right? And so they're bringing this up to leadership around, 'Hey, we need a lot of money to invest now to build this out.' Anay Shah (02:14:12): They're doing their annual budgets, they're projecting years in advance, they're getting into their mid-year review. Ben Shwab Eidelson (02:14:19): Microsoft's in this weird transitional phase. They have Bing that they're still growing; they have other services. Xbox is booming, Azure is still early innings. So it's not the dominant use of their internal server build-out, but it's a lot of different teams to juggle and predict. And so this forecasting challenge is a real one. You bring this to leadership, and what are you there to say? Anay Shah (02:14:38): And you've talked to all the business groups to figure out what they're thinking, and you go to Ballmer with your prepared notes, and within a few sentences you get interrupted, and Steve Ballmer's like, 'Ah, the business groups don't know what they need. Let me tell you how to do this.'" Ben Shwab Eidelson (02:14:52): And he says, "Give me the Excel sheet. You just draw a straight line from how growth has been going the last few years, and you just project it out, and you just build the data centers that that line shows you to do." Anay Shah (02:15:04): Just a straight line from here to 2020. The business groups, they don't know what the plan is. Ben Shwab Eidelson (02:15:08): There's actually a lot of wisdom to what Ballmer's saying because you realize that every team is trying to be excited about what they're building and Azure is going to have all this demand, but no one really knows—especially more than 12, maybe 18 months out. So how can you plan if you're building long-term, multi-year build-outs for what anyone's going to need? Anay Shah (02:15:28): This moment of growth? It was quite difficult. And Janes' team found that he was consistently under-forecasting. But he did go back many, many years later, and he was kind of curious: how accurate were his forecasts compared to the straight line in 2020? Ben Shwab Eidelson (02:15:43): He looked back, and Ballmer was off by how much? Anay Shah (02:15:46): 5%. Ben Shwab Eidelson (02:15:47): 5%. So they should have just followed the line. So sure enough, Microsoft keeps at it, and the power team at Microsoft—the data center team at Microsoft—becomes one of the biggest builders in this industry. They shift from leasing square footages to megawatt-based deals, as they become a major, major hyperscaler. Anay Shah (02:16:08): There's a big learning journey for them, right, because they were a software company, and they had to grow very quickly into an infrastructure company. It was not an easy pivot to make, and they had to do that globally. The Microsoft team had the blessing of this entrenched enterprise customer base that was very loyal, very profitable, and very global. And so their go-to-market strategy was actually to follow their customers. And that meant that within a few years of launching Azure, they were getting calls from customers in Frankfurt, in Amsterdam, in Singapore. If their German multinational had data sovereignty questions, Microsoft would go and stand up capacity in the region. And so by 2015, they had announced over 20 regions, which was actually more than Amazon at this time. Each anchored on an interconnection-rich metro and typically supported by entering with colocation facilities—perhaps the Digital Realties of the world—before moving on to dedicated Microsoft campuses once they built up enough critical demand. But this go-to-market strategy forced them to innovate and expand all over the globe, and it drove massive results quite quickly. Ben Shwab Eidelson (02:17:33): And to that point, you fast forward to today, and Microsoft's business is in significant part the Azure business. Azure is a core growth driver. It's now 75 billion of revenue a year and growing 40% year-over-year. And so this company that was 'the software company' ('we write software, we sell software, those are our core platforms, those are our products, that's what we do') is no longer just a software company; they are, in large part, a data center company. So we've gone through AWS, Google and GCP, Microsoft and Azure. But there was another company that many thought for many, many years was just a toy. Let's talk about Facebook. Anay Shah (02:18:16): The Facebook, the Facebook. Ben Shwab Eidelson (02:18:19): In February 2004, Facebook's running out of a single server in Zuckerberg's dorm room at Harvard University. Early days caused crashes. In fact, many of his earlier products, including FaceMash, the 'hot or not' for Harvard campus, melted down the servers in the Kirkland House, and the IT department at Harvard was not pleased. But then it becomes a real company, right? Anay Shah (02:18:40): And they start expanding to other campuses, and you've got different schools putting on different servers in different data centers. Ben Shwab Eidelson (02:18:49): In a way that I think creates automatic resiliency because if the server crashed for, whatever, Penn, it's not going to bring down Harvard. Anay Shah (02:18:56): And this isn't their concern, right? They're building this viral, game-changing platform that is exploding, probably beyond their expectations. And new campuses are signing up. So they're not really concerned about the infrastructure, but it's just growing organically. By the time they launch News Feed, you've got hundreds of millions of people refreshing this site, uploading photos by the billions, opening up Messenger just as a reflex of the thumb. Ben Shwab Eidelson (02:19:23): And the company's spending all this money; they raised VC dollars and they're not happy spending all this money on fancy servers and colo cages. Anay Shah (02:19:29): And so they're realizing, 'Why is everyone making so much money on us?' There's got to be a better way to do this. And there are two broad ways that they go about this. One is similar to what we've seen with Amazon, Google, and Microsoft, which is the thought that, 'Hey, we can do this better, cheaper if we build our own custom data center.' And so their first purpose-built facility was announced on January 10th and began operations in 2011 in Prineville, Oregon. And they soon followed with North Carolina, Sweden, and Iowa, and expanded rapidly. But Prineville was, just like all the others, quite intentional and quite strategic. Ben Shwab Eidelson (02:20:09): Yeah. And so similarly, in this kind of Northwest region, you had cheap power, you had a dry, cool climate, and you had Oregon's incentives starting to come into play where they offered long property tax abatements on any improvements they made. All of this combined to make a great site for their first 'Lighthouse' data center. And if you look at the numbers of server growth, if you go back just to 2008, they had 10,000 servers; in 2009, 30,000 servers. Supposedly by 2010, when they are really kicking off and building the Prineville project, they have around 60,000 servers. They're trying to figure out how to scale up. Anay Shah (02:20:42): And so they decide to engineer the building and the servers together. This very purpose-built facility. And just like the others, they realized that they can beat industry standards. And so they very famously and publicly came out with this first purpose-built facility with a very aggressive target of a 1.15 PUE. Industry average is 1.5. Historically, they were north of 2, and they were able to report 38% less energy, 25% lower cost against prior facilities. And do you think they ended up beating their PUE? Ben Shwab Eidelson (02:21:15): No way. Anay Shah (02:21:16): Smoked it: 1.07. Ben Shwab Eidelson (02:21:20): So that means that only 7% of the energy going to run the data center went to anything other than powering the IT equipment. That is phenomenal. Anay Shah (02:21:30): Amazing. --- Ben Shwab Eidelson (02:21:30): Did they keep this to themselves, how they did this? I think this is what sets Facebook's approach apart. Anay Shah (02:21:38): This first part they needed to do, and they executed extremely well. But the second thing they did was far more revolutionary. In a very secretive world of data center development where each hyperscaler kept their builds to themselves, in April of 2011, Facebook open sourced their blueprints. They announced the Open Compute Project. Ben Shwab Eidelson (02:22:01): It's hard to overstate how radically different an approach this was. This is an industry that kept the design of the servers in the data centers extremely secretive. They viewed that as core IP, differentiation, and notions of security. Google would publish papers, especially on things like PUE, and be visible about metrics that they wanted to highlight. But they famously didn't let anyone into their data centers until this really changed the game. Anay Shah (02:22:29): Their motivation is pretty interesting. They look around at all the other big data center companies and they're seeing all the margins that are being made. They're like, 'Well, wait a minute, if we publish our blueprints and we get everyone else to buy in and publish theirs, that means we can drive down the costs by standardizing what we're building.' Ben Shwab Eidelson (02:22:50): Ultimately, what they want is for their suppliers to be in increased competition. The best way to do that is not to make one deal with one supplier, but to say, 'Hey, suppliers, this is what we need, this is what we like. You make this, we'll buy it as long as it's the cheapest one out there.' Anay Shah (02:23:04): Yeah, it flips the power dynamic. Now, instead of the vendors and suppliers dictating what the specs are and having multiple different specs for multiple different customers, they're able to standardize it and say, 'You're going to respond to the Open Compute Project standards.' Ben Shwab Eidelson (02:23:17): And it was directionally aligned with all the innovation that we saw at Google. There's actually a big parallel echo here, right? Google built all of these in-house software orchestration tools that we talked about that made the data center reliable. Facebook grew up in that open-source world and benefited from that, and was able to build on top of that. So it's not surprising that they're the ones to look at the hardware design and say, 'Why is this any different? We don't want to be the sole developers of this part of our infrastructure.' It actually benefits us to have other developers and the ecosystem in part because — and this really gets to a business model motivation — we're not trying to sell that innovation. We're not trying to sell anything to another business other than ads on our feed. The better and the cheaper this infrastructure is, the better our business margins are going to be. Anay Shah (02:24:05): Yeah, we want to take this cost item and reduce it, so we can put more of our budget, more of our focus, onto the core business. They're looking at it and they're like, 'Well, the industry is using a 19-inch rack because that's what the telcos did.' 'Well, that's not suiting what we needed. There's no reason for the 19-inch rack. It was just a legacy element.' So Facebook decides to do it differently. They widen it to 21 inches and publish that to make it more efficient and fit their needs better. Ben Shwab Eidelson (02:24:33): It's this funny thing how companies are open strategically, so it's important to be clear-eyed about what they're open about and what they're not. Would Facebook publish publicly the exact way that their News Feed ranking works? No. But that's very different than this infrastructure layer that benefits them when the ecosystem adopts it. I think one thing that's amazing about it is how much the industry rallied around it and started to join it. OCP ripples across the industry. Microsoft starts bringing their designs into it. Google contributes big improvements, like we talked about, their 48-volt design. Telcos and everyone joins this, and it has the intended desired flywheel effect. Anay Shah (02:25:12): It did normalize the idea of transparency, and this was right around the time where companies like Google at first were publishing their PUE. So, this notion that we can be transparent with our results as a way to drive the industry forward, bring down costs, and make this whole part of the business easier for everyone, spurred more competition. Ben Shwab Eidelson (02:25:33): This kicked off the sustainability race among these four hyperscalers. I would say this started all the way back in 2007 when Google announced that they would achieve operational carbon neutrality. Anay Shah (02:25:43): Each of the companies followed, and they were serious. Right? Microsoft committed to carbon-neutral operations in 2012. It drove site selection, their power purchase agreements, and how they negotiated and determined who they were going to work with and their relationship with their utilities. Ben Shwab Eidelson (02:26:01): And it just accelerates. Amazon kicks off the Climate Pledge to get net-zero by 2040. You have zero-waste goals. Microsoft announced water-positive by 2030 in 2020. Google catches up with a similar announcement a year later. So you just have this era of accelerating commitments and goals. I think it's good to ask, why is this? Does it just feel good? Having worked at some of these companies, I do genuinely think the leadership wants the company to have a positive environmental impact, and it is the economically right thing to do as well. These goals of efficiency lead to your data centers, which are a growing item of operational scale, costing you less and less. The other thing this does is catalyze different renewable buying and procurement habits, right? These teams, like the one led by Brian Janis at Microsoft, become energy-buying procurement machines. They're looking to help accelerate that next solar project, that next wind project, to then be able to buy — via what's called a virtual power purchase agreement — the accounting for the clean electrons. Google pushes this even one step further to say, 'Hey, we want to buy electrons that are generated at the same time that our data center is using electrons.' So we want this notion of 24/7 carbon-free electricity. This really pushes the clean energy ecosystem forward, I think, in a material way. If we look at these three commercial cloud businesses of AWS, GCP, and Azure, they are behemoths, as we said. Azure hit $75 billion last year, GCP surpassed $50 billion, and AWS was on a run rate of $111 billion for that business. You total that up, that's $236 billion a year spent only on the direct cloud infrastructure businesses. Anay Shah (02:27:51): Let's put this in perspective. That's $236 billion across these three companies spent on the infrastructure. US consumers spend $500 billion annually on electricity. So we're talking about half the spend of all residential electricity across this country. Ben Shwab Eidelson (02:28:10): So I think it is safe to say that we have now entered the utility era of computing. It just so happens that the utilities are Amazon, Microsoft, and Google. Meanwhile, Meta, first of all, doesn't want to buy power from those three utilities. They also use so much themselves that they want to build their own and have no interest in selling access to it. They just need their own data centers to run everything that they're doing in-house at such an immense scale. So we end the 2010s with a very mature cloud, right? I think it's almost in the background. People don't even need to talk about and explain the cloud anymore. It's like, 'Of course, this is how you're going to start and run your company.' The functionality is all grown up. All of the companies are now running containerized Kubernetes things. Your services are portable. Yes, they have their functional differences and their sales differences, but they're all kind of mature and stable, multi-region. All of this stuff going on. Anay Shah (02:29:04): Nobody asks, 'Do you have a cloud strategy anymore?' It just is the way business is done. Ben Shwab Eidelson (02:29:08): You also have the new startups that have built out the missing pieces. You have the Snowflakes of the world that are helping build more specialized databases. You have Datadog helping you with orchestration — real significant public-scale businesses that have built the missing components of the cloud moment. Anay Shah (02:29:23): And the feeling right now is one of maturity of the cloud infrastructure, maturity of a new large-scale data center build-out, and at the same time, a huge pressure to continue to build. There is a global race; it is fierce, it is competitive, and it has many, many more players than just these hyperscalers. To give you a sense, Microsoft scaled from 35 regions to 75 regions over the period of a couple of years. So they were adding an entire region a month. This is data center region build-outs. Deal size was ballooning at the same time. A decade ago, a large lease might have been 5 megawatts. You fast forward to the end of the 2010s, and every one of the hyperscalers is reserving 100-megawatt campuses. So, a 20x growth in the size of the data center over this decade. Ben Shwab Eidelson (02:30:21): These two big points fit. The whole idea that the cloud now can be taken for granted and invisible is because of these scaled, global build-outs. You don't get to do that if the thing isn't just working as usage is exploding across the entire industry. Anay Shah (02:30:35): The industry has matured around private equity money coming in. You've got developers treating this as a real estate asset class, and they're stockpiling land, pre-building substations ahead of demand because they know what the hyperscaler or the next large company is going to need, and they're going to snap up that capacity. So this build-out is fierce on land. Ben Shwab Eidelson (02:30:56): Not only is it fierce on land, it's fierce in the seas. It's not enough just to have your own servers and your own buildings and your own power infrastructure. You need confidence that you can stay connected. The best way to have confidence is to put your own cables underwater. So instead of just renting capacity, which of course they continue to do, Microsoft, Meta, Google, and others start financing and building their own undersea network. This way, they can control end-to-end. You think about Google's vertical integration: it starts with the box, it goes to the rack, then it goes to the building, then it goes to the campus, then it goes to the wires that connect the campus. That's how you get confidence in what you're building. This leads to improved economics and improved performance. Anay Shah (02:31:38): And performance continues to drive forward. But by the end of the 2010s, PUE had plateaued around an incredible 1.1. Only 10% of the power being used in this facility is not directly for the server and IT equipment. So, as we talked about in the sustainability race, the bragging rights had shifted. The marketing teams had moved off of PUE, and now they're talking about carbon intensity (CUE). Now we're talking about water intensity (WUE), and the holy grail metric here is who is launching 24/7 real-time renewable-powered data centers. Everyone's putting out their climate pledges, building into their plans, and driving the entire renewable energy and PPA world forward through this genuine focus on carbon intensity. Ben Shwab Eidelson (02:32:28): Intensity. The cloud by 2020 was doing what it needed to do. So we now had this scaled, mature global technology for storing things, computing things, connecting all of us, streaming content, and powering video calls. Why might that be helpful as we go into 2020? Anay Shah (02:32:48): Well, Ben, we're now upon everyone's favorite moment in recent history: the dark days of the COVID pandemic — a time where we all had to stay at home and a time where society, systems, public health, everything was stretched to the max. The same actually goes for our data center infrastructure. While the 2010s saw fast, consistent growth of cloud computing, COVID compressed five years of adoption into about 18 months with everyone at home. Video calls, gaming, online collaboration, telehealth, e-commerce, SaaS — everything surged all at once. We all remember the endless video calls, but let's put that into perspective a little bit. Ben Shwab Eidelson (02:33:35): Going into this era, Zoom had something like 10 million folks as daily meeting participants. Fast forward to March 2020: they hit 200 million in four months. One month later, 300 million. So, just explosive growth. Google Meet similarly saw 3 million new users per day in April. Peak usage was up 30x since January, and gaming was exploding. They're at home, looking for entertainment, so Steam is shattering records. Anay Shah (02:34:03): Netflix and YouTube famously had to throttle down the network speed to lower resolutions to avoid essentially breaking the Internet. Ben Shwab Eidelson (02:34:11): I was at Stripe at the time, and to the e-commerce point, it was just this explosive moment where, sure, you saw some businesses, perhaps in travel in particular, struggle and go under. But the majority of Stripe's users were exploding. Everyone was ordering stuff online. Instacart and DoorDash were booming. It's easy to say five years of growth in 18 months. What does it actually feel like when you're working on the infrastructure? It feels like everything is breaking, and you're trying to make sure no one in the outside world feels it. We managed to accomplish that, and I'm very proud of that. The API stayed up, and payments were processed even as they scaled rapidly. Anay Shah (02:34:45): Another company in the crosshairs that seized the day was obviously Zoom. They had to scale infrastructure from 10 million to 300 million users in months. They added servers in colocation metros, they used AWS, they used Azure, and they expanded aggressively on Oracle Cloud Infrastructure. They were just doing everything they could to ensure that those video calls didn't have delays. Ben Shwab Eidelson (02:35:09): Under the hood, the way Zoom works, you can think of it split into two: the Zoom Control Plane, which was making sure that it would respond to your logins and understand what you are as a participant in a meeting. But then you actually had the video feed, and that would have these meeting zones where Zoom would make sure to connect you to the closest meeting zone in a carrier-dense building. So it's kind of very similar to the CDN networks that we talked about earlier with Netflix, but it had this dual, bi-directional nature to it. They were in a race to add more of these peering machines in those telco hotels that we talked about earlier. Anay Shah (02:35:41): And Zoom became proof that the cloud's elastic promise was real. In the greatest stress test of its time, capacity could materialize as fast as humanity demanded it. This was the moment. We were in an era of exploding demand, and there are a few key themes that affect the data center world in this moment. One of them is famously known as ZIRP. On March 15, 2020, the U.S. Federal Reserve cut its target rate to the range of 0 to 25 basis points. What does that mean in the infrastructure world? That basically lowers the hurdle rate for your risk for speculative builds. So now campuses and shells can be built out way ahead of leases being signed because the carrying cost of that infrastructure was so low, because your cost of capital is so low. Ben Shwab Eidelson (02:36:33): You're buying a house. This was the time when your interest rate on the mortgage was as low as possible, so you could buy more house. Well, if you are a company trying to build data centers, it's the same story. You can buy more data centers. Your same amount of principal is going to go much further in building out more. Anay Shah (02:36:50): As a company in this infrastructure environment, you can take more risk. Ben Shwab Eidelson (02:36:54): The hyperscalers took advantage of this and started pouring money into building new data centers. But so did Blackstone. In 2021, Blackstone purchased QTS Realty Trust, one of the largest data center builders in the industry, for $10 billion. Anay Shah (02:37:07): There's so much capital floating around, and the beauty of the data center asset class was that it had started to look very predictable. It was looking like utility-style returns. Everyone's investing in the REITs and the infrastructure debt. This is fantastic for the counties that were building this out, namely Loudoun County, which we've talked about before. Right? Their budgets were suffering in the COVID era, but because they're Data Center Alley, they saw their server equipment tax go past $400 million annually, and that prevented them from needing to raise property taxes to fill their budget gaps. Ben Shwab Eidelson (02:37:46): I think at some point their vacancy was below 1% in all of this. So there's just so much demand for space, colocation, interconnection. Everything is just flying off the shelves. Anay Shah (02:37:56): Another major theme here is what is defining the build-out. What you find now is that it starts to grow the panic around the urgency for electrical supply. It was both the amount of supply of power that you want, as well as the time to power, because there is this capital looking to go to work. The question was actually, 'How soon can you get me the power?' Ben Shwab Eidelson (02:38:23): This is new, right? The scale of the data centers earlier — these 5, 10, 15-megawatt data centers — didn't necessitate a big conversation with the utility. We're now talking into the 50 to 100-megawatt builds, and this changes the dynamic with suppliers. We had folks in Ireland saying for the first time, 'We're going to pass regulation that says the utility can block a data center from coming in because it's too much load.' Anay Shah (02:38:47): You can't just walk up to a utility and plug in for 100 megawatts. For the first time, grid capacity and planning was a throttling factor in how fast data infrastructure could grow. Ben Shwab Eidelson (02:39:01): The era from 2010 to 2018-2019, which was this big shift to the cloud that we talked about, was one of increased efficiency in these systems that ended up balancing out the increase in demand and capacity. So there's actually relatively flat power-use growth despite there being so much growth. While there was more demand, it was a steady increase, and the efficiency jumps could go in concert with the growth jumps. Anay Shah (02:39:30): So you buy a new phone, you buy a new computer — most of it's all the same, except you just get more processing power because of Moore's Law. That brought about these efficiency gains. We were building a lot of new data centers and compute, but total energy consumption of data centers actually remained relatively flat throughout this period. It was largely due to a few factors. One is that we moved into the... Ben Shwab Eidelson (02:39:54): ...cloud, and everything we talked about earlier with utilization can then come into play, right? Anay Shah (02:39:59): Exactly. One study found that the cloud is 93% more efficient than running your compute power on-premises because you have less wasted assets combined with everything we talked about with PUE and driving down that efficiency. Ben Shwab Eidelson (02:40:14): Well, now all of a sudden you had COVID plus ZIRP, where you're not waiting to get more efficient; you're just trying to build more and more. Anay Shah (02:40:22): We'd also kind of run out of that low-hanging fruit, right? PUE is at 1.1. Ben Shwab Eidelson (02:40:28): So if you look at the big four that we talked about before — Amazon, Microsoft, Google, and Meta — between 2017 and 2021, in those four years, they doubled their energy use to 72 terawatt-hours in 2021. Anay Shah (02:40:40): So much greater demand, combined with a loss of the PUE benefits and the cloud efficiency, means we're in a new paradigm of power usage, and the industry is having to reckon with that. Ben Shwab Eidelson (02:40:52): It's pretty amazing that we had such a flat period for how much growth that we just talked through. I mean, the whole scale-up of the cloud led to growing usage, while the efficiency gains were so significant. We've run out of those tricks. Right? Anay Shah (02:41:05): Right. Ben Shwab Eidelson (02:41:06): The hot and cold aisles are contained. We've virtualized everything; we've containerized everything. We need to come up with some new tricks. I hold out some optimism that necessity is the mother of invention, and we'll figure out some new tricks because we'll have to. Anay Shah (02:41:18): We have to. --- Ben Shwab Eidelson (02:41:20): That being said, there is an extreme way that a new power user is showing up in this era. Let's talk about the crypto miners for a bit. Anay Shah (02:41:29): Let's talk a little bit about crypto because it's not just a hobby, but actually ends up being a competitive buyer in the data center infrastructure build-out. Ben Shwab Eidelson (02:41:40): By 2022, 100 to 150 terawatt hours. So that's almost 50% of what the rest of the data centers are consuming now going to crypto mining. Go back to 2009, Bitcoin launches, and it's pretty much just a dark corners of the internet thing, right? Someone mining on a PC in their bedroom. Then, finally, folks figure if you buy GPUs off the shelf, you can go faster, and people start even designing custom ASICs that are designed to do nothing but mine Bitcoin and other coins. By the mid-2010s, mines had become significant power loads, and folks had started to build these out wherever they could find cheap electricity. Anay Shah (02:42:14): And while the technology behind crypto was interesting from a business perspective, it was less of an IT business and more of a power arbitrage business, because it was all based on how efficiently you could mine and where you're doing it. Ben Shwab Eidelson (02:42:29): Hard math computation as many times as you can. This is purely solving problems using power. And this is very, very different than everything we've said about data centers before. This is just compute as cheaply as possible. Anay Shah (02:42:43): And this really comes to a collision point in the real world where you had markets like Iceland and Eastern Washington where Bitcoin miners were outbidding these cloud providers for the cheap hydro and the power that was available. Ben Shwab Eidelson (02:42:57): Then the crypto industry starts to realize that there's power that's not useful to others. Let's look for ways and places to put crypto mining that you couldn't even put a data center or a normal industrial load. You're in an oil field and you're flaring excess gas. What if you put a turbine there, burn that, and power a crypto mine? This is how Crusoe, which later became one of the premier AI data center build-outs, got started finding that cheap power source. And so, it's interesting how many of these power-related innovations came from the crypto moment. Anay Shah (02:43:27): Then, crypto loads were different. They were large, but they were fairly mobile. So that was an advantage in terms of where and how quickly they could place. But it also meant that policy could step in and shape the industry fairly quickly. Ben Shwab Eidelson (02:43:39): By 2021, most crypto mining was happening in China. There was the cheapest power. It was close to where the hardware was being manufactured. But in 2021, China cracked down. There was no more crypto mining. So all the miners fled to Texas, to Kazakhstan, to Canada. And so, by early 2022, the US share of crypto miners was back up to almost 40%. This is up from 3 to 4% in 2020. Anay Shah (02:44:03): What started out as a hobby and took over the fintech world and financial infrastructure forced real estate, power, and the compute industry to wrestle with how to connect very large, very flexible loads to stressed grids. And what innovations can be born out of trying to wrestle with that. On that point, crypto as an industry didn't care much about its footprint. But the rest of the industry, and the data center industry in particular, continued to be very focused on the carbon intensity of their operations. Now, these hyperscalers were tracking their carbon footprints with extreme precision. Ben Shwab Eidelson (02:44:40): By 2020, Amazon became the world's largest buyer of renewable energy. It's a scale thing. Amazon signed the climate pledge. And for Amazon to hit net zero, they didn't just have to decarbonize their data centers and offices; they had fulfillment centers and travel of unprecedented scale, delivering all the packages. Anay Shah (02:44:59): And this is beyond just the quote-unquote "right thing to do." As we've talked about, oftentimes the cheapest, most economical way to get new load onto the grid and scale is through renewable power. And in a zero-interest-rate environment, it actually makes these PPAs more and more attractive, and it lowers the hurdle rate. So, a wind project that might otherwise have been too expensive in this environment is profitable and able to be signed on to a Microsoft new build-out. Ben Shwab Eidelson (02:45:30): It's hard to appreciate the flywheel of more demand for the data center services driving data center build-out, which can be funded by a zero-interest-rate environment, which then all drives all the renewable energy build-out, which is similarly funded by the zero-interest-rate environment. You just had an infrastructure build flywheel running that set us all up for what might come next. Anay Shah (02:45:53): It was an extremely timely moment in the evolution of data centers and actually served as a dress rehearsal for the unprecedented scale that we are going to see in the AI boom. This moment during COVID, with cheap capital flooding the system, was driving a massive new build-out. Power was starting to become a much more limiting reagent in the development of our infrastructure, and there was a breaking down of our global supply chains which was feeding this build-out. All this happened at this moment, right before a massive boom. So, now we've spoken about the companies building the big infrastructure, the big servers powering our life, and we have a $4 trillion elephant in the room that I think it's time to introduce. Ben Shwab Eidelson (02:46:40): It's now time to talk about Nvidia. Nvidia started as a company that was trying to figure out how to make gaming run faster on computers. So, they made gaming chips. They did this for years and years and years. And I remember them recruiting many a student out of the Stanford EE department to go work on gaming chips. Anay Shah (02:46:59): They were very much tied to the gaming industry. Their growth was correlated to it. Ben Shwab Eidelson (02:47:05): But it turns out the same thing that you're doing in a game — which is doing a bunch of math to figure out how to render a pixel on a screen — is what you sometimes need to do complex science: a bunch of tough math problems at once. And if you're going to build a cryptocurrency, what are you doing? As we said earlier: a bunch of tough math problems at once. Anay Shah (02:47:21): A lot of compute, matrix multiplication at... Ben Shwab Eidelson (02:47:24): Scale, to such an extent that their stock price starts to get tied to what's going on in crypto mining. Anay Shah (02:47:29): First, they had been tied to the game industry; now they became even more correlated to crypto. And then, in 2018, you have the crypto winter. That fall in the crypto industry dragged Nvidia's stock down 17% in one day. Ben Shwab Eidelson (02:47:45): Over that period, they got cut in half. Anay Shah (02:47:47): And so, enter COVID, and you hit a crypto boom-bust cycle. But machine learning algorithms are spitting out more and more recommendations to us as we're sitting on YouTube and Netflix and Instagram. And you know what powers a recommendation model? An Nvidia GPU. And so, growth of these training models takes off. Ben Shwab Eidelson (02:48:08): And you can see it as a line item, right? In their earnings, they have this carve-out for what they call their data center business, which is really the growth of this business. You see 60% growth. Anay Shah (02:48:18): This hit nearly $4 billion in just a quarter. And this consisted mostly of their A100 chips. They were selling thousands of these. Ben Shwab Eidelson (02:48:26): Right. And so, these A100 chips became the backbone of the early era of training models. So, in 2020, Microsoft announced the next stage of their partnership with OpenAI. They committed to buying 10,000 or more of these A100 chips to build a special data center, a special supercomputer for OpenAI to continue developing their models. And remember, at this time, OpenAI was not really a household name; it was... Anay Shah (02:48:47): ...a research project, right? Ben Shwab Eidelson (02:48:48): Yeah, I mean, that's right. It was a leading, well-funded AI lab with this deep collaboration with Microsoft, building models that were available via API. And in the same year, they published a paper that really demonstrated something that they had discovered and what they call their scaling laws, showing that the bigger the compute and the larger the data set you threw at the problem, the better the model. And Google was discovering very much the same thing: the more chips, the better the model. Anay Shah (02:49:15): How convenient. The more chips, the better the model. Ben Shwab Eidelson (02:49:18): Music to Jensen's ears, I think. Anay Shah (02:49:21): Indeed. And so, in 2022, right as the crypto crash was coming, OpenAI released a new model, GPT-3.5. It was a huge upgrade from GPT-3, and they had built it in their new Microsoft data center. And they were thinking about new ways to show to the world the value of what they've been able to build. Ben Shwab Eidelson (02:49:41): Are you saying not everyone wants to use an API sandbox to understand how good a model is? Anay Shah (02:49:45): Right now it's a little bit hard to access, but maybe if you put a simple box as a UI and create a little chat feature on the GPT, it'd be a... Ben Shwab Eidelson (02:49:59): Nice demo at least, right? Anay Shah (02:50:00): Right. Few people might be interested, so they decided to put out this chat function. On November 30, 2022, ChatGPT launched. In five days, they had a million users. This was far and away beyond the team's expectations, as well as their capacity planning. According to Sam Altman, they expected an order of magnitude less interest by January 2023. ChatGPT has become the fastest-growing consumer software application in history. So, after getting a million users in five days, they crossed 100 million users in just two months. Ben Shwab Eidelson (02:50:41): By this last summer, ChatGPT's website was among the top five most visited sites in the world, right after Instagram. So there was real consumer and enterprise pull. The world had been waiting for AI to be a thing for a very long time, but this was the moment where everyone said, "Okay, this is it. This is the time." Anay Shah (02:50:59): It ended up with over a billion prompts a day. People were loving this thing. Ben Shwab Eidelson (02:51:02): And everyone knew, to make it better, we just needed more chips. Why is Jensen so delighted? Where are we now? Anay Shah (02:51:09): Three years later, more chips, better models, right? Three years ago, in 2022, Nvidia's data center revenue was growing rapidly: $4 billion in revenue that quarter. Last quarter, their data center revenue was a staggering $39 billion. That's it growing nearly 10x from what it was in three years. Ben Shwab Eidelson (02:51:30): To add $36 billion in quarterly revenue in three years, it's unprecedented. Anay Shah (02:51:37): This is powered by this discovery of the scaling laws. Ben Shwab Eidelson (02:51:41): And the timing of their product launch could not have been better. They had the A100. That was essentially for training — scaling your training. But they launched the H100 in 2022. That was designed for this moment. It was the rocket boost of a chip. It added new special math modes tailored for transformers. It added more high-speed memory. As a result, it slashed training time and made inference faster. Anay Shah (02:52:07): And these H100s are flying off the shelf, from selling a million and a half in 2023 to over 2 million in 2024. And the big players, having learned from past years of the cloud boom and the COVID demand boom, are ordering these in the hundreds of thousands. Ben Shwab Eidelson (02:52:23): These things are not cheap. Each chip is like a car, right? $40,000 apiece, supposedly. You couldn't buy one chip at a time. Anay Shah (02:52:32): No. Why would you sell one car at a time when you can put them in boxes? Ben Shwab Eidelson (02:52:35): They really sold in two models, right? There are training boxes and inference boxes. The training box is an 8-GPU box with a CPU in the middle. They're hardwired with this NVLink connection. What's really interesting about this is it's all about making it effectively one big GPU. It's all about creating this really high-bandwidth connection between these GPUs. Because when you're training, that's what you want. In NVLink, if anyone's ever built a computer, you would have what's called a PCI connection in a motherboard. The way your Nvidia graphics card would plug into your computer was using PCI. It felt like a really fast connection. But it's not fast enough for this. So Nvidia invented NVLink. It's up to 15 times faster than that connection. So these eight GPUs can communicate extremely quickly. One training box is about $500K to buy. Then they also launched these inference boxes. And so, this is two GPUs coupled with a bunch of memory. To be clear, inference is when you're asking ChatGPT a question and it's responding. That process of responding is inference. You want a lot of memory to be able to understand all the content in that response. These boxes were designed with a lot of fast memory, connected to be able to share context across the model. Anay Shah (02:53:46): Now, selling these GPUs in a box actually has a physical space implication, right? So you've heard us talk about the racks and racks in the data center as far as the eye can see. These racks have slots for servers, and those servers have a traditional power consumption. But when you put in a specialized server, it starts to change the power consumption of that rack and therefore of the square footage and of the data center. So let's unpack, just for a moment, what's happening at the rack level when AI arrives. Ben Shwab Eidelson (02:54:16): Historically, one rack would be maybe 400 watts. So you'd build a rack with 20 to 30 of these servers. You'd have some networking equipment. So, somewhere around 5 to 10 kilowatts of power going to a rack — that's like 10 hair dryers running at the same time, to give you a sense. Now, these GPU boxes arrived — those eight-GPU training boxes. Each of those is up to 10 kilowatts by itself. So it's the same power consumption and heat as the whole rack was. But you could now pack a rack physically with four to eight of those boxes. So, all of a sudden, you're an order of magnitude up in power consumption, up to 90 kilowatts in a rack. This has a bunch of implications, not just for power, but also for cooling. An inference box is somewhere in between; so a whole rack, maybe, is up to 40 kilowatts. Anay Shah (02:55:01): One of the things driving this power conversation we've been having is, again, not only the size of these data centers, but now the density of power required within these data centers. This is what makes an AI-driven data center so much different from a traditional data center. Ben Shwab Eidelson (02:55:21): Just 20 of these training racks in a pod is one megawatt of power. That's on the order of a small, 800-person town powering electricity. So, if you get 200 of these racks, you get to an 8,000-person neighborhood very quickly. You get to a gigawatt of Seattle-scale power. Anay Shah (02:55:41): As you pack these GPUs that have higher power density, they run hotter, and you can't let them run hotter. So now, how you cool your GPUs and your racks needs to evolve. --- Ben Shwab Eidelson (02:55:55): The problem is, air can only move heat so quickly. And if you were trying to do that, even with the inference racks, you would need leaf blower-level fans to move the air out. And so you need to start bringing more cooling down to the rack level. So the first thing that you can look at is retrofitting the racks with what's called rear door heat exchangers. This puts some water cooling directly alongside the rack so that when air is moving over those hot chips, some of that air, instead of just going straight into the hot aisle, can get extracted via some tubes of water running inside the rack. That is a nice convenient half-step because you don't have to change how you're actually cooling the overall system. The next jump is the jump to direct chip liquid cooling. This is where you mount cold plates directly on your chips and you run water right near the chip to actually extract the heat away, instead of expecting air and heat sinks to do it. And so, increasingly, especially with the most powerful GPUs—and to be clear, each generation of GPUs gets more powerful and therefore has more heat to remove—we are moving to a water-cooled world. Anay Shah (02:57:03): We've crossed the threshold of physics where air can cool this amount of heat being emitted at this level of density from the racks. And so now it is a liquid-cooling world. Now, at the end of the day, you've got to move the heat from inside to outside. But it's very environment-dependent. In the right climates, you can expel it outside. And that's why we saw some of these data centers looking at Finland and Oregon and certain geographies that enable that to happen. And this consumes very little water. But in other climates, you have large evaporated water cooling towers. This is where the problem of the water impact on the local community starts to come into play. Ben Shwab Eidelson (02:57:45): Fundamentally, what we're trying to do is we're removing heat through that phase change, right? You're evaporating water; that's a massive transfer of heat. These towers increase the surface area of the water. Water evaporates out. It looks like steam going up from these cooling towers. And it's a very effective and efficient way to cool things down. But it can use up your water. Anay Shah (02:58:03): And so you end up with this energy-water trade-off in certain environments. And that becomes a big planning focus not only for data centers that are working with their local community, but also those that are power-constrained and built by companies that care about carbon intensity. One study Google did in 2022 found that water-cooled data centers use about 10% less energy, which means they emit about 10% less carbon emissions than many of the air-cooled data centers. Now, the problem is, you can't do this as easily in water-stressed areas. Ben Shwab Eidelson (02:58:38): So there's essentially this knob you can dial up and down. To be clear, it's possible to build data centers that don't use any water, but they're going to use more electricity to run an air conditioner in your house. You're not using a bunch of water, but you're taking a ton of energy. And so I would argue if we care about water, the best thing we can do is have clean electricity to make this an easier trade-off to make. Anay Shah (02:59:00): So if you zoom out and look at water, globally, data centers consume over 550 billion liters of water annually, which is a big number. To put that into context, a single 100-megawatt data center can consume 2 million liters a day, or the amount equivalent to 6,500 U.S. households per day, both direct and indirect water usage. So this is a very big, real, and current issue for data centers in terms of where they site their data center, how they get permitted, and the ongoing operations. Ben Shwab Eidelson (02:59:35): The local question is hyper-important. Some reporting that The New York Times did about the situation in Georgia where, supposedly, when Meta broke ground on their billion-dollar data center build-out, water taps in some residents nearby went dry. And there's some back and forth around whether or not you can prove direct causality. Still working out whether or not that is actually the case, caused by that data center. But it is known to be true that that data center is using about 10% of the county's total water and it's driving water prices in the region to go up. Anay Shah (03:00:06): So now let's dive into what is the largest constraint to data center build-outs today, and that is power. We've talked about the evolution of power moving up the decision stack. From the early 2010s through the COVID era, and now in the AI era, power is really the limiting reagent. Ben Shwab Eidelson (03:00:30): The image in my mind is: we have the largest, most complex machine that humankind has built in the power grid that has brought us such evolution in how we live our life. And it is colliding with the new, complex, most interesting machine of the data center. And this collision and the force of it that we're living through right now is defining this period. Anay Shah (03:00:53): And it is no small collision. It is a function of two things, right? It's a function of speed and a function of scale. So, as we've seen, there is an absolute arms race on compute, because the faster you can get compute online, the better your models are, the more revenue you're going to make. And timing matters a lot here because a model trained in 2025 can become obsolete by 2027. So if you have a two-year delay in getting your data center online because of the power grid, it becomes a dealbreaker. It has to happen now, coupled with the scale of these data centers. And so, as we talked about, a typical data center started as 5 to 10 megawatts. Then, just a few years ago, we were in the 50 to 100 megawatts. And now this year we're starting to see gigawatt-scale announcements. Now, to be clear, these are campuses that make up a gigawatt. There are multiple facilities, but they are still being built out cohesively and will put a strain on the grid and the local community at a gigawatt scale. Now, to put a gigawatt in perspective. Ben Shwab Eidelson (03:02:00): This is the scale of a city's power consumption, right? Maybe a Pittsburgh or Cleveland. And Google's data center electricity use doubled over four years and was up to 30 million megawatt-hours in 2024 from 14 and a half million megawatt-hours in 2020. To put that all into perspective, that's about 3 million U.S. homes, or around three-quarters of a percent of all U.S. electricity consumption. Anay Shah (03:02:22): Which checks out, right? If data centers are currently about 4% of U.S. electricity use, I guess it makes sense that Google's about a fifth of that. Ben Shwab Eidelson (03:02:31): And it's worth pausing there for a second to talk about that 4% electricity use. When I hear that number, it actually seems shockingly small. For all of the discussion of data centers and electricity, it's like, okay, it's 4%. Let's say it goes to 8%. Compared to industry, compared to cooling and heating buildings. It all feels small. So why might this be such an issue of conflict? Anay Shah (03:02:54): It's a great question. It might have to do with the fact that it's misleading to think about it in terms of total national electricity use, because a data center has a localized and concentrated impact, right? You're not spreading this load across multiple utilities. And not only is it localized and concentrated, it's localized and concentrated in similar areas. Because, as we've been talking about, the network effect of the value of a data center being positioned near the undersea cables and near other network points is where you get a lot of performance gains. And so you end up concentrating yourself in Virginia, in California, in Texas. There's just basically 10 states where you're seeing new data centers come online. And so there's a tremendous impact at a local level—at the electricity prices for that community, that county, and in that state—but not necessarily at a national level. Ben Shwab Eidelson (03:03:52): We talk gigawatt scale. That is all about these large training clusters. When we're talking about inference—which is, hey, ChatGPT, processing your response—you actually probably want that spread out, right? You want that closer to the edge, and that's where you need it close to the interconnect, because you want it to not have latency when interacting with the user and their data. Anay Shah (03:04:10): So let's talk a little bit about why you need a gigawatt data center or a 5-gigawatt data center to do the best training possible. Ben Shwab Eidelson (03:04:17): You don't really want to think about it as one GPU or a rack of GPUs or a building of GPUs. You want, as much as possible, the whole data center to act together. Because the way that a transformer actually trains is that it makes guesses about what should come next, and then it compares that to reality, and then it tunes the weights across it. And you're doing that collectively across the whole model. And to do that, you need all of the computers working on the problem to be able to communicate quickly. Otherwise, the whole thing is training slowly. The faster those exchanges can happen, the better. And so you want the connectivity between chips to be as fast as possible. You want the connectivity between boxes of chips to be as fast as possible. And so it does not work if all of a sudden half of your computers are on the East Coast and half are on the West Coast, because the speed of light across the country is going to slow you down by multiple orders of magnitude than within one campus, within one center. And that's why Meta wants to build a 5-gigawatt campus because that's going to get them the biggest training model possible. Anay Shah (03:05:21): There's also an interesting nuance here where training models hit higher load factors than an inference model. Ben Shwab Eidelson (03:05:30): And in a typical data center, you're trying to run this all at once and you're utilizing ideally all your expensive chips. And so all of that leads to a higher utilization and more power and more compute. Anay Shah (03:05:43): So we talked about the speed. More time to power is time to revenue. We've talked about the scale, now going from 100 megawatts to gigawatts. So this creates an objective function when we're thinking about the power constraint on data center growth. And that ends up being: how do we secure large 24/7 power where you need it, when you need it, quickly and ideally as clean as possible? Should be easy, right? Ben Shwab Eidelson (03:06:13): Oh, wait. Not to mention there are supply chain bottlenecks in transformers, bottlenecks in gas turbines, and bottlenecks— Anay Shah (03:06:20): In labor and in specialty skills. Ben Shwab Eidelson (03:06:22): We don't have the most up-to-date, freshest grid with transmission lines. So this gets into a very tense moment and an area that arguably breeds innovation. Anay Shah (03:06:33): So the reality is, getting affordable power that is 24/7, quickly and cleanly, is going to be very rare. That Goldilocks situation doesn't really exist today. And so we're going to have to compromise on at least one of those variables. Now you've got a few different options to get these data centers online, right? You can either hook it up to the grid and use power that already exists. You can build new generation of power that's off the grid and power that data center, do some sort of hybrid in between, and over the long term, we can build out large new generation like nuclear, geothermal, and others. Ben Shwab Eidelson (03:07:14): Watching this closely, there are both a lot of moments of concern, right, because we're at times doing things like keeping a coal plant running longer than we might have otherwise. But there are also a lot of interesting moments of innovation. A lot of growth in storage to help supplement what's going on and smooth out power generation, and co-locating these in new hybrid ways. Anay Shah (03:07:34): And there's a tension that's largely dictated by timing, right? There's a near-term pain that local communities are facing when you bring a data center online and potentially increase electricity prices. But there's also cause for optimism because you've got well-funded companies who have an interest in long-term sustainable power. And the fact of the matter is, the most affordable, long-term sustainable power we have is going to be clean and renewable. And so this moment actually offers an opportunity for us to accelerate a lot of new clean build on the grid and reshape our electricity infrastructure. Ben Shwab Eidelson (03:08:10): I think, at the end of the day, there's also a big issue of incentives. These hyperscalers, they want to build these models. Now, many of them are used to mostly still building their businesses on software-based timescales. And the reality is, despite all the discussion of the utilities and electricity, for them, it's a bottleneck. The actual cost of energy for them to build the biggest models is like 2 to 6%. It is all in the people and the hardware. It is all in the chips and the humans. And so for them, it's all, as you said earlier, this speed to getting out there. And that's not how our system is set up. That's not how our incentive structures are set up, right? Anay Shah (03:08:51): That's right, because the utility supplying that power operates on an entirely different paradigm. In a typical electric utility, power delivered in 2027 is valued at the same amount as power delivered in 2030. Utilities don't differentiate those products, but for the customer—a Google or Meta—there's a massive difference for their business. They need power today. Ben Shwab Eidelson (03:09:17): How much more would they pay if they could get power now versus three years from now? Anay Shah (03:09:21): And how much could we unlock in terms of investment into our utilities if they could take advantage of the fact that companies are willing to pay a lot more today than in two years? Ben Shwab Eidelson (03:09:31): Brian Janis calls this the Watt-bit spread. It's the economic arbitrage between an available Watt and the ability to turn that into a bit. Reality is, the speed of regulatory change in utility tariff pricing—which is a fancy way of saying how quickly the utility can change anything about their economics—is slow. And so despite there being an obvious market demand structure, it's not going to change in the next 12 months the way that utilities price their product. Anay Shah (03:10:03): Utilities are public goods, right? They power our lives and our schools and our hospitals. And so for good reason, they are regulated. But because they are regulated, they take time to change. And moreover, we have a federated system. We have some 3,000 different utilities across the United States. And so making that change across this country, or even in the 10 states where data centers are getting built out, will take a lot of time. Ben Shwab Eidelson (03:10:27): Power has become the dominant question of: where can you build your data center? Because you can't go do it without that. But it didn't remove the fact that the other questions still remain. For example, you need people to go build a data center; you need a lot of electricians; you need all the components and the supply chain and the things to build the transformers to step down the power. Anay Shah (03:10:48): Based on how we've been talking about this, you might think that data centers are almost entirely a U.S. story. And while the U.S. is the dominant builder of data centers, there is actually a global story here and global infrastructure being built out, and a strong need for sovereignty for other nations. So in order to help understand what's going on here, let's go back in time a little bit to understand the drama that's setting in globally. Ben Shwab Eidelson (03:11:19): I think it's worth looking at just fairly recently. In 2018, Europe passed GDPR, and that set this world baseline for privacy. Thank you. All the cookie pop-ups, but also really tight controls on where EU data sits and what happens if you have EU resident data outside of the EU. And so this is really, I think, a reaction to where people are storing data about our citizens. And so there are controls about what happens if you're holding or transferring data outside of the EU. Anay Shah (03:11:51): In 2018, the U.S. passed the CLOUD Act, which essentially lets U.S. authorities lawfully demand data from U.S. providers, even if that data sits on servers outside of— Ben Shwab Eidelson (03:12:03): the U.S. And so the net effect is if you're working with EU data at scale, you start to need to have servers and hard drives in the EU. And this becomes, I think, almost another lock-in that the big cloud providers start to have because they go and build the infrastructure to do that. Anay Shah (03:12:19): And we saw this in the Microsoft story, right? A big part of their early scaling was following their enterprise customers globally and needing to build out data centers in Germany as one of the primary cases. Ben Shwab Eidelson (03:12:31): And so if the EU-U.S. looks like a little tiff drama, I think it's time to move to the major geopolitical data center drama, which some have— Anay Shah (03:12:41): deemed the new Cold War. And there's a book called *The Chips War*, which centers around this notion of the U.S. and now China in a fight over everything. Ben Shwab Eidelson (03:12:57): But centered on this technology, right? So, I mean, let's go back to 2012, which I think really kicked off this last decade. Fifteen years of animosity here. The House Intelligence Committee report warned U.S. carriers away from using Huawei and ZTE equipment, saying that we shouldn't use this in any sensitive government system. Now that, of course, starts to spiral outside of just government use. Anay Shah (03:13:21): That was first with the carriers. Beijing responded on a different front: the data front. So in 2017, China's Cybersecurity Law, followed by other laws, pushed data localization and tighter state control. And so the U.S. companies had to respond by localizing operations, which essentially means AWS, Apple, Azure: they either had to sell off to Chinese partners or find a Chinese government-linked company to be a partner. Ben Shwab Eidelson (03:13:49): That's right. So it's not just that those companies' hard disks and servers had to be in China, but they actually had to be owned and operated by a Chinese company. And so then the U.S. reacted even further. They barred all federal agencies from buying or using equipment from a long list of covered Chinese vendors. And they barred the FCC from approving any new authorizations from Huawei, ZTE, and others. So this just ramped up further and further. Anay Shah (03:14:14): This brewing war was not just limited to land, but also involves our friends, the undersea cables. Ben Shwab Eidelson (03:14:21): Right. Anay Shah (03:14:21): And so in 2016, Google and Facebook partnered with a Hong Kong-based company to build the Pacific Light Cable Network, a massive 12,000-kilometer undersea cable linking Los Angeles to Hong Kong, Taiwan, and the Philippines. Part of that global network of undersea cables we were talking about earlier. Seems like a great idea. What could go wrong? Ben Shwab Eidelson (03:14:44): And fast forward four years later, to 2020. U.S. national security officials say, "We don't know about this Hong Kong landing point anymore." We think that's going to be a vehicle for Beijing to have surveillance across this important backbone of the internet. And so the FCC ultimately blocked that route. And so now the cable hits Taiwan and the Philippines, and those are lit up. But the Hong Kong branch that's been laid, I believe, is just lying dark. Anay Shah (03:15:10): So I'm sitting here in Los Angeles without that cable to Hong Kong. Now, moving back to land and to chips. The battle continues. And so in 2022, the Commerce Department rolls out these sweeping controls on advanced AI chips and fab tools. And so, remember, this is the moment where we've gone through a COVID boom. We're seeing an uptake in usage from crypto and streaming. And now AI, and the Commerce Department tightens these controls again in 2023 and again in 2024. Ben Shwab Eidelson (03:15:43): And this feels very active, right? I mean, literally every quarter of Nvidia's earnings, there are discussions of this. A few weeks ago, the U.S. opened a channel that said Nvidia could ship certain chips that are dialed back, like the H20. But they have to give the U.S. government a 15% cut of the revenue. It's a pretty unprecedented pay-to-export arrangement, like a lot of things that are unprecedented that are going on. But if you zoom out, it raises some good geopolitical strategic questions on how we want to engage, right? Anay Shah (03:16:11): Yeah. It's foundational to the new economy that's being built on this advanced compute power. And it remains to be seen how this plays out. It may backfire. Right. China is not terribly keen on being dependent on U.S. production, so they are infusing massive amounts of capital in their chip sector. It's still a step behind Nvidia in terms of raw performance and software ecosystem dominance. But given China's resources, you have to assume that that gap is going to narrow and narrow quickly. --- Ben Shwab Eidelson (03:16:46): There's a reason China wanted domestic energy. They want to have their own domestic production of silicon here, and so they've invested heavily and they're catching up. Some estimates are that they're 60 to 70% of Nvidia performance; it depends on what they're doing. But they're investing out of a $50 billion fund to improve chip development. Even more so, they're passing policy that says data centers need to source at least 50% of their chips domestically. So they're making sure to really stand up not just the supply side of the market, but also simultaneously the demand side of the market. Anay Shah (03:17:17): The story is less about whether Chinese chips can equal Nvidia's top-end designs and more about how quickly that's going to happen given the demand and the capital that they're putting in. So in short, at the moment, the US is still ahead, but the China chip ecosystem is accelerating rapidly. Their backlog of demand for Nvidia chips, which is being withheld, is going to matter less and less over time. Ben Shwab Eidelson (03:17:43): The other dynamic that you have geopolitically is governments are seeing the AI data center boom as an important future part of their economy. A deal that exemplifies this is the Emirates deal with OpenAI around this UAE Stargate buildout, a one-gigawatt AI cluster in Abu Dhabi, and all the ecosystem players from Oracle to Nvidia to Cisco to SoftBank coming in to finance this and collaborating on the buildout. I think it just raises a lot of interesting questions: What about the location of a data center matters to a government? Anay Shah (03:18:16): And the map is stark. If you look at where AI data centers are located today, only 32 nations have them, and most of them are in the Northern Hemisphere. You have large swaths of Latin America and Africa as fully dark. Governments are deeply, deeply concerned because they are wondering that if you continue to rent compute power from faraway data centers, you remain at the whims and you remain vulnerable to foreign entities and foreign companies, and you aren't able to support domestic enterprise or domestic scientific research or academia with the same level of control. So this idea of compute sovereignty is top of mind for a lot of people, and a lot of emerging markets are worried that the AI era runs the risk of leaving them even further behind economically. Ben Shwab Eidelson (03:19:09): There's almost a rough analogy to energy independence, right? Do you have it regardless of your relationship with another country? That feels like the important thread here. Anay Shah (03:19:18): The other tricky dimension here, which if you talk a little bit about, is the actual impact on the climate, which gets a lot of attention these days, rightly so. There are a few questions that continue to come up, and one that Ben, maybe your mom has asked you, I know mine has, is if a ChatGPT query is bad for the climate. If I care about the climate, should I really be using AI? And we have some data on that. Ben Shwab Eidelson (03:19:44): There's been some reporting on this, and I think it's an area that's become a pretty hot-button issue for folks. There's been, I think, some people even shaming others for using these tools because of the climate impact. So it's worth looking at the latest numbers. Google actually just put out a publicly readable paper on Gemini where they go down to the details on the energy consumption and the carbon intensity of that energy. The median Gemini text prompt, which for all intents and purposes I think could be viewed as fairly equivalent to a Claude or a ChatGPT or a Copilot prompt, uses 0.24 watt-hours of energy and emits 0.03 grams of carbon dioxide equivalent and consumes 0.26 milliliters or about five drops of water. Those are figures that are substantially lower than what the public estimates have been. For some kind of sense of scale, that per-prompt energy impact is about the same as watching your TV for less than nine seconds. Why do we think the public reporting might be so far off? Anay Shah (03:20:40): There have been a lot of improvements over time, and so it's important to kind of ground ourselves in this moment. We know, through that same report, that the AI systems that we're using are becoming more efficient. There's constant innovation in the software and the hardware to drive more efficiency. So over 12 months, the energy of a standard Gemini text prompt dropped by 33-fold. Ben Shwab Eidelson (03:21:04): Wow. Anay Shah (03:21:05): A 33x improvement in the energy consumption. Even more so, from a total carbon footprint per se, we have 44x, all while delivering higher quality responses. So we are moving in a world where the efficiency gains are continuing, and this question of "is your query bad for the climate?" is fairly insignificant. Hannah Ritchie from Our World in Data has independently corroborated this fact. It's an interesting cocktail conversation, but largely misses the forest from the trees. Ben Shwab Eidelson (03:21:37): I think the pressure is good positive pressure because one individual query you should not feel bad about. But the pressure for all of these companies to monitor this and drive these numbers down and get these efficiency improvements is a fantastic thing. That's right. And to drive the deployment of clean energy so that the carbon intensity of the energy drives down is a good flywheel to keep pressure on. Anay Shah (03:21:56): Indeed. So, Mom, keep asking a question because Google will then continue to improve. Ben Shwab Eidelson (03:22:00): I think there's another area that's been under-discussed. Anay Shah (03:22:03): You don't hear a lot of discussion on the actual carbon intensity of building these very large pieces of infrastructure, pieces of real estate. That's called embodied carbon. So what is the embodied carbon impact of the steel, the cement, the energy used to build a shell and stand up the facility in and of itself? Ben Shwab Eidelson (03:22:27): Thankfully, Google did another report on this and shared that in their analysis. Running an AI data center, the operational emissions, which really means the energy going into running the data center ongoing, is going to be about 70 to 90% of the total emissions. Manufacturing emissions, which is really the manufacturer of the server components, the memory, the flash storage, the GPUs, is going to be around 25%. And then the data center construction, so this is the steel and cement and the logistics around that, are around 5%. So this is all to say, I think when you look at this, we should probably be paying a bit more attention to the manufacturing of the components and make sure that we're looking at the LCAs for solid-state disk drives and GPUs and making sure we're taking that into account. But the attention's probably correctly centered on the electricity. Anay Shah (03:23:14): And on this point of focusing in on the manufacturing, the procurement muscle of one of these companies that are building this out is actually a very big leverage. So when Microsoft requires its suppliers to use 100% carbon-free energy by 2030, it will pull their suppliers, all the fab building and the motherboard building and the assemblers. It's moving everyone down the supply chain — it's called Scope 3 emissions — into a cleaner world, which has tremendous power. Ben Shwab Eidelson (03:23:43): I think the other thing that's interesting is some of the circularity conversations are starting to happen. So I was at a talk where the Microsoft CSO was talking about how they recently launched, I think it was just in April of this year, a circularity program where they would take the rare earth material in the hard disk drives that they're using in the data centers, and they have a new way to recycle those while disposing of the data. That yields a 90% recovery of those rare earth materials, which also helps stand up a US supply chain around rare earth materials, which we don't really have today, which is pretty phenomenal. Anay Shah (03:24:13): Yeah. Tremendous. And it's going to be a growing discussion over the coming years as we mine for more and more of these critical minerals and the geopolitical control over who has those minerals. Ben Shwab Eidelson (03:24:25): All right, so given all of that operational load, is the main contributor to greenhouse gases something like 70% or more of the emissions? How should we think about the scale of that from a meta-climate perspective? Anay Shah (03:24:38): So when we talk about operations, remember we're talking about the energy consumption and not the materials that go into building the data center. Recent studies show that data centers account for about 4% of total U.S. electricity consumption. And with more than half of that electricity derived from fossil fuels, that means that data centers generate more than 105 million tons of CO2 every year. Ben Shwab Eidelson (03:25:04): Until reading the study, I didn't realize that data centers' carbon intensity is actually higher than average, exceeding the U.S. average by 48%. Anay Shah (03:25:12): The big point here is how grid-dependent that impact is. If these data centers are located on a dirtier grid, let's say a coal-heavy grid in the Mid-Atlantic region of Virginia, then it's going to have a very different climate profile than a data center that's in Eastern Washington that's entirely powered by hydro. So when we talk about 105 million tons of CO2, how should we think about that? Ben Shwab Eidelson (03:25:36): A few comparisons to help think about the scale of 105 million tons of CO2 emissions. One is you look at US aviation emissions: this is about half of that. Another would be looking at enteric methane, which is a fancy way of saying cow burps, is around 178 million tons. So that means data centers are about 60% of the equivalent of that. Or all US passenger vehicles are 1,000 million tons, or a gigaton. And so data centers are about 10% of that today. Now, if you look globally, this goes up by approximately a factor of three, as do the rest of these things. So overall, data centers are not insignificant. They're worth tracking on the map here, but quite fractional compared to passenger vehicles, big emitters like livestock, and a third to a half of something like global aviation. The interesting thing, I think, what makes this a hot topic, is not just the current scale, but the projections, right? If all of a sudden we double on our current grid in the US, then now we're caught up to the emissions of aviation in the U.S. And if you keep going from that and you keep the trend line up, you could imagine data centers becoming a fairly dominant story in emissions. Anay Shah (03:26:50): The other leg of the stool in this conversation is how it's affecting the local communities that these data centers are being placed in. One of the themes that we've found come out through this is the impact of these data centers are concentrated and they are disproportionately felt by the environment, the ecology, and the people that immediately surround it. We metaphorically went to Memphis, where XAI is building a very large data center on the banks of the Mississippi, 15 minutes from downtown. And there, Elon is commissioning a 200,000 GPU unit data center. There's been a significant backlash from the Memphis community for good reason. Ben Shwab Eidelson (03:27:37): I think a lot of it centers on how are we going to provide power to that? Because the utility said that they can provide XAI power for about 50 megawatts of load, but XAI wants triple that amount. That's a lot of GPUs, and 50 megawatts isn't going to cut it. So they brought in gas turbines — 35 turbines — which could theoretically power 420 megawatts. The problem with that — I mean, it's good they're solving their power problem — but the other problem with that is these are highly polluting gas turbines. So they have the potential to emit a couple thousand tons of smog-forming nitrous oxides each year. For context, that's more than the smog caused by the Memphis airport. So it's like you're adding another airport of smog to the region. Anay Shah (03:28:18): And this impact is not continuous, right, because these data centers ebb and flow in terms of how much power they are consuming. So it's important to look at these peak moments. We found public satellite data from NASA and the European Space Agency that shows that, on average, nitrogen dioxide concentration increased by 3% when comparing for a year before in this area. But in the times of peak consumption, we're talking about a 79% increase in nitrogen dioxide concentration from pre-XAI levels in that area. So you can imagine it's like sitting in a traffic jam, and those moments have outsized impact on the community at that moment in time. Ben Shwab Eidelson (03:29:00): Yeah. And so there's been a lot of pushback from this pollution and health risk standpoint. It's hitting South Memphis neighborhoods that already have elevated asthma and cancer rates from past industrial waste. This temporary electricity generation infrastructure isn't going to sustain in the long term. So all of this is kind of a case study in both the local impacts and also the time-to-power issue. Anay Shah (03:29:24): And this notion of time to power, essentially what it means is there is enough demand to get these data centers online quickly. The companies are willing to pay more and more. So when you have that kind of economic pressure, the community resistance sometimes can only go so far. As we're seeing in this case, XAI, despite the pressures, is currently building out a second location a few miles away, which will be double the size. Ben Shwab Eidelson (03:29:50): Of the first: half a million of these GPUs. Anay Shah (03:29:53): I mean, just insane. That's the current moment we're living in. Ben Shwab Eidelson (03:29:59): So far we've been trying to keep it focused on what has been built more than what is proposed. But I think it's worth taking an extreme view on some of what's coming. Meta has announced their big long-term project, their big training center, the Hyperion project, located in Richland Parish, Louisiana. It's a $10 billion buildout, online by 2030. They'll have a few steps in between. The goal is for this to be a five-gigawatt canvas covering over 2,000 acres. Anay Shah (03:30:28): What is 2,000 acres? Ben Shwab Eidelson (03:30:29): Yeah. So a different way to visualize this is this data center is about the size of Lower Manhattan. It is at a scale that is kind of unfathomable, right? That we're going to build buildings of that scale in a few years here to power such a new technology. Anay Shah (03:30:48): That Louisiana project is a great example of what we are living, the step-change function that we are living through right now. This is the biggest tech infrastructure project since either the 1960s, the dawn of the computer age, or even the 1880s, the heyday of the railroad period. Ben Shwab Eidelson (03:31:09): I think Nvidia is on pace to capture the highest share of market-wide capital spending since IBM peaked in their percentage of that in 1969. And so a lot of comparisons are being made to other past booms like the Gilded Age or the telco buildout. Anay Shah (03:31:25): And when you mentioned booms, obviously you end up thinking about busts, right? So we talked about the telecom boom of the 1990s, which contributed to the dot-com crash and a bust on fiber. You also look back at the 1870s and the huge railroad boom that led to a crash, and both of these pose the question of, like, did we overbuild? Are we outrunning our demand? In both of those cases, it's not that the CapEx spenders were wrong; they were just early. We saw this vividly in the fiber overbuild. Ben Shwab Eidelson (03:32:00): Right. Anay Shah (03:32:01): By the year 2000, we were only using 3% of the fiber we had laid. But it was absolutely necessary to power the next couple decades. And that's what it looks like for AI is that there may be an overbuild in this moment, but the foundational nature of this technology suggests that it's going to get utilized. Ben Shwab Eidelson (03:32:20): Yeah, I think there's such a rush here because the potential AI market creates this economic imperative to go and plug in as many GPUs as quickly as possible, and that will build the largest and smartest AI model, which will create a moat. I think there's an interesting question here, though, of what is the infrastructure? Because yes, we still use the railroad tracks and we still use the fiber, but remember that all the companies that were building the fiber and the ones that were on top of that, it was too early for them to capture that value. But what's in part different about this boom is how it's being funded now. Yes, there are VC dollars and other dollars going into it, but at least historically, and if you look at the major capital expense, it is these big companies we've spent the episode talking about — it is Amazon, Microsoft, Google buying from Nvidia. And they're using the fact that their business models have thrown off billions and billions of dollars every quarter to then go buy the chips and build this out. So there's a real kind of concentration in some of the buildout of the infrastructure. Anay Shah (03:33:20): Those previous boom-busts financed by the banks end up drawing in the rest of the economy. And while there are some private credit actors and venture capitalist actors, because it's largely financed by free cash flow of these private companies, a bust hurts them, which will drag down the stock market, but is hopefully insulating the rest of the capital providers that are not involved. Ben Shwab Eidelson (03:33:43): There is supposedly now more private credit starting to come in to fund this, and that could create linkages to other parts of the system. We will see how that plays out. We've talked about the story of data centers up until present day. Let's zoom out for a second and talk about just where we are today from a sense of scale. Anay Shah (03:34:04): Let's start with the most basic question: How many data centers exist today? There are globally approximately 11,800 data centers worldwide. Now, of course, the counting of this gets nuanced when you think about the closets that still exist with servers as they did back in the day. But by and large, we're talking about independent construction. And the US is by far the largest with over 5,000 data centers, followed by Germany, the UK, China, and Canada. Ben Shwab Eidelson (03:34:33): So the US has almost half the data centers. Wow. Anay Shah (03:34:36): Yeah, heavy concentration in the U.S. Obviously other countries are catching up. China is building very quickly. So this begs the question of who owns all of these 11,000 data centers and who's spending these hundreds of billions of dollars. And you can kind of think of it in four broad categories. That's representative of the US but also it fits globally. And so the first category are our friends, the colocation centers, the ones we've talked about: Equinix, Digital Realty, and others. There are about a dozen of these that are building 10 to 100 megawatt blocks in the US and around the world. The second category are the hyperscalers: the Facebooks, Amazons, Microsofts; you can add in Apple, Oracle in there. They account for almost half of global data center capacity, and they're the lion's share of new growth. But importantly, you still have thousands of private server rooms in banks and retail facilities and public institutions that are out there. So while large in count, they're relatively smaller in capacity, but today still make up about 35%. And finally, you've got legacy telcos that are still owning and operating data centers. Now the interesting thing about this is the trend, right? You're seeing enterprise and public sector data centers trending down, and new build increasingly going to hyperscalers and these purpose-built colocation facilities. Ben Shwab Eidelson (03:35:54): So how much actual physical real estate space are all of these data centers taking up? This is like anything that gets into this level of detail. It can be hard to pull exact numbers, but one reasonable estimate is that maybe there's one and a half billion square feet globally in data center buildout. For context, if you took an average median US home of around 2,200 square feet, that's around 730,000 of those homes of square footage, or 28,000 football fields. Anay Shah (03:36:20): Importantly, including end zones. Ben Shwab Eidelson (03:36:24): That feels like a big number. But for some comparison to some other infrastructure in our life: If you just took the U.S. interstate system, so this is, you know, I-5, I-90, the big interstates, just the asphalt of those interstates is 10 times the square footage. So let's talk about power. Anay Shah (03:36:40): Remember the impact of data centers on power: it's quite localized and concentrated to its physical location. But let's talk about it in a sense of scale. We mentioned that in the US it's about 4.5% of total electricity. That's roughly 17 million households' annual use. How can I conceive of that power? Ben Shwab Eidelson (03:36:59): Yeah, I think if you look at the electricity consumption of the city of New York, it's about three New Yorks to power all the data centers in the US. Or if you look at all data centers globally, it's about equivalent to the power consumption of the United Kingdom. If you look at other comparable industries, it's pretty close to the power consumption of the chemical industry or the primary metals industry. This is extracting iron ore and smelting aluminum. Anay Shah (03:37:26): Similarly, on the water question, there's a tremendous amount of nuance in how to aggregate total water consumption. But let's just do the average of the average. Let's take direct water for cooling, indirect water from the power generated for the plant, and get a sense of how much water this is using US data. --- Ben Shwab Eidelson (03:37:45): Centers together consume about 250 million gallons of water per day. That's about a quarter of what the city of New York uses. Anay Shah (03:37:53): So we've talked about the space, the data centers, how many of them, who owns them, how is it being powered and cooled? What's actually inside of them? Let's think about the scale of the compute power. It's actually a little bit difficult to figure out what humanity's overall compute power is, but let's just say there are about 50 to 100 million servers globally. Ben Shwab Eidelson (03:38:13): Yeah. And so, I think the best way to get at this was to go back to the electricity usage numbers and then look at how much compute power we get per unit of electricity. What's surprising here is that if you run the numbers, we've grown humanity's compute power around 40x over the last decade, but have only grown power consumption about two and a half times because we've gotten more efficient. And if you look at that new compute power that's come online, only around 10% of data centers are AI-focused today. Of course, that's a growing percentage. Anay Shah (03:38:45): So compute power is one part of what the data center is doing. Let's give a sense of the scale on storage. Ben Shwab Eidelson (03:38:51): Yeah. So in the last decade, we've 3x to 4x total installed storage capacity to around 15 zettabytes. Anay Shah (03:38:57): And so we're speaking to a friend of the show, Byron, who made me understand the modern marvel of hard drives. So imagine the head of the hard drive is scaled up to the size of a Boeing 747. It's flying at 560 miles an hour, and it is about the thickness of one piece of paper above a football field. That hard drive head is reading and writing to every single blade of grass moving at 560 miles an hour. Ben Shwab Eidelson (03:39:27): That is absolutely insane. And so we have all these hard drives and all this compute. Now they're connected via these submarine cables. How many of those do we have? Anay Shah (03:39:34): We have approximately 600 active submarine cables that are powering 99% of international usage on the internet. Ben Shwab Eidelson (03:39:45): So in total, we're about 100 times the global bandwidth that we were a decade ago. And we've seen internet bandwidth continue to grow 25% year over year. Anay Shah (03:39:53): And so these numbers are today's snapshot. Now, there are a lot of projections out there of where growth is going to go over the next five years. It can get fairly outlandish, but suffice to say, growth is continuing to accelerate now and in the years to come. Ben Shwab Eidelson (03:40:10): All right, well, we talked about Byron's plane reading and writing blades of grass. Now it's time to land our plane. Anay, let's do the themes. What did you realize bubbled up for you over the course of this research and conversation? Anay Shah (03:40:24): It's been a journey. I did not know a lot coming in and have learned way more about the ins and outs of data centers than I ever imagined. A few big things will continue to stick with me. One of the first books we read is called *Tubes*, and it's this nonfiction, nerdy detective story to discover where this thing called the internet lives. It turns out the cloud is quite physical, but even more interesting, it's strangely concentrated. The internet has almost infinite edges, but it's got a shockingly small number of centers and has been built in this hub-and-spoke structure. And so for me, that starts with May East, right? Ashburn, Virginia, home to 13% of global data capacity and has once carried north of 30% of the world's internet traffic. That is the undisputed capital of the internet. Ben Shwab Eidelson (03:41:25): And it's so wild that that became the hub just by being the hub. There's just this center of gravity that spiraled on itself, a black hole of data center investment, almost by accident, right? Yeah. Anay Shah (03:41:38): The internet's structure is based on this mesh connectivity of global cities on coastal shores. It's Virginia, New York, D.C., London, Paris, Amsterdam, Tokyo, Seoul, Lagos, all connected by these 600 undersea cables that are literally powering the world's economy. And so I think about data centers that started off occupying closets, growing to whole floors, then buildings, then warehouses. And I start to think of the cloud as a building, as a factory, where a bit comes in, gets massaged, gets put together in the right way, and then it gets sent out to its destination, and it really brings the internet home. Ben Shwab Eidelson (03:42:21): For me, this connects to one of my reflections, which is threaded through the invisibility of this infrastructure and what makes data centers and the connecting internet almost the perfect abstracted infrastructure. And what I mean by that is you can access them, you can use them, you can get all of the power of them without ever seeing them or touching them. Now, with us living in a mostly wirelessly blanketed internet ourselves, you have access to all of these warehouses and all these buildings via all these undersea cables. And if you're an engineer developing software, you can deploy to all these regions around the world and never actually look at a CPU or a hard drive in your life. It is, I think, the best-abstracted physical infrastructure that humanity has built. It is physical, it is silicon, it is electricity, it is fiber. There is no real magic. It is all physics. But the only infrastructure I can think of in humanity that is as well abstracted is maybe money, but money is actually not physical anymore. In the same way, this actually is still doing physical work. And that struck me. Anay Shah (03:43:34): It does. It feels very rare to have global infrastructure that is defining our life and how the world operates, to be so. Ben Shwab Eidelson (03:43:44): Invisible, and so frequently in our life. Even though it is invisible, our entire day is mediated by this infrastructure. But if your entire day is spent on the train, you're very aware of where the tracks are, and you can see them, and you're on the train. That's the engine, right? Anay Shah (03:44:01): Even fire, we probably used it selectively throughout the day. The railroads, we got on and off. The car, we got in and out of. Electricity, we turned the lights on and off. This is a modern marvel that we're only not using when we sleep. And even then, it's doing stuff for us. Ben Shwab Eidelson (03:44:20): It's tracking my sleep. I'm wearing a ring that's tracking my sleep, sending bits to Oura's servers. Yeah, as you were saying that, it's like you're getting on and off a train. You're kind of turning electricity on and off in a transactional moment. This is almost more like you're living in it, right? You're living in this infrastructure, but we never see it. Anay Shah (03:44:44): I just had a flash of *Ready Player One*. Ben Shwab Eidelson (03:44:46): Yes. Anay Shah (03:44:47): Putting on the full suit and immersing myself in another reality. We're close, when you think about it this way. Ben Shwab Eidelson (03:44:54): I think that's right. I think this gets to one of my other realizations when comparing this to other physical infrastructure, which is, despite it being truly physical—buildings and silicon and electricity—we are upgrading it continuously and rapidly. Part of that is because of Moore's Law. Part of that is because of the improvement of the hardware and our techniques for cooling and all of these things. But I feel that with most other infrastructure, you get to, "Okay, this is a reasonably good way to build a train track. Now we need to go build more of it," or, "This is a reasonably good way to transport electricity. So now we're going to build more transmission." And certainly you get a little bit better at transmission, but we don't just get a little bit better here. We're getting 40x better. In the last decade, which was off the previous decade, it's gotten much better. So it's this weird thing where, yes, it's infrastructure, but it is changing what it is. We're getting at such an order of magnitude scale so quickly. And I can't think of other analogies to that. Maybe the first 20, 30 years of electricity felt like this, but this feels... something feels even more drastic here because what is being delivered is not commoditized exactly the same way as electricity. Anay Shah (03:46:02): That sense of movement that you're describing reminds me of another one of my takeaways, which is where I'm feeling stuck. And that's on the question of power. To me, it's easy to think about the complexities of why the power problem is so hard to solve, and yet the answers are right in front of us. The problem is huge, right? Data centers are pulling more and more electricity from the grid. The problem is hard because the grid is strained, the turbines and transformers are backlogged, and we don't really have utilities that are built to capture the incentive structures to value what hyperscalers are willing to pay today. And it's going to take years to reform the grid to actually meet the needs. And at the same time, the fastest, cheapest energy is available today. It's solar, storage, putting renewable energy online quickly and cheaply. That is already accounting for 90% of new capacity going on the grid. Despite every effort from our current administration to execute an ideological war on clean power and claim that we're in some national energy emergency, and yet deny the grid affordable electrons being placed on it. So if we can just get out of our own way, we can put a lot of power on the grid fairly quickly. The other side of that story is the way we're building data centers: accounting for 24/7 peak load. And there's a famous study that circulated earlier this year from Duke University that suggested that if we can limit grid-facing power of data centers by just 1% a year—90 hours out of the entire year—we can unlock 100 gigawatts of load. We can unlock the equivalent of two nuclear fleets on the US energy grid. Ben Shwab Eidelson (03:47:54): Right. You saw Google do some recent announcements where they're collaborating with some of the utility providers to do exactly that. There are new startups that we see all the time that are building businesses around that concept. This connects really closely to my major takeaway on the power story, which is that limitations drive innovation, especially when they come up against a legacy incumbent system—something that generally works, and reasonably well. Which for all of our general disgruntledness about utilities in our life, it is phenomenal that the electricity works most of the time as well as it does. And it has not gotten a kick in the behind in a long time in terms of needing to try and evolve. There have been decades of people talking about a smarter grid, but nothing's forced the need like the AI data center competition and the geopolitical competition with China, which drives innovation across the stack. It's exactly what you're saying in terms of driving the next cheap, clean electron on the grid in the forms of renewable energy. But it's also pushing us on transmission and permitting, and it's also pushing us inside the data center to think even further about AI models. And we've invested in companies that improve the efficiency of actually running and training the model. Anay Shah (03:49:12): That's right. Ben Shwab Eidelson (03:49:12): And so up and down this whole stack, a point we found again from talking to people and from research, these types of crunches drive efficiency innovation and the evolution of the grid. Anay Shah (03:49:24): And arguably, according to folks who have been in this for decades, we've been fairly lazy on innovation. It's been a lot of low-hanging fruit. And so this type of forcing function can drive step-change improvements in something as complicated as our national grid. Ben Shwab Eidelson (03:49:41): So to me, all of that points to long-term optimism: first, in solving the problems, but second, in solving them in a way that leads to more clean energy deployment. However, it also means needing to recognize and not dismiss the near-term local impacts for both power and water in these environments. Anay Shah (03:50:01): I love that point because as we talked through these stories, that theme kept recurring of how it is different when you think about the problem locally than when you think about the problem nationally. And I think that's really important for people to understand as this is more and more headline news. Ben Shwab Eidelson (03:50:16): My personal takeaway is less and less anxiety about that next incremental Gemini Query, ChatGPT, Anthropic use case or storing a video, and a greater desire to have more awareness locally within a given county that's struggling with air quality or water shortages. Because that's where activism needs to center within communities in the moment, rather than trying to slow the valuable uses and progress that we're all building toward. Anay Shah (03:50:45): Absolutely. This point on communities and geography and where things are located brings up another takeaway that's close to my heart as a former international politics major and someone who spent 20 years living and working across emerging markets. And that's the geopolitical implications of the moment we're living through right now. And so we talked a little bit about this, but to me, it really crystallizes into a twofold idea: One is the notion that we're living through this chip war, right? That microchips have replaced oil as one of the world's most critical resources and it's a key determinant of economic and military power. And we're in a new Cold War. This time, it's largely with China. As Americans, we don't want a country like China accessing our data or having superior performance over us. And so we're actively preventing US companies from selling chips to China. But it's not so straightforward, right? Because you think of, "Okay, Nvidia as an American company, we can prevent that." But Nvidia is the architect of the chips, right? The manufacturer of over 90% of the world's advanced chips is a Taiwanese company, TSMC, and they're the ones actually constructing it. So this geopolitical tension has various layers to it, but the notion that we're living through a new Cold War and the chip is the center of that story is quite striking. Ben Shwab Eidelson (03:52:07): I think in the long term, or mid to long term, this type of technology is a genie that can't be put back into the bottle. China, in particular, is hyper-aware of what they want to domestically build for themselves here and actually is in a great position from a supply chain perspective to pull that off. Even if they're a couple years behind. You've seen them spin up AI labs, spin up their own domestic chip manufacturing, and do impressive jobs. And you can say, "Well, 18 months ahead," but what is that in the scale of this foundational technology? Anay Shah (03:52:40): Yeah, 18 months is a blink of an eye. Ben Shwab Eidelson (03:52:42): I lean again toward long-term optimism that this competition creates good incentives for us to rethink our fundamental constraints, which are mostly on the clean energy deployment side. One big thing China has going for it that we don't have is its ability to grow generation of clean energy much, much faster than we do in the US. Again, because of their solar panel supply chain, their battery supply chain. Anay Shah (03:53:07): Their government structure, their ability to build, is unmatched. Ben Shwab Eidelson (03:53:11): And so this is giving a lot of our structures a kick to rethink what's slowing us down. If this is so existentially important to stay at least on par, if not ahead on. Anay Shah (03:53:22): I agree strongly. In this notion of the chip war, I think it's interesting. In this moment, I think over the medium to long term, it's less. So here's where I don't share as much optimism: the AI boom and data center build-out that's potentially creating a new digital divide, exacerbating global inequality on yet another dimension. So we've talked about how US hyperscalers run almost 50% of data center compute. The U.S., China, and the European Union host more than half of all powerful data centers. What does that mean for the billions of other people living around the world? And what is the implication for a country that is renting compute power from a faraway data center? It's potentially higher costs, slower connection speeds, and compliance to different laws. But more importantly, as we unpack this—as we're trying to wrestle with this idea—it's this question of having a critical national security and economic infrastructure input owned, operated, and housed outside of your control. Ben Shwab Eidelson (03:54:28): The techno-optimist in me is just screaming to refute this, and I'll tell you why. And I think we can study the internet as an example. This feeds into, I think, another meta theme for us, which is that these companies are the modern-day utilities, but they're actually the opposite in a way of the governing structure of an electricity utility, which is hyper-localized and granted local monopoly. These are global utilities. They're providing much more non-commodity functionality, everything down to a bespoke AI model that they're training, down to more commodity storage. And the business model of a utility is to, as we've seen again and again, provide more as cheaply as possible to as many customers as possible. And you've seen the biggest internet companies grow by incentivizing and subsidizing free services to more and more people. And that has led to a lot of the world getting connected that was not connected just 10 years ago. There's reason to be cynical about what Google and Meta are doing, trying to get more of the world connected. Don't they just want more customers? Obviously, yes, they want more customers. But in doing so, a lot more of the world has access to the internet than ever before, and information and knowledge. And I think that's where I fall on the side of optimism: that there's nothing more that OpenAI wants than more people having access to OpenAI. Now, that to me goes to the individual human level, which is very different than how a nation-state or the government in control of a nation-state might feel about it. But I lean on the individual human in a global context, and what powers I want them and their children to have in the decades to come. And that's where I get a source of optimism. Anay Shah (03:56:07): I mean, it's true. Sixty-six-plus percent of the world's population is now connected to the internet, and that number is increasing. I like the optimism. I wonder if the nature of this technology, particularly AI compute power, functions differently than access to the internet, access to electricity, and access to other similar infrastructures—having that controlled by private equity and public companies that have very specific incentive structures. Ben Shwab Eidelson (03:56:39): I think there's a big thing that you just hit on for me, which is my optimism doesn't really account for the hosting government of hyperscalers. And so it's not just that these are companies, but they're American companies. If they're American companies, they exist a little bit at the pleasure of the American government. And that means that another country's access to them exists at the pleasure of the American government. And that, I think, gets to a more complex nuance, and to me, it just drives value to the idea of more global models and more global competition, even if it's not hosted within a given country, so that it's not just one or two governments that have control over these technologies. Anay Shah (03:57:22): More models trained on different languages, of course, and different speech patterns. And so, how this unregulated utility—as we're talking about, this new digital utility, this new critical infrastructure that society will increasingly depend on—will continue to be seen as a public good and delivered to more and more people as that... You know, they say hindsight is 20/20, but there's something even more interesting about creating a historical narrative to something like data centers. And it's the marvel of the narrative process, this story, that starts to make so much sense of how one thing came after another. So you start off with IBM and Watson's journey and the introduction of the punch card that is storing data, and then the machines that start to compute off that storage, and that grows and evolves. And you've got the Cold War introducing the need for connecting networks, and suddenly you've got compute and storage power. Network connections bring that together, and that continues to evolve. And then you get the explosion of the '90s. And in this time, first slightly accidental and then more purposeful infrastructure starts to get built in very important nodes, and the fiber boom happens. I think I'll now always associate the late '90s with the overbuild of fiber and this notion that by the time the dot-com bust happened, we only used 3% of the fiber that we built. It's wild. And yet that was exactly what we needed to go through the run-up of the 2000s and the growth of consumer and enterprise internet, mobile, and streaming, and then eventually the cloud. Ben Shwab Eidelson (03:59:10): And then you had crypto creating this market for the AI chips before we had the models. Cycles of building ahead of the curve, there being some bust, but then there being this new utilization of those fundamental building blocks. Anay Shah (03:59:22): Absolutely. And these brilliant people located within very important companies that took the lead to build what is now foundational infrastructure for the world. Ben Shwab Eidelson (03:59:34): And then there's another dance that's happening—in where storage and compute lives for the average consumer or average business—where you go from it being all on the mainframe. Then it moves eventually to the PC. And you feel so powerful with your PC, but you have no collaboration, no backup, no redundancy. Then it moves back to the server room. Then in the mobile era, your connection is really slow. Early days, you think about the first iPhone: it was a really slow edge connection. So your phone actually had to do most of the stuff on the phone. But now we live in this world where the connection's back, fat, and happy. And so we do everything now on the server except for rendering the UI on the phone. And so this dance—of where both compute and storage happens, mediated by the communication bandwidth—is a really interesting dance that I see played out in the story. Anay Shah (04:00:26): That dance then drove the cloud boom of the 2010s and the maturing of enterprise cloud and infrastructure, where now you're able to do everything you want at higher and higher utilization in the cloud. And all of that build-out, and that fine-tuning and optimization, allowed us to enter March of 2020—the dark days of the beginning of the pandemic—and switch over from in-person to online from the perspective of the internet, largely without much of a hiccup. Ben Shwab Eidelson (04:01:04): Yeah, going to slightly lower resolution Netflix videos for a month. I think we handled it pretty well from a technology infrastructure perspective. Anay Shah (04:01:13): Correct. And how that just so happened to juice the infrastructure even more, to lay the foundation for GPUs to finally be utilized for what was growing in the background: machine learning and large language model training. And then November of 2022, putting a chatbot on top of a model, and the AI boom and the infrastructure that was laid to capture that moment, and the cash coffers of the hyperscalers that were ready to meet that moment, are now propelling the greatest technological infrastructure build-out in human history. What a story. Ben Shwab Eidelson (04:01:59): Yeah. Do you think it's an overbuild? Anay Shah (04:02:02): There are things that I worry about in this world, as we noted from the geopolitical situation, but once I strap in, the optimist in me takes over. So I am fully convinced that we will utilize every photon, electron, and square footage. I hope that this is the opportunity where we collectively can seize the moment to get more clean, firm power online, more affordable power online, and where we can use technology and renewable power to reduce the depletion of water resources in critical areas. The opportunity to build a newly-skilled workforce to build out this new infrastructure. The opportunity to modernize our aging grid, our transmission lines, the way we do siting and permitting, the way we do environmental review processes, the way utilities are incentivized to meet demand. The opportunity to spur local economic development in communities all over the country, and in the world. The opportunity to have large enterprises build out infrastructure that will be a net benefit to the consumer where they're locating their assets. And I think it's really important for us to be mindful of the cost of moving so quickly and not thinking through the implications of water, power, climate, communities, and global equity. And my real hope is that when you have this much capital and this much demand coming on to reshape a foundational technological infrastructure, we come out the other end with an ability to—as one of our new friends that we interviewed said—build these assets as ecologically invisible as possible. I think we have the opportunity to do that. And when we come out the other end having done that, I think our children's generation will look more kindly on us. --- Ben Shwab Eidelson (04:04:04): I think so, yeah. My big takeaway is that, of all the things we might study on this show, this one feels like the major piece of infrastructure that we're living in the middle of the step change. We are in the sharp part of the curve that's going up. This level of capital investment and build-out is what it looked like if you were to put yourself back into the initial build-out of the electricity grid or the railroad system, but further compressed. What's so interesting about that is you get to experience personally how quickly you adapt to amazing infrastructure and power—how quickly we've adapted, I think, to having the latest large language models at our fingertips. Now I have Claude Code, and I'm a few weeks into using that. I have what feels like new powers, but I'm kind of getting used to it again. We lived through most of this story and have vivid memories. Like, I remember Blockbuster, and I remember when Netflix was a DVD rental service. That was because it was too expensive to move streaming video at that scale, and now it's blindingly cheap. The business model changes; the products completely change. We are living through this cycle of infrastructure iteration and evolution at a pace that's rapidly changing what we can do with it. I don't know when we'll again get to the upper step of that curve, but we are living in the step change, and I'm just trying to relish that experience. Anay Shah (04:05:35): The rocket ship is in takeoff, feeling some G-forces. That's right. Ben Shwab Eidelson (04:05:39): But you get used to it, don't you? Anay Shah (04:05:41): Very quickly. Shockingly fast. I mean, even since we launched the fund two years ago to now, our workflows have changed dramatically. Ben Shwab Eidelson (04:05:53): That's right. Anay Shah (04:05:54): Despite the fact that you and I continue to look at each other 12 hours a day in the same screen. Yeah, yeah. The normalization of humans is astounding. And then you start thinking about our kids and how they're growing up in... Ben Shwab Eidelson (04:06:06): This moment: speaking to ChatGPT, Santa Claus if they want, or making a game. My four-year-old programmed a game that runs in JavaScript by telling Claude what he wanted. Anay Shah (04:06:18): Amazing. Ben Shwab Eidelson (04:06:20): It is wild, wild times. Anay Shah (04:06:22): We are living through a step change moment. Thanks for listening. Hopefully you're that much more informed about the invisible infrastructure that's not so invisible in powering the modern world. Ben Shwab Eidelson (04:06:39): You know how you got this podcast into your ears? It's amazing. That's right. Anay Shah (04:06:43): If you enjoyed this and have friends or colleagues that you think may find the story of data centers valuable, please send it their way. Ben Shwab Eidelson (04:06:51): And please make sure you're subscribed to Step Change in your podcast player of choice so you find out when our next episode comes out, which at this current pace will be at least once more this year, I hope. And sign up for emails from us at StepChange Show. Podcast ratings make a huge difference to help people discover this, so we always appreciate them and love hearing from listeners. Shoot us an email anytime. It'll bounce from your data centers to ours over at hi StepChange Show. Anay Shah (04:07:20): At Step Change, we invest in early-stage companies accelerating energy abundance and building critical infrastructure. So if you're a founder working on software to help make all of this work, from data center management to efficiency to power generation, reach out to us. We'd love to check, chat, and learn more. And last but not least, we talked to a number of folks in the data center and cloud worlds who have helped us tremendously with this research. Deep appreciation and gratitude to Christian Baledi, Peter Gross, Brian Janis, Sean James, Byron... Ben Shwab Eidelson (04:07:52): Rakitsas, Brandon Middaugh, Nat Bullard, John Kumi. And a thank you to Ben Gilbert and David Rosenthal for all of their Acquired episodes on AWS, Microsoft, Google, and Nvidia. And a big thank you to Nick Petrie, our editor, and to our wives, Shiva and Ana, for hearing us talk about data centers for the last six months. And, all right, until next time, thank you, Sam. Ready for more?
--------------------------------------------------

Title: Meta (META) Stock Backed With $900 Price Target as Reels Ads Expand
URL: https://finance.yahoo.com/news/meta-meta-stock-backed-900-153005647.html
Time Published: 2025-09-17T15:30:05Z
Description: Meta Platforms, Inc. (NASDAQ:META) is one of the AI Stocks Analysts Say You Should Watch Closely. On Septemer 15, Citizens JMP analyst Andrew Boone...
--------------------------------------------------

Title: The Federal Reserve Cuts Interest Rate by a Quarter-Percentage-Point, Signals 2 More Before the End of the Year
URL: https://www.barchart.com/story/news/34867935/stocks-tread-water-as-fomc-decision-nears
Time Published: 2025-09-17T15:27:26Z
Description: The S&P 500 Index ($SPX ) (SPY ) today is up -0.17%, the Dow Jones Industrials Index ($DOWI ) (DIA ) is up +0.77%, and the Nasdaq 100 Index ($IUXX ) (QQQ...
--------------------------------------------------

Title: Today in Apple history: Apple stock smashes through $700 barrier
URL: https://www.cultofmac.com/apple-history/apple-stock-hits-700-dollars
Time Published: 2025-09-17T14:48:43Z
Full Content:
By Luke Dormehl • 7:48 am, September 17, 2025 September 17, 2012: On the back of record iPhone 5 preorders of 2 million in 24 hours, Apple’s stock price hits a new all-time high. For the first time in history, AAPL breaks the $700 mark in after-hours trading. Passing the milestone cements Cupertino’s place as the world’s most valuable publicly traded company. Amazingly, the new record is $270 a share higher than at the start of the year. Apple stock rose 65% in just nine months. By any measurable metric, Apple enjoyed soaring success in 2012. A stock price breaking $700 was just the latest in a series of dizzying records the company set in a few short years. In January 2006, Apple surpassed the value of Dell, the company whose founder once said he’d shut down Apple and refund its shareholders. A few years later, Apple overtook Microsoft — besting the tech giant that dominated the 1990s. In August 2011, Apple briefly overtook ExxonMobil to become the world’s most valuable company. And Apple’s stock price just kept climbing from there. At the end of February 2012, Apple surpassed the $500 billion market cap, then smashed through $600 billion in April. By September 2012, Apple was valued at upward of $650 billion. To put that in perspective, Apple was worth more than second-place ExxonMobil plus the entire net worth of Google at the time. Not bad for a company that had fallen to $12.88 a share on September 29, 2000. Apple hasn’t exactly renounced its winning ways since then. It became the first publicly traded U.S. company to pass the $800 billion, $900 billion, $1 trillion, $1.5 trillion and $2 trillion valuations. In early 2022, it surged past the $3 trillion line briefly before dropping amid a wider market pull-back. And it did the same thing in mid-2023. (Phew!) At the time of writing, AAPL is trading at around $240 a share. However, it’s not fair to compare this stock price with Apple’s $700 milestone from 2012, due to a 7-to-1 stock split in 2014 and a subsequent 4-to-1 split in 2020. As the stock prices of some AI-focused companies surged in recent years, Nvidia and Microsoft surpassed Apple’s market valuation. Currently, Apple’s market cap sits at $3.554 trillion, making it the third most valuable publicly traded company in the world, with Google’s parent Alphabet, Amazon and Meta hot on its heels. Daily round-ups or a weekly refresher, straight from Cult of Mac to your inbox. Our daily roundup of Apple news, reviews and how-tos. Plus the best Apple tweets, fun polls and inspiring Steve Jobs bons mots. Our readers say: "Love what you do" -- Christi Cardenas. "Absolutely love the content!" -- Harshita Arora. "Genuinely one of the highlights of my inbox" -- Lee Barnett. The week's best Apple news, reviews and how-tos from Cult of Mac, every Saturday morning. Our readers say: "Thank you guys for always posting cool stuff" -- Vaughn Nevins. "Very informative" -- Kenly Xavier. Copyright © 2005 - 2025 - Cult Of Mac. All rights reserved.
--------------------------------------------------

Title: Has Elon Musk really been awarded a $1 trillion pay deal?
URL: https://www.aljazeera.com/news/2025/9/17/has-elon-musk-really-been-awarded-a-1-trillion-pay-deal
Time Published: 2025-09-17T14:22:48Z
Full Content:
And why has the South African entrepreneur been busy buying up shares in Tesla? By Alex Kozul-Wright Share Save Tesla shares jumped 6 percent on Monday after CEO Elon Musk disclosed that he had bought $1bn worth of the company’s stock. The move reinforces Musk’s push for greater control over Tesla and comes a week after the company’s board offered him a $1 trillion pay package over the next decade. Musk’s stock purchase – his first open-market buy-up of shares since 2020 – comes at a critical time for Tesla, as it races to transform into an artificial intelligence and robotics firm whilst also grappling with falling sales of electric vehicles (EVs). But Musk’s pay packet has come in for intense criticism. Last weekend, Pope Leo decried the widening pay gap between corporate bosses such as Elon Musk – whose estimated wealth now stands at $367bn – and ordinary working people, which he said was a major factor in growing global unrest. On September 12, Musk, 54, purchased 2.57 million shares (which represents less than one percent of Tesla’s market capitalisation), paying between $372 and $397 per share as the price varied through the day, according to regulatory filings. He now owns almost 20 percent of Tesla, which seemingly pleases its investors. Tesla’s share price rose to around $422 on Monday – still 12 percent lower than its all-time high of $479 (reached in December 2024). Following his recent move, Musk posted on X that the increase in Tesla’s value was “foretold in the prophecy”. While Musk wasn’t an original founder of Tesla – he invested in the company one year after it was established – he became chairman in 2004. The South African entrepreneur has consistently demanded a bigger stake and more voting power at Tesla, having previously said he would prefer to build AI products and robots outside of Tesla if he cannot control 25 percent voting power in the firm. Musk sold more than $20bn of Tesla’s stock (or 4.6 percent of its market cap) in 2022 to fund his acquisition of Twitter, now X, for $44bn. He also owns private holdings in SpaceX, Neuralink and The Boring Company. The Tesla CEO will have to meet certain performance-related criteria first. To unlock the full $1 trillion payout, Musk will have to raise the company’s valuation from roughly $1 trillion today to $8.5 trillion over the next 10 years. He will also have to sell one million autonomous taxis and one million robots and increase Tesla’s profits by more than 24 times what it earned last year. Tesla currently operates a few dozen autonomous taxis in a limited area in the city where it is headquartered, Austin, Texas in the US. Known as “robotaxis“, they are self-driving vehicles but are accompanied by human “safety supervisors”, who can intervene if problems occur. On the robotics side, the company unveiled its first humanoid robot – Optimus – in 2022. In 2024, Musk claimed that Tesla would deploy robots for “internal use [ie for use inside its own factories]” in 2025, and that it would have produced 5,000 units by then. Neither pledge has been met so far. Musk also recently said that “80 percent of Tesla’s [future] value will be Optimus”. After Musk joined Tesla in 2004, he took very little cash pay. Instead, he chose to be paid in equity. Then, in 2018, shareholders approved a landmark 10-year pay package for Musk – linked to various operational targets – estimated at $2.6bn. As Tesla’s market value surged after the start of 2020 (when it was trading at just $29.50 a share), many of those pay objectives were met, and Musk received a large number of additional Tesla shares. Due to broad stock market gains since the COVID-19 pandemic, Musk’s earnings are estimated to have climbed by $40bn-$60bn. Though Musk’s pay windfall at Tesla has attracted regulatory scrutiny for overcompensation, especially from Delaware’s Court of Chancery, most of the company’s shareholders have repeatedly ratified the CEO’s payment packages. Tesla doesn’t disclose non-executive salaries, so it is hard to say how Musk’s income compares to that of the average worker there. However, corporate pay in the US has generally rallied in recent decades compared to that of workers. According to the Economic Policy Institute, average pay for CEOs at S&P 500 companies – the 500 biggest listed firms in the US – rose by almost 1,000 percent over the 50-year period leading up to 2024. By contrast, a typical worker at an S&P 500 company has seen his or her pay packet rise by just 27 percent (adjusted for inflation) over the same period. Stated differently, the CEO-to-worker pay ratio has increased from 30:1 to 350:1 over the past five decades. In an interview last week with Crux, a Catholic news website, Pope Leo singled out Elon Musk as an example of the kind of wealth he said was corroding “the value of human life, of the family, of the value of society”. Asked about Tesla’s proposed $1 trillion pay packet, Leo responded: “What does that mean, and what’s that about? If [personal wealth accumulation] is the only thing that has any value any more, then we are in big trouble.” Despite its recent uptick, so far this year, Tesla’s stock market performance has been among the worst of the “Magnificent 7” group of tech giants – which also includes Alphabet, Amazon, Apple, Meta, Microsoft and Nvidia – having lost around 2 percent of its value this year so far. Tesla’s most recent quarterly results showed profit losses amid falling demand for electric vehicles and increased import production costs associated with US President Donald Trump’s trade tariffs. Looking ahead, earnings look set to continue falling. Sales of Tesla cars in the US will likely fall further in the last three months of 2025, as Trump has refused to extend a tax credit for EV purchases for US consumers after October. Up to now, the rebate has played a crucial role in making American EVs more affordable. Follow Al Jazeera English:
--------------------------------------------------

Title: Mark Zuckerberg wowed Meta skeptics this time last year. He's about to try to do it again.
URL: https://www.businessinsider.com/mark-zuckerberg-trying-to-wow-skeptics-ai-glasses-meta-connect-2025-9
Time Published: 2025-09-17T12:02:01Z
Full Content:
Meta's vision of AI-powered glasses as the next must-have device is about to face a major test. CEO Mark Zuckerberg set the bar high at Meta Connect in 2024 with the not-for-mass-market technology in its Orion glasses. Now he has to take the next step in marketable wearables — and get consumers to shell out. Zuckerberg is expected to take the stage at Meta's Menlo Park headquarters for the company's annual Meta Connect conference to lay out his vision for Meta glasses, headsets, and the future of contextual AI. The stakes are high. Meta is spending billions on AI research and hardware while competitors like Apple, OpenAI, and Google are chasing the same market. Meta Connect is Zuckerberg's chance to prove the company's sprawling investments in wearables, software, and AI are converging into something consumers would potentially pay more for. Last year, Zuckerberg generated major buzz with a prototype of the Orion glasses, Meta's first true attempt at fully holographic AR. He also laid out a vision of wider adoption of AI glasses. Though still a developer-only product, Orion impressed early testers, including Nvidia CEO Jensen Huang and Reddit's Steve Huffman. The demo also earned positive reviews from Business Insider's own Peter Kafka, who said he would "buy them in a heartbeat." At the event, Zuckerberg also rolled out updates to existing products, such as transparent Ray-Ban Meta frames, new celebrity voices for Meta AI, and a budget-friendly Quest 3S headset at $299. Meta's stock rose more than 2% during Zuckerberg's keynote, and the Ray-Ban smart glasses also became a hit, with sales surpassing 2 million units by early 2025. In May, Zuckerberg announced that Meta AI now has around 1 billion monthly active users across Meta's apps. Leaks and reports suggest Zuckerberg will use this year's keynote to shift focus from experimental prototypes to products that it hopes will be widely adopted. Orion is equipped with cutting-edge technology and costs $10,000 to produce per pair. Meta's test this year is whether it can release a stepping-stone product in terms of price point and capabilities, and whether Zuckerberg could convince consumers to pay more for a pair of glasses than the current Ray-Ban Meta model, which starts at $299. Here is what we might see: This year's Meta Connect event comes during an escalating talent war in Silicon Valley. Not including the billions that Reality Labs, the division behind AR/VR, has lost in 2025, Meta has been dangling multimillion-dollar compensation packages to lure AI researchers and hardware engineers, and it seems to be working. Earlier this year, the company launched Meta Superintelligence Labs under Scale AI CEO Alexandr Wang. To bring Wang on board, Meta invested $14.8 billion in the startup for a 49% non-voting stake. In July, Meta lured away a co-creator of ChatGPT from OpenAI and appointed him chief scientist of its Superintelligence Labs. The company has also recruited three researchers behind OpenAI's Zurich office — Lucas Beyer, Alexander Kolesnikov, and Xiaohua Zhai — who had spent time at Google's DeepMind. With all this AI talent and new hardware, Meta needs to wow consumers and Wall Street again. Jump to
--------------------------------------------------

Title: Rs 14 crore to Rs 390 crore! Facebook's early investor strikes gold again, this time with Urban Company IPO
URL: https://economictimes.indiatimes.com/markets/stocks/news/rs-14-crore-to-rs-390-crore-facebooks-early-investor-strikes-gold-again-this-time-with-urban-company-ipo/articleshow/123937117.cms
Time Published: 2025-09-17T06:18:45Z
Full Content:
Accel’s early investment in Urban Company turned Rs 14.3 crore into a Rs 390 crore payout after the home services platform debuted with a 57.5% premium over its Rs 103 IPO. Accel first backed the home services startup over a decade ago, buying shares at an average price of Rs 3.77 apiece. The firm still holds shares valued at over Rs 1,100 crore. (What's moving Sensex and Nifty Track latest market news, stock tips, Budget 2025, Share Market on Budget 2025 and expert advice, on ETMarkets. Also, ETMarkets.com is now on Telegram. For fastest news alerts on financial markets, investment strategies and stocks alerts, subscribe to our Telegram feeds .) Subscribe to ET Prime and read the Economic Times ePaper Online.and Sensex Today. Top Trending Stocks: SBI Share Price, Axis Bank Share Price, HDFC Bank Share Price, Infosys Share Price, Wipro Share Price, NTPC Share Price (What's moving Sensex and Nifty Track latest market news, stock tips, Budget 2025, Share Market on Budget 2025 and expert advice, on ETMarkets. Also, ETMarkets.com is now on Telegram. For fastest news alerts on financial markets, investment strategies and stocks alerts, subscribe to our Telegram feeds .) Subscribe to ET Prime and read the Economic Times ePaper Online.and Sensex Today. Top Trending Stocks: SBI Share Price, Axis Bank Share Price, HDFC Bank Share Price, Infosys Share Price, Wipro Share Price, NTPC Share Price The foresight of Peyush Bansal made Lenskart IPO-ready. What next? Damodaran vs. proxy firms: High priests of governance spar over own ethics Distributors outdo banks, grab 70% of INR2.53 lakh crore mutual fund assets Metro, Vande Bharat are changing train travel, and the course of this wagon maker too Policy measures that can help small exporters beat Trump tariff Stock picks of the week: 5 stocks with consistent score improvement and upside potential of up to 29% in 1 year All Mutual Funds Top Tax Saving Mutual Funds Better Than Fixed Deposits Low Cost High Return Funds Best Hybrid Funds Best Large Cap Funds SIP’s starting Rs. 500 Top Performing Mid Caps Promising Multi Cap Funds Top Rated Funds Top Performing Index Funds Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Slideshow Private Companies Top Prime Articles Top Story Listing Top Definitions Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Worry not. You’re just a step away. It seems like you're already an ETPrime member with Login using your ET Prime credentials to enjoy all member benefits Log out of your current logged-in account and log in again using your ET Prime credentials to enjoy all member benefits. Big Price Drop! Flat 40% Off Offer Exclusively For You Save up to Rs. 700/- ON ET PRIME MEMBERSHIP Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get Flat 40% Off Then ₹ 1749 for 1 year Offer Exclusively For You ET Prime at ₹ 49 for 1 month Then ₹ 1749 for 1 year Special Offer Get flat 40% off on ETPrime What’s Included with ETPrime Membership Trump temper on H-1B visas is forcing Indians to do these things to stay put in US What Adani’s US indictment means for India Inc’s overseas fundraising Why veterans like Reliance, L&T are on acquisition spree? Aswath Damodaran has an answer. Will China’s dollar bond sale in Saudi Arabia trump the US in financial world? Huawei launches its own OS to compete with Google and Apple. But can it win beyond China? The problem with lab grown diamonds Why a falling rupee is a better option for the economy A list of top 20 momentum stocks that have delivered massive returns in one year Investment Ideas Grow your wealth with stock ideas & sectoral trends. Stock Reports Plus Buy low & sell high with access to Stock Score, Upside potential & more. BigBull Portfolio Get to know where the market bulls are investing to identify the right stocks. Stock Analyzer Check the score based on the company's fundamentals, solvency, growth, risk & ownership to decide the right stocks. Market Mood Analyze the market sentiments & identify the trend reversal for strategic decisions. Stock Talk Live at 9 AM Daily Ask your stock queries & get assured replies by ET appointed, SEBI registered experts. ePaper - Print View Read the PDF version of ET newspaper. Download & access it offline anytime. ePaper - Digital View Read your daily newspaper in Digital View & get it delivered to your inbox everyday. Wealth Edition Manage your money efficiently with this weekly money management guide. TOI ePaper Read the PDF version of TOI newspaper. Download & access it offline anytime. Deep Explainers Explore the In-depth explanation of complex topics for everyday life decisions. Health+ Stories Get fitter with daily health insights committed to your well-being. Personal Finance+ Stories Manage your wealth better with in-depth insights & updates on finance. New York Times Exclusives Stay globally informed with exclusive story from New York Times. TimesPrime Subscription Access 20+ premium subscriptions like Spotify, Uber One & more. Docubay Subscription Stream new documentaries from all across the world every day. Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Stories you might be interested in
--------------------------------------------------

Title: Finally, the Fed
URL: https://finance.yahoo.com/news/finally-fed-043205658.html
Time Published: 2025-09-17T04:32:05Z
Description: After weeks of market gyration and agonising over every speck of U.S. data, the day is finally upon us when the Federal Reserve delivers its appraisal of how...
--------------------------------------------------

Title: Why Citizens JMP Sees Profitability Tailwinds for Alphabet (GOOGL)
URL: https://finance.yahoo.com/news/why-citizens-jmp-sees-profitability-030419304.html
Time Published: 2025-09-17T03:04:19Z
Description: Alphabet Inc. (NASDAQ:GOOGL) is one of the AI Stocks in the Spotlight for Investors. On September 11, Citizens JMP analyst Andrew Boone reiterated a Market...
--------------------------------------------------

Title: 10 Holiday Marketing Strategies to Stand Out in AI Search - Africa Talks Business
URL: https://www.africatalksbusiness.com/2025/09/16/10-holiday-marketing-strategies-to-stand-out-in-ai-search/
Time Published: 2025-09-17T01:01:23Z
Full Content:
‘Tis the season. Everyone is gearing up for that time of year — holiday shopping. Is your businesses’ search engine optimization (SEO) strategy ready for the holidays? If you just broke out into a cold sweat thinking about it, don’t worry — we’re here to help. We’ll explore tips, strategies, and budget-friendly tricks to make sure your business is found this holiday season (and beyond) — and how you can use your customer relationship management (CRM) tools to turn your top searchers into your top customers. What you’ll learn: AI search, including Google’s AI Overviews, Bing Copilot answers, and other LLM-powered engines, increasingly shapes how shoppers discover products during the holidays. A holiday AI SEO marketing strategy is a tactic to help small businesses appear in search engines and get seen for the holiday season — any season, really. Good SEO means making sure your digital presence is fully optimized, from your web copy to your store’s product descriptions to your opening hours, no matter what season it is. Let’s dig into why this is important. Effective SEO increases your brand’s visibility, driving more qualified leads to your website. These leads can then be captured and managed within your CRM system. In the reverse, content optimized for SEO can provide valuable information for your sales and marketing teams, enriching the data within your CRM. Also, by optimizing your website with the latest in AI-enabled SEO, you can personalize their experience and tailor your CRM efforts to nurture them more effectively. The future of SEO is AI-powered, and small businesses can get in on it too. The holidays are one of the few times of year when being small is an advantage. Surveys show 93% of U.S. consumers plan to “shop small” during the holiday season, and the U.S. Chamber of Commerce reports that eight in ten small businesses say Q4 is critical for annual profit. Modern LLMs are trained to detect and surface businesses that align with high-intent queries like “local gifts near me,” “independent bookstores with fast shipping,” or “family-owned bakery Christmas pies.” To be recognized, SMBs need to make these signals explicit. Practical steps: How a CRM can help: A CRM allows you to segment your customer base and understand their past purchases and preferences. This means you can create highly targeted promotions, gift guides, and website copy that resonate with specific customer groups, increasing the likelihood they’ll click through from search or answer engine results. No matter where you are on your journey as a small business, you can get started with Starter Suite — the all-in-one CRM for SMBs. Customer reviews are vital, but they’re not the only trust signal that matters in AI search. Answer engines cross-reference multiple sources before including a business in an answer. Local press mentions, chamber of commerce directories, event sponsorships, and nonprofit partnerships are all citations that machines can parse and validate. Practical steps: How a CRM can help: A CRM can help manage these efforts by centralizing contact information, tracking outreach, segmenting customer data, automating reminders for updates, and managing event planning. By proactively managing these diverse trust signals, businesses can improve their visibility and credibility in AI search.. Shoppers are no longer just typing. The proliferation of voice assistants and visual search tools means your business must be discoverable beyond traditional text queries. In fact, Google Lens reports billions of monthly image searches alone, so SMBs should think beyond text. Get your website voice-ready: Make your images findable: Multimodal AI discovery isn’t hypothetical; it’s happening now. A shopper might snap a photo of a candle on TikTok and ask, “Where can I buy this locally?” in search, and land on your store if your assets are properly tagged. How a CRM can help: By organizing customer data and preferences, a CRM enables businesses to create more targeted and relevant content that can be surfaced through voice and visual searches. Also, CRM data can help businesses keep branding and product information consistent across their online presence. Large retailers publish long holiday gift guides on their sites, but SMBs can win by publishing specific guides focused on certain topics or personas. In an AI-first world, specificity wins. AI overviews often surface micro-content that maps tightly to niche intents: “teacher gifts under $25,” “last-minute local pickup in Brooklyn,” “eco-friendly hostess gifts.” These focused guides often map directly to LLM-driven answer boxes, which pull concise, niche content to satisfy specific shopping intents. For SMBs, creating multiple focused guides is both achievable and impactful. Instead of pouring effort into a 2,000-word omnibus guide, launch three to five smaller landing pages: Each page should feature short answer capsules, such as: “Hand-poured soy candle, $22, available for same-day pickup. Perfect for teacher gifts – eco-friendly and long-lasting.” These capsules can be lifted directly into AI summaries, while the pages themselves capture long-tail traffic. Cross-link guides to relevant product pages and vice versa, ensuring both human shoppers and AI systems see the connections. How a CRM can help: By segmenting customer data, a CRM enables businesses to identify niche intents and create relevant guides that capture long-tail traffic. Additionally, a CRM can help businesses cross-link guides to relevant product pages. Learn how autonomous AI can scale your small business for efficient growth in our free e-book. Google’s guidance emphasizes E-E-A-T: Experience, Expertise, Authoritativeness, and Trustworthiness. This humanization isn’t just about trust; it’s a critical differentiator. SMBs have an edge here. While big-box retailers rely on mass-produced descriptions, you can signal authenticity with small touches. What works: Research shows consumers increasingly value authenticity: 88% say they’re more likely to buy again after a great service experience, according to the latest State of the Connected Customer report. AI raters are trained to look for this, too — pages that show lived experience (“we tested,” “our team recommends”) are more likely to surface than generic, stock copy. How a CRM can help: A CRM can help small businesses demonstrate E-E-A-T and authenticity by managing customer interactions and data that inform personalized content, such as customer testimonials, product recommendations, and staff-curated content. Additionally, a CRM can help businesses track customer interactions and feedback, allowing them to refine their content and improve overall customer experience. AI engines don’t just crawl websites; they also increasingly draw from community conversations on social media. Mentions in Reddit threads, TikTok videos, or Quora answers can seed authority signals that flow back into AI-curated summaries, acting as powerful, unlinked brand mentions. For example, shoppers often post on Reddit boards like r/Gifts or r/BuyItForLife asking for holiday ideas. If a satisfied customer mentions your handmade product there, it becomes part of the training data that models like Perplexity or ChatGPT may cite. Similarly, TikTok’s algorithm surfaces local businesses for trending holiday gift tags, and AI scrapes those public posts. Practical plays: AI engines are trained to identify consensus, along with other things. A handful of high-signal mentions in the right community can outweigh dozens of low-quality backlinks. For SMBs, that’s a winnable path to visibility. How a CRM can help: A CRM can track customer mentions, reviews, and feedback, helping businesses to identify opportunities to encourage happy customers to share their experiences and showcase their products in community forums. By integrating social media data, a CRM can also help businesses monitor their authority signals and adjust their marketing strategies accordingly. Cross-platform visibility means your business information is identical, and present, across the digital platforms that AI reconciles: your site, Google Business Profile (GBP), marketplaces (Amazon, Etsy, Walmart), and social shops. AI systems (and people) mistrust mismatches, and guidance from Google stresses consistent, helpful, non-commodity content. Parity checklist (check weekly during peak holiday times): Why it matters: AI will show fewer options. Brands with coherent facts across touchpoints are safer to include. How a CRM can help: A CRM centralizes your customer and product data, ensuring consistency across your website, Google Business Profile, marketplaces, and social media platforms. By managing this data in one place, businesses can easily update product information, pricing, and other details across multiple channels, reducing the risk of mismatches and inconsistencies. Set up your digital storefront, engage customers, and sell more using a commerce-ready platform with integrated tools for every sale. Google’s holiday research highlights how spur-of-the-moment mobile sessions drive decisions; four in five smartphone holiday shoppers use their phone in spare moments, and those micro-moments add up. Here’s how to make sure your SMB site is set up for these micro-moments. 90-second purchase checklist: How a CRM can help: A CRM can help small businesses capture shoppers on the go by managing detail-heavy customer interactions and data that inform targeted marketing efforts, such as SMS reminders and instant promotions. Small Business Saturday, taking place every fall, is a moment to compound visibility: spikes in local search, social shares, and reviews build authority that carries into December. The SBA cites long-running consumer participation and cumulative spend; Constant Contact found one-third of annual revenue can hinge on the holiday season for many SMBs. Here’s how you can get it on the Small Business Saturday traffic in the week leading up to November 5: How a CRM can help: With a CRM, you can automate and personalize your marketing efforts, such as sending targeted emails and SMS messages, and track customer behavior and preferences for future marketing strategies. Additionally, a CRM can help you manage your business’s online presence by storing customer data, preferences, and interactions in one place, making it easier to execute any Small Business Saturday marketing plans. The biggest SMB constraint isn’t strategy; it’s time. That’s why automation should be targeted at the areas where consistency matters most to AI: feeds, inventory, and customer communication. Automation frees you to focus on building trust (content, reviews, partnerships). For many SMBs, platforms like Salesforce Starter Suite and Pro Suite make this achievable – unifying storefront, CRM, and simple marketing automation in one place. Automation priorities: How a CRM can help: By unifying storefront, CRM, and marketing automation in one place, a CRM enables businesses to refresh product feeds multiple times a day, automate “out of stock” updates, and schedule timely reminders for shipping cutoffs. By taking this busy work off your plate, a CRM lets you focus on the bigger picture. Learn how your business can benefit from this out-of-the-box CRM. Salesforce for Small Business Holidays don’t just test large retailers; they reveal how small businesses can carve out visibility in AI-powered shopping flows. For SMBs, the differentiator is agility: being clear, consistent, and credible across local and digital touchpoints. A CRM system plays a crucial role in this agility by unifying customer data, product information, and marketing efforts in one place. Small businesses that invest in structured data, authentic content, and cross-platform consistency—made easier through a CRM—will get their products surfaced in AI-curated answers at the very moment shoppers are ready to buy. And by using a CRM, SMBs can ensure that their customer interactions, product feeds, and marketing communications are consistent and accurate across all platforms, which is critical for AI visibility. Start your journey with a free trial of Starter Suite today. Looking for more customization? Explore Pro Suite. Already a Salesforce customer? Activate Foundations to try out Agentforce today. AI supported the writers and editors who created this article. How does AI search change holiday marketing for small businesses? AI search emphasizes structured data, local relevance, and authentic content, shifting focus from traditional keyword stuffing to providing clear, consistent, and human-centric information that AI can easily understand and surface. What is “E-E-A-T” and why is it important for small businesses in AI search? E-E-A-T stands for Experience, Expertise, Authoritativeness, and Trustworthiness. For small businesses, it’s crucial because AI values authentic, lived experience and signals of trustworthiness. Highlighting founder stories, staff picks, and behind-the-scenes content can demonstrate E-E-A-T and improve AI visibility. How can small businesses improve social media to improve their AI search visibility? AI engines increasingly draw information from community conversations on social media platforms like TikTok and Reddit. Encouraging customers to post authentic content and engaging in niche forums can create strong authority signals that AI models may cite, acting as powerful, unlinked brand mentions. What are “micro-moments” in holiday shopping, and how can SMBs optimize for them? Micro-moments refer to brief, spur-of-the-moment mobile sessions where shoppers make quick decisions. SMBs can optimize by ensuring fast website loading speeds, offering quick payment options like guest checkout, providing “buy online, pick up in store” (BOPIS) capabilities, and offering instant gift cards or SMS reminders for shipping or availability cutoffs. What role does a CRM system play in a small business’ holiday AI marketing strategy? A CRM system centralizes customer data, product information, and marketing efforts, ensuring consistency across all digital touchpoints. This allows businesses to automate tasks like inventory updates and personalized communications, freeing up time and ensuring data accuracy, which is critical for AI visibility.
--------------------------------------------------