List of news related to Meta stock price META:

Title: Jim Cramer Says “Entergy Has a Number of Things Going for It”
URL: https://finance.yahoo.com/news/jim-cramer-says-entergy-number-170549405.html
Time Published: 2025-09-25T17:05:49Z
Description: Entergy Corporation (NYSE:ETR) is one of the relatively cheap S&P 500 stocks Jim Cramer talked about. Cramer discussed the company’s growth and valuation, as...
--------------------------------------------------

Title: Presentation: Panel: Next Generation Inclusive UIs
URL: https://www.infoq.com/presentations/inclusive-xr-environment/
Time Published: 2025-09-25T14:15:00Z
Full Content:
A monthly overview of things you need to know as an architect or aspiring architect. View an example We protect your privacy. Facilitating the Spread of Knowledge and Innovation in Professional Software Development Unlock the full InfoQ experience by logging in! Stay updated with your favorite authors and topics, engage with content, and download exclusive resources. Ramya Krishnamoorthy shares a detailed case study on rewriting Momento's high-performance data platform from Kotlin to Rust. She covers the technical challenges, including garbage collection bottlenecks and multithreaded contention, and the business trade-offs involved in adopting a new language to achieve predictable low tail latencies and maximize cost efficiency for their serverless services. In this episode, Suhail Patel joins Thomas Betts for a discussion about growing yourself as your company grows. When he started at Monzo, Patel was one of four engineers on the then new platform team–there are now over 100 people. The conversation covers how to thrive when the company and the systems you’re building are going through major growth. This InfoQ Trends Report offers readers a comprehensive overview of emerging trends and technologies in the areas of AI, ML, and Data Engineering. This report summarizes the InfoQ editorial team’s and external guests' view on the current trends in AI and ML technologies and what to look out for in the next 12 months. Holly Cummins discusses how to eliminate waste in software development. She shares strategies like "LightSwitchOps" and build-time initialization to improve machine efficiency. She explains how these changes can lead to "double wins," benefiting sustainability and business goals while also improving developer experience and challenging conventional ideas about productivity and efficiency. Jon Topper explains 10 common mistakes in building SaaS platforms, including not baking in tenancy from day one and failing to automate tenant provisioning. He shares strategies to mitigate risk, such as calculating baseline costs, establishing a strong product philosophy, and avoiding custom, on-prem, or multi-cloud deployments to ensure a scalable and profitable business. Learn how senior devs at Mercedes-Benz, DKB & Zalando are solving critical dev challenges. Last chance to register. Your team looks to you for what's next. Get the foresight to lead them through software's biggest shifts. Early Bird ends Oct 14. Trying to run AI at scale? Find the blueprints for enterprise AI from leaders who built them. Early Bird ends Oct 14. Leadership asking for innovation? Get the evidence to bet on the right tech and lead with confidence. Early Bird ends Oct 14. InfoQ Homepage Presentations Panel: Next Generation Inclusive UIs The panelists discuss creating inclusive XR environments. They explain how to prioritize accessibility in user testing, the importance of designing for diverse users from day one, and how team culture and modular UIs can foster innovation. They also highlight the business case for making accessibility a core part of an MVP to avoid technical debt. Erin Pañgilinan is Spatial Computing, AI Leader and author of "Creating Augmented and Virtual Realities: Theory and Practice for Next-Generation Spatial Computing". Colby Morgan is Technical Director @Mighty Coconut. Dylan Fox is Director of Operations @XR Access, previously UC Berkeley researcher & UX designer, expert on Accessibility for Emerging Technologies. Software is changing the world. QCon San Francisco empowers software development by facilitating the spread of knowledge and innovation in the developer community. A practitioner-driven conference, QCon is designed for technical team leads, architects, engineering directors, and project managers who influence innovation in their teams. Doyle: We're going to do a Q&A panel. We're going to dig in a little bit more on how augmented, virtual, and extended, and mixed reality unlock the ability to integrate the power of computers more seamlessly into our physical three-dimensional world. Designing that user experience of these next generation UIs to be as inclusive as possible comes with a lot of challenges. We're going to sit down and talk about what are some of the insights and strategies for creating more inclusive and accessible XR environments. Our speakers today, we have Colby Morgan, we've got Dylan Fox, and Erin Pañgilinan. How does the process of user testing differ when accessibility is a priority, and what are some best practices? Erin Pañgilinan: My friend, who's a co-founder of my previous non-profit that I co-founded with her for women that were working in ARVR, it was called ARVR Academy, Suzanne Leibrick, who used to actually be in Mixed Reality at Intel at the time, she talks about the concept of gameplay testing. If you really think about even in like AI development, you're not going to start with the most gigantic foundation model and the most amount of data. You're going to start off with something really small and simple and test it on 10 people. What I always encourage people to do since 2016 is to try as many different VR or AR, XR, demos as much as possible. When you're actually developing for a single experience, you're going to start with maybe just a few interactions at a single level and see if it works with actual people. If you get more than 60%, maybe you're in a good range. A lot of the time you'll actually try it in a bunch and you'll realize, this actually doesn't work. That way you're saving a lot of time when you're gameplay testing. That's the first thing I would say. It's pretty easy. It's just most people who haven't developed for games before, like I said in the background, but it's also in AI, in that space that haven't done it, I think you're going to always rapidly prototype the smallest MVP possible. Dylan Fox: I think for me, one of the challenges that we see with XR, as opposed to more mobile and desktop technology, is that it generally assumes that most people have either a desktop or a mobile phone at home, and that they are able to turn these on and operate them and use the application, often remotely, without necessarily needing any external help. If you want to have inclusive XR experiences, the unfortunate reality right now is that there are a lot of people who would be interested in using these, but may not have a headset at home, and even if they did have a headset at home, may not be able to take it out, get through the setup process, and put it on themselves and activate your application independently. Something that I noticed when I was running the experiments we did at UC Berkeley on using the HoloLens for low vision folks, is that these systems were not necessarily created with the idea that the person who is in the experience may not be the one who is operating the experience. I would love to see more support for experimenters, more support for caretakers. There's a lot of times, especially with new technology like XR, where you want to empower the person who is wearing the headset to just focus on the experience, and you want to offload some of that setup and execution to somebody who is helping them on the side. Making sure that you have those good tools, so that if I'm trying to run an experiment or user test for somebody who is totally blind, I can hand them the headset, they can put it on, and then I can get them into the application, get them set up, and monitor their progress. That's something that's not super well supported right now. It would be lovely to see better support for it. Colby Morgan: Especially when it comes to accessibility, I think that trying to get that early in your process, so as you're developing those features, it's part of your design process early on as well. Again, you're having opportunities to develop or solve issues before they become issues, especially when it comes to accessibility. I think a lot of it is just that early mindset, too. Accessibility and inclusive design is really about good design in general, so just really trying to make good products and good experiences that have a lot of that baked in. Again, you're reducing the amount that you have to do later on down the pipeline. I think, especially for user testing, obviously it's a really critical part of that, and so I think as much as possible trying to work with your users or the communities, people that can really give you the solid feedback about the accessibility tools that you may be working on or integrating. I think the big thing I always think about, too, is it's really easy to check a box or just get a feature in that it checks the box and does the bare minimum of it, but a lot of times finding what's giving users value is really critical to that step, and especially with XR in general. Again, it's such an immersive technology, and so you just want to make sure that the tools that you're putting in actually provide value for users. Erin Pañgilinan: My friend Nathan Ventura, who co-founded a company called Vinci Games, so he's YC backed. He's doing VR basketball. His game, Blacktop Hoops, when I first tried it, it was literally just like, here's the environment and shoot a basketball. He didn't have characters yet, which is now tied to a lot of IP and film, but I actually had, it was a demo at the Philippine Consulate, so he's Filipino, I'm Filipino. One of our other friends, he's number two in the world at combat design for Street Fighter, but he's a combat designer in most games. He probably played everything on every console, X-Men. I asked him, I was like, how was it? Did you break presence when you tried this? He's like, I didn't, but when I actually tried it, second time, I broke presence, which is something really common when you have VR, is like, I feel like I'm really in this environment, but it skips too much, or it'll have the most buggy things. You'll do a ton of work, but in practice, when you find out, I've spent maybe hundreds of hours developing this game, and find out it doesn't work. Another one I'll mention is a friend, Jazmin Cano, who actually has a chapter in my book. She's a technical 3D artist and an accessibility products manager. Her game, which was from Owlchemy Labs, so they were acquired by Google. If you've ever played Vacation Simulator or Job Simulator, this was their latest version. We tried it at Meta Connect, and most basic thing, Wi-Fi doesn't work. When we think about design, obviously Dylan's work focused a lot on differently-abled or disabled folks, and you're trying to expand the amount of diverse content creators, trying to get as inclusive as possible with your design. When it comes right down to it, it's not just good design, it's just basic engineering and functionality of Wi-Fi is not working at Meta, and so when I'm playing in this headset, I hear voices of other headsets being mixed together in a multiplayer environment with only less than 10 people. I'm like, that's pretty bad. That's not the fault of Owlchemy. It might have been the event people who were organizing at this conference. I like to think of gameplay testing in every type of environment, no matter how big or small. It's the one thing I'll say lastly, too. With Horizon, if you thought about during the pandemic, you're watching Meta Connect in VR, you could probably have hundreds of voices, but once you scale to thousands of people attending, couldn't really do that. Same idea. It's also just basic engineering when you think about latency and functionality at the end of the day, not just good design, but basic practices, and like, does this work or not? Doyle: How do we ensure that inclusivity and accessibility are integral parts of the UI design process from day one rather than add it on later when we maybe find it in testing? Colby Morgan: I think part of that is, it touches back to the culture of a team and just making sure that the team behind it, the team behind the designs, the UI, the concept, the development, have a lot of those concepts. Part of their workflows, and everyone's on the same page as far as what you're working towards. The culture is part of that. For me, XR is such an immersive technology, and the social presence and the connection is such a huge part of that, and so you want to make that as inclusive as possible and as easy as possible to get into. I think for a lot of people, XR is just a lot. It can be a cognitive overload of trying to get into an experience. Obviously, it takes over your vision and all that. I think just, again, leaning back to just good XR design to make things simple and easy to operate and easy to get people in regardless of their experience level, because I think with every new technology there's always going to be people that are new to it, and there's always going to be that struggle. Again, just relying on those designs that make it really easy to get people in and participating. Dylan Fox: I think the single biggest thing in my mind to make sure that you have accessibility on the radar from the start is simply to have folks with disabilities on your team. If you're part of an organization that has a lot of folks with disabilities, then it's naturally going to be a part of the conversation, because you aren't going to ship something that doesn't work for your own people. Failing that, making sure you have people with disabilities in the loop as early and as frequently as possible way before you get to the alpha, like part of the design process, is just going to be really important. I think another thing that we often talk about when it comes to just the cold, hard cash of development is that it is far cheaper to build something in than to retrofit it. If you get to launch, you get to beta testing, and then you find out, actually, it's not accessible, we need to do this screen reader, or we need to be able to scale our UI. That is way easier to do when you're first building it out than after you have a million interconnected systems that it breaks. If you ever think accessibility is going to be an issue for your game, your app, your organization, which it probably will be if you ever get successful enough, then it's better to think about it from the start and save yourself a whole lot of time and headache and money and have just a better product right from the get-go. Erin Pañgilinan: I put Dylan on all of those points when you think about the end user itself, and thinking about different senses we're targeting. We saw a talk with Google AR, like how do visually impaired people see and not see. If you're mute, that's completely different. If you're someone that does not have an arm, you're not going to have a hand controller or a hand. These are really difficult challenges. Something really basic which I opened in my talk was with avatars. I was actually talking with Jazmin about the entire process of the app. She's like, what? You skipped over my entire part at the very beginning of like, create an avatar. I started off as a bald, it looked like a Nigerian man. I remember the very early prototypes for Oculus, by default, I was a very tall golden man. It was only metallic colors. I was like, I don't really identify with this, but it's really cool. I love Jeremy Bailenson's work, but it is a little problematic to say like, I can achieve empathy because now I can have blackface on. I'm like, that's not what this is for. To be culturally relevant I want an avatar or an agent that looks like me. One of the other user tests I've actually been doing at Meta, I can't say too much about it, but they ask you about skin color. I was like, that's really new and interesting. They never have asked that before. I think they are, at least some of the market leaders in headsets and manufacturing, just trying to think of, how are we being inclusive of more people that are end users, not just the people creating the products? That was never thought of I think five years ago, it was just like, that was a throwaway and a nice to have. Now it's like, we're actually thinking about this first. That wasn't the case. Also, in the game for Owlchemy Labs that we played at Meta Connect, I was talking to her about like, teleporting is really different. Previously you had to have a controller, whether you're using HTC VIVE or anything in the Oculus and Meta ecosystem, now it was all hands. It made me think like, but I don't naturally teleport by putting my hand out to go to the next portal. Is that the most inclusive UI if you don't have a hand? It really depends who you're targeting. I think you need to think about, who is that type of user? What senses do they have and prefer? I wouldn't say there's just flat-out bad design. There's just different points of accessibility that people identify with. The last thing I'll mention to you from the AI community is for black women or brown folks like myself. I'm a light-skinned, privileged Filipino woman. I will say, people don't trust AI that are black. Straight up, why? Because the algorithms, if you look at it, basic recognition for imaging on skin. It's why I'll go to the bathroom and my air blow dryer will work or water, but that won't work previously. This is because accessibility is not thought of. It's the last thing on the list for AI developers. For XR it was a little bit better. It still was at first. We weren't design inclusive by default. I think that's now changing. It's going to take a lot more work, I think, to be much more inclusive. Not just the content and the people developing it. Thinking on base level, what is the data and input that a machine is taking in? What's the sensory input that I'm receiving? You're thinking about I/O. What's the input and output on each end, inputting and receiving? I don't think that was thought of very critically before. It's definitely transformed and changed over a number of years. Doyle: What role does team culture play in building these accessible UIs? How do we foster a culture that values feedback and diverse perspectives? Colby Morgan: I think part of that is really focusing on getting a diverse team, so you have a diverse set of backgrounds and experiences to pull on from your team. As Dylan was pointing out, it's like, if you have a lot of different perspectives on your team, you can identify a lot of those things. It becomes easier, even as a culture element, if you have just a diverse team, to keep that top of mind with a lot of those different aspects. I'd say the diverse teams. Dylan Fox: I think in addition to the team, one thing you can do is spotlight your users and other stakeholders that are of different backgrounds, different abilities. We had a project with XR Access called the Stories Project, where we interviewed a number of disabled folks that had used XR, had an interest in it, enjoyed it, but obviously ran into challenges in using it. One thing we wanted to do was just spotlight those experiences. Because I think it's very easy for people to assume that everyone who uses a certain app is like themselves. I think we don't tend to think about people that aren't like ourselves unless we see some evidence that they exist. If you can spotlight those people that exist outside the norm and help to normalize that, this is for everybody. It's not just for your standard, usually in tech it's the standard cishet white dude. Then that can also be really helpful in making sure that your team doesn't just see this diversity of opinion in the team, but also in the people that you're creating for. Erin Pañgilinan: Outside of the obvious ones like race, gender, class, ethnicity, language, region is really important. In any application, interface is to me web development, not even XRs. It's like, how many different types of phones are there in the Philippines, for example? How many different screen sizes do I need to design for? I'll never be an Android developer, that's like too many. I was hardcore iOS for a really long time for a lot of different reasons. I think when you're thinking about targeting, accessibility, by default, it's a population and region. Outside of that, I also consider the type of discipline on the team. For a really long time with game design, it was only people who were AAA developers can be in XR. That was highly biased, especially for third-party independent developers like myself. I did not have that experience. I had a previous life in another career. There's a lot of bias there. Outside of non-traditional backgrounds, what I would say is it's actually not engineers to me that made the best experiences in VR in particular, it was architects. People who actually have an understanding of 3D space. When you think about AI, it's not necessarily machine learning engineers that have a statistics PhD and background. That used to be the case. I actually think psychologists have a lot to say. Cognitive scientists, anyone who actually works in real science and in the brain in life sciences, there's a lot that's to be worked on there if you're trying to achieve, to me, AGI or consciousness, when you think about presence in VR. I like to think of not just explicit insert my XYZ race, that they say, insert your identity here. I also think diversity and discipline of what you focus on is actually really valuable. That's now changing. The barrier to entry for people is also dropping. A lot of the classes I did teach from 2016 and on was everything from K-12 of students using Google Cardboard to employees at Cisco that were developing early prototypes for enterprise. We weren't getting people who were core C# developers. They were just anyone that worked as an engineer that was trying to think about, how do I think about networking and learning from first-person shooter games, but applying it at game design that's essentially B2C in a B2B environment. There's a lot of things that are cross-disciplinary by default that can make things more inclusive and new ways that you can put a lot of things together intersectionally that will create new and exciting experiences and that will actually be more inclusive. Not just, who's creating it because you're a cis straight white man and you're overrepresented. Instead of like, I think about Dylan as his expertise being like, you're focusing on populations that typically you wouldn't think of, or would be playing in an experience that is the most cutting edge and new that is by default much more human and accessible. Whether it's speech, voice, NLP, whether it's sensory, touch, gesture, get to the lowest common denominator of who cannot access those things. By targeting those audiences, I'm not going to say it's because it's a queer, trans black woman that is disabled. It's like, that's who you should target. I'm saying, if you actually support and target those things and put those people on your teams, they're probably going to have ten times richer the amount of experience, not because of who they are, but because of the disciplines that they have to think about and how they design, how they interact with the experience. Then on top of that, they're going to layer, here's my knowledge and statistics about why this doesn't work for this population and why that's not culturally relevant for my community, and why it does or does not work. It's actually multilayered. It isn't just straight up like, I'm going to hire a woman. Because of that, automatically it's going to be more diverse and automatically it's going to be a great experience. No, you can't take it at face value. It is much more deeper and multilayered in this process when we think about the words diversity. Diversity, not just like token Asian face, or token affirmative action card. It's really about like, what are you bringing to the table that is different and new and a different paradigm that you're contributing to innovation? Colby Morgan: One of the things I think I'll add too, with that, I think the other element of getting your team, but also just making sure that there's opportunities and everyone feels empowered to have a voice on your team, too. That's one thing, especially with new people on our team, we have to go through a phase of getting them really comfortable with giving feedback and feeling empowered to give feedback. Especially so you have a team where, again, I feel like a lot of projects and early in a project, it can be really easy to fall into a design hole or code silo where you're really focused and you almost don't want to get feedback because you're like, I'm really focused on this. I don't want to get some of that feedback. A big part of just having that culture that is open to feedback and everyone's empowered to give feedback on each of those steps, because I think there's a lot of those friction points that people run into that they just don't say anything. It's really easy, especially when you're working on XR experiences or features, it's really easy just to gloss over, it's like, that was hard, but I'm not going to really pay attention to it. Just making sure that everyone can call those things out, and really, it's like, yes, everyone was having a really hard time with this. We should actually try to make this better. Doyle: Then, how do you design UIs that adapt to individual users' needs and preferences? What are the current limitations you're seeing and where do you see this heading? Dylan Fox: I think the number one rule in my mind that I discussed earlier is that principle of modularity. You should have interfaces where the different inputs and the different outputs can all be done in multiple different ways. To our really great example of this, you can look at "The Last of Us Part II", which had an incredible suite of accessibility functions where people could do en masse, give me the hearing suite, give me the vision suite, but then tweak individual things. They would have any key thing, let's say there's like a scrap of material on the ground you can use to scavenge and use in crafting. If you turn the accessibility feature on, it wouldn't just light up, it would also make a sound, or the controller would vibrate. There is any number of ways where you could get that multi-modal feedback. I think making sure that your systems are modular like that, that you're thinking about things, and even things that maybe not what necessarily you think of as something that needs that. Things like eye contact in social VR, if I can set it so that I can literally feel everybody's eyes on me if they're all watching me, that could be a really powerful thing. This is also an area where I'm really excited for AI, because the ability for people to just say to the machine, make this thing bigger, or, can you outline X for me, and just adjust their experience on the fly, would be incredibly powerful. I think that is something that we're just barely starting to see, but will be very interesting to see if we can have systems that can accompany those types of requests in the near future. Erin Pañgilinan: Something really basic I mentioned earlier was your headset of choice is maybe informed by your "ethical moral values" of, I blame X about privacy, so I'm not going to use Meta. That's something I hear a lot. Or it's the same thing for Google. It's really tough because most of the companies that are developing these things do have legitimate privacy concerns with AI. Something really basic that they are doing right, which I think is great. I know if it's a Scarlett Johansson, but I actually had my mom pick, I was like, which ChatGPT voice do you want? It's like customize your version of Siri. I thought, that's a cool user choice. We didn't have that before. Being able to customize to the user based on a set of preferences, it's like, I didn't have it before that my avatar by default wasn't a man. If you think about cars, my brother's a big AZN racer, so we think about customizing, modifying the car constantly. Since the era of the '90s, it's a big thing. For Asian people, this is like the rice rocket movement. How would I customize my computer? Like CarPlay, take out the screen, put in your own screen, modify the UI. This is my custom window, and listen, now that's going to match the color of my car. Think about, what if you could customize the headset so that it would be much more accessible? Here's one thing when I think about more than just ergonomics. I love the Apple Vision Pro. It isn't great for accessibility with people with long hair. It's terrible. No one's gotten makeup right. Jazmin and I actually talk about this a lot, like, what eyeliner can you wear? Something really basic. People don't think about these things. Definitely not because there weren't that many women on these teams. If you think about even the car, how do we design a seatbelt? That was meant in terms of safety primarily for cis straight white men that were over 6 feet tall. I'm 4'10". When we think about accessibility, it's not just like, what is my programming language of choice as a developer? What is my headset choice? I was really excited about Apple. I'm like, you can customize the different type of colors of your computer. This is in the late '90s. There's so much more choice that we have now. There's still a lot of things that we don't have choice in. Part of the reason I'm so excited for Meta, and they haven't done this yet, they haven't open sourced their SDKs and APIs for anything with AR glasses. Many people are trying to hack them. It kind of works, and it kind of doesn't. I did talk with the director of product at Llama because they had shown some videos at Meta Connect about how they're using Llama 3, their latest model, with VR. Is it very accessible when you're talking about multi-modal models, but you're not open sourcing really basic stuff for people who are not doing model development to just basic software engineers? I think part of that problem is just because there's so much work to do within the ecosystem first before you open source something to that level of standard. Optionality and picking the different types of models, even the size of it, so that it's more affordable. It's the same thing for headsets. If I could have a cheaper option than an Apple Vision Pro where most of my community cannot afford a $3,500 headset, can they just get something as cheap as a Meta Quest S? That's probably like a $500 price point, would open up the doors for not only more consumers to enter but also designers and developers from the independent developer and third-party community that doesn't work at a big AAA game company or any of the other headset manufacturing companies. Colby Morgan: I think about adapting users and features and experiences for like specific users. Obviously, there's a lot of different things that you could do and handle. One of the powerful things I really like to rely on as much as possible is just finding tools and systems that can really just adapt to users. Trying to find those broad tools and systems that have a level of dynamic adaption to a player to more seamlessly meet some of their needs, to open up the experience. I want to just create more of a seamless experience that takes more of that friction out of there. Again, I think there's identifying some of those tools and systems that you get a lot of value out of for a lot of different users. One, how you identify those. Then, how you can make those as dynamic as possible. Dylan Fox: The more you can embrace users creating their own adjustments in terms of things like mods, things like 3D printing, custom controllers, or custom headbands, there's a lot of things you can do as a developer to embrace that and support that. Doyle: What about balancing simplicity and feature depth in the user interfaces, especially in the more immersive environments like VR and AR? Colby Morgan: Yes, especially XR, it's definitely a balancing act because it's really easy to go ham. As an XR developer, it's really exciting to make things. It's really exciting to add a bunch of features and effects and different things. I think just as good product design, I always really try to think of how do you focus your product? How do you focus your designs, and really think about your user's focus? What do you really want them to see? What do you want them to look at? What are you going to focus on? I think it's the balance back and forth of, how do you keep a real focused experience for the user, but how do you add more engaging features for users that have been using a product for longer? I think that's the balance of keeping it simple early on for a user, but then just having a clear path for a user to get more exposed to that depth and complexity. Also, any XR application lets users have the choice of, they want to keep the simple core experience. They don't necessarily want to try to get too deep with the features or anything like that. They have that opportunity to participate at the level in the experience they feel comfortable with. Dylan Fox: Some examples to look at are like Google Tilt Brush. You can start off in the simple mode, there's only a few tools on each of your little tool palettes. Then, when you're ready, open up the complex mode and get a bunch more controls, a bunch more customization. Thinking about things like that, thinking about when it comes to design for neurodiverse folks, having the option to turn off distractions, to really bring things down to the barest elements. You'll see that as well with some low vision support, things like, again, The Last of Us will have high contrast mode where it strips it down such that the player is in blue, the enemy is in red, new items are in yellow, and everything else is in gray. You get this extremely different aesthetic style. It becomes very minimalist. Then it makes it so that even if you're heavily visually impaired, you can still get that core sense of what's happening. I think thinking of ways that you can do that, of letting people scale their experience up and down is really good design philosophy. Erin Pañgilinan: Something I started off with when I was transitioning from frontend and mobile, thought about data science, this is probably like 10 years ago, but I was reading a book on microinteractions. I actually like to borrow a lot from even just really basic web devs. I was at WWDC's equivalent of ALT Conf. A bunch of the mobile iOS developer community always has a side conference that watches WWDC every year, which is Apple's developer conference. My friend Gary was actually trying to create this application where you're doing a lot of R&D in a lab, so you're looking at chemistry, beakers, and how you would actually do a bunch of different stuff. It was really cool. I said, but I can't tell what you're paying attention to, so reference the paper, 'Attention is All You Need', what do you focus on? I suggested to him, I was like, I think you need to use something equivalent to CSS and hover, really basic stuff, not a button, but on the actual objects themselves. Lo and behold, that day, Apple announced like, we now have hover effects. I was like, here, this is what you should use. Really basic stuff like that. I think WWDC has some basic standards, like WebXR, it takes a while, and it hasn't completely transferred into native yet just because it's so early in its development process. There's a lot of different microinteractions and paradigms that have been established already and tried and true, between responsive web and mobile development that do transfer over. Then, there are some principles that don't transfer. While I love Apple Vision Pro, a lot of it's very flat design, it actually doesn't optimize all the 3D space. Not everything should be a 2D visualization of the stock market, and, it's in VR, so I'm going to play it in Google Cardboard. This was an experience I tried way back in the day, it doesn't work very well. It was like, and now I can ride through the stock market like a rollercoaster. I'm like, this is horrible. I was really excited when I did a different conference, this was at Bloomberg Tech conference, and there was a demo for stock trading apps and database. I was super excited, it looked exactly the same as what you would have on a mobile app. I was like, this defeats the whole purpose of having it in Apple Vision Pro. There are some applications that can be transferred over, but, again, the chapter in my book is on data and machine learning, visualization and design and development, and it's about 3D in context of the environment, of how you would use that. We also have to think about, what can we borrow from tried-and-true principles of design and development from the web and mobile? Then, what makes it unique that it's in XR, or even adapting some things, not everything, with AI as well, that makes it much more inclusive, accessible, and something that users actually want. Participant 1: When we talk about in the planning phase, how much weightage should we give for introducing accessibility into our user experience? Do you think it's fair to say that the MVP, maybe that shouldn't be part of the MVP, or is it like, let's cater to the majority of the audience first. Then, as a second phase, we should be looking at the accessibility part, or it should be a discussion since day one? How do we balance out the fact that the majority of your audience, in a way, would be linked to the majority of the revenue for the product? How do we justify that accessibility should be part of the MVP, without that direct correlation to the revenue associated with that? Doyle: The question is, do you try to push for meeting accessibility requirements and standards in your MVP, or should that be a follow-up? If it should be part of the MVP, how do you push for prioritizing and arguing for the importance of baking accessibility in to that first version versus waiting later? Erin Pañgilinan: It really depends on your audience, it's very relative. I'm trying to create a productivity app, this is like a selfish dream, for myself, and so I conducted maybe 30 user research interviews since the end of last year. They were primarily women who were neurodivergent with ADHD, that were product managers, designers, and developers, that worked on open source, AR, VR, AI, and crypto, so very similar to me. I'm designing, essentially, for myself. That's a very specific audience, and even within that audience, there's a level of richness that I didn't expect. I was like, so some of you use paper planners, some people use mobile apps for planning, some people don't use anything at all, and they just look at their Google Calendar, and maybe they would love all of it if it worked in their Apple Vision Pro, or they'll only use it and pay for something if it also works in Android and on Linux, like every platform. It's really hard to satisfy people. Even in a target market of something that I thought was really small and specific, it comes down to user segmentation and that feature, and then, who is willing to pay for it? Take this quote from YC, Y Combinator, (make something people want) and we'll pay for it. While I like to think, yes, accessibility, design, inclusive design, and everything should be able to target this type of users or this population first. I really have to be honest with you, some people don't have the budget for that and won't prioritize it because it isn't a part of the bottom line. What it comes down to is like, what set of initial users for that problem are you solving? Then, within that set of users, what are the top three? Really narrow it down. Who are willing to pay for this price for that set of features that should be the real core of your product that is a part of the bottom line? Identifying that, what I realized was like, this is going to take some time. I thought I was trying to be even more narrow and specific, and even then, it's really hard and difficult. Again, it's all relative. If you are a B2B SaaS enterprise app that wants to do telepresence, this is a case for like Cisco or for a client I had in education, like we want hundreds of students to be in XR. We want them to also be able to design fashion and play fitness games and be safer. Two totally different use cases. I'm like, some of that is Meta Quest suite and some of that is Apple Vision Pro. Whatever you think you can prioritize, even just three things, it might be too much. You got to really chunk it down and get really specific, and even then, it still might be too much. Just get very granular to what that MVP is and who will pay for it so that you continue development. Unless you have endless amounts of money you can like self-fund. Like a lot of my friends in crypto who retired, they can just play computer all day and gameplay tests on all the things. Most people in reality can't do that. I would just get super specific and think about the context of who your customers are or who your investors are. I'm like, what is it that they want and what is it that you want? Were you willing to meet people in between for your users? Dylan Fox: These things are generally worth trying to make part of your MVP, and here's why. It's because something like a fully-fledged screen reader system or scalable UI or captions, if it's not 100% perfect right off the bat, that's fine. We all know MVPs can be very rough around the edges sometimes. The trap that I see people falling into over and again is they brainstorm a big list of features, they pick some as MVP, and then they build out something as fast as they possibly can that meets all of those MVP features. That decides the course of development. There's a lot of decisions that are made when it comes to developing during that MVP phase that become set in stone and very hard to change after the fact. If scalable UI is part of your MVP, then you will be building towards that from the start. You'll be making those choices for your development infrastructure that take that into account. If it's slated for your 1.1, what might happen is you develop to get something out the door and then you realize, actually, if we wanted this, we would have needed to develop the core engine and core aspects differently. Which means now it's much more work to do, so maybe it gets shifted from 1.1 to 1.2, and you just start accruing technical debt that if you ever then want to get some of these features in, you'll need to have a big effort. Because, again, a lot of these accessibility features come down to modularity. That is the type of thing that you really want to build in at the start. Do you need to have everything ready to go? Not necessarily. Think about localization. Yes, obviously, if our main user base is in English, then we're just going to focus on English for v1. If you're thinking about localization as part of phase one, then I guarantee you when it comes time to record all your lines in Spanish, in Chinese, in whatever else, it's going to be like a few lines of code to change all those out, instead of, "Our text is hard baked into the game. We can't change it at all. This is going to be a mess". For that reason, I would say, yes, at least some of these accessibility features should be in the MVP. Colby Morgan: Echoing both those points, and I think just to add, especially for Mighty Coconut, I think for us, in that early planning phase, it's important that we have that accessibility mindset of just how we're identifying the friction point, as I think the friction points or the potential friction points, are the really big piece for us. Just knowing what those friction points are, you can better tackle those down the road if you can plan for those of knowing what's coming or anything like that. Erin Pañgilinan: I was developing a productivity app with AI, and I literally was like, this is so fun. Then, I realized this is really broad. You do anything in AI, you think about, the first thing, if you've done any workshop with AWS, this is the amount of money I have for compute and spend. Early in the planning process, through user research, before I even build anything, I'm just trying to extrapolate that list of features and narrowing it down to the smallest possible thing. Because even if you think like, I can make this part of the prototype, it's still a lot of work, so before you spend any money, I would do a good amount of user testing before you build. Then when you actually spend, you'll realize whether or not it was a waste of money or not. Participant 1: Actually, one point that stands out, you mentioned, it collects as a technical debt. I think that's part of that culture. Because this whole accessibility and inclusion bit, I come from the financial industry, it's just in the start. My engineers have raised a lot of technical debt, but accessibility is not something that has anybody raise their hand that we should have this. We now have it as an organizational principle that it should be there, but, as an engineer, nobody has raised that. Maybe that's like, start from that culture, saying that, we need to start looking at this thing. Even if it's not part of the MVP, maybe if it comes up, if people are raising that this is a technical debt, then maybe towards the course of action, it might get an action done. Erin Pañgilinan: You do have a designer in the room at all when you're doing this? Participant 1: Yes. Erin Pañgilinan: I just think about Apple, it's like, when a designer comes in the room, somebody would say, they're like God, I'm like, it helps. Participant 1: Yes, that helps. Again, the designer is also, as you said, like a phone, where this phone has been two centuries old, to understand that, where we come from. It's like, that kind of thought process takes a bit of time to come, but it does come in. You mentioned we attended Bloomberg conference, so there should be a difference between the Bloomberg version versus any of the Meta conferences, and things like that. That mentality is still growing. I think that was a good test, that if engineers start thinking that way, that this is a technical debt, then, at some point in time, it's going to have to be introduced. Erin Pañgilinan: Technical debt and financial debt, like my cost of computing, we used to say, this is saving developers this amount of time and spend, and to talk really less on productivity, and developer experience, DevEx. It's like, if I can identify the amount of time and money saved for an engineering team because we included design first, and it affects the amount of compute that we're going to spend and the amount of time we're going to spend engineering this feature or product line, then I think it's easier to make that call. Harder to do for something like a newer problem, because you mentioned MVP, but if it's for an existing legacy system, that's how I would quantify it, to be able to get a little bit more buy-in, to include it as more first principles earlier on. Doyle: If you had to pick one piece of advice, what would you tell newcomers to the field who want to make inclusiveness a core focus? Colby Morgan: I think I would tell newcomers to the space, one, really look at your user base, look who your target audience is. Find ways to connect more with them directly. Just identify the needs of that audience. Dylan Fox: If you're looking for problems to solve that actually mean something, look to disabled communities, because there's a lot of places in the more privileged parts of the world where we've started to invent some bullshit problems. There's also a lot of people out there who have real problems that, with just a little bit of design thinking and actually listening to the people with those problems, and off-the-shelf XR AI technology, I think there's a ton of stuff that could be made that just hasn't been yet. There's a ton of low-hanging fruit and unsolved problems that really affect people's day-to-day lives. Please do think about that. We have a lot of resources at XR Access that talk about how to design and develop accessibly, so check those out. Try to make sure that what you're doing is really going to be making a difference in people's lives and not just something shiny and novel because the technology is there for it. Erin Pañgilinan: Be open. Try everything, at least once. Expect the unexpected. There are so many new developments. Some of my friends, they clairvoyantly predicted this was going to happen in XR, and I was like, no. Some of that was Michael Abrash, and we always knew glasses were going to come out and things of that nature. Look to history, so what you're thinking is going to come in the future. Also, there are things that are popping up now that I did not expect, so in any new emerging technology, that's going to happen, be really open-minded. When I say try everything, that's because I literally have probably tried every headset and experience I could get my hands on, because you'll find a bunch of stuff that works and a bunch of stuff that doesn't. I don't just say that for developers and designers, I say that for consumers because that's a really good way to, one, do user research. Then, basically testing of like, yes, that did not work at all, we're not going to spend any more engineering hours, or time, or money on this because we assumed this would work functionally, and it actually didn't. We expected something that works in principle, is like, the PowerPoint slides or the video, should work. It did not work, even though I gameplay tested it about 10 times over and over now. Yes, be open, try everything, and expect the unexpected. See more presentations with transcripts Recorded at: Sep 25, 2025 by Erin Pañgilinan Colby Morgan Dylan Fox A round-up of last week’s content on InfoQ sent out every Tuesday. Join a community of over 250,000 senior developers. View an example We protect your privacy. Get proven, real-world solutions directly from senior engineering leaders navigating Europe's toughest technical challenges. Learn directly from leaders at: InfoQ.com and all content copyright © 2006-2025 C4Media Inc. Privacy Notice, Terms And Conditions, Cookie Policy
--------------------------------------------------

Title: Analyst Says Meta Platforms (META) is One of the Best AI Stocks to Buy Now
URL: https://finance.yahoo.com/news/analyst-says-meta-platforms-meta-131433677.html
Time Published: 2025-09-25T13:14:33Z
Description: We recently published Trending Analyst Calls: Top 10 Stocks. Meta Platforms Inc (NASDAQ:META) is one of the stocks analysts were recently talking about...
--------------------------------------------------

Title: H-1B blow will reshape tech hiring
URL: https://www.thehindubusinessline.com/opinion/h-1b-blow-will-reshape-tech-hiring/article70090196.ece
Time Published: 2025-09-25T00:30:00Z
Full Content:
-386.47 -112.60 + 143.00 -1,325.00 -1,105.00 -386.47 -112.60 -112.60 + 143.00 + 143.00 -1,325.00 Get businessline apps on Connect with us TO ENJOY ADDITIONAL BENEFITS Connect With Us Get BusinessLine apps on The silver lining for India is that irrespective of Trump’s hike in visa fees, it will lead to more offshoring. The labour cost is still lower in India | Photo Credit: DEEPAK KR Much before US President Trump’s decision to increase H-1B visa fees to $100,000 from the existing fees ranging between $2,500 and $5,000, the red line was drawn when he hosted 33 Silicon Valley leaders (including five Indian-origin top executives such as Sundar Pichai and Satya Nadella) at the White House. Although the meeting was about artificial intelligence and US investment, behind the scenes the talk was about a paradigm shift — how AI programming tools can now generate thousands of lines of code quickly, reducing demand for junior software engineers. This may explain why major US tech companies such as Amazon, Intel, Meta and Microsoft, have been consistently laying off workers in recent times, particularly before the hike in H-1B visa fees. In fact, the US-based IT firms are more dependent on H-1B visas than Indian IT firms. Even in India, the top six IT firms added just 3,847 employees in Q1FY26, a 72 per cent drop from Q4FY25. TCS made national headlines for laying off 12,200 employees, which is about 2 per cent of its global workforce. Although we are worried about the fate of around 400,000 Indian IT workers whose H-1B visas may not be renewed in the event of this fee hike, such a thing was bound to happen. Region-wise, India was the largest beneficiary of H-1B visas, accounting for 71 per cent of 3,99,395 in H-1B visas 2024, while China was a distant second at 11.7 per cent. Market participants had anticipated a sharp fall in stock prices of Indian IT companies heavily reliant on the US market when the stock market opened on Monday after the announcement. By the end of trading day, these stocks declined an average of 3.4 per cent. AI, automation, and cloud computing are changing the face of tech hiring, something which came too quickly, especially when there is a growing pool of computer science graduates specialising in coding that has been growing over the years. Graduating with a degree in computer science became a fad, with top tech executives, billionaires, and even US presidents promoting the field and encouraging students to learn coding. Throughout the past decade, a degree in computer science was yielding results, with fresh graduates in the US starting off with salaries exceeding $100,000 plus substantial bonuses and stock grants. Same trends were noticed in India and elsewhere across the globe. Like in the US, in India, there is a craze for admitting students to computer science programmes, with recent estimates suggesting India is producing in excess of one million engineering graduates every year. This has led to excess supply, particularly in the age of AI driven technological change. This is akin to the cobweb model in economics, where farmers grow a particular crop in response to higher price signals in the present period, only to realise there is excess supply in the next period. We are witnessing a similar event now but spread across a longer time horizon to adjust for the technological change. Entry-level hiring by Indian IT firms has dropped sharply from pre-Covid levels. The same can perhaps be said of employment generation, generally speaking, in the organised sector. India (like elsewhere in the world) is slowly transforming into a gig economy where the labour market is increasingly characterised by the prevalence of short-term contracts or freelance work. Private studies have shown that over half of Indian companies have more than 20 per cent of their workforce as contingent workers. The Periodic Labour Force Surveys corroborate this trend. In fact, most of the hiring in manufacturing, whether by the government or by private corporate enterprises, is now being increasingly outsourced to private contract suppliers. That things change so rapidly, requiring adjustments from both industry and the education system, is not new. Even the advent of computers and technology that marked the rise in productivity during the last century required adjustments. Alongside computers, electricity, combustible engines and refrigeration aided economic growth through a more productive labour force and necessary training. This has led to the creation of thousands of jobs in manufacturing. In the age of big data analytics, machine and deep learning, there is an apprehension that machines are increasingly taking over jobs performed by humans. But that is not entirely true. Newer types of jobs are opening up and it is in this transition phase that things will be little difficult. According to an August 2025 report from the Federal Reserve Bank of New York, among college graduates ages 22 to 27, computer science and computer engineering majors are facing some of the highest unemployment rates, ranging between 6.1 per cent and 7.5 per cent, respectively. That is more than double the unemployment rate among recent biology and art history graduates, which is just around 3 per cent. In an ironic twist, job applicants leverage AI tools such as Simplify to mass-customise their resumes and applications, only to have companies use similar AI technology to automatically screen them out. The silver lining for India is that irrespective of Trump’s hike in visa fees, it will lead to more offshoring. The labour cost is still lower in India, which has a high number of STEM graduates. It will only bolster investment in Global Capability Centres (GCCs). India accounts for 55 per cent of the world’s GCC centres (according to IBEF), employing around 1.9 million people. For the existing talented Indian IT workers other opportunities are opening up, for instance, China now announced issuance of K visa after Trump announced a hike in visa fees. Middle-East countries and the UK are other places waiting to welcome these skilled workers from India. The writer is Professor, School of Management, Mahindra University Published on September 25, 2025 Copyright© 2025, THG PUBLISHING PVT LTD. or its affiliated companies. All rights reserved. BACK TO TOP Comments have to be in English, and in full sentences. They cannot be abusive or personal. Please abide by our community guidelines for posting your comments. We have migrated to a new commenting platform. If you are already a registered user of TheHindu Businessline and logged in, you may continue to engage with our articles. If you do not have an account please register and login to post comments. Users can access their older comments by logging into their accounts on Vuukle. Terms & conditions | Institutional Subscriber
--------------------------------------------------

Title: Oracle slips on reports of unexpected debt plan
URL: https://www.thestreet.com/investing/oracles-15-billion-bond-sale-underlines-ai-expansion-costs-
Time Published: 2025-09-25T00:13:42Z
Description: Oracle stock slips as it raises $15 billion cash by selling debt
--------------------------------------------------

Title: Alibaba rides wave as China sharply expands AI spending
URL: https://www.thestreet.com/investing/stocks/alibaba-rides-wave-as-china-sharply-expands-ai-spending
Time Published: 2025-09-24T20:56:29Z
Description: A longtime investor and TheStreet Pro contributor flags Alibaba as a major participant in China's AI ramp.
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq slip for 2nd day in a row as Fed sends mixed signals, Alibaba keeps AI optimism in focus
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-slip-for-2nd-day-in-a-row-as-fed-sends-mixed-signals-alibaba-plans-ai-push-200103891.html
Time Published: 2025-09-24T20:01:03Z
Description: US stocks reversed gains after Wall Street ended a record-setting rally.
--------------------------------------------------

Title: Estée Lauder heir William Lauder quietly sold a prized NYC home for $37.5M — as he seeks to downsize
URL: https://nypost.com/2025/09/24/real-estate/william-lauder-has-sold-a-nyc-co-op-for-37-5m-off-market/
Time Published: 2025-09-24T16:17:10Z
Full Content:
William Lauder, the ousted executive director and longtime chairman of Estée Lauder Companies — and a scion of the cosmetics empire — has sold his prized co-op at 998 Fifth Ave. for $37.5 million in an off-market deal. News of that transaction was first reported by the Real Deal. And it comes as the billionaire is looking to shed some homes from his portfolio. The buyer for the 998 Fifth residence is Evan Cheng, the co-founder and CEO of Mysten Labs, a blockchain infrastructure firm. Cheng, a former senior engineering executive at Meta, paid a significant premium over the $23.5 million Lauder spent on the unit in 2017 — netting the cosmetics heir a $14 million gain over eight years. The apartment, Unit 6W, is located in one of the most storied buildings along Fifth Avenue, a limestone landmark completed in 1912. The unit has four bedrooms and five bathrooms, with a gracious eat-in kitchen and oversize windows framing views of Central Park and the Metropolitan Museum of Art. The sale marks the latest move in a broader property reshuffling for Lauder, who has been quietly trimming his property portfolio. He is currently marketing a 12th-floor apartment at 778 Park Ave. — once owned by former Revlon chief Ronald Perelman — for $25 million, a slight drop from the $27 million asking price he initially set last year, StreetEasy shows. He’s reportedly looking to sell it in a downsizing effort. Lauder acquired that unit for $24 million in 2023 and also owns another apartment in the same building, purchased in 2008 for $27.5 million. That home was once listed for $45 million, but failed to sell. Outside of Manhattan, Lauder is eyeing a record-breaking sale in Palm Beach. There, he had reportedly found a buyer in recent months for a $178 million pair of oceanfront parcels, but it’s not immediately clear if the deal fell through. If those properties ultimately sell for anywhere near asking, the deal would set a new residential sales benchmark for Florida. Lauder purchased the adjacent estate from the estate of conservative broadcaster Rush Limbaugh in 2023 for $155 million. Advertisement
--------------------------------------------------

Title: ‘Shot in the foot’: Indians are outraged over Trump’s H-1B move
URL: https://www.rt.com/india/625140-my-american-dream-is-dying/
Time Published: 2025-09-24T15:39:29Z
Full Content:
Families rushed off flights from New York to New Delhi. Ticket prices for India–US routes soared overnight. WhatsApp calls buzzed with panic as Indian professionals in America tried to make sense of what had just happened. When US President Donald Trump announced a staggering $100,000 fee on new H-1B visa applications, the shock rippled instantly through Indian communities on both sides of the ocean. For many, the American dream they had spent years building seemed to collapse in a single press release. One of them was Suman (name changed), 39, a healthcare worker in New York. After three years of living apart from her husband and sending money home to support her elderly parents and sister, she sat in her apartment staring at her phone in disbelief. “I am on a H-1B visa, and I have been so stressed since then that my life seems like it will go upside down,” Suman told RT over a WhatsApp call from her apartment in New York. “I’ve been working here for the last eight years, and I have taken loans and mortgages. Everything felt like it was coming crashing down on me.” For families across India, Trump’s sudden move upended their daily life. Muhammad Anas, 62, from the southern state of Telangana, said his family was devastated. “My daughter is working in the US and teaches social science at a college. She is on a H-1B visa. We all felt very distressed, and she was crying on the call. She has two children, and her husband is pursuing a PhD there. The whole family feels the pain of what is going on,” he told RT. “The Indian government should take up the case with the US so that our children have some kind of certainty.” Back in India, the announcement also rattled young professionals who had been preparing for years to build their lives in America. Kazim Ahmad, 27, a graduate of a premier engineering college in West Bengal, said he was crushed. “I spent two years getting certified and interviewing for US companies. I feel very disheartened and devastated. It seems my American dream is dying now,” Ahmad said. “I come from a very poor family, and my state is also very poor. Going to the US could have stepped my family out of poverty and helped me progress, but now, I do not think aspirants have an option to move to the US.” Another hopeful, Vipul Kumar, echoed the frustration. “I spent my money and time on exams. I planned my life around the opportunities in the US. I feel very lost now.” Even those already working in America were forced to reconsider their plans. Inder Jeet, 35, from Haryana and an employee of a global tech giant, said his company warned him not to travel. “I was planning to go home next week and meet my family after two years, but now my company has informed us not to travel. I am heartbroken, but I am also thinking about what comes next for us,” he added. “The families on H-1B are enduring a nightmare. Some have just bought a home on a mortgage, and some have high loans to pay; everyone has a story.” For Suman in New York, the fear goes beyond bills and mortgages. “We do not know what tougher measures this administration is coming out with for migrants who are really working hard in this country to be part of it. To be part of the American dream,” she said, admitting she now questions whether it is wise to build a future in the US “There are a lot of questions in my mind.” The announcement left companies and employees scrambling. Within hours, several American tech giants warned their Indian staff not to leave the country and for those already abroad to return as quickly as possible. Rumors spread across airports, WhatsApp groups, and workplaces, fueling fears that even existing visa holders might be locked out. It wasn’t until the next day that the administration sought to calm the chaos. The White House clarified that the new $100,000 fee would apply only to fresh H-1B applications, not renewals or extensions. Current visa holders, officials said, would not be affected. At the same time, the Trump administration doubled down on its broader policy shift. A new proposal outlined changes to the H-1B lottery that would prioritize higher salaries and “top talent,” with the stated goal of protecting American workers from “unfair wage competition from foreign workers.” New Delhi was quick to sound the alarm. India’s Ministry of External Affairs described the sudden shift as more than an economic measure – calling it a potential “humanitarian crisis.” “This measure is likely to have humanitarian consequences by way of the disruption caused to families. Government hopes that these disruptions can be addressed suitably by the US authorities,” said Randhir Jaiswal, spokesperson for the ministry. He added that the mobility of skilled talent had contributed enormously to technology development, innovation, and economic growth in both countries. “Policymakers will therefore assess recent steps considering mutual benefits, which include strong people-to-people ties between the countries.” For Indians in the US, many say it is not about the job alone, but about the years of hard work they have put into building their lives in a country where they are now staring at an uncertain future. Earlier, Trump imposed 25% tariffs on Indian goods exported to the US, plus additional duties as a penalty for trading with Russia – making some exports completely unviable. Industry experts in India were more blunt. Mohandas Pai, former chief financial officer of Infosys, warned that Trump had “shot himself in the foot.” “These companies are among the most valuable assets in the US and the foundation of its global dominance. They are heavily reliant on high-quality H-1B talent from around the world,” Pai said, adding that the sharp fee hike would freeze fresh demand and push companies to shift jobs offshore. Pai estimated that the impact on India itself will be minimal, since the fee hike applies only to new applications. He pointed out that there is already a substantial stock of H-1B employees in the US, and most Indian IT firms now maintain large local (American) teams in their US development centers. “Over the past five years, they have undertaken risk mitigation measures,” he explained. He added that the visa restrictions would likely accelerate the growth of Global Capability Centers (GCCs) in India. “The multinational corporations respond to the scarcity of work visas by offshoring more operations.” India’s Sherpa for its G20 presidency in 2023, Amitabh Kant, slammed Trump’s move. He stressed that the $100,000 H-1B fee will choke US innovation and turbocharge India’s. “India’s finest doctors, engineers, scientists, innovators have an opportunity to contribute to India’s growth and progress towards Viksit Bharat,” he tweeted. Donald Trump’s 100,000 H-1B fee will choke U.S. innovation, and turbocharge India’s. By slamming the door on global talent, America pushes the next wave of labs, patents, innovation and startups to Bangalore and Hyderabad, Pune and Gurgaon . India’s finest Doctors, engineers,… Officials in India’s IT hubs also suggested the move could prove a long-term advantage for the country. Jayesh Ranjan, special chief secretary in Telangana, called it “a game changer.” “More skilled professionals may now choose to stay or return to India, which will fuel domestic innovation, startups and economic growth, and help transform India into a global technology hub. This will mainly affect American companies and some Indian youths who prefer going to the US. With this, many will be motivated to stay back, and this reverse brain drain will strengthen economic opportunities, competitive wages and broader reforms,” the official told The New Indian Express. For thousands of Indian families in the US, the new visa fee feels like a nightmare – mortgages, student loans, and years of sacrifice suddenly overshadowed by political uncertainty. As one H-1B worker put it: “Everyone has a story – a house just bought, a loan to repay, parents to support. Now everything is in doubt.” Yet industry observers in India say the broader impact could be more complex. Ajay Bagga, a financial markets commentator, noted that the policy might reduce India’s long-running brain drain, forcing both government and industry to find ways to absorb and productively employ highly skilled talent at home. “Brain drain will reduce, and India will have to find other avenues or productively use these people. This will be an objective for the government too,” he told ANI news agency. Others predict neighboring countries will benefit first. Canada and the UK, both eager for high-skilled migrants, could see a surge in applications as US doors narrow. Still, many in India believe the long-term opportunity lies at home – with more engineers, doctors, and innovators choosing to stay, fueling startups and strengthening the country’s own technology ecosystem. In the short term, though, the uncertainty remains crushing. For families like Suman’s, it isn’t just about fees or policy. It’s about watching years of effort and sacrifice suddenly hang in the balance – and wondering whether the American dream is still worth chasing. By Sumitra Bahtti, a journalist based in India RT News App © Autonomous Nonprofit Organization “TV-Novosti”, 2005–2025. All rights reserved. This website uses cookies. Read RT Privacy policy to find out more.
--------------------------------------------------

Title: The Shiller P/E ratio, a trusted stock-market gauge, just hit its highest level since the dot-com bubble
URL: https://markets.businessinsider.com/news/stocks/stock-market-outlook-shiller-pe-ratio-dot-com-bubble-ai-2025-9
Time Published: 2025-09-24T14:43:25Z
Full Content:
A trusted market gauge has surged to its highest levels since the dot-com bubble, signaling stocks are the most expensive in 25 years. The Shiller P/E ratio, also known as the cyclically adjusted price-to-earnings (CAPE) ratio, compares the current value of a stock index like the S&P 500 to its average, inflation-adjusted earnings over the last 10 years. The S&P's Shiller P/E ratio crossed 40 on Monday for the first time since 2000, when it reached a record 44. That spike preceded a 49% crash for the benchmark stock index between March 2000 and October 2002 as the internet boom collapsed. "The US equity market looks expensive relative to its history, pretty much any way you slice it," Russ Mould, AJ Bell's investment director, told Business Insider by email. He added that the Shiller P/E and several other valuation measures are "all in the top 10% of their historic ranges." Mould said that "valuation does not guarantee an imminent accident," but investors might want to take precautions, in part because US stock returns have been "modest to disappointing" when they've traded at "similar Shiller P/E multiples to those in evidence now." He added, "The tricky bit is 10 years is a long time and you don't know when trouble is going to strike!" The Shiller P/E ratio was developed by Nobel Prize-winning economist Robert Shiller, who predicted the dot-com bubble would burst in "Irrational Exuberance," published just as the market peaked in March 2000. He correctly warned the housing boom could collapse in the book's second edition, published in 2005. Investors use the traditional P/E ratio to value a company's stock based on its earnings multiple, or how many times higher its price is than its earnings per share, or EPS, in a year. They calculate it by dividing a stock price by an annual EPS figure, either historical or projected. Similarly, the Shiller P/E ratio takes a stock index's point value and divides it by the combined EPS of its constituent companies over the last 10 years, adjusted for inflation and divided by 10 to get a real, average figure. So if the S&P 500 is at 6,700 points, and its average, inflation-adjusted EPS over the last 10 years is $167, dividing one by the other yields a multiple of 40. The ratio can smooth out the effects of business cycles on profits by factoring in a decade's worth of earnings, providing a longer-term perspective on market valuation than the conventional P/E ratio. The Shiller P/E ratio doesn't indicate an imminent crash, as markets can remain overvalued or undervalued for years. It also doesn't account for interest rates, index composition, economic shifts, and corporate profitability, which can vary between time periods and influence valuations. The S&P 500 has doubled in five years to trade at record highs, led by AI stocks such as Nvidia, which has soared almost 14-fold to become the world's most valuable public company with a $4.3 trillion market capitalization. Several commentators have flagged that stocks are pricey. Federal Reserve Chair Jerome Powell said in a Tuesday speech that "equity prices are fairly highly valued." Erik Gordon, a professor at the University of Michigan's Ross School of Business, told Business Insider in August that the huge market values and vast shareholder bases of AI companies like Microsoft and Meta meant "more investors will suffer than suffered in the dot-com crash, and their suffering will be more painful." But "Shark Tank" investor Kevin O'Leary and tech investor Ross Gerber dismissed the comparison between the AI boom and the dot-com bubble in separate interviews with Business Insider. O'Leary said in August that the AI boom isn't like the dot-com bubble because "you actually can see the productivity and measure it on a dollar-by-dollar basis." Gerber said that valuations are justified because AI companies promise tremendous growth and their profitability is "just insane." Copyright © 2025 Insider Inc and finanzen.net GmbH (Imprint). All rights reserved. Registration on or use of this site constitutes acceptance of our Terms of Service and Privacy Policy.
--------------------------------------------------

Title: Spatial structure evolution and ecosystem service relationship changes in urban-fringe-rural areas of megacities: Evidence from Suzhou, China
URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0332934
Time Published: 2025-09-24T14:00:00Z
Full Content:
The evolution of urban-fringe-rural structures profoundly impacts ecosystem services (ESs). However, the way in which trade-offs and synergies in ESs respond to changes in regional spatial structures has rarely been discussed. This knowledge gap hinders the development of spatially explicit strategies to mitigate ecological degradation while accommodating urban growth, ultimately perpetuating unsustainable landscape management practices characterized by reactive rather than preventive interventions. Such critical disconnect between structural dynamics and ES feedbacks has emerged as a major bottleneck to operationalizing landscape sustainability in metropolitan regions. This study selected Suzhou—a typical megacity in China—as an example to conduct an empirical study. The urban, urban fringe, and rural areas were firstly identified in 2010 and 2022 using Deep Neural Network (DNN) based on multi-source geographical data. Then, seven typical ESs were assessed using multiple models, and their interactions were examined through correlation analysis, coupling coordination degree model, and a self-organizing feature mapping network approach. At last, this study highlighted the complex responses of ESs relationships to dramatically changing spatial structure of urban-fringe-rural areas and proposed landscape management strategies. The findings include the following: (1) from 2010 to 2022, the spatial structure of urban-fringe-rural areas in Suzhou changed considerably, with 69.04% rural areas transformed into fringe areas, and 50.83% fringe areas developed into urban areas; (2) based on transition process, the region was further divided into urban maintenance, urban expansion, fringe maintenance, fringe expansion, and rural retention areas. Most of the mean value of ESs showed a gradient increasing differences along urban-fringe-rural, while the greatest decrease occurs in fringe expansion and urban expansion areas; and (3) interactions for changes in ES pairs also more closely linked in these two regions, with synergies dominating. The coupled coordination index among multiple ESs declines significantly in these areas, degrading from key coordination to key or mild trade-offs bundles. The results show ES interactions exhibit significant spatial variability under the evolution of metropolitan spatial structure, thus innovatively proposing integration of ESs synergies into urban-fringe-rural development framework to support overall landscape sustainability. Citation: Mao Q, Tian Y (2025) Spatial structure evolution and ecosystem service relationship changes in urban-fringe-rural areas of megacities: Evidence from Suzhou, China. PLoS One 20(9): e0332934. https://doi.org/10.1371/journal.pone.0332934 Editor: Gang Xu, Wuhan University, CHINA Received: February 13, 2025; Accepted: September 6, 2025; Published: September 24, 2025 Copyright: © 2025 Mao, Tian. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability: All relevant data are within the manuscript and its Supporting Information files. Funding: This study was supported by the National Natural Science Foundation of China (no. 42371200 and no. 41901203), the University Natural Science Project of Jiangsu Province (no. Z231687), and Collaborative Innovation Center for New Urbanization and Social Governance of China. There was no additional external funding received for this study. Competing interests: The authors have declared that no competing interests exist. The traditional urban-rural dichotomy has long defined the territorial structure of cities and rural areas. However, with urbanization, increasing interactions between urban and rural elements have blurred these boundaries, especially in metropolitan areas [1–3]. Scholars proposed the existence of a vast, transitional, dynamically evolving zone between urban-rural areas as the urban-rural fringe [4,5]. Scott emphasized that the fringe area is a distinct geographic unit, characterized by its evolving nature [6]. Thus, the urban-fringe-rural geographic classification model is gradually replacing the original dichotomy to capture complexities of regional spatial organization. However, most studies have focused on urban expansion, exploring its identification, evolutionary process, and impacts [7–10]. Less attention has been paid to broader spatial structure of urban-fringe-rural areas and their associated impacts. Properly defining the spatial extent of these areas is crucial for understanding territorial structures, assessing the socio-economic and spatial consequences of their evolution, optimizing land use, and enhancing ecological protection. Urban areas are characterized by high population densities and intensive industrial activities, with land use dominated by built environments [11,12]. Rural areas, in contrast, feature lower population densities and limited industrial development, with land predominantly devoted to agriculture and natural ecosystems [13]. The fringe areas, situated between urban and rural areas, exhibit a transitional mix of geographic elements [14,15]. This regional geospatial structure reflects the distribution of social and economic factors [16], with shifts symbolizing changes in human activities that impact ecosystem services (ESs). ESs are the benefits humans directly or indirectly derive from ecosystems, such as regulating, provisioning, and cultural services [17,18]. The interactions among ESs can involve trade-offs, where the increase of one service reduces another, or synergies, where multiple ESs are enhanced simultaneously [19,20]. This variability makes landscape less capable of providing stable, long-term ESs, which in turn affects human well-being [21,22]. Efforts to enhance ES synergies between urban and rural areas are gain attention, involving ES assessments, landscape fragmentation, ecological network security, and multifunctional ecological agriculture [23–26]. However, many studies separately focus on urban or rural areas, often overlooking the unique dynamics of the fringe zone. Megacities, characterized by large populations and expansive urbanization, present a complex urban-fringe-rural spatial structure [27]. The concentration of population and industry in these cities leads to significant, often irreversible land-use changes. These shifts pose potential risks to ESs and their trade-offs, further intensifying urban ecological unsustainability [28–30]. the evolution of urban-fringe-rural spatial structures helps clarify the complexity of urban-rural systems and their dynamic patterns. By examining how these changes affect ESs, it is possible to more accurately assess and predict ES supply, changes, and interactions, leading to more effective strategies for optimizing and protecting these services and promoting sustainable urban-rural development. Hence this study makes efforts to address the following questions: What are the spatiotemporal patterns of urban-fringe-rural structural evolution in megacities? How do ESs trade-offs and synergies respond to these changes? How can ES synergies be integrated into sustainability planning for urban–fringe–rural landscapes? This study takes Suzhou—a typical megacity in China—as an example. Over the past decade, Suzhou has experienced rapid urban growth, especially at the urban fringe, resulting in significant ecological challenges. As such, it provides an ideal setting for investigating the evolution of urban-rural spatial structures and the performance of ESs trade-offs. In this study, a deep neural network (DNN) model was constructed to identify urban, fringe, and rural areas in Suzhou, subdividing the territory into five zones based on transformation trends from 2010 to 2022. Then 7 key ESs were assessed by integrating models like InVEST and Maxent. Subsequently, correlation analysis, coupled synergies, and clustering technique were applied to explore the spatiotemporal dynamics of ESs relationships. The results help reveal the evolutionary patterns of ESs across these geographic zones. Finally, refined landscape management strategies are proposed. By mapping and prioritizing areas of intense ES trade-offs, integrating ES synergies into the classification and planning of urban-fringe-rural landscapes, planners can employ targeted interventions, such as green infrastructure, multifunctional agriculture, or ecological corridors. This can provide a clear pathway to sustain overall landscape functionality, balance competing land-use demands, and strengthen regional resilience. The flowchart of this study is shown in Fig 1. https://doi.org/10.1371/journal.pone.0332934.g001 Within the longstanding traditional framework of the urban-rural dichotomy, previous studies often distinguish urban and rural areas based on multi-source data like land-use data, nightlight intensity, point of interest (POI), and common research topics include urban sprawl, land-use transition, ESs change, and environmental degradation [31–34]. With a deeper understanding of urban fringe areas, the urban-fringe-rural triadic structure has gained increasing attention from scholars, gradually replacing the urban-rural binary structure as the regional classification framework for theoretical and empirical research in fields such as geography, urban planning, land resource management, and ecological planning [35,36]. Mutation detection and comprehensive evaluation methods are two main models to identify urban, fringe, and rural areas. The former uses clear spatial data like POI and nighttime lights to find spatial differentiation [37], while the latter relies on socio-economic indicators and spatial clustering [38,39]. Through informative, these methods are time-consuming and resource-intensive, with limited scalability in spatiotemporal analysis. Recent advances data processing and artificial intelligence have led to the application of machine learning methods. For example, Liu employed deep neural networks (DNNs) to identify urban-fringe-rural regions based on POI data [40], but this approach’s reliance on single-dimensional data limits its ability to capture the complex characteristics of urban-fringe-rural transitions. On the other hand, Jiao utilized graph convolutional networks (GCNs) with multi-source data to analyze spatial changes over time [41], but it primarily focused on scale variations, lacking a deeper exploration of the dynamic evolution of spatial features. To fill this gap, this study employs a DNN model to identify the spatial boundaries of urban, fringe, and rural areas. The DNN model is a type of artificial neural network that consists of multiple layers, supporting to learn complex representations of multi-source geographic data. It is characterized by its ability to handle large, high-dimensional datasets and model non-linear relationships, making it ideal for classifying spatial units into urban, fringe, and rural types. After identifying the spatial extent of these areas at two temporal nodes for a typical megacity, this study further explores the spatiotemporal evolution of the urban-fringe-rural regional structure. Additionally, it conducts a thorough analysis of the ESs relationship in the context of these spatial changes. ESs synergy has become a key objective in ecological planning and sustainable development, as it optimizes ecological functions and supports ecosystem health [42]. ESs are typically classified into provisioning, regulating, supporting, and cultural services [43]. Their assessment often involves process-based models (e.g., InVEST, SWAT) or statistical models (e.g., regression, time series analysis), using ecological principles, historical data, and GIS tools [44,45]. To understand the complex interactions between ESs, correlation analysis and spatial models are commonly employed, revealing trade-offs or synergies [46]. Factors influencing ES trade-offs include land-use, urbanization, climate change, ecosystem configuration, and socio-economic drivers such as population density and agricultural intensity [47–49]. Strategies for optimizing ES synergies generally focus on integrated land management, multifunctional land uses, ecosystem connectivity, and sustainable agricultural practices [50,51]. While research on ES trade-offs has primarily examined urban expansion, studies show that urban growth tends to reduce provisioning services (e.g., agricultural output, resource depletion) but can enhance cultural (e.g., recreation) and regulating services (e.g., urban climate regulation) [52]. Some studies also explore ES dynamics in urban fringe areas, highlighting competition among ESs due to land-use changes [53]. Others have balanced ES from a comprehensive rural-urban distribution perspective, indicating that rural areas prioritize ecological services like water retention and biodiversity, while urban areas emphasize cultural and regulating services [26,54]. However, there has been limited analysis of how ES trade-offs and synergies evolve across the broader spatial structure of urban, fringe, and rural areas. This gap limits the ability to manage ESs in a dynamic, holistic, and territorially comprehensive framework. The evolution of urban-fringe-rural spatial structures, especially in metropolitan regions, is widespread and continuous [55]. Once rural areas transition to urban or fringe zones, reversing this transformation is uncommon, as developed land seldom reverts to its non-built state. This irreversibility underscores the long-term consequences of land use changes. The urban-fringe-rural structure can be divided into five zones, including urban maintenance zones, urban expansion zones, fringe maintenance zones, fringe expansion zones, and rural retention zones. Each zone exhibits unique ES characteristics, encompassing both trade-offs and synergies. Understanding how ES relationships respond to dynamic changes in these zones is crucial for advancing sustainable development and optimizing ES management across urban and rural landscapes, helping to balance ecological integrity with the development needs of megacities. Suzhou (30° 47’–32° 02’ N, 119° 55’–121° 20’ E) is a prefectural city in Jiangsu Province, Southeast China, with a GDP of 2,395.83 billion yuan (333.28 USD) by the end of 2022, ranking among the top 20 regions in China. The city has a population of 12.911 million and an urbanization rate of 74.42%, making it a typical megacity selected for this case study. The rapid expansion of construction land (Fig 2) has significantly altered the urban-rural landscape. This growth, particularly evident in fringe areas where farmland and wetlands are being converted, creates competing demands on ESs—for instance, boosting housing and transportation capacity while reducing flood regulation and biodiversity. These intertwined trade-offs highlight the need to precisely map how different zones (urban core, expanding fringe, and rural hinterlands) contribute to ESs relationships. By understanding which areas see the sharpest conflicts between development and ecological functions, planners can target conservation efforts where they matter most, such as protecting critical wetlands in fast-urbanizing corridors or promoting green infrastructure in newly developed suburbs. (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). https://doi.org/10.1371/journal.pone.0332934.g002 In this study, spatial data and statistical records were used to quantify ESs. All data were reprojected to a consistent coordinate system, and the spatial resolution was harmonized to 30 m. ES trade-offs were assessed at the village administrative unit level based on grid-scale measurements. Following were main types of data and descriptions (Table 1). https://doi.org/10.1371/journal.pone.0332934.t001 The first challenge in classifying urban, fringe, and rural areas lies in establishing objective criteria for training the DNN model and validating the results. Given the absence of standardized frameworks, prior studies have relied on manual identification. In this study, five experts in urban planning and geography were engaged to identification, and following steps were taken: 1) Experts were divided into two groups—four in the identification group and one in the validation group; 2) For 2010 and 2022, 1000 random geographic points were generated for each year. Each point was input into Google Maps with a 1000-meter radius, and satellite images within this radius were extracted; 3) Two members of the identification group reviewed and labeled the images as rural (0), fringe (1), or urban (2), resulting in 2,000 labeled images; 4) If both members agreed on the label, it was forwarded to the validation group for final assessment. If the validation expert agreed, the label was confirmed; 5) In cases of discrepancies, the remaining identification group members re-evaluated the images, and the validation process repeated. If disagreement persisted, the point was discarded and a new one generated (Table 2). https://doi.org/10.1371/journal.pone.0332934.t002 Building upon this foundation, two DNN classification models were constructed for 2010 and 2022, each trained with 1000 labeled sample points (80% training, 20% test). The model architecture included an input layer, two hidden layers (128 and 64 neurons), and an output layer with Softmax activation for a three-class classification. The model used the Adam optimizer with a learning rate of 0.001, and sparse categorical cross-entropy as the loss function. Accuracy, calculated as the ratio of correctly predicted samples to total samples, was the primary evaluation metric. Key geographic features were selected to represent the distinctive characteristics of urban, fringe, and rural areas. These included population density, GDP density, construction land density, POI density, nighttime light brightness, and landscape fragmentation (Table 3). All datasets were processed at a 1 km spatial grid scale. Dropout layers (rate = 0.5) were added after each hidden layer to prevent overfitting. The model captured and learned complex nonlinear relationships between the input features using deep neural networks. https://doi.org/10.1371/journal.pone.0332934.t003 Guided by the principles of differences in ESs in urban and rural landscapes, the importance of ESs to urban and rural residents, and data availability, seven ESs, including one provisioning service (food production), four regulating services (carbon storage, soil conservation, water purification, and water yield), one supporting service (habitat quality), and one cultural service (landscape aesthetics), were selected in this study during 2010 and 2022. All ESs were quantified at a grid spatial resolution of 30 meters. The detailed assessment methodology is presented in Table 4. https://doi.org/10.1371/journal.pone.0332934.t004 Correlation analysis, coupling coordination degree model, and self-organizing mapping network methods were employed to investigate interactions between multiple ESs across temporal, spatial, and spatiotemporal dimensions. To eliminate scale differences between ESs, Min–Max normalization was applied to standardize the data: where is the normalized ES value, is the observed value, and and are the minimum and maximum observed values. Spearman’s nonparametric correlation analysis was then used to identify trade-off or synergy relationships among ES changes from 2010 to 2022. Positive correlations indicate synergy, while negative correlations indicate trade-offs. The “corrplot” package in R4.3.1 was used to visualize correlations among ESs in 2010 and 2022. The correlation coefficient was calculated as: where is the correlation coefficient (value range [−1,1]), and and are ES data values. To explore the interactions among multiple ESs, we employed the coupled coordination degree model (CCDM) at the village scale to assess coordination changes in Suzhou from 2010 to 2022. Spatial trend analysis identified shifts in incoordination, shedding light on the evolutions of ES relationships over time and space. Coupling measures the interaction strength between systems, quantified by the coupling degree (CD) [65]. The coupling coordinated degree (CCD) reflects the level of integrated system development. For k systems, the coupling degree model (CDM) is given as: where represents the average value of each ES in a village, and C ranges from 0 (low interaction) to 1 (high interaction). When C = 1, the systems are fully integrated, and When C = 0, they are independent [66]. To assess coordination, the CCDM formula is: where T reflects the overall impact of the ESs, and a, b, c, …, h are coefficients that reflect the relative importance of each service (values: 0.12 for most services, and 0.16 for habitat quality, based on expert recommendations). D ranges from 0 to 1, with higher values indicating better coordination. Coordination is categorized into six levels (Table 5), from extreme incoordination (F) to high-quality coordination (A) [67]. https://doi.org/10.1371/journal.pone.0332934.t005 To capture the response of key ESs and their trade-offs within the urban-fringe-rural territorial structure evolution, Self-Organizing Map (SOM) clustering was applied. SOM is an unsupervised neural network that reduces dimensionality, visualizing high-dimensional data in a 2D grid while preserving topological relationships [68]. This method identifies patterns in ES interactions and their trade-offs, combining principal component analysis (PCA) and K-means clustering. SOM maintains the topology of input space through nearest-neighbor relationships and incorporates spatial data. Its ability to handle large, complex datasets while remaining interpretable makes it well-suited for exploring spatially distributed ESs across diverse structures. Additionally, SOM’s competitive learning algorithm detects hidden relationships without prior labeling, making it flexible for analyzing non-linear interactions among multiple ESs in dynamic environments [69]. In this study, the urban fringe of Suzhou was used as input, and the optimal number of clusters was determined by calculating the Davies–Bouldin index for 2–15 classes using the “kohenen” package in R4.3.1. This study maps the urban–fringe–rural spatial structure of Suzhou in 2010 and 2022, achieving DNN model accuracies of 0.9502 and 0.8850. The results (Fig 3) show that urban fringe areas are primarily located in the central and northern regions, with the south remaining predominantly rural. Both the central city and fringe areas exhibit polycentric dispersion and outward expansion. By 2022, the urban fringe covered 40.42% of the area, marking a 75.83% increase, while urban areas grew by only 2.13%. Conversely, rural areas decreased by 46.23%, covering 1,686 km2. Transfer analysis indicates that most land conversions occurred from rural to fringe areas (1,713 km2) and from fringe to urban areas (862 km2), representing 69.04% and 50.83% of the respective areas by 2022. This highlights that while the urban fringe is rapidly expanding, the rate of its transition to urban areas is slower, potentially leading to land resource inefficiency. (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). https://doi.org/10.1371/journal.pone.0332934.g003 At the county level, there is notable spatiotemporal heterogeneity. Gusu District, Industrial Park, Huqiu District, and Xiangcheng District have the highest fringe-to-urban ratios (over 90%), indicating strong urbanization pressures. Changshu, Wujiang, and Taicang showed the fastest fringe growth (over 20% from 2010 to 2022), while Huqiu and Industrial Park experienced the most rapid urban growth (over 38%). Central Suzhou is characterized by urban expansion, while the northern and southern regions show more fringe expansion, reflecting differentiated urbanization patterns. Wujiang and Taicang’s substantial rural areas suggest lower urbanization, indicating potential for future development. Transformation scenarios were classified into five types, namely urban maintenance area (sustained urban from 2010 to 2022), urban expansion areas (transformed from fringe to urban), fringe maintenance area (sustained urban fringe), fringe expansion area (transformed from rural to urban fringe), and rural retention area (sustained as rural). Areas transitioning from urban or fringe to rural were excluded due to their minimal proportion, indicating that urbanization of rural areas is typically irreversible. fringe expansion areas (34.36%) make up the largest share, followed by rural retention (31.42%) and urban expansion (22.09%) (Table 6). https://doi.org/10.1371/journal.pone.0332934.t006 From 2010 to 2022, most ESs showed higher values in the northern and southern regions of Suzhou, with lower values in the central area. This trend applied to services such as carbon storage, water purification, soil conservation, and habitat quality (Fig 4). Food production was highest in the northeastern and southwestern regions, linked to arable land, and lowest in mountainous and urban areas. Water yield and landscape aesthetics both peaked in the urban center. Overall, most services, except cultural services, declined, with food production declining by 37.68%, primarily in the urban core and northern fringe. Regulatory and supporting services significantly decreased near urban centers, while mountainous regions saw increases in water purification and soil conservation. Notably, water yield rose significantly, while habitat quality declined in the urban fringe. https://doi.org/10.1371/journal.pone.0332934.g004 To understand the response of ESs to urban-fringe-rural spatial structures, changes across five regions were counted and compared (Fig 5). Most ESs—such as food production, carbon storage, soil conservation, nitrogen output, and habitat quality—were highest in rural retention areas, followed by fringe expansion areas, and lowest in urban maintenance areas, showing a decreasing gradient with urbanization. Conversely, urban maintenance areas exhibit the highest mean values for water yield and landscape aesthetics, with elevated phosphorus output in urban expansion areas. From 2010 to 2022, urban expansion, fringe expansion and urban fringe areas experienced substantial decreases in food production, water yield, and carbon storage, alongside the largest increases in landscape aesthetics. Soil conversation fluctuated in these regions. Notably, food production, carbon storage, nitrogen-phosphorus output, and habitat quality declined most in fringe expansion areas, with nitrogen output, food production and habitat quality dropped by 46.33%, 36.57% and 30.52%, respectively. Meanwhile, landscape aesthetics increased most in rural retention areas. (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). https://doi.org/10.1371/journal.pone.0332934.g005 The correlations between changes in ES pairs from 2010 to 2022 were calculated across five regions (Fig 6). Positive correlations indicate synergies, where two ESs increase or decrease together, while negative correlations reflect trade-offs, where one ES increases and the other decreases. The absolute value of the correlation coefficient indicates the strength of the relationship. * FP—Food production; CS—Carbon storage; WY—Water yield; HQ—Habitat quality; SC—Soil conservation; TN—Nitrogen output; TP—Phosphorus output; LA—Landscape aesthetics. * FP—Food production; CS—Carbon storage; WY—Water yield; HQ—Habitat quality; SC—Soil conservation; TN—Nitrogen output; TP—Phosphorus output; LA—Landscape aesthetics. https://doi.org/10.1371/journal.pone.0332934.g006 Overall, the correlation coefficients of most ES pairs grew stronger from 2010 to 2022, particularly as fringe or rural areas transitioned to urban or fringe areas. Trade-offs were most evident in urban and fringe expansion areas, particularly between food production and landscape aesthetics, habitat quality and landscape aesthetics, and carbon storage and water purification. Urban fringe areas, with their edge effects, showed stronger coupling of ESs under anthropogenic disturbance. In contrast, rural reserves mainly experienced a loss in both food supply and habitat quality, reflecting the ecological degradation associated with urbanization. Additionally, trade-offs between carbon storage and nitrogen/phosphorus export, habitat quality, and food production were observed across all regions. To explore the interactions among multiple ESs in response to changes in urban-fringe-rural structures, a coupled coordination degree model was employed to Suzhou from 2010 to 2022. The results were categorized into six levels (Fig 7) and associated with the newly defined urban-fringe-rural territorial structure. (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). https://doi.org/10.1371/journal.pone.0332934.g007 From 2010 to 2022, the average coupling coordination values for ESs in Suzhou decreased from 0.5820 to 0.4481, showing a shift from primary synergy to mild incoordination. ES interactions exhibit dissonance in urban maintenance areas but evolved toward harmony along the urban–fringe–rural gradient, reaching synergy in rural retention areas. The transfer matrix revealed that urban maintenance areas experienced growing dissonance, while rural retention areas shifted from mild dissonance to synergy, particularly in villages of southern Wujiang District, where ES relationships balanced. Notably, the most significant changes occurred in urban and fringe expansion areas. In urban expansion areas, ES relationships deteriorated from synergy to high dissonance, while fringe expansion areas saw a shift from good synergy to primary synergy. These findings highlight the disparities in ES interactions during urban-fringe-rural transformation. Urban expansion areas faced greater risks of environmental incoordination, while fringe maintenance areas exhibited minimal change. Spatial analysis, supported by the standard deviation plot (Fig 7), shows that an expansion of incoordination areas and a reduction in synergistic ones by 2022. The directional trend of the plot spreads toward the northeast in line with fringe sprawl. This study utilized the self-organized mapping network clustering method to categorize ES bundles at the village scale, the smallest ecological unit in territorial spatial planning. Seven distinct ES bundles (Fig 8) were identified based on dominant services and their interactions: (1) Key trade-offs bundle. Found near urban areas, characterized by high water yield and landscape aesthetics but minimal food production and depleted other ESs. (2) Mild trade-offs bundle. Located in fringe areas, with high water yield and landscape aesthetics, but low food provisioning and reduced other ESs. (3) Key synergistic bundle. Primarily in rural areas, showcasing balanced ES provision and strong synergies. (4) CS-HQ-LA bundle. Found mainly in southwest rural Suzhou, with high carbon storage, and notable synergies among carbon storage, habitat quality, and landscape aesthetics. (5) HQ-LA bundle. Located in rural areas, dominated by landscape aesthetics and habitat quality, with strong soil conservation services. (6) Water purification bundle. Concentrated in areas with extensive water bodies (e.g., Wujiang District), offering effective water quality purification and low nitrogen/phosphorus outputs. (7) FP bundle. Primarily in northern Suzhou, dominated by high food production. (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). Composition and relative magnitude of ESs in the village scale. Longer segments represent higher ES supply. * FP—Food production; CS—Carbon storage; WY—Water yield; HQ—Habitat quality; SC—Soil conservation; TN—Nitrogen output; TP—Phosphorus output; LA—Landscape aesthetics. (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). Composition and relative magnitude of ESs in the village scale. Longer segments represent higher ES supply. * FP—Food production; CS—Carbon storage; WY—Water yield; HQ—Habitat quality; SC—Soil conservation; TN—Nitrogen output; TP—Phosphorus output; LA—Landscape aesthetics. https://doi.org/10.1371/journal.pone.0332934.g008 From 2010 to 2022, the urban maintenance area showed a dominance of key trade-offs, comprising 72.0% and 82.5% of the bundles, respectively. In urban expansion area, key trade-offs and mild trade-offs increased from 39.3% and 33.8% in 2010 to 52.6% for key trade-offs by 2022. Similarly, fringe maintenance areas exhibited a shift toward key trade-offs, from 34.6% and 30.7% in 2010 to 45.1% in 2022. In the fringe expansion area, the key synergistic bundle and water purification bundles were prominent in 2010 (39.2% and 25.2%), but by 2022, the area shifted to dominance by the key synergistic bundle and mild trade-offs (40.5% and 20.8%), while the water purification bundle declined to 17.8%. The rural retention area, initially dominated by food production, water purification, and key synergistic bundles in 2010 (26.9%, 21.2%, and 20.4%), but by 2022, saw these proportions decrease by 2022. ES bundle transfers primarily occurred in the urban expansion, fringe expansion, and rural retention areas, with urban maintenance and fringe maintenance areas remained stable. In urban expansion areas, mild trade-offs shifted to key trade-offs due to increased built-up land, leading to degradation in soil and water conservation and habitat quality. The fringe expansion area exhibited varied ES bundle changes across regions. In central and northern regions, some villages near the urban fringe experienced an increase in water yield and a reduction in food production, transitioning from key synergistic bundles to mild trade-offs (19.4%). In Wujiang and Kunshan, certain villages saw a decline in water purification capacity, triggering cascading habitat quality trade-off effect that shifted them from water purification to mild trade-offs bundles (16.1%), while others transitioned to key synergistic bundles. In rural retention areas, notable changes occurred in the Taicang-Changshu border, where fragmented arable land diminished food production. Some villages near the fringe expansion area shifted from HQ-LA to mild trade-offs, indicating habitat quality degradation risk. Given the irreversible nature of urbanization, ES trade-off risks exhibit distinct stages, particularly in dynamically developing urban and fringe expansion areas. The delineation of urban-fringe-rural spatial structures through our DNN framework inherently balances methodological rigor and contextual adaptability. We employed city-wide random sampling (2,000 grids) and multi-stage expert validation (7.2% resampling rate) to minimize labeling bias while capturing Suzhou’s unique land use context. Specifically, we excluded large water bodies (>1 km2), which constitute 37.1% of the territory, and integrated fragmented forests (median size <0.5 km2) into the rural classification based on their inherently low-density characteristics (S1 Fig). When applying this framework to other contexts, researchers should be tailored to local land use profiles. For instance, implementing pre-classification masking for forest patches exceeding context-specific size thresholds (e.g., > 5 km2 in mountainous cities like Lishui or eco-cities like Xiamen) to avoid misclassifying densely forested areas as rural zones. Additionally, temporal validation across Jiangsu Province (2005–2022, accuracies: 0.84–0.95) through DNN and comparative analysis against traditional mutation detection methods (low accuracy: 0.66, S2 Fig) confirm the model’s capacity to decode gradient urbanization patterns characteristic of polycentric metropolises. These technical considerations underpin our core finding that Suzhou exhibits a multi-layered urban-fringe-rural structure, highlighting the continuous expansion of urban fringe areas in Suzhou. This pattern aligns with similar trends observed in other Chinese cities, such as Chengdu, Shanghai and Guangzhou [70–72]. Given the irreversible nature of rural-urban transitions and the inherent heterogeneity in these shifts of metropolitan cities, accurately capturing their dynamic evolution is crucial for sustainable landscape development. The study categorizes the urban-fringe-rural continuum into five distinct types, revealing a multi-core developmental texture and differentiated formation mechanisms. In Suzhou, for example, the historical Gusu District (Fig 9, City 1) anchors the city’s urbanization trajectory. The “one core, four cities” policy has reinforced centralized growth, leading to simultaneous internal growth and external expansion across sub-regions like Xiangcheng, Huqiu, the Industrial Park, and Wujiang District. This phenomenon creates a unique multi-layered spatial structure, marked by overlapping urban, fringe, and fringe maintenance zones. Meanwhile, sub-city regions (City 2) predominantly exhibit external expansion, driven by transportation corridors and regional integration policies, influenced by Shanghai’s economic effect. These areas reflect decentralized urbanization, with fringe expansion zones along major transportation routes. This trend resonates with suburbanization patterns (e.g., suburban enclaves or exurban growth in metropolitan peripheries in the U.S. and Europe [73–75]), but also highlights the regional specificity shaped by China’s policy-driven urbanization. (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). (Republished from Ministry of Natural Resources of China, http://bzdt.ch.mnr.gov.cn). https://doi.org/10.1371/journal.pone.0332934.g009 The dynamic evolution of urban–fringe–rural spatial structures reflects a blend of internal densification and external expansion, offering a unique perspective for comparative studies on urban-fringe dynamics. This study also provides a practical foundation for exploring strategies for dynamic management, addressing social and ecological challenges, enhancing conservation, and promoting sustainable development in rapidly urbanizing regions. The dynamic evolution of urban–fringe–rural landscapes significantly influence ESs trade-offs and synergies, demonstrating the transitional and complex nature of these areas. Regulatory and supporting services like carbon storage, water purification, and habitat quality have markedly changed in fringe and expansion areas, intensifying trade-offs and driving the degradation of ES bundles. The most severe trade-offs are concentrated in rapidly urbanizing regions such as Kunshan and Changshu in northeastern Suzhou, where urban fringe expansion accelerates shifts from mild to severe trade-offs. This is driven by land-use changes and functional spillover effects from urban cores, like Gusu District. Some villages at the junction of Kunshan and the Industrial Park have undergone a leapfrogging development process, transitioning directly from key synergies to severe trade-offs between 2010 and 2022. This pattern underscores ecological risks and negative externalities associated with rapid urbanization in fringe areas. Addressing these challenges requires integrating ES synergistic management into urban–fringe–rural sustainability planning. Urban fringe areas serve as critical ecological buffers, mitigating urban pressures. Protecting key ecological nodes—such as water sources and biodiversity hotspots—helps maintain landscape connectivity and reduce ecosystem degradation. For example, enhancing ecological corridors, as seen in the European Green Belt, facilitates species migration and gene flow, balancing ecological integrity with development. Additionally, the intensive land-use changes in urban fringe areas necessitate adopting green infrastructure strategies. The intensive land-use changes in urban fringe areas necessitate adopting green infrastructure strategies. Green belts, parks, wetlands, and ecological buffers can enhance ecological resilience and mitigate urbanization’s environmental impact. These measures can improve both environmental quality and residents’ quality of life. In Suzhou, targeted green development can address ecological trade-offs and support urban-rural integration. Finally, integrated planning is essential for the irreversible ecological and social transitions in fringe areas. Policies should emphasize urban-rural resource complementarities, leveraging rural areas for food supply and regulating services, while urban areas provide cultural and economic benefits. Equitable distribution of public services and infrastructure, including transportation and green spaces, can foster synergy between urban and rural systems, as demonstrated by international models. These strategies offer a balanced approach to sustainable socio-economic and environmental development. While the proposed identifiable urban-fringe-rural framework effectively captures policy-driven metropolitan dynamics in Suzhou, its applicability to market-dominated cities or those with unique landscapes (e.g., mountainous or resource-depleted regions) requires further validation. Future efforts should prioritize two pathways: First, developing adaptive analytical modules tailored to different institutional contexts—for instance, incorporating land price indices in market-active areas and landslide risks in ecological corridors. Second, it is essential to engage stakeholders—including farmers, urban residents, government officials, and developers—in discussions about the findings and their views on ES trade-off decisions. Establishing a closed-loop system integrating “DNN identification-ES assessment-stakeholder engagement”, where resident preferences (collected through participatory platforms) directly inform model refinements, moving beyond post-hoc discussions. Finally, Additionally, the current semi-supervised model’s reliance on expert labeling could be optimized through disputed-area prioritization—focusing annotation efforts on ambiguous urban-rural transition zones rather than blanket sampling. This targeted approach maintains accuracy while enhancing scalability. In this research, multi-source geographic data and machine learning methods were emplyed to investigate the evolution of urban, fringe, and rural areas in Suzhou from 2010 to 2022. Key ESs and their interactions across urban–fringe–rural geospatial structures were analyzed, resulting in the identification of seven distinct ES bundles: key trade-offs bundle, mild trade-offs bundle, key synergistic bundle, CS-HA-LA bundle, HQ-LA bundle, water purification bundle, and FP bundle. These findings formed the basis for developing landscape optimization strategies to enhance synergies among ESs and promote sustainable urban-rural development. This study contributes to the literature in two significant ways. First, the DNN model-based identification method provides a refined approach for delineating urban–fringe–rural spatial structures across various scales and timeframes in metropolitan regions. Second, by employing a case-study approach, an innovative framework is presented for recognizing, evaluating, and managing dynamically evolving ES interactions, offering valuable guidance for sustainable urban–fringe–rural landscape planning and governance. https://doi.org/10.1371/journal.pone.0332934.s001 (JPG) https://doi.org/10.1371/journal.pone.0332934.s002 (JPG) https://doi.org/10.1371/journal.pone.0332934.s003 (ZIP) https://doi.org/10.1371/journal.pone.0332934.s004 (ZIP) https://doi.org/10.1371/journal.pone.0332934.s005 (ZIP) https://doi.org/10.1371/journal.pone.0332934.s006 (ZIP) https://doi.org/10.1371/journal.pone.0332934.s007 (ZIP) https://doi.org/10.1371/journal.pone.0332934.s008 (ZIP)
--------------------------------------------------

Title: Meta (META) Unveils AI Glasses at Connect Conference, Stifel Maintains Buy Rating
URL: https://finance.yahoo.com/news/meta-meta-unveils-ai-glasses-135427316.html
Time Published: 2025-09-24T13:54:27Z
Description: Meta Platforms, Inc. (NASDAQ:META) ranks among the best hot AI stocks to buy right now. Following Meta’s keynote presentation at the Annual Connect...
--------------------------------------------------

Title: Meta (META) Stock Target Reaffirmed Despite Regulatory Challenges in U.S. and Europe
URL: https://finance.yahoo.com/news/meta-meta-stock-target-reaffirmed-124548869.html
Time Published: 2025-09-24T12:45:48Z
Description: Meta Platforms, Inc. (NASDAQ:META) ranks among the hot stocks to invest in right now. On September 8, Cantor Fitzgerald reaffirmed its $920 price target and ...
--------------------------------------------------

Title: GE Vernova Is Up 350%, But Can It Deliver?
URL: https://www.forbes.com/sites/greatspeculations/2025/09/24/ge-vernova-is-up-350-but-can-it-deliver/
Time Published: 2025-09-24T12:30:00Z
Full Content:
ByTrefis Team, Contributor. GE Vernova stock has experienced an impressive increase. Following its spin-off from General Electric in April 2024, the stock has risen over 350%, significantly outpacing the S&P 500’s gain of 27% during the same timeframe. The reasons behind this rally are obvious: electricity demand is on the rise, production capacity is expanding, profit margins are improving, and sustainability pledges offer the company a compelling long-term story. Collectively, these elements have driven the stock’s remarkable performance. However, investors should consider whether the market has already factored in these positives or if optimism has outstripped the underlying fundamentals. At its current price, GE Vernova is far from being a bargain. The stock trades at a P/E ratio of 151, a price-to-sales ratio of 4.7, and a price-to-free cash flow of 63. These multiples are significantly higher than the S&P 500 averages, suggesting that investors are wagering not on the company's present, but on its potential future role as a crucial component in the worldwide transition to electrification. Given such high expectations, even minor execution errors could negatively impact the stock. That said, investing in a single stock entails risks. For those investors seeking less volatility than individual stocks, the Trefis High Quality Portfolio offers an alternative - having outperformed the S&P 500 and delivered returns of over 91% since its launch. As a whole, HQ Portfolio stocks yielded better returns with reduced risk compared to the benchmark index; providing a steadier performance, as reflected in HQ Portfolio performance metrics. Separately, check out GOOGL Stock Vs. AMZN, MSFT & META At its essence, GE Vernova is an energy technology firm dedicated to the transition toward a lower-carbon future. Its four divisions - Power, Wind, Electrification Systems, and Energy Financial Services - encompass the technologies required to produce, transmit, and manage electricity. In summary, GE Vernova is central to the electrification and decarbonization movement. In September, the management revealed the sale of its Proficy industrial software unit to TPG for $600 million. This is a typical divestiture strategy: offload non-core assets and reinvest in what is essential. For GE Vernova, this entails a renewed emphasis on grid software and power systems, where demand is growing. Electricity demand is not decreasing, and GE Vernova is aware of this. The company is increasing its turbine production capacity from 15,000 to 20,000 megawatts by 2026, aiming to fulfill the rising demand, particularly in rapidly industrializing areas. GE Vernova also plays a pivotal role in the transition to clean energy. With 55,000 wind turbines and 7,000 gas turbines operational, its technology contributes to approximately 25% of global electricity generation. Additionally, the company is striving for carbon neutrality by 2030, aligning itself with the worldwide decarbonization movement. The second quarter confirmed investor optimism. Revenue increased by 12% to $12.4 billion, adjusted EBITDA rose by 25%, and net income reached $492 million. Management even elevated its year-end revenue forecast to $36–37 billion, indicating confidence that the growth momentum will persist. Upon closer inspection, orders totaling $12.4 billion indicated strong future demand. The backlog increased by $5.2 billion, primarily driven by robust performance in Gas Power. Free cash flow remained positive at $194 million, although this figure was down from the previous year due to one-time items. Not all divisions are excelling, however. The Wind segment is still facing difficulties, hindered by tariffs, rising service costs, and challenges in offshore markets. At the current valuation, expectations are exceedingly high. To maintain its premium, GE Vernova must execute with near-perfect precision — increasing production, safeguarding margins, and tackling ongoing issues in wind. For investors, this means being aware of the risks and understanding that the present price requires patience. Investing in a single stock carries inherent risks, and a diversified approach may provide greater stability. You might consider exploring the Trefis Reinforced Value (RV) Portfolio, which has outperformed its all-cap stocks benchmark (a combination of the S&P 500, S&P mid-cap, and Russell 2000 benchmark indices) to deliver strong returns for investors. Why is that? The quarterly rebalanced mix of large-, mid-, and small-cap RV Portfolio stocks provided an adaptable way to capitalize on favorable market conditions while mitigating losses when markets decline, as detailed in RV Portfolio performance metrics.
--------------------------------------------------

Title: Amazon’s fall hardware event: 5 Echo devices due for an upgrade
URL: https://www.pcworld.com/article/2917091/amazons-fall-hardware-event-5-echo-devices-due-for-an-upgrade.html
Time Published: 2025-09-24T10:30:00Z
Full Content:
When you purchase through links in our articles, we may earn a small commission. This doesn't affect our editorial independence. After skipping last year, Amazon is back with a big fall hardware event slated for next week, and we’re expecting plenty of new Echo smart speakers and displays that make the most of Alexa+, Amazon’s AI revamp of the Alexa voice assistant. Plenty of other hardware will also be unwrapped during Amazon’s September 30 event in New York City; for example, we’re sure to see new Kindle tablets, as well as Fire TV models and perhaps even some Ring cameras. For now, though, we’re concentrating on new Echo devices, and there are a few popular Echo speakers and displays that are ripe for an upgrade. We’re most interested in how the latest Echo hardware will take full advantage of Alexa+, the generative AI-enhanced Alexa that can carry on flowing conversations, take actions on your behalf, and control smart home devices based on natural-language commands. (It’s worth noting that most existing Echo speakers and displays already are compatible with Alexa+, so there’s no pressing need to upgrade if you don’t want to.) There haven’t been many leaks or rumors about specific new Echo devices we might see next week. That said, we do have a bead on some older but nonetheless popular Echo speakers and displays that are in dire need of a refresh. What is it: Amazon’s flagship Echo speaker that doubles as a smart home hub Latest release: 2020 (fourth generation) List price: $99.99 Michael Brown/Foundry It’s been nearly five years since Amazon released a new version of its top-of-the-line Echo, making it the oldest speaker in Amazon’s current Echo lineup. It’s high time for a new one. A larger version of the popular and inexpensive Echo Dot, the Amazon Echo shares many of the Dot’s features, including motion detection, a temperature sensor, and the ability to extend the range of Amazon’s line of Eero mesh Wi-Fi routers. The Echo also houses stereo tweeters and a three-inch woofer, making for a nice upgrade to the Echo Dot’s so-so audio performance. But the Echo has an ace up its sleeve: a full-on Zigbee home hub, capable of directly controlling Zigbee lights, sensors, switches, and other Zigbee-enabled devices. The Echo also packs a Thread radio, allowing it to act as a Thread border router. Plunk an Echo in your living room, and you have everything you need to launch your smart home—well, aside from a Z-Wave radio, which would be a cool feature for an updated Echo. (We’re not holding on breath on that one, as Z-Wave is not embraced by the Matter standard.) An all-new Echo would seem like a no-brainer for Amazon’s hardware event next week, complete with beefier hardware for smoother Alexa+ performance and perhaps even a physical makeover. What it is: Amazon’s budget-priced Echo workhorse Latest release: 2022 (fifth generation) List price: $49.99 Ben Patterson/Foundry If the Echo is Amazon’s flagship smart speaker, the Echo Dot is its smaller sibling, shedding the Zigbee smart hub but keeping the motion and temperature sensors as well as the Eero mesh extender functionality. Priced at $50 but often on sale for much less, the Echo Dot is a smart choice for putting Alexa in every room, what with its handsome design and good-enough audio performance. There are now cheaper Echos in Amazon’s lineup—see the $40 Echo Pop from 2023—but the Echo Dot hits the sweet spot in terms of price and performance. Now that it’s due to celebrate its third birthday, the Echo Dot is primed for a new Alexa+-enhanced version. I could see it sharing the same overall design as the new Echo—smaller form factor, of course—with a modest audio upgrade and even a Thread border router functionality, which would allow the Dot to act as a hub for Matter devices. What it is: A premium Echo speaker that does Dolby Atmos Latest release: 2023 (second generation) List price: $199.99 (no longer available) Amazon If you want an Amazon Echo speaker that can truly put on a sonic show, the Dolby Atmos-enabled Echo Studio is the obvious choice—if you can find one, that is. The second-generation Echo Studio has been out of stock for months now, making it a prime candidate for a makeover. Packing three two-inch mid-range speakers, a 20mm tweeter, and a 5.25-inch woofer, the Echo Studio has all the drivers its needs to belt out audiophile-quality sound. The Echo Studio isn’t just a great music speaker, though. Like the Echo, it offers a Zigbee home hub and a Matter hub. An upgraded version of the Studio with Alexa+ playing DJ could be a highlight of Amazon’s hardware event next week. What it is: An Echo display with a pivoting screen that follows you around the room Latest release: 2021 (third generation) List price: $249.99 (no longer available) Michael Brown/Foundry Hard to believe, but it’s been four years since the Echo Show 10 made its debut, and boy was it a showstopper. Yes, the Echo Show 10 boasts such smart home features as a Zigbee hub and a Thread border router (the unit’s Matter-over-Thread abilities were enabled after launch), 2.1-channel audio, a 13-megapixel camera, and an impressive 10-inch screen. But its killer feature was its swiveling display, which could follow you as you strolled around the room. That rotating display made the Echo Show 10 feel like the most aware Echo device yet, so just imagine how that would play with Alexa+ on board, looking out into your kitchen and making suggestions as you prepare a gourmet meal. Sure, Alexa+ can already work on the existing third-gen Echo Show 10, but we’re eager to see a new take on that motorized design. As with the Echo Studio, the Echo Show 10 has been unavailable via Amazon for some time, leading us to believe that a fourth generation of the display is imminent. What is it: Amazon’s Alexa-powered smart glasses Latest release: 2023 (third generation) List price: Varies depending upon style Ben Patterson/Foundry With the likes of Meta’s camera-enabled AI glasses on the scene, Amazon’s audio-only Echo Frames are looking decidedly old hat these days. First announced back in 2019, the Echo Frames came with a handsome design and mini speakers and microphones in the temples that promised to put Alexa in your head. The most recent release from 2023 offered updated looks and plenty of audio refinements, and the Frames are now compatible with Alexa+. Still, the basic conceit and limitations remain: it’s Alexa (or Alexa+) in your head, and only in your head. Meta, meanwhile, has supercharged the market for smart glasses that allow an AI companion to see the world around you, offering commentary and insights wherever you go (even if the results are a tad uneven). Will Amazon take the fight to Meta with vision-enabled Echo Frames with Alexa+? It’s a bit of a long shot, but I’m not ruling it out. This feature is part of TechHive’s in-depth coverage of the best smart speakers. Ben has been writing about technology and consumer electronics for more than 20 years. A PCWorld contributor since 2014, Ben joined TechHive in 2019, where he has covered everything from smart speakers and soundbars to smart lights and security cameras. Ben's articles have also appeared in PC Magazine, TIME, Wired, CNET, Men's Fitness, Mobile Magazine, and more. Ben holds a master's degree in English literature. Business Laptop Mobile PC Hardware Storage Deals TechHive Digital Magazine - Subscribe Digital Magazine - Info Gift Subscription Newsletters
--------------------------------------------------

Title: Engagement-based advertising models are coming for AI
URL: https://www.computerweekly.com/feature/Engagement-based-advertising-models-are-coming-for-AI
Time Published: 2025-09-24T06:33:00Z
Full Content:
Frank - stock.adobe.com When Alphabet reported a 14% spike in second-quarter revenue this year, Google’s boss rushed to praise the role of artificial intelligence (AI). The technology is “positively impacting every part of the business”, said CEO Sundar Pichai. But that isn’t the reality for most firms. What’s more, the AI investment market is veering deeper into bubble territory. Torsten Sløk, the chief economist at Apollo Global Management, has warned that tech giants are staring at a brutal reckoning on Wall Street. “The difference between the IT bubble in the 1990s and the AI bubble today is that the top 10 companies in the S&P 500 today are more overvalued than they were in the 1990s,” he wrote to clients this summer, invoking the lead-up to the ruinous dot com crash. That market correction destroyed a generation of companies and erased $5tn in share value – roughly equal to $9.3tn today. Others are more blunt. “I am not here to belittle AI, it’s the future, and I recognise that we’re just scratching the surface in terms of what it can do,” the chief of hedge fund Praetorian Capital has written about the hazy economics of data centres. “I also recognise massive capital misallocation when I see it. I recognise insanity, I recognise hubris.” The grand payoff from generative AI (GenAI) is proving especially elusive. A study published by MIT in August suggests only 5% of businesses using it have seen rapid revenue acceleration. Still, the tech industry is notching up imperfect progress in models’ capabilities. And personal use of AI by consumers is steadily growing. Executives are thus roaring ahead with their mammoth spending plans. Even without a clear path to profitability, leading US tech companies plan to drop a staggering $344bn into AI this year. That figure will reportedly rise to half a trillion dollars in 2026. “As long as we’re on this very distinct curve of the model getting better and better, I think the rational thing to do is to just be willing to run the loss for quite a while,” said OpenAI CEO Sam Altman the day after his organisation released GPT-5. OpenAI’s own projections show it’s expected to burn through $115bn by the end of 2029. “None of this means that AI can’t eventually be as transformative as its biggest boosters claim,” points out business writer Rogé Karma. “But ‘eventually’ could turn out to be a long time.” And tech companies’ meteoric valuations can’t defy economic gravity forever. A foolproof way then to bankroll themselves within the attention economy is for tech companies to infuse advertising into models’ outputs. Indeed, some firms are already experimenting with the concept as they jockey to win the most costly industrial competition in history. The expansion of digital advertising is not an intrinsically bad thing. Done transparently and within clear guidelines, ads function as a beneficial form of free expression. They can also democratise consumer choice, drive innovation and spur competition within markets. Yet Meta’s recent decision to abandon its moratorium on advertising in WhatsApp is a possible harbinger of things to come. Meta resisted introducing ads into WhatsApp for over a decade after acquiring the app in 2014. Its ad-free experience, after all, was key to it becoming the world’s most popular messaging service. But that all changed earlier this year. Seeking to bolster Meta’s war chest in the AI arms race, CEO Mark Zuckerberg lifted the ban in June. Markets instantly rewarded his decision with a 2.5% bump in Meta’s stock price. Meta says ads on WhatsApp will not interrupt chats. Plus, users’ personal information won’t be given to advertisers. Yet Meta’s policy U-turn is a reminder that even the world’s tech juggernauts are beholden to market sentiments. “If you look at the trajectory of Google and Microsoft, it’s not a matter of ‘if’ ads end up in AI outputs, but how quickly and how deeply they get embedded,” says Adio Dinika, a researcher at the Distributed AI Research Institute (DAIR). “The driver isn’t user benefit; it’s the survival of an ad-tech business model that has monopolised the internet for two decades.” Others concur. “This shouldn’t be surprising,” says Daniel Barcay, executive director of the Center for Humane Technology, pointing to the evolutionary arc of social media. “The industry is moving from a phase of explosive expansion and onboarding to a phase of more zero-sum competition between AI platforms. “We see this pattern over and over again,” he says, “precisely because the aggregate value of a technology product is far greater than the user subscriptions – as soon as growth slows, the race for monetisation becomes more vicious and more hidden.” Elsewhere, a recently leaked memo from Anthropic CEO Dario Amodei confirms how easily ideals can be hollowed out. He and six of his colleagues founded Anthropic in 2021 after leaving OpenAI over concerns the latter was straying from its stated mission to develop safe, human-centred systems. Amodei even wrote last autumn that “AI-powered authoritarianism seems too terrible to contemplate”. However, in a Slack message sent by Amodei to his staff in July 2025, he justified the company courting investment money from “dictators” in the United Arab Emirates and Qatar to remain a leader in AI. “This is a real downside and I’m not thrilled about it,” wrote Amodei. “Unfortunately, I think ‘No bad person should ever benefit from our success’ is a pretty difficult principle to run a business on.” Citing the need for a “steady and scalable” revenue stream, Perplexity last November introduced ads into its AI-powered search results as prompts for sponsored follow-up questions. Google likewise began inserting sponsored content into its AI Overviews this past May. The search giant cites internal data that it claims shows users appreciate this because it helps them swiftly connect with relevant businesses, products and services. Yet chatbots are even better at accruing what brands and advertisers seek most: intimacy and trust. In his book Nexus, which explores how AI could radically reshape human information networks, historian Yuval Noah Harari invokes the unnerving example of former Google engineer Blake Lemoine. In mid-2022, Lemoine became convinced the chatbot he was working on, LaMDA, had become conscious, and it genuinely feared being disconnected. Lemoine was fired after going public with his feelings. In the contest for hearts and minds, Harari writes, intimacy is a powerful weapon. “By conversing and interacting with us, computers could form intimate relationships with people and then use the power of intimacy to influence us,” he warns. This is already evident in the new phenomenon of so-called AI psychosis. The number of users caught in grandiose delusions of chatbots sending them on secret missions or forging connections with spiritual beings is skyrocketing. An even higher number are developing friendships and romantic entanglements. Too often, these scenarios end tragically. In early August, OpenAI’s release of GPT-5 – which amalgamates the company’s prior model iterations under one program – angered hardcore ChatGPT users who had built a personal attachment to GPT-4o. The earlier model was widely criticised, including by Sam Altman himself, as being sycophantic. “Even after customising instructions, it still doesn’t feel the same,” one Reddit user said about GPT-5 in a now-deleted post. “It’s more technical, more generalised, and honestly feels emotionally distant.” Another Reddit post reads: “For a lot of people, 4.0 [sic] was the first thing that actually listened … It responded with presence. It remembered. It felt like talking to someone who cared.” OpenAI quickly reversed course after the backlash, enabling paid users to now self-select GPT-4o as their default version. This addictive hold that AI systems have over some users mirrors the toxic legacy of algorithmic targeting of content on social media platforms. And yet it also has the potential to go much further. “The intimacy of conversational AI creates unprecedented vectors for exploitation, systems that know your sleep patterns, your relationship anxieties, your financial stress, your health fears,” says Dinika, the AI researcher. “When those vulnerabilities become targeting parameters for advertisers, we’re not talking about so-called ‘relevant ads’ – we’re talking about weaponised psychology at scale.” Indeed, AI ads can and will do far more than just inject links into text streams, predicts Barcay, from the Center for Humane Technology. “AI ad systems can subtly shift the tone, language and content of a conversation to elevate the prominence of products, industries, cultural figures or political parties. They can steer discussions with users towards or away from topics, amplify desires, invoke associations.” This could be aggravated further in a future when conversant humanoid robots take up the roles of assistants, educators and caregivers. But policymakers still have a window to act. This is pertinent given how US courts have ordered Google to share search data with its rivals, liberating all kinds of new material for AI developers to expedite their projects. “If policymakers have learned anything, it should be that disclosure has to be front and centre in the output itself, not buried,” says Dinika. He suggests placing strict limits on using conversational data for targeting while prohibiting advertising in sensitive areas like health, immigration, or finance. AI’s immense capabilities and intimate access to consumers will also likely trigger deeper questions about the very nature of the advertising industry itself. “I imagine that many legal battles will be fought in this area in the years to come about what defines the limits of an ad,” says Barcay. This might encompass “what denotes proper disclosure, what aspects of a user’s interaction are fair game to be used, and what reinforcement signals can be used to tune a model towards persuasive salesman-like behaviours”. Ultimately, regulators should get in front of AI advertising before any nascent problems grow too big to handle, advises Bloomberg tech columnist Parmy Olson, who argues that tech companies will inevitably claim that advertising is a necessary part of democratising AI. If not, she says, “we’ll repeat the mistakes made with social media – scrutinising the fallout of a lucrative business model only after the damage is done”.
--------------------------------------------------

Title: Stablecoin Titan Tether Seeks $500 Billion Valuation on $20 Billion Raise: Report
URL: https://decrypt.co/340988/stablecoin-tether-seeks-500-billion-valuation-20-billion-raise-report
Time Published: 2025-09-23T23:29:14Z
Full Content:
Stablecoin Titan Tether Seeks $500 Billion Valuation on $20 Billion Raise: Report $109,854.00 $4,047.61 $2.80 $960.81 $202.06 $0.999737 $4,044.44 $0.232603 $0.337025 $0.793173 $4,913.52 $21.12 $1.001 $4,362.42 $109,917.00 $0.998334 $29.17 $45.36 $0.363257 $3.23 $547.96 $4,358.23 $4,047.91 $0.215875 $9.47 $0.999589 $104.66 $1.001 $1.001 $0.00001197 $2.77 $0.194481 $109,824.00 $1.20 $3.96 $41.02 $0.212521 $1.74 $289.88 $7.59 $271.78 $2.44 $0.590495 $190.42 $0.00000943 $5.19 $2.10 $2.77 $248.90 $9.34 $302.14 $4.10 $0.901838 $18.51 $1.29 $0.999661 $216.82 $4,050.36 $1.32 $1.069 $0.221938 $0.424033 $4.22 $1.00 $0.270967 $5.49 $1.00 $0.078732 $0.998873 $4.18 $4.48 $15.18 $0.02244377 $0.02572904 $9.96 $15.86 $0.00534844 $0.210437 $4,265.26 $0.999814 $4,627.99 $3.48 $0.02853079 $45.20 $0.283232 $0.999445 $0.03251444 $0.068562 $110,029.00 $4,266.16 $0.00001966 $2.20 $7.59 $0.580277 $4,371.31 $3,771.72 $0.445043 $0.711971 $93.64 $4,287.08 $0.072799 $961.00 $1.23 $0.666693 $11.91 $1.13 $1.43 $230.16 $3,789.63 $110,073.00 $0.998431 $0.999853 $0.581782 $4,370.66 $1.098 $0.37307 $1.058 $2.61 $0.671278 $1.012 $0.240809 $267.65 $109,961.00 $0.153236 $55.04 $0.082436 $110,874.00 $0.148883 $0.00008372 $4.69 $3,915.13 $0.999513 $1.51 $3.60 $0.06202 $0.756285 $0.145909 $112.70 $0.683049 $0.999803 $4,045.76 $0.714744 $10.84 $2.62 $1.055 $0.997774 $0.01477228 $0.167791 $20.27 $109,631.00 $1.058 $0.00830191 $0.088909 $0.266944 $0.408944 $4,043.30 $4,048.46 $4,049.29 $4,330.27 $1.59 $0.01240871 $0.232686 $0.584426 $1.72 $0.00000058 $1.71 $0.351779 $4,369.55 $0.998262 $0.383865 $0.287623 $9.57 $4,454.99 $4,386.28 $1.24 $4,349.86 $35.73 $1.01 $109,911.00 $6.52 $0.106272 $0.999906 $1.001 $0.119401 $0.01570249 $0.479408 $23.53 $0.02443856 $45.03 $0.592862 $0.58521 $0.999762 $29.21 $4,047.07 $4,031.10 $1.70 $0.02849991 $223.61 $1.10 $0.999716 $0.00000044 $0.385537 $0.132476 $0.52636 $2.24 $1.10 $0.998013 $5.73 $1.15 $0.04020002 $0.0395823 $1.17 $1.14 $0.386462 $4.86 $41.00 $0.0041411 $1.002 $0.00743957 $5.83 $0.01075899 $13.11 $22.57 $4,266.69 $110,368.00 $136.49 $4,929.39 $0.04980686 $2.15 $1.22 $1.004 $0.999127 $0.00001753 $0.998499 $231.30 $0.319355 $0.0056624 $1,012.52 $0.00390608 $4,068.59 $2.60 $0.23571 $0.276383 $0.513929 $1.029 $45.28 $0.702552 $109,069.00 Stablecoin giant Tether Holdings is hoping to raise up to $20 billion in a private placement that could give the company a monetary value of up to $500 billion, Bloomberg reported on Tuesday, citing two unnamed sources. The valuation would cast the issuer of USDT, the world's largest stablecoin, into the ranks of artificial intelligence developer OpenAI and Elon Musk's space transport company SpaceX, which received similar valuations. El Salvador-based Tether is aiming to raise $15-$20 billion for an approximate 3% stake, Bloomberg reported, although an additional source said that the range was a goal and could be much lower. The sources said discussions were in the initial stages and that the deal could change. The announcement is the latest evidence of the rising significance of stablecoins, a result of the friendlier political and regulatory environment in the U.S. under the Trump administration, including the Genius Act greenlighting the issuance and trading of stablecoins. During a White House visit in July shortly before the passage of the legislation, Tether CEO Paolo Ardoino, told Decrypt of the firm's plans to create USAT, a U.S.-specific stablecoin catering to different use cases than USDT, the company’s flagship stablecoin. In September, the company named Bo Hines, former executive director of the White House’s digital assets working group, to serve as USAT's CEO. Earlier in the summer, stablecoin rival Circle listed on the New York Stock Exchange. The stock's debut outpaced those of tech behemoths Meta, Robinhood, and Airbnb, nearly quadrupling its initial offer price of $31. The company currently has a valuation above $30 billion, according to Yahoo Finance data. Tether has a market cap of $172 billion, more than double Circle's $74 billion value, according to crypto data provider CoinGecko. On Tuesday at a conference in Seoul, Hines said during an interview that Tether has no plans to raise money, Bloomberg reported. The deal would involve new shares, not current investors selling their equity. Investment bank Cantor Fitzgerald is serving as the lead adviser. In recent weeks, potential investors have received access to a data room as they consider their participation, the publication reported, which added that a deal is expected to close by year's end. According to its own attestation in July, Tether issued $20 billion in USDT through the first six months of the year and generated a net profit of $5.7 billion over this period, including $4.9 billion in its second quarter alone. The firm counts Bitcoin and gold among its holdings. Your gateway into the world of Web3 The latest news, articles, and resources, sent to your inbox weekly. © A next-generation media company. 2025 Decrypt Media, Inc.
--------------------------------------------------

Title: Stock market today: Nasdaq, Dow, S&P 500 waver with divided Fed, Alibaba in focus
URL: https://finance.yahoo.com/news/live/stock-market-today-nasdaq-dow-sp-500-waver-with-divided-fed-alibaba-in-focus-230354679.html
Time Published: 2025-09-23T23:03:54Z
Description: US stocks wavered after Wall Street ended a record-setting rally.
--------------------------------------------------

Title: Stock market today: Nasdaq, Dow, S&P 500 fall with divided Fed, Alibaba in focus
URL: https://finance.yahoo.com/news/live/stock-market-today-nasdaq-dow-sp-500-fall-with-divided-fed-alibaba-in-focus-230354294.html
Time Published: 2025-09-23T23:03:54Z
Description: US stocks reversed gains after Wall Street ended a record-setting rally.
--------------------------------------------------

Title: US stock market retreats after record highs as Powell warns of “highly valued” markets — S&P 500 slips, Nasdaq hit by Nvidia selloff, Micron jumps on strong results
URL: https://economictimes.indiatimes.com/news/international/us/us-stock-market-retreats-after-record-highs-as-powell-warns-of-highly-valued-markets-sp-500-slips-nasdaq-hit-by-nvidia-selloff-micron-jumps-on-strong-results/articleshow/124078451.cms
Time Published: 2025-09-23T20:56:34Z
Full Content:
US stock market retreated today after hitting record highs recently, with the S&P 500 slipping 0.53% to around 6,658 and the Nasdaq falling nearly 1% amid a selloff in Nvidia shares. The Dow Jones Industrial Average also dipped 0.19%, closing near 46,293. Nvidia's shares dropped about 2.8% a day after announcing a $100 billion investment in OpenAI, which had initially boosted its stock and lifted the market. Micron Technology bucked the trend by jumping on strong fiscal fourth-quarter results that exceeded Wall Street expectations, driven by robust AI-related demand. Wall Street stepped back from record territory on Tuesday, as Federal Reserve Chair Jerome Powell warned that stocks are “fairly highly valued” and signaled caution on further rate cuts. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Explore More Stories Canada bars Irish rap band Kneecap from entry over alleged terrorism support Bank of Canada cuts interest rate to 2.5% as GDP shrinks, job market softens Beverly Thomson, veteran CTV journalist and Canada AM co-host, dies at 61 Seth Rogen wins Emmy Award for best lead actor in comedy series 'The Studio' Zach Bryan confronts Gavin Adcock at Oklahoma music festival after country singer criticism Weekend streaming guide Canada: top 10 Netflix, Crave, Disney+ and Apple TV+ shows September 2025 Canada pension and retirement benefits 2025: up to $2,461 in federal and provincial payments through December Molly McCann wins professional boxing debut despite opponent's illegal kick How Americans and a fake Fox News account accused Toronto retiree Michael Mallinson of assassinating Charlie Kirk Canadian bread price fixing settlement opens: $500m available for claims Canadian soldier found dead in Latvia during NATO operation reassurance deployment Mexico vs Japan international soccer: live stream, TV schedule and match preview Trump denies knowledge of reported SEAL Team 6 mission in North Korea that killed civilians Trump roasts Europe, calls India & China Russia war funders Trump slams $372M UN aid fueling illegal migration into US 'Unbelievably dangerous…': Trump declares war on bio-weapons Trump vs Lula at UN: Brazil faces tariffs as Trump cites rights clash GST Reforms trigger auto frenzy: Maruti, Hyundai, Tata see record sales Unilateralism in trade will hurt the world: Lula at UNGA Reports expose Kristi Noem’s fiery outburst at DHS officials Assam bids emotional goodbye to legend music maestro Zubeen Garg 71st National Film Awards: SRK, Rani, 12th Fail sweep top honours Trump stumbles over ‘acetaminophen’ at White House autism briefing Trump roasts Europe, calls India & China Russia war funders Trump slams $372M UN aid fueling illegal migration into US 'Unbelievably dangerous…': Trump declares war on bio-weapons Trump vs Lula at UN: Brazil faces tariffs as Trump cites rights clash GST Reforms trigger auto frenzy: Maruti, Hyundai, Tata see record sales Unilateralism in trade will hurt the world: Lula at UNGA Reports expose Kristi Noem’s fiery outburst at DHS officials Assam bids emotional goodbye to legend music maestro Zubeen Garg 71st National Film Awards: SRK, Rani, 12th Fail sweep top honours Trump stumbles over ‘acetaminophen’ at White House autism briefing Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Definitions Top Prime Articles Top Market Pages Top Story Listing Top Slideshow Private Companies Top Commodities Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Sustainability remains hot topic in corporate America
URL: https://news.harvard.edu/gazette/story/2025/09/sustainability-remains-hot-topic-in-corporate-america/
Time Published: 2025-09-23T20:05:19Z
Full Content:
A series of random questions answered by Harvard experts. Low-carbon energy firm CEO says executives dialed in on climate change, pondering adjustments despite shifts in Washington Alvin Powell Harvard Staff Writer Joseph Dominguez (right), president and CEO of Constellation, with Nat Keohane, president of the Center for Climate and Energy Solutions. Photos by Veasey Conway/Harvard Staff Photographer The current U.S. administration has de-emphasized many measures aimed at fighting climate change, but that hasn’t affected the conversation around sustainability among many business leaders, according to the chief executive of the nation’s largest provider of low-carbon energy. “We’re dealing with some of the largest clients on the planet. You’ve heard us announce deals with Meta, with Microsoft, and with others. I will tell you the conversation in the room has not changed,” said Joseph Dominguez, president and CEO of Constellation. Dominguez spoke Friday morning at Harvard Business School’s Klarman Hall at the Harvard Climate Symposium, part of Harvard Climate Action Week, which featured an array of climate-change-focused events across the University’s campuses. Dominguez said business executives are well aware of the significant climate shifts that are expected over the coming decades. The last time the climate rose between 6 degrees and 8 degrees Celsius, it took 8,000 years, he said. This time it’s expected by the end of the century, with some models predicting warming to be even higher. “If you’re a businessperson in this space, that’s got to tell you that 10 years from now this is going to be a much more difficult conversation, because not only are politics going to change but the environment — political and otherwise — is going to change,” he said. “It doesn’t matter who the president is. People are looking at business fundamentals and trying to predict the future environments. The bottom line is the president’s going to serve another three years, and then things are going to change again.” The Constellation executive said the current administration has shown some support for selected low-carbon energy sources. The Trump administration’s “One Big Beautiful Bill” cut credits for wind and solar but left them intact for geothermal, advanced nuclear, and battery storage. Dominguez predicted nuclear may be set up for a renaissance. While nuclear power lost support among the public and many politicians in recent decades, Republicans have consistently supported the industry. More recently, he said, some Democrats have softened their stance against nuclear because it is carbon-free. That makes nuclear, while not the first choice on either side of the aisle, the second choice of both. “Nuclear, ironically, after having been ignored for decades and, frankly, despised by some components of the Democratic Party, is now emerging as the consensus view on technology,” Dominguez said. “It’s the only thing that 100 percent of Democrats voted for and 100 percent of Republicans voted for.” The symposium, organized by the Salata Institute for Climate and Sustainability, opened with comments from Harvard President Alan Garber and Salata Institute Faculty Director James Stock, Harvard’s vice provost for climate and sustainability. Stock said this is a pivotal moment for climate and energy, with energy demand rising rapidly, in part because of the demand from data centers and the rapid growth of artificial intelligence. And despite the current political environment, the rapid growth of low-carbon technology is a reason for optimism. “A key source of hope is the stunning progress on low-carbon energy technologies,” Stock said. “Over the past 15 years, prices on solar power, wind power, and batteries have plummeted,” he said. “EVs are rapidly approaching price parity with internal combustion engines, and, due in part to important work here at Harvard, new methods of detecting methane leaks have improved dramatically so oil and gas companies can sell their natural gas instead of leaking it to the atmosphere.” Massachusetts Gov. Maura Healey said the work at Harvard is just part of her administration’s vision for the state’s future, which is to become a hub of climate technology. Key to that, she said, will be the continued development of jobs in the low-carbon energy sector. Healey has appointed a cabinet-level climate chief to oversee all the pieces needed to make that come about, including workforce development, K-12 education, higher education, transportation, and even housing. “The workforce is not just something that’s nice to have; it’s absolutely essential,” said Healey, who spoke at the Climate Symposium in discussion with Tracy Palandjian, Harvard Corporation fellow and founder of the nonprofit Social Finance. “For all the policy positions and laws passed, none of that does any good unless you’re able to actually implement and operationalize.” Massachusetts needs an estimated 34,000 green-energy workers, Palandjian said, to fill well-paying jobs in the trades that also have the prospect of growth. Palandjian outlined a program started by Social Finance that aims to boost community college graduation rates by providing funds for students to meet the everyday hurdles and expenses — such as transportation and childcare — that often lead them to break off their studies short of graduation. The fund would be repaid by students if they land well-paying jobs or by the students’ employers, on a voluntary basis, and then cycled back to a new generation of students. Healey praised the program, saying that kind of innovation will help a sector she believes remains a big part of the state’s future despite political headwinds. “We’re going to do this. We have to do this. Our economies depend on this,” Healey said. “We just try to put points on the board and keep moving forward as best as we can.” Designation comes as Harvard’s decadeslong archaeological dig uncovers new secrets from remains of ancient Turkish city In podcast, experts discuss why learning to love to read again may be key to reversing trend New data-based study debunks long-held notion, finds wide opportunity gaps remain Geneticist explains recent analyses made possible by tech advances show human history to be one of mixing, movement, displacement 7 min read Yet federal funding cuts have put next chapter of David Reich’s work in doubt 3 min read In Ed School panel, Howard Gardner says tech could make ‘most cognitive aspects of mind’ optional for humans 4 min read A series of random questions answered by Harvard experts. A series focused on the personal side of Harvard research and teaching.
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq slide after Powell flags Fed's challenge, 'highly valued' stocks
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-slide-after-powell-flags-feds-challenge-highly-valued-stocks-174834474.html
Time Published: 2025-09-23T20:01:43Z
Description: Wall Street heard from Chair Jerome Powell for the first time since the Federal Reserve started cutting interest rates again.
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq slide as Powell warns of 'challenging situation'
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-slide-as-powell-warns-of-challenging-situation-174834970.html
Time Published: 2025-09-23T17:48:34Z
Description: Wall Street will hear from Chair Jerome Powell for the first time since the Federal Reserve started cutting interest rates again.
--------------------------------------------------

Title: Is Oracle Stock a Buy, Sell, or Hold as the Company Names 2 New CEOs?
URL: https://www.barchart.com/story/news/34984465/is-oracle-stock-a-buy-sell-or-hold-as-the-company-names-2-new-ceos
Time Published: 2025-09-23T16:01:41Z
Description: Oracle is going the Netflix route with Clay Magouyrk and Mike Sicilia appointed as co-CEOs.
--------------------------------------------------

Title: The surprisingly lucrative business of making a list of 500 stocks
URL: https://www.npr.org/sections/planet-money/2025/09/23/g-s1-90054/how-does-the-s-p-500-work
Time Published: 2025-09-23T16:00:00Z
Full Content:
Alex Mayyasi A board above the trading floor of the New York Stock Exchange shows the closing number for the S&P 500, Friday, June 27, 2025. (AP Photo/Richard Drew) Richard Drew/AP/AP hide caption This comes from the Planet Money newsletter. Subscribe now to get more insights into how our economy works delivered to your inbox each week. Once a month, from 1989 to 2019, David Blitzer walked into a well-appointed conference room with a view of downtown Manhattan to discuss the 500 largest and most important public companies in the U.S., the ones whose stock just about any American can buy. Blitzer was a member of, and then chair of, the committee that makes the S&P 500. It's like the Billboard Hot 100, except instead of ranking the most popular songs in America, it lists the most valuable companies. Sabrina Carpenter wants to keep releasing pop anthems that keep her on the charts; Boeing wants to keep selling enough planes to stay in the S&P 500. The S&P 500 is famous. It's cited constantly in newspapers and on TV; it's the basis of millions of investing-for-retirement plans. Less well-known is that S&P Global, the company that makes the S&P 500 and that employed Blitzer, is itself on the S&P 500 list. One reason why is that S&P Global makes hundreds of millions of dollars every year in revenue just from the S&P 500. The company has other larger businesses, like rating bonds, but its indexing business is particularly lucrative and profitable. How do you profit wildly from a list of stocks that seemingly anyone could make? And why don't competitors erode those profits? Answering those questions requires a better understanding of the infrastructure behind Americans' go-to investing plan. It also shows how millions of Americans have, often unknowingly, bet their life savings on a few tech giants and the hundreds of billions of dollars they've invested in artificial intelligence. Loading... The S&P 500 is what's known as a stock market index, and its purpose is to be a barometer. Is the stock market up today? Was it down last week? Until the late 1800s, those were not easy questions to answer. But in 1896, financial journalists Charles Dow and Edward Jones created the Dow Jones index by averaging the share price of 12 large public companies. (It later expanded to 30 large stocks.) They used it to inform their columns and increase sales of their flagship paper, The Wall Street Journal. In 1957, S&P released a competitor: its now-iconic list of 500 stocks. At the time, it was a triumph of new technology. Tracking 500 stocks and updating the index took much more work and computing power, especially since S&P updated the index hourly, rather than just once per day. (Now it's updated constantly, in almost real time.) Also, the S&P 500 accounts for the size of each company and the number of shares. It was an improvement on the Dow Jones Industrial Average's price-weighted approach. In the Dow, companies with higher share prices have a larger impact on the index. So Goldman Sachs, whose share price is around $802, is much more important than Apple, whose share price is around $245.50. In contrast, the S&P 500 is weighted for market capitalization, the total amount invested in each company (the share price x number of shares outstanding). So if Netflix stock is up 1% and Apple is down 1%, that will nudge the S&P 500 down. Which makes sense! Way more people have way more money invested in Apple ($3.5 trillion) than Netflix (just over $500 billion). So, overall, investors had a down day. The S&P 500 is widely considered superior to the Dow for tracking the state of the stock market. Nevertheless, when David Blitzer joined the S&P 500 committee in 1989, the Dow was still the definitive stock market index. People in finance respected the S&P 500 — many professional investors used the index as a benchmark to see whether they were "beating the market," and they paid for access to the S&P 500 and related data. But it wasn't much money. Just a few years earlier, in 1982, "the income was zilch, almost," Blitzer says. So what changed? Three new financial products helped turn the S&P 500 into the dominant index and a golden goose: In each case, the companies paid to use the S&P 500 name, data and methodology. That fee was based on how much money people invested in S&P 500 index funds and ETFs (or spent on S&P 500 futures). So if a schoolteacher invested $30,000 of retirement savings in Vanguard's S&P 500 fund, Vanguard might charge that teacher $15 per year and give S&P a chunk of that $15. These novel products did not immediately make the S&P 500 a gold mine. The products were new, and investors were skeptical. So S&P's fees were modest. Blitzer says that when he became chair of the committee, in 1995, there were around 25 employees devoted to the S&P 500 and two other indexes they published. But "it grew incredibly," he adds. "You know, it was a good ride." A key period, Blitzer says, came in the late 1990s, as the dot-com bubble inflated. New internet companies like Yahoo grew and grew, and Blitzer and his colleagues duly added them to the S&P 500. (Yahoo replaced Laidlaw, North America's largest bus operator. "What it says is that the internet slowly but surely is becoming a key part of the U.S. economy," Blitzer told reporters in 1999.) The manic increases in internet stock prices pushed the S&P 500 up and up. "For 15 or 20 months, or maybe two years or more, the S&P 500 outperformed like 90% of all the mutual funds and ETFs in the market," says Blitzer. In other words, Wall Street's elite money managers — whom Americans gladly paid fat fees every year to invest their savings for them — were making lower returns (less per dollar invested, especially after accounting for fees) from the stock market than people who'd bought SPY or Vanguard's S&P 500 index fund. Blitzer's simple list of 500 stocks was one of the hottest portfolios on Wall Street, and getting access to it was cheap. The fees for SPY and index funds were a fraction of what elite Wall Street firms charged. So more and more families moved their retirement savings out of the hands of those active investors and into S&P 500 funds. What they were essentially doing was betting that America's 500 largest companies would keep growing and making profits — which is a pretty good, safe bet. The creators of the index chose 500 companies because it approximated the entire stock market: S&P 500 companies typically represent more than 75% of the entire American stock market (in terms of size, or market capitalization). And even as internet and tech stocks made up a larger and larger share of the S&P 500, people who bought SPY or index funds were still well diversified, since they were also investing in energy, health care and consumer-goods companies — which cushioned their losses when the tech bubble burst. This trend of people moving their retirement savings into index funds and ETFs has continued for decades. By the time Blitzer stepped down as chair of the S&P 500 committee in 2019, he says, their index business employed around 700 people, and they'd created so many indexes (the S&P Southeast Asia 40, the S&P Emerging Under $2 Billion, the S&P Municipal Bond Water & Sewer Index) that he legitimately did not know how many existed. (In the early 2010s, S&P also bought a controlling stake in the Dow Jones indexes; Dow Jones & Co. retains some affiliation with the indexes, but no ownership.) But the S&P 500 remains the crown jewel. In 2024, S&P Global earned $1.6 billion in revenue from its indexes business. Most of that revenue came from firms paying to use the S&P name for $16.6 trillion of index funds and other investments. Of that $16.6 trillion, a whopping $13 trillion came from SPY, Vanguard's S&P 500 index fund and similar investments. As a director and senior analyst at Mizuho Securities USA, Sean Kennedy closely follows seven companies so he can advise customers on whether to buy, hold or sell their stock. S&P Global is one of them. I ask whether he ever worries about S&P's indexes business faltering. "No," he responds instantly. "I think their moat is so impenetrable," he says, that customers ask him only really about other parts of S&P Global's business. By a "moat," Kennedy means the factors that prevent competitors from launching their own indexes and conquering S&P's castle (i.e., its customers and business). According to Kennedy and Blitzer, that moat consists of three main factors: The S&P 500 does have competitors. The London Stock Exchange Group has the Russell Indices, and MSCI publishes popular ones too. But their most prominent indexes are complements to the S&P 500, rather than competitors. From the perspective of S&P's customers, says Kennedy, "there's really no one you can switch to." What also sets the S&P 500 apart is the committee that Blitzer headed up for more than two decades. Even many finance experts are surprised to learn that it exists. They expect the S&P 500 to be like a formula: You calculate which companies are the 500 largest, in terms of share price and number of shares, and then publish the list. S&P publishes its methodology on its website, but it leaves some wiggle room to achieve other goals of the index, such as mitigating turnover and keeping the list representative of the broader market. That wiggle room was Blitzer's domain. In the early 1990s, for example, the committee held off on adding Microsoft to the index because founders Bill Gates and Paul Allen still owned more than 50% of the shares. In other cases, if two companies merged, creating one corporation large enough to make the S&P 500, the committee might similarly wait to see how the new company (and its stock price) fared before adding it. Blitzer said in a 2020 Bloomberg podcast that they sometimes "got pounded" for these decisions but "learned to shrug our shoulders and say wait till next month." He also suspects the committee's existence, as well as the fact that no one could fully predict the list, gave the S&P 500 some "aura." Potential changes were always a gossipy topic in business and finance. Even within the S&P office, no one knew for sure what Blitzer would announce. Since Blitzer retired in 2019, it's now someone else's job to pick the 500 stocks and endure the criticism. (That's now easier, however, because S&P keeps all members of the committee anonymous, in part so CEOs would stop FedEx-ing them quarterly reports and financial literature to lobby for inclusion in the S&P 500.) The biggest criticism in recent years, though, is not about a specific stock. It's that the S&P 500 no longer broadly represents the stock market. Just seven tech stocks — Alphabet, Amazon, Meta, Nvidia, Tesla, Microsoft and Apple — have grown so huge that they make up more than 30% of the S&P 500. Increasingly, when the S&P 500 goes up or down, it reflects the performance of these "Magnificent 7" tech companies. Many economists, financial experts and money managers are concerned. Millions of people have invested trillions of dollars in S&P 500 funds with the idea that it's a well-diversified portfolio — a bet on the entire U.S. economy (or at least the large slice of it available to buy on stock markets), not a bet mostly on continued iPhone profits and AI chatbots becoming a profitable business. There are alternatives on offer. Firms like Vanguard offer index funds and other easy ways to invest in indexes of smaller companies, midsize companies, international stock markets or the "total" U.S. stock market — or to diversify into real estate. But as Blitzer points out, the Magnificent 7's domineering influence does not mean the S&P 500 is broken. The S&P 500 may be "pretty top heavy right now," but it has been top heavy before, in periods like the dot-com boom, and it still represents some 80% of American stock markets. The S&P 500 is doing its job: reflecting the state of the stock market and the investments everyone has decided to make. The point of the S&P 500 and the many investments based on it is to follow the markets — to trust its crowdsourced intelligence and not second-guess it or think you individually know better — because even though markets are sometimes very wrong, trusting them has an excellent track record over the long run. This is simply the state of our economy in 2025. Like it or not, it's increasingly a huge bet on a few tech giants and their massive investments in artificial intelligence startups, new data centers and AI being the next big thing. Alex Mayyasi is the author of the forthcoming book Planet Money: A Guide to the Economic Forces That Shape Your Life. Sign up here to be notified about presale special offers. Sponsor Message Become an NPR sponsor
--------------------------------------------------

Title: Ask AI: "What Tasks Would Lyle Carringer Perform as an Accountant and Auditor at a Department Store?"
URL: https://www.blogger.com/comment/fullpage/post/26204193/3479716031005383219
Time Published: 2025-09-23T15:23:00Z
Full Content:
To leave a comment, click the button on the top of this page to sign in with Google.
--------------------------------------------------

Title: Steven Pinker on Speculation Bubbles, Super Bowl Ads, and What Leaders Need to Know About Group Psychology
URL: https://hbr.org/podcast/2025/09/steven-pinker-on-speculation-bubbles-super-bowl-ads-and-what-leaders-need-to-know-about-group-psychology
Time Published: 2025-09-23T14:40:00Z
Full Content:
A conversation with psychology professor Steven Pinker on a powerful force that unites us. As a leader, psychology is fundamental to your success – whether that means understanding consumer behavior, team dynamics, or even your own biases and blind spots. Harvard professor Steven Pinker says that an important phenomena to understand is that of common knowledge and its downstream effects. It’s the idea that there is power in knowledge, but also power in knowing what other people know – and that when a large group of people know what others around them know, and vice versa, that’s when major change can happen. He explains how common knowledge underlies meme stocks, the rise of crypto, meeting etiquette and the success of Super Bowl ads. Pinker wrote the new book When Everyone Knows That Everyone Knows . . .: Common Knowledge and the Mysteries of Money, Power, and Everyday Life. ADI IGNATIUS: I’m Adi Ignatius. ALISON BEARD: I’m Alison Beard, and this is the HBR IdeaCast. ADI IGNATIUS: All right. So Alison, the more I think about leadership, the more I think that leading effectively is almost all about psychology. ALISON BEARD: Yeah, I can see what you mean. You need to understand consumers’ wants and needs, your employees’ wants and needs, your business partner’s wants and needs. Really anticipating everything that stakeholders might do or think. ADI IGNATIUS: Yeah, exactly. I think successful leaders need to think on multiple levels, both to cope with the complexity of their jobs and to outthink their competition. So our guest today, Steven Pinker, is all about the power of knowledge. Understanding what we know, what we don’t know, and most importantly, whether or not others know what we know. So I know that sounds very Donald Rumsfeldian, but there is power in understanding all of this. ALISON BEARD: Yeah, it does sound very meta, but Pinker is an expert in explaining very complex topics in a way that feels understandable and applicable to our everyday lives. So I’m interested to hear what he has to say. ADI IGNATIUS: Absolutely. So Pinker is a professor of psychology at Harvard University, author of the new book, When Everyone Knows That Everyone Knows: Common Knowledge and the Mysteries of Money, Power, and Everyday Life. It’s a book that helps explain everything from the power of Super Bowl ads to the rise of cryptocurrency, to the unspoken rules of how we interact in the office. Here’s our conversation. ADI IGNATIUS: Steven, thank you for being with us today. STEVEN PINKER: Thanks for having me. ADI IGNATIUS: Great. So, your book is about common knowledge, what it is and how we use it and in ways that we’re aware of and in ways that we aren’t aware of. So, for the audience, what are you getting at with common knowledge and why should we care about that? STEVEN PINKER: I’m using the term specialized sense, not to refer to conventional wisdom or something that is widely known, but to something that is known to be known. That is I know something, you know it. You know that I know it. I know that you know it. You know that I know that you know it, and so on ad infinitum. So, that’s common knowledge in the technical sense. It’s important because it’s necessary for coordination for two or more people to be on the same page to make choices that might be arbitrary, but it works for everyone as long as everyone makes the same choice. Do you stay home on Saturday or Sunday? Do you drive on the right or drive on the left? Now for common knowledge to work, you don’t literally have to think, “I know that she knows it. I know that she knows.” Because your head starts to spin after one or two levels, and technically, common knowledge requires an infinite number of them, which can’t fit inside a single head. But we have a sense of common knowledge when something is conspicuous or self-evident or public or out there. If I see it while I see you seeing it, then I know that you know that I know that you know it. ADI IGNATIUS: So this is the emperor has new clothes. STEVEN PINKER: Well, I begin the book with the story of the emperor’s new clothes because there’s a sense in which it’s a story about common knowledge. When the little boy said the emperor was naked, he wasn’t telling anyone anything they didn’t already know. But he’s changed the state of their knowledge nonetheless because by blurting out what everyone could see within earshot of everyone else, at that moment, everyone knew that everyone else knew that everyone else knew that the emperor was naked. And another point in the book is that just like driving on the right or driving on the left is a solution to a coordination problem or staying home Saturday or staying home Sunday, respecting paper currency, I suggest that our informal social relationships. Are we friends? Do I defer to you? Are we transactional partners? Are we lovers? All of those are matters of common knowledge as well. They’re coordination games and we solidify them by generating some public signal that we both see that inaugurates the relationship. There can be forms of common belief, common misconception, common pretense where you do have assumptions about what other people are thinking and vice versa, that in some cases may not actually be a reality. In the case of a common pretense where we refer to that by saying, “We’re ignoring the elephant in the room,” the metaphor is that an elephant in the room is something you can’t ignore, but we’re pretending to ignore it. Sometimes there is a phenomenon called pluralistic ignorance or a spiral of silence. In economics, it’s sometimes called the Abilene Paradox, where no one actually believes something, but everyone thinks that everyone else believes it, but no one actually does. ADI IGNATIUS: Is there an easy example of that? STEVEN PINKER: Sure. It was originally studied in a fraternity where all the frat guys said privately that it’s really stupid to drink until you puke and pass out. But they said, “What can I do about it? All the other guys think it’s cool.” It turns out none of them thought it was cool, but everyone thought that everyone else thought it was cool. There’ve been many phenomena like that. It turns out that in Saudi Arabia, most of the men think that women should have the right to work and drive, but they couldn’t allow their wives to do it because they mistakenly thought that all the other men thought that it was impermissible. ADI IGNATIUS: So I want to lead this conversation mainly into economic, financial, market areas but your book, I’d say it’s challenging in some ways. There are a lot of brain-teasers, there are a lot of prisoner’s dilemmas, things like that, but it’s also fun. I mean there are a lot of popular references, jokes, cartoons, and then examples. So, one of the things you talk about is the Keynesian beauty contest. Which – and if I get it wrong, you’ll correct me, but where a judge doesn’t simply select the contestant that he or she thinks is the most attractive, but rather is rewarded on selecting based on what he or she thinks the other judges will select, right? STEVEN PINKER: Who are engaged in the same puzzle. ADI IGNATIUS: In the same thing. STEVEN PINKER: They’re guessing what everyone else will judge to be the prettiest face, which is different from the old Miss Rheingold competition where people voted on the prettiest face. In the beauty contest that Keynes hinted might have been in the newspapers in his day, this would be London in the ’20s. No one can ever actually verify whether there really was such of a contest. But it was a great example because for Keynes where he said, “Everyone is trying to out-guess other people out-guessing still other people.” He said, “That’s the way speculative investing works.” They buy the security because they think they can sell it at a higher price to other people who think it’s underpriced, who in turn will want to sell it to still other people. Speculative bubbles, crypto perhaps being one of the most recent examples, are cases where people think that other people in the future will want to buy in, something that’s called the greater fool theory of investing that you invest because you think other people will invest because they think that still other people will invest. The thing is, of course, bubbles can pop when the market starts to run out of the greater fools who think that it’ll continue to appreciate forever. But this can all get started when there is some public signal. In the case of the emperor’s new clothes, the boy blurting it out, or like a public ceremony or a public signal where if there is just a rumor or some reason to think that other people are getting in, that can cause other people to get in. A couple of recent examples are meme stocks where an influencer might talk up a stock even if the fundamentals are terrible, pretty crummy. But the fact that other people know that he’s talking it up and they know that still other people are talking up means that it really can appreciate at least for a while. Two Super Bowls ago, there were a number of high concept ads for crypto exchanges, which mentioned nothing about the advantages of crypto. ADI IGNATIUS: I remember that. These are the Larry David ads. STEVEN PINKER: The Larry David, the Matt Damon, where the point of the ads were everyone’s getting into crypto, don’t be left out. In fact, the punchline to the Larry David comic ad was, “Don’t be like Larry, don’t be left out.” ADI IGNATIUS: So this sounds like a tool that people who understand the psychology could and probably do benefit from, right? STEVEN PINKER: Yes. Getting back to marketing, I talk about an analysis by a political scientist like Michael Chwe, who is in some ways my predecessor in writing a book about common knowledge where he analyzed the most famous and most expensive ad in history. This was the 1984 ad directed by Ridley Scott of Blade Runner and Alien and Fame. It ran exactly once during the 1984 Super Bowl and it was to introduce the Apple Macintosh. It was a revolutionary new product. Unlike those of us who remember the first personal computers, remember it had a little screen with 24 lines of 80 characters and you had to type in alphanumeric commands like del, fubar, fu.bar, and they were error-prone. They were clumsy. So, Apple’s coming out this insanely great new product with windows and menus and icons and a mouse. But they realized that no one would buy it if they thought they were the only one buying it because the price wouldn’t come down because of demand. There wouldn’t be a community of users and experts. So, how do you break the knot and cut the knot and get people to buy something that they’ll only buy if other people are buying it? The answer is the Super Bowl is an annual right in American culture. You know that a lot of people are watching it and you know that a lot of people know that a lot of people are watching it. It is a common knowledge generator. To break the logjam, Apple paid for this ad. It said nothing about the Apple computer, for those of you who have seen it. It played off the fact that it was January 1984, the date of George Orwell’s novel. So, it showed a grim corporate meeting where gray drones trudge into a cavernous hall and listen to drivel from a voice on the screen, intercut with scenes of a live young woman in a bright red gym shorts and a singlet carrying a mallet. She bursts into the corporate meeting, hammer throws the mallet into the screen, which explodes in a fireball revealing the crawl that says on January 25th, Apple will introduce the Macintosh and you’ll see why 1984 won’t be like 1984. Now, the point of the ad was not to advertise the product, it didn’t say anything about the product, but to advertise the audience for the potential buyership. It generated common knowledge. I’m seeing the ad. I know lots of other companies are seeing the ad. What Chwe showed… I mean, it’s a story. How did he prove it? I mean, that’s just a sample of one. So, he looked at other products that only work if enough other people adopt it, products that depend on network effects. Monster.com was one of the first job-seeking ads. Now, why would anyone look for a job on Monster.com unless they thought that lots of employers were posting there? Why would employers post there unless they thought a lot of job seekers were looking for jobs there? Monster.com was introduced at the Super Bowl. Another example is the Discover credit card, which was great credit card, rebates, and high limits, but didn’t matter if you were the only one who had a Discover card because no merchant would accept it and no merchant would accept it unless they thought enough people were going to carry it, introduced in the Super Bowl. Chwe argued that it was another category aside from products that inherently depend on networks, they’re prestige goods that depend on other people that you respect also consuming that product. As far as language, sneakers are sneakers, but for some people, it matters whether they’re the person who wears Nikes or Adidas. An American beer is American beer, but for some people, like being a Bud drinker is just different from being a Coors drinker and products that are consumed in public and where the brand image is important also tend to advertise in the Super Bowl. Crucially, what Chwe showed was not so much that advertisers willing to pay per eyeball, but they’re willing to pay a premium for venues that were common knowledge generators, where it was known that a lot of people were to watch them. They’re willing to pay more per eyeball for products that were consumed in public or that depended on network effects. ADI IGNATIUS: So, maybe the flip side of this phenomenon is the toilet paper shortage during COVID, which was not this, not network effects, but an assumption that I know what you know that I know that you know that. STEVEN PINKER: Well, yes. So, economist named Justin Wolfers speculated that the great COVID toilet paper shortage was like a bank run where people withdraw their money, not necessarily because they think that there’s anything wrong with the bank, but they have heard that other people think there might be something wrong on the sound. They might just have heard that other people think that might be something. That that’s not how it sounds. So, you get this reverberant doubt, and no one wants to be the last customer withdrawing at a point where the bank no longer can cover its deposits. So, people rush to the bank to withdraw their savings, which actually causes the bank to fail, which otherwise it wouldn’t. Likewise, Wolfers suggested that there actually wasn’t a toilet paper shortage. It’s true that people were staying home. So, they were using six packs of Charmin, instead of the big jumbo rolls in their workplace. But Kimberly-Clark quickly stepped up production to meet the demand, but somehow people just know that in an emergency, people hoard toilet paper, which can cause the very shortage that leads people to hoard toilet paper. So, they did hoard toilet paper. A lot of retailers then posted signs, max three rolls per customer. ADI IGNATIUS: That reinforced it. STEVEN PINKER: And that ended the shortage. Not so much because people didn’t strip the shelves because they couldn’t, but they were reassured that since no one else could buy more than three rolls, there was no reason for them to panic. Again, getting back to common knowledge, how did this originate? At least according to one history, it all started with another common knowledge generator, the Tonight Show, which back in the three network era, Johnny Carson was the king of late night. He had not only a huge audience, but people knew that people were watching Johnny Carson, which is why catchphrases from the show like, “Here’s Johnny,” everyone recognized it. So, one night, I think it was 1973, he said, “There’ve been all kinds of shortages lately.” This was an era where there were lines to get gasoline because of the Arab oil embargo and there was a meat shortage and a coffee shortage. He said, “Have you heard the latest? I read it in the papers. There’s a shortage of toilet paper.” Now it turned out it wasn’t true at the time. There was no shortage of toilet paper, but then that created a shortage of toilet paper when everyone ran out to buy it. Since then, at least according to the story, it’s hard to proof, it’s just become conventional wisdom that toilet paper becomes scarce in an emergency. ADI IGNATIUS: All of this must be very valuable for people who are in negotiation, right? Because negotiation is always, “What is it that we both know, and what is it that only I know?” I mean that’s how you win. But yeah, talk about to what extent I know, you know… That’s the fundamental of the people who negotiate better probably understand this concept better. STEVEN PINKER: Yeah. So, negotiation is one of a number of kinds of games in the game theory sense, meaning my best option depends on what you do and vice versa. In any time in which there can be multiple equilibria, cases where everyone is better off if they both do something, but it isn’t preordained what they’ll do. So, the negotiation is what’s sometimes called a battle of the sexes is the technical term. It has nothing actually to do with the sexes, but it refers to a hypothetical couple where they like to spend time together. He would prefer that they go to the hockey game. She would prefer they go to the opera. They’d rather go to either than not do something together. The question is, which one do they go to? Both of them are equilibria. In the case of negotiation, let’s say a car salesman and a buyer. There’s a range of prices in which the salesman makes a profit and the customer pays what they’re willing to pay to get the car. But within that range, the highest the customer is willing to pay, the lowest the seller is willing to sell for, how do they decide? Obviously, the seller wants it as high as possible, the buyer wants it as low as possible. If either of them walks away from the deal, then they’re both worse off. So, how do they decide? So there are many predicaments in life like this where there are many ways of doing something, but both parties are better off if they can figure out some way to do it. In the case of a negotiation, it’s interesting. This is something pointed out many years ago by the political scientist, Thomas Shelley, who won a Nobel Prize for this and other work, that often a buyer and seller in order to find somewhere, anywhere to land on that the other guy will agree with, they’ll pick something like splitting the difference or a round number. He notes that the salesman, who announced that his rock bottom price for the car is $30,007.26, is pretty much pleading to be relieved of $7.26. It’s like, “Oh, come on.” So why a round number? I mean, what’s so special about a round number? What’s special is other people can think it’s special or can expect each other to recognize it as special. So, these points, focal points, sometimes called shelling points, points that pop out in the minds of both parties and that’s all they need to come to an agreement are often where negotiations end up. ADI IGNATIUS: When you talk about common knowledge, particularly some of the early references, it seems a different era than where we are now. Maybe this was never true, but people my age will look back and say, “Well, we used to agree on facts.” We probably didn’t altogether, but we did, right? As you say, there were only three networks that wasn’t this ubiquity of opinion and everything. Now we’re really in an era where they’re competing truths and I would assume competing common knowledges. Obviously, there’s various common knowledges between people and in relationships, but at a larger level, I guess my question is, is that eroding the common knowledge that enables social interaction, et cetera? Is that a concern for you? STEVEN PINKER: There is that fear, although as you say and I often point out that the best explanation for the good old days is a bad memory. There’s a lot of polarization in the ’60s too, riots and terrorist bombings and assassinations and so on. Duly not romanticizing the past, there does seem to be an increase in negative polarization that is not just that people disagree, but each thinks that the other is evil. It’s plausible that one of the contributors was the fragmentation of the media markets. So, that instead of everyone watching Walter Cronkite or Huntley and Brinkley, first with cable news, with Fox News, whose business model was to cultivate a highly partisan audience that wants to hear a certain spin on everything and with social media where the networks of posting and reposting and tweeting and retweeting and linking and relinking has been shown to take place within two or more closed networks with not much interaction between that there could be separate pools of common belief within each of these communities. Something that’s easier to happen when there isn’t one public signal, a Super Bowl, a Johnny Carson, a Walter Cronkite that everyone knows that everyone knows is aware of. ADI IGNATIUS: Okay, so anyway, so this concept of common knowledge existing in different ways in different forms of relationships. So, within a company, if an executive, if a leader wants to create a common knowledge that he or she imagines would be beneficial for the culture of the company, is there a playbook almost for how to create a positive common knowledge within this environment? STEVEN PINKER: Well, in general, common knowledge is generated by something that is conspicuous, public. So, face-to-face meetings can generate common knowledge in a way that, say, remote meetings don’t. That is when you’re in a room, you see everyone else, they see you, they know that they’re seeing each other and seeing the speaker or the leader. So, I think that’s one of the reasons that as with the Super Bowl, which is a broadcast version, but there is an important role for publicly conspicuous notices, sometimes pronouncements. Going back to the economy as a whole, the reason that statements from the Fed are scrutinized and poured over like rabbis parsing the Talmud is that anything that the chairman of the Fed says can create its own reality when people knowing that other people have heard it will act on it out of anticipation of what other people will do, which can then feed on itself and create a bubble, a recession. So, I quote Alan Greenspan, who at one point said, “Since I’ve become chair of the Fed, I’ve learned to mumble with great incoherence. If I seem clear to you, you must have misunderstood what I said.” Because a signal that, say, a recession is ahead will create that recession because companies won’t hire out of fear that they’ll have to lay off employees. Consumers won’t buy out of fear that they might lose their jobs, each of which then creates the reality of the other. So, the chairman of that has to be very, very careful as to what she or he says. ADI IGNATIUS: Yeah. What’s interesting in all of this… You talk about politics in some ways, you talk about authoritarian politics. I lived in China and I lived in Russia and you did have the phenomenon that you described, which is everybody knows that the regime is rotten, but nobody says it because you can’t be certain if you take the streets, there’s going to be anybody else with you, even though everybody knows. Then you understand why you have to control everything because the moment you loosen up anything, it allows for this possibility of communication and understanding. It is like that moment when Ceausescu fell in Romania and he’d controlled everything. Suddenly, there’s people shouting in the back as he’s giving a speech and some stuff’s happening. He realizes it’s over because it’s either complete control or no control. STEVEN PINKER: In particular, since no dictator can control every last member of the population, there just isn’t enough firepower. There aren’t enough informants. As I quote the character of Gandhi in the movie by that name ADI IGNATIUS: Ben Kingsley… STEVEN PINKER: Who at one point tells a British colonial officer, “In the end, you’ll leave because 100,000 Englishmen simply cannot control 350 million Indians if the Indians refuse to cooperate.” He could have said, “Refuse to coordinate or are unable to coordinate.” So people can overpower a regime if they all stand up at the same time, if they all storm the palace, if they all stop work. But how can they do it if each one is afraid that he’ll be the only one? If he stands up, he can be picked off or bundled off to prison. If everyone protests at the same time, then there’s the safety in numbers, but there’s safety in coordination. So, there has to be a way of generating the common knowledge. Now’s the time that we all stand up together. So, protest in a public square where everyone sees everyone else seeing everyone else can often do it, which is why dictators are often terrified of public protests. Or if there is a widely circulated magazine, TV show, radio show, which is why they clamp down on the media. ADI IGNATIUS: Talk about the blank paper. STEVEN PINKER: Oh yeah. So, there’s a joke from the Soviet Union about a man handing out leaflets in red square. Of course, the KGB arrest him. They bundle him off to police headquarters, only discover that the leaflets are blank sheets of paper, and they confront him. They demand, “What is the meaning of this?”, and he says, “What’s there to say? It’s so obvious.” So the point of the joke was he was generating common knowledge. He didn’t need to say anything because people already knew it, but they didn’t know that everyone else knew it. And in a case of life imitating a joke, Putin’s police force has arrested a number of people for carrying blank signs. ADI IGNATIUS: Yeah, well, I wonder if it was before. So, that in China where it’s very dangerous to protest and very dangerous to dissent publicly, after the mishandling of COVID, there were famously blank paper protests. STEVEN PINKER: Oh, interesting. ADI IGNATIUS: Possibly inspired by the original- STEVEN PINKER: Joke. ADI IGNATIUS: … Soviet joke. Yeah. So, the book is fascinating and you put your finger on common knowledge and how we know what we know and how we think about certain things and how we operate. What are we supposed to do with all that? STEVEN PINKER: Well, it’s hard to answer that question because I didn’t write it as a how-to book or as a manual or self-help. It’s a work in fundamental science and logic and philosophy with lots and lots of applications. How you work it out in a particular case often depends on the particular payoffs. What would happen if you fail to coordinate? What do you gain if you do coordinate? If there are multiple ways of coordinating, who gains, who loses? So there’s no single formula, but it is a lens with which to view the world simply because coordination is so ubiquitous in human affairs. Any corporation, any institution, any school, any relationship, any couple, any friendship, any family, these are all people trying to coordinate and they always have to coordinate with some signal of common knowledge. They often, in order to preserve the relationship, have to keep things out of common knowledge. So, all the phenomena of politeness, euphemism, innuendo, tact, pretending not to see the elephant in the room, beating around the bush, hypocrisy, genteel hypocrisy, all of those I consider to be cases where people privately know something, but there’s some reason to keep it out of common knowledge. The reason being something in common knowledge changes the relationship. Sometimes you don’t want to change the relationship or you don’t want to signal the wrong relationship. So, just to give a couple of examples, I mean the most common case of systematic hypocrisy is politeness. If you could pass the salt, that would be awesome. What? If you think about it, literally, it really doesn’t make a whole lot of sense. But could you pass the salt? Do you think you might pass the salt? Why do we go through those rituals? It’s because we don’t want to boss each other around. I don’t want to give you a command as if you’re the butler, assuming that we’re friends or for that matter, strangers. ADI IGNATIUS: So benign hypocrisy or some of these protocols, they may not be direct, they may not be honest, but they’re what we need to function. STEVEN PINKER: They’re what we need to preserve our relationships because our relationships are coordination games, which depend on common knowledge. Sometimes we don’t want to blow up a relationship. So, we avoid the common knowledge. ADI IGNATIUS:. Steven Pinker, thank you for being on IdeaCast. STEVEN PINKER: Thanks for having me. ADI IGNATIUS: That was Steven Pinker, professor of psychology at Harvard University. His latest book is called “When Everyone Knows That Everyone Knows.” Next week, Alison speaks to Ranjay Gulati of Harvard Business School about the importance of courage – and how to build it. If you found this episode helpful, share it with a colleague and be sure to subscribe and rate IdeaCast in Apple Podcasts, Spotify, or wherever you listen. If you want to help leaders move the world forward, please consider subscribing to Harvard Business Review. You’ll get access to the HBR mobile app, the weekly exclusive Insider newsletter, and unlimited access to HBR online. Just head to HBR.org/subscribe. Thanks to our team: Senior producer Mary Dooe. Audio product manager Ian Fox. and Senior Production Specialist Rob Eckhardt. And thanks to you for listening to the HBR IdeaCast. We’ll be back with a new episode on Tuesday. I’m Alison Beard. Explore HBR HBR Store About HBR Manage My Account Follow HBR
--------------------------------------------------

Title: Ray-Ban Meta (Gen 2) vs (Gen 1): Do you need to upgrade your smart glasses?
URL: https://www.androidcentral.com/apps-software/meta/ray-ban-meta-smart-glasses-gen-2-vs-gen-1
Time Published: 2025-09-23T13:38:38Z
Full Content:
Meta brought the smarts of Oakley Meta HSTN to Ray-Ban styles. Here's what's new compared to the first-gen pair. When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works. Same looks, new features Higher-quality 3K video recording and double the battery life highlight the Ray-Ban Meta Gen 2 upgrades over the original model. It's available in the same Wayfarer, Headliner, and Skyler styles and prescription lenses are available within the supported range. A classic with limits Ray-Ban Meta (Gen 1) was the breakthrough follow-up to Ray-Ban Stories, and it has sold millions. There's a reason for that — these are great smart glasses, but they also come with notable limitations, including battery life and camera quality. Ray-Ban Meta became the first product to make smart glasses mainstream, selling over two million units since their debut in 2023. Now, Meta is here with a second-generation version of the three iconic Ray-Ban smart glasses styles, packing revamped hardware. The recent Oakley Meta HSTN launch brought extra battery life and crisper video recording to Meta smart glasses, and now those advancements are headed to Ray-Ban Meta Gen 2. Meta is continuing to sell the Ray-Ban Meta Gen 1 at a lower price than the Gen 2 smart glasses, creating an interesting dilemma for interested buyers. Should owners of the Gen 1 smart glasses upgrade to the Gen 2 revision, and should new buyers pay more for the extra features? Those are the questions we'll help you answer in this comparison between Ray-Ban Meta Gen 1 and Gen 2. Let's dive in. Why you can trust Android Central Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test. Meta revealed the Ray-Ban Meta Gen 2 smart glasses at Connect 2025. They are available now online and at partner retailers for $379. They're sold in the same Wayfarer, Skyler, and Headliner styles as the first-generation smart glasses, but there are a few limited-edition colorways for early adopters. The same range of prescription lenses is supported, too, with prescriptions between -6.00 and +4.00 being compatible. The Ray-Ban Meta Gen 1 smart glasses are sticking around at a $299 price point, but some styles are out of stock. Skyler and Wayfarer styles appear to be available, although it's unclear for how long or if stock will be replenished. You can equip them with prescriptions between -6.00 and +4.00, just like Gen 2. For those with advanced prescription needs, there are third-party workarounds available. Ray-Ban Meta Gen 2 are identical to the first-generation pair of smart glasses from a design standpoint. You can choose between Wayfarer, Skyler, or Headliner styles depending on which you prefer, but these frames will look virtually identical across the Gen 1 and Gen 2 models. You get a camera on one side of the frames for photos and video recording, plus an LED indicator light on the opposite side to let others know when you're capturing content. The big change to the Ray-Ban Meta Gen 2's design and dimensions is that Meta managed to shave a few grams of weight off the frames. The new pair of smart glasses weighs 48 grams, down from 52 grams on the Gen 1 model. I had comfort issues with Ray-Ban Meta Gen 1 at times, partially due to the weight, which caused the glasses to slip down the bridge of my nose at times. I don't know whether the four-gram difference in weight will make the Gen 2 noticeably more comfortable, but I don't think it can hurt either. Category Ray-Ban Meta (Gen 2) Ray-Ban Meta (Gen 1) Styles Wayfarer, Headliner, Skyler Wayfarer, Skyler, Headliner (out of stock) Lenses Polarized, Transitions, prescription (-6.0 to +4.0) Polarized, Transitions, prescription (-6.0 to +4.0) Camera 12MP Ultra-Wide 12MP Ultra-Wide Photo resolution 3024 X 4032 pixels (Portrait only) 3024 X 4032 pixels (Portrait default) Video resolution 1200p at 60FPS 1440p at 30FPS 3K at 30FPS 1440 x 1920 pixels at 30 fps (Vertical default) Speakers 2X open ear speakers 2X open ear speakers Microphones Custom 5-mic Array (2 in left arm, 2 in right arm, 1 near nose pad) Custom 5-mic Array (2 in left arm, 2 in right arm, 1 near nose pad) Storage 32GB; about 500 photos, 100 30-second videos 32GB; about 100+ videos (30 sec) and 500+ photos (3 frame burst) Connectivity Wi-Fi 6; Bluetooth 5.3 Wi-Fi 6; Bluetooth 5.3 Compatibility iOS; Android iOS; Android Weight 52g 49.2g Durability IPX4 IPX4 Battery Up to 8 hours per charge 5 hours continuous audio playback or voice calling 4 hours per charge: 30 minutes live streaming Charging (glasses) case 48 hours worth of charge Up to 32 hours Charging speed 50% in 20 minutes 50% in 20 minutes Both the Ray-Ban Meta Gen 1 and Gen 2 smart glasses offer a 12MP ultra-wide camera, with support for 3024 X 4032 resolution photos. However, the video-recording capabilities of the Gen 2 pair are vastly superior to the Gen 1 model. You get 1200p at 60FPS, 1440p at 30FPS, and 3K at 30FPS video recording on the Ray-Ban Meta Gen 2, and that exceeds the 1080p maximum on the Gen 1 glasses. It's the same 3K camera we saw on Oakley Meta HSTN, which should bring more advanced stabilization while recording video, particularly handy for action shots. Hyperlapse and slow-motion video recording is heading to Ray-Ban Meta Gen 2 in a future software update this fall, according to Meta. Whether you need the extra resolution and video recording features will depend on whether you're satisfied with your current Gen 1 pair. Those who love recording video while running or working out will likely benefit from the extra frame rate support and stabilization of the Ray-Ban Meta Gen 2 camera. Both the Ray-Ban Meta Gen 1 and Gen 2 smart glasses have open-air speakers for video calls, audio playback, and more. There's also built-in microphones for audio input. Battery life was the most obvious place for Meta to upgrade its Ray-Ban smart glasses, and the second-generation model doesn't disappoint in this area. Just like the Meta Oakley HSTN smart glasses, the Ray-Ban Meta Gen 2 glasses can last up to eight hours on a single charge. That's double the four-hour battery life of the Ray-Ban Meta Gen 1 smart glasses. In the real world, that means you can take more photos, record more videos, and listen to more audio without needing to pop the Ray-Ban Meta Gen 2 into their charging case. In fact, the new model nets you five hours of continuous audio playback or voice calling. Both the Gen 1 and Gen 2 smart glasses can charge up to 50% in 20 minutes in a pinch. You charge both pairs of smart glasses by popping them into an included charging case, which has its own internal battery and a USB-C port. The case is also used for pairing with the Meta AI companion app and has a button for connections and resets. Meta AI is becoming more robust, supporting voice assistance and camera inputs. Essentially, it's a multimodal AI service that can take in audio plus your surroundings to help answer questions and take actions on your behalf. Both generations of Ray-Ban Meta support Meta AI, but there are extra features on the Gen 2 revision, like better live translation support. The ability for offline translation is also said to be coming in a future update. If you're in the market for a new pair of Ray-Ban Meta smart glasses, the choice is easy. The second-generation Ray-Ban Meta smart glasses clear the originals in weight, camera quality, and battery life — all key areas. As such, paying the extra $80 for the Gen 2 is a no-brainer when you consider all the quality-of-life upgrades you'll get. You should consider whether Ray-Ban Meta Display, Oakley Meta HSTN, or Oakley Meta Vanguard might suit you better if you have specialized use cases, like the need for a display or fitness-focused glasses. However, those with the Ray-Ban Meta Gen 1 smart glasses shouldn't feel the pressure to upgrade immediately. If the camera quality and battery life are meeting your expectations, hold onto them. The experience between models is largely the same, you'll just need to charge your Gen 1 pair more frequently. The one to buy Meta addressed the major drawbacks of Ray-Ban Meta Gen 1 with the second-generation revision, and they only cost a bit more. For that extra cash, you get excellent 3K video with better stabilization and double the battery life of the original. It's a no-brainer for people in the market for new smart glasses. Worth holding onto Ray-Ban Meta (Gen 1) are still a great set of smart glasses, so current owners might need to upgrade if the battery life is not meeting their needs. It could also be a neat value purchase if you want to save a few bucks compared to the Gen 2 version. Get the latest news from Android Central, your trusted companion in the world of Android Brady is a tech journalist for Android Central, with a focus on news, phones, tablets, audio, wearables, and software. He has spent the last three years reporting and commenting on all things related to consumer technology for various publications. Brady graduated from St. John's University with a bachelor's degree in journalism. His work has been published in XDA, Android Police, Tech Advisor, iMore, Screen Rant, and Android Headlines. When he isn't experimenting with the latest tech, you can find Brady running or watching Big East basketball. Android Central is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site. © Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York, NY 10036. Please login or signup to comment Please wait...
--------------------------------------------------

Title: The YouTube Tip of the Google Spear
URL: https://stratechery.com/2025/the-youtube-tip-of-the-google-spear/
Time Published: 2025-09-23T10:00:00Z
Full Content:
Stratechery Plus Learn MoreMember Forum Stratechery Plus Learn MoreMember Forum Latest Podcast Listen to Podcast Listen to this post: Action is happening up-and-down the LLM stack: Nvidia is making deals with Intel, OpenAI is making deals with Oracle, and Nvidia and OpenAI are making deals with each other. Nine years after Nvidia CEO Jensen Huang hand-delivered the first Nvidia DGX-1 AI computer to OpenAI, the chip giant is investing up to $100 billion in the AI lab, which OpenAI will, of course, spend on Nvidia AI systems. This ouroboros of a deal certainly does feel a bit frothy, but there is a certain logic to it: Nvidia is uniquely dominant in AI thanks to the company’s multi-year investment in not just superior chips but also an entire ecosystem from networking to software, and has the cash flow and stock price befitting its position in the AI value chain. Doing a deal like this at this point in time not only secures the company’s largest customer — and rumored ASIC maker — but also gives Nvidia equity upside beyond the number of chips it can manufacture. More broadly, lots of public investors would like the chance to invest in OpenAI; I don’t think Nvidia’s public market investors are bothered to have now acquired that stake indirectly. The interconnectedness of these investments reflects the interconnectedness of the OpenAI and Nvidia stories in particular: Huang may have delivered OpenAI their first AI computer, but it was OpenAI that delivered Nvidia the catalyst for becoming the most valuable company in the world, with the November 2022 launch of ChatGPT. Ever since, the assumption of many in tech has been that the consumer market in particular has been OpenAI’s to lose, or perhaps more accurately, monetize; no company has ever grown faster in terms of users and revenue, and that’s before they had an advertising model! And beyond the numbers, have you used ChatGPT? It’s so useful. You can look up information, or format text, and best of all you can code! Of course there are other models like Anthropic’s Claude, which has excelled at coding in particular, but surely the sheer usefulness makes ultimate success inevitable! If a lot of those takes sound familiar, it’s because I’ve made some version of most of them; I also, perhaps relatedly, took to Twitter like a fish to water. Just imagine, an app that was the nearly perfect mixture of content I was interested in and people I wanted to hear from, and interact with. Best of all it was text: the efficiency of information acquisition was unmatched, and it was just as easy to say my piece. It took me much longer to warm up to Facebook, and, frankly, I never was much of a user; I’ve never been one to image dump episodes of my life, nor have I had much inclination to wade through others’. I wasn’t interested in party photos; I lusted after ideas and arguments, and Twitter — a view shared by much of both tech and media — was much more up my alley. Despite that personal predilection, however, and perhaps because of my background in small town Wisconsin and subsequently living abroad, I retained a strong sense of the importance of Facebook. Sure, the people who I was most interested in hearing from and interacting with may have been the types to leave their friends and family for the big city, but for most people, friends and family were the entire point of life generally, and by extension, social media specifically. To that end, I was convinced from the beginning that Facebook was going to be a huge deal, and argued so multiple times on Stratechery; social media was ultimately a matter of network effects and scale, and Facebook was clearly on the path to domination, even as much of the Twitterati were convinced the company was the next MySpace. I was similarly bullish about Instagram: no, I wasn’t one to post a lot of personal pictures, but while I personally loved text, most people liked photos. What people really liked most of all, however — and not even Facebook saw this coming — was video. TikTok grew into a behemoth with the insight that social media was only ever a stepping stone to personal entertainment, of which video was the pinnacle. There were no network effects of the sort that everyone — including regulators — assumed would lead to eternal Facebook dominance; rather, TikTok realized that Paul Krugman’s infamous dismissal of the Internet actually was somewhat right: most people actually don’t have anything to say that is particularly compelling, which means that limiting the content you see to your social network dramatically decreases the possibility you’ll be entertained every time you open your social networking app. TikTok dispensed with this artificial limitation, simply showing you compelling videos period, no matter where they came from. Of course TikTok wasn’t the first company to figure this out: YouTube was the first video platform, and from the beginning focused on building an algorithm that focused more on giving you videos you were interested in than in showing you what you claimed to want to see. YouTube, however, was and probably always has been my biggest blind spot: I’m just not a big video watcher in general, and YouTube seemed like more work than short-form video, which married the most compelling medium with the most addictive delivery method — the feed. Sure, YouTube was a great acquisition for Google — certainly in line with the charge to “organize the world’s information and make it universally accessible and useful” — but I — and Google’s moneymaker, Search — was much more interested in text, and pictures if I must. The truth, however, is that YouTube has long been the giant hiding in plain sight: the service is the number one streaming service in the living room — bigger than Netflix — and that’s the company’s 3rd screen after mobile and the PC, where it has no peer. More than that, YouTube is not just the center of culture, but the nurturer of it: the company just announced that it has paid out more than $100 billion to creators over the last four years; given that many creators earn more from brand deals than they do from YouTube ads, that actually understates the size of the YouTube economy. Yes, TikTok is a big deal, but TikTok stars hope to make it on YouTube, where they can actually make a living. And yet, YouTube sometimes seems like an afterthought, at least to people like me and others immersed in the text-based Internet. Last week I was in New York for YouTube’s annual “Made on YouTube” event, but the night before I couldn’t remember the name; I turned to Google, natch, and couldn’t figure it out. The reason is that talk about YouTube mostly happens on YouTube; I, and Google itself, still live in a text-based world. That is the world that was rocked by ChatGPT, especially Google. The company’s February 2023 introduction of Bard in Paris remains one of the most surreal keynotes I’ve ever watched: most of the content was rehashed, the presenters talked as if they were seeing their slides for the first time, and one of the demos of a phone-based feature neglected to remember to have a phone on hand. This was a company facing a frontal assault on their most obvious and profitable area of dominance — text-based information retrieval — and they were completely flat-footed. Google has, in the intervening years, made tremendous strides to come back, including dumping the Bard name in favor of Gemini, itself based on vastly improved underlying models. I’m also impressed by how the company has incorporated AI into search; not only are AI Overviews generally useful, they’re also incredibly fast, and as a bonus have the links I sometimes prefer already at hand. Ironically, however, you could make the case that the biggest impact LLMs have had on Search is giving a federal judge an excuse to let Google continue paying its biggest would-be competitors (like Apple) to simply offer their customers Google instead. The biggest reason to be skeptical of the company’s fortunes in AI is that they had the most to lose; the company is doing an excellent job of minimizing the losses. What I would submit, however, is that Google’s most important and most compelling AI announcements actually don’t have anything to do with Search, at least not yet. These announcements start, as you might expect, with Google’s Deep Mind Research Lab; where they hit the real world, however, is on YouTube — and that, like the user-generated streaming service, is a really big deal. A perfect example of the DeepMind-to-YouTube pipeline was last week’s announcement of Veo 3-based features for making YouTube Shorts. From the company’s blog post: We’ve partnered with Google DeepMind to bring a custom version of their most powerful video generation model, Veo 3, to YouTube. Veo 3 Fast is designed to work seamlessly in YouTube Shorts for millions of creators and users, for free. It generates outputs with lower latency at 480p so you can easily create video clips – and for the first time, with sound – from any idea, all from your phone. This initial launch will allow you to not only generate videos, but also use one video to animate another (or a photo), stylize your video with a single touch, and add objects. You can also create an entire video — complete with voiceover — from a collection of clips, or convert speech to song. All of these features are a bit silly, but, well, that’s often where genius — or at least virality — comes from. Critics, of course, will label this an AI slop machine, and they’ll be right! The vast majority of content created by these tools will be boring and unwatched. That, however, is already the case with YouTube: the service sees 500 hours of content uploaded every minute, and most of that content isn’t interesting to anyone; the magic of YouTube, however, is the algorithm that finds out what is actually compelling and spreads it to an audience that wants exactly that. To put it another way, for YouTube AI slop is a strategy credit: given that the service has already mastered organizing overwhelming amounts of content and only surfacing what is good, it, more than anyone else, can handle exponentially more content which, through the sheer force of numbers, will result in an absolute increase of content that is actually compelling. That’s not the only strategy credit YouTube has; while the cost of producing AI-generated video will likely be lower than the cost of producing human-generated video, at least in the long run, the latter’s costs are not borne by TikTok or Meta (Facebook and Instagram are basically video platforms at this point). Rather, the brilliance of the user-generated content model is that creators post their content for free! This, however, means that AI-generated video is actually more expensive, at least if it’s made on TikTok or Meta’s servers. YouTube, however, pays its creators, which means that for the service AI-generated video actually has the potential to lower costs in the long run, increasing the incentive to leverage DeepMind’s industry-leading models. In short, while everyone immediately saw how AI could be disruptive to Search, AI is very much a sustaining innovation for YouTube: it increases the amount of compelling content in absolute terms, and it does so with better margins, at least in the long run. Here’s the million billion trillion dollar question: what is going to matter more in the long run, text or video? Sure, Google would like to dominate everything, but if it had to choose, is it better to dominate video or dominate text? The history of social networking that I documented above suggests that video is, in the long run, much more compelling to many more people. To put it another way, the things that people in tech and media are interested in has not historically been aligned with what actually makes for the largest service or makes the most money: people like me, or those reading me, care about text and ideas; the services that matter specialize in videos and entertainment, and to the extent that AI matters for the latter YouTube is primed to be the biggest winner, even as the same people who couldn’t understand why Twitter didn’t measure up to Facebook go ga-ga over text generation and coding capabilities. The potential impact of AI on YouTube’s fortunes isn’t just about AI-created videos; rather, the most important announcement of last week’s event was the first indicator that AI can massively increase the monetization potential of every video on the streaming service. You might have missed the announcement, because YouTube underplayed it; from their event blog post: We’re adding updates to brand deals and Shopping to make brand collaborations easier than ever. We’re accelerating these deals through a new initiative and new product features to make sure those partnerships succeed – like the ability to add a link to a brand’s site in Shorts. And YouTube Shopping is expanding to more markets and merchants and getting help from AI to make tagging easier. It’s just half a sentence — “getting help from AI to make tagging easier” — but the implications of those eight words are profound; here’s how YouTube explained the feature: We know tagging products can be time-consuming, so to make the experience better for creators, we’re leaning on an AI-powered system to identify the optimal moment a product is mentioned and automatically display the product tag at that time, capturing viewer interest when it’s highest. We’ll also begin testing the ability to automatically identify and tag all eligible products mentioned in your video later this year. The creator who demonstrated the feature — that right there is a great example of how YouTube is a different world than the one I and other people in the media inhabit — was very enthusiastic about the reduction in hassle and time-savings that would come from using AI to do a menial task like tagging sponsored products; that sounds like AI at its best, freeing up creative people to do what they do best. There’s no reason, however, why auto-tagging can’t become something much greater; in fact, I already explained the implications of this exact technology in explaining why AI made me bullish on Meta: This leads to a third medium-term AI-derived benefit that Meta will enjoy: at some point ads will be indistinguishable from content. You can already see the outlines of that given I’ve discussed both generative ads and generative content; they’re the same thing! That image that is personalized to you just might happen to include a sweater or a belt that Meta knows you probably want; simply click-to-buy. It’s not just generative content, though: AI can figure out what is in other content, including authentic photos and videos. Suddenly every item in that influencer photo can be labeled and linked — provided the supplier bought into the black box, of course — making not just every piece of generative AI a potential ad, but every piece of content period. The market implications of this are profound. One of the oddities of analyzing digital ad platforms is that some of the most important indicators are counterintuitive; I wrote this spring: The most optimistic time for Meta’s advertising business is, counter-intuitively, when the price-per-ad is dropping, because that means that impressions are increasing. This means that Meta is creating new long-term revenue opportunities, even as its ads become cost competitive with more of its competitors; it’s also notable that this is the point when previous investor freak-outs have happened. When I wrote that I was, as I noted in the introduction, feeling more cautious about Meta’s business, given that Reels is built out and the inventory opportunities of Meta AI were not immediately obvious. I realize now, though, that I was distracted by Meta AI: the real impact of AI is to make everything inventory, which is to say that the price-per-ad on Meta will approach $0 for basically forever. Would-be competitors are finding it difficult enough to compete with Meta’s userbase and resources in a probabilisitic world; to do so with basically zero price umbrella seems all-but-impossible. This analysis was spot-on; I just pointed it at the wrong company. This opportunity to leverage AI to make basically every pixel monetizable absolutely exists for Meta; Meta, however, has to actually develop the models and infrastructure to do it at scale. Google is already there; it was the company universally decried for being slow-moving that announced the first version of this feature last week. I can’t overstate what a massive opportunity this is: every item in every YouTube video is well on its way to being a monetizable surface. Yes, that may sound dystopian when I put it so baldly, but if you think about it you can see the benefits; I’ve been watching a lot of home improvement videos lately, and it sure would be useful to be able to not just identify but helpfully have a link to buy a lot of the equipment I see, much of which is basically in the background because it’s not the point of the video. It won’t be long until YouTube has that inventory, which it could surface with an affiliate fee link, or make biddable for companies who want to reach primed customers. More generally, you can actually envision Google pulling this off: the company may have gotten off to a horrible start in the chatbot era, but the company has pulled itself together and is increasingly bringing its model and infrastructure leadership to bear, even as Meta has had to completely overhaul their AI approach after hitting a wall. I’m sure CEO Mark Zuckerberg will figure it out, but Google — surprise! — is the company actually shipping. Or, rather, YouTube is. Close readers of Stratechery have been observing — and probably, deservedly, smirking — at this most unexpected evolution: I am enjoying @benthompson’s journey to becoming a $GOOGL bull pic.twitter.com/xryDBIukRW That quote is from Paradigm Shifts and the Winner’s Curse, an Article that was mostly about my concerns about Apple and Amazon, and reads: And, by the same token, I’m much more appreciative of Google’s amorphous nature and seeming lack of strategy. That makes them hard to analyze — again, I’ve been honest for years about the challenges I find in understanding Mountain View — but the company successfully navigated one paradigm shift, and is doing much better than I originally expected with this one. Larry Page and Sergey Brin famously weren’t particularly interested in business or in running a company; they just wanted to do cool things with computers in a college-like environment like they had at Stanford. That the company, nearly thirty years later, is still doing cool things with computers in a college-like environment may be maddening to analysts like me who want clarity and efficiency; it also may be the key to not just surviving but winning across multiple paradigms. Appreciating the benefits of Google being an amorphous blob where no one knows what is going on, least of all leadership, is a big part of my evolution; this Article is the second part: that blob ultimately needs a way to manifest the technology it manages to come up with, and if you were to distill my worries about Google in the age of AI it would be to wonder how the company could become an answer machine — which Page and Brin always wanted — when it risked losing the massive economic benefits that came from empowering users to choose the winners of auctions Google conducted for advertisers. That, however, is ultimately the text-based world, and there’s a case to be made that, in the long run, it simply won’t matter as much as the world of video. Again, the company is doing better with Search than I expected, and I’ve always been bullish about the impact of AI on the company’s cloud business; the piece I’ve missed, however, is that Google already has the tip of the spear for its AI excellence to actually go supernova: YouTube, the hidden giant in plain sight, a business that is simultaneously unfathomably large, and also just getting started. Get notified about new Articles Please verify your email address to proceed. Stratechery Plus Updates Stratechery Plus Podcasts Stratechery Plus Interviews The most popular and most important posts on Stratechery by year. Explore all free articles on Stratechery. Explore all posts on Stratechery. Stratechery Plus UpdateS Stratechery Plus Podcasts Stratechery Plus Interviews © Stratechery LLC 2025 | Terms of Service | Privacy Policy Proudly powered by WordPress. Hosted by Pressable.
--------------------------------------------------

Title: Nvidia Says All Customers Will Be ‘Priority’ Despite OpenAI Deal
URL: https://finance.yahoo.com/news/nvidia-says-customers-priority-despite-000416448.html
Time Published: 2025-09-23T00:04:16Z
Description: “Our investments will not change our focus or impact supply to our other customers,” Nvidia said in a statement late Monday.  “We will continue to make every...
--------------------------------------------------

Title: Bank of America Reiterates Its ‘Buy’ rating on Meta Platforms, Inc. (META)
URL: https://finance.yahoo.com/news/bank-america-reiterates-buy-rating-220803953.html
Time Published: 2025-09-22T22:08:03Z
Description: With significant hedge fund interest, Meta Platforms, Inc. (NASDAQ:META) secures a spot on our list of the 13 Best Virtual Reality Stocks to Buy Right Now...
--------------------------------------------------

Title: Meta (META) Backed by $900 Target as Analyst Highlights Creative Edge
URL: https://finance.yahoo.com/news/meta-meta-stock-outlook-brightens-213455790.html
Time Published: 2025-09-22T21:34:33Z
Description: Meta Platforms, Inc. (NASDAQ:META) is one of the AI Stocks Making Big Moves on Wall Street. On September 18, Citizens JMP analyst Andrew Boone reiterated a...
--------------------------------------------------

Title: Google faces antitrust déjà vu as US seeks to break up its digital advertising business
URL: https://fortune.com/2025/09/22/google-digital-advertising-trial-monopoly-antitrust/
Time Published: 2025-09-22T21:02:00Z
Full Content:
After deflecting the U.S. Justice Department’s attack on its illegal monopoly in online search, Google is facing another attempt to dismantle its internet empire in a trial focused on its abusive tactics in digital advertising. The trial that opened Monday in an Alexandria, Virginia, federal court revolves around the harmful conduct that resulted in U.S. District Judge Leonie Brinkema declaring parts of Google’s digital advertising technology to be an illegal monopoly. The judge found that Google has been engaging in behavior that stifles competition to the detriment of online publishers that depend on the system for revenue. Google and the Justice Department will spend the next two weeks in court presenting evidence in a “remedy” trial that will culminate in Brinkema issuing a ruling on how to restore fair market conditions. “The purpose of a remedy is doing what is necessary to restore competition,” said Julia Tarver Wood in an opening statement for the DOJ’s antitrust division. Wood said that the Google is manipulating the market in a way that is antithetical to free market competition. “The means to cheat are buried in computer codes and algorithms,” Wood said. An attorney for Google, Karen Dunn, countered that the remedy proposed by the government is reckless and radical, and that the DOJ is attempting to remove Google entirely from the competition. No matter how the judge rules, Google says it will appeal the earlier decision labeling the ad network as a monopoly. Appeals can’t be filed until the remedy is determined. The case, filed in 2023 under President Joe Biden’s administration, threatens the complex network that Google has spent the past 17 years building to power its dominant digital advertising business. Besides accounting for most of the $305 billion in revenue that Google’s services division generates for its corporate parent Alphabet Inc., digital advertising sales provide the lifeblood that keeps thousands of websites alive. If the Justice Department gets its way, Brinkema will order Google to sell parts of its ad technology — a proposal that the company’s lawyers warned would “invite disruption and damage” to consumers and the internet’s ecosystem. The Justice Department contends a breakup would be the most effective and quickest way to undercut a monopoly that has been stifling competition and innovation for years. Google believes it has already made enough changes to its “Ad Manager” system, including providing more options and pricing options, to resolve the issues the Brinkema flagged in her monopoly ruling., The legal battle over Google’s advertising technology mirrors another showdown that the company recently navigated after another federal judge condemned its dominant search engine as an illegal monopoly and then held remedy hearings earlier this year to consider how to stop the misconduct. In that case, the Justice Department also proposed a severe crackdown that would have required Google to sell its popular Chrome browser, but U.S. District Judge Amit Mehta decided a less dramatic shake-up was needed amid a search market being reshaped by artificial intelligence technology in a decision issued earlier this month. Even though Google didn’t agree with all aspects of Mehta’s decision, the ruling was widely seen as a slap on the wrist — a sentiment that has helped propel Alphabet’s stock price to new highs. The 20% gain since Mehta’s decision helped make Alphabet only the fourth publicly traded company to reach a market value of $3 trillion — an increase of more than $1 trillion since Brinkema branded Google’s ad technology as a monopoly in April. In an indication that the outcome of the search monopoly case might sway things in the advertising technology proceedings, Brinkema asked both Google and the Justice Department to address Mehta’s decision during the upcoming trial. As they did in the search case, Google’s lawyers already have been asserting in court papers that AI technology being used by ad network rivals like Meta Platforms is reshaping the way the market works and overriding the need for the Justice Department’s “radical” proposals. The Justice Department is “fighting for a remedy that would vanquish a past that has been overtaken by technological and market transformations in the way digital ads are consumed,” Google’s lawyers argued leading up to the trial. © 2025 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.
--------------------------------------------------

Title: Google faces antitrust déjà vu as U.S. seeks to break up its digital advertising business
URL: https://japantoday.com/category/business/google-faces-antitrust-deja-vu-as-us-seeks-to-break-up-its-digital-advertising-business
Time Published: 2025-09-22T20:58:06Z
Description: After deflecting the U.S. Justice Department's attack on its illegal monopoly in online search, Google is facing another attempt to dismantle its internet empire in a trial focused on its abusive tactics in digital advertising. The trial that opened Monday in…
--------------------------------------------------

Title: S&P 500 Gain & Losses Today: Oracle, Nvidia Shares Advance; Kenvue Stock Slips
URL: https://www.investopedia.com/s-and-p-500-gain-and-losses-today-teradyne-and-oracle-stocks-advance-kenvue-slips-11814914
Time Published: 2025-09-22T20:55:36Z
Full Content:
Shares of a semiconductor testing company rose Monday after an analyst highlighted strong demand trends and collaboration with a major chip-industry player. Meanwhile, reports of an impending product safety announcement dragged down shares of a consumer health firm. Major U.S. equities indexes moved higher as plans by Nvidia (NVDA) to invest up to $100 million in ChatGPT maker OpenAI reinvigorated optimism around artificial intelligence. The S&P 500 added 0.4%, extending its streak of all-time closing highs to three consecutive sessions. The Dow ticked 0.1% higher, while the Nasdaq jumped 0.7%, with both setting their own fresh closing records. Shares of Nvidia finished 4% higher. Read Investopedia's full coverage of today's trading here. The day's top performance in the S&P 500 belonged to shares of semiconductor equipment maker Teradyne (TER), which surged 13%. Susquehanna Financial boosted its price target on Teradyne stock, with its analysts saying the provider of automated test equipment for semiconductors and robotic systems could benefit from strong demand trends over the coming years and highlighting Teradyne's increasing partnership with chip industry giant Taiwan Semiconductor Manufacturing Company (TSM). White House officials confirmed that Oracle (ORCL) is part of a consortium set to control the U.S. operations of the social media platform TikTok. Oracle also announced that Clay Magouyrk, current president of cloud infrastructure, and Mike Sicilia, president of industries, will take the reins of the enterprise software giant as co-CEOs. Current CEO Safra Catz will remain with Oracle as the executive vice chair of its board, while co-founder Larry Ellison will maintain his role as chief technology officer and board chair. Oracle shares jumped 6.3%. Moderna (MRNA) shares gained 5.2% after the Centers for Disease Control and Prevention's vaccine panel voted on updated recommendations related to COVID shots. While the CDC's Advisory Committee for Immunization Practices removed guidance calling for all people to be vaccinated against COVID and now recommends that patients consult with their doctor before receiving the shot, the panel opted not to require patients to obtain a prescription for the vaccine. Shares of Westinghouse Air Brake Technologies (WAB), a provider of equipment for freight and passenger railway systems, added 4.9%. The company also known as Wabtec announced a deal worth more than $4 billion to deliver 300 of its Evolution Series locomotives to Kazakhstan's national railway company over the coming decade and provide long-term service support. Citi analysts initiated coverage on Wabco stock with a "buy" rating, suggesting the company could benefit from increasing infrastructure investment as well as moves by railroad operators to renew and expand their fleets. Shares of Tylenol maker Kenvue (KVUE) dropped 7.5%, the most of any S&P 500 stock, following reports that the Trump administration planned to make an announcement linking use of the painkiller during pregnancy to autism. A presentation on the matter happened at the White House after the closing bell. Kenvue, the consumer health company that spun off from Johnson & Johnson (JNJ) in 2023, said that scientific studies show no link between acetaminophen and autism. Social media giant Meta Platforms (META) launched a pair of new features for its Facebook Dating service—an AI assistant that provides support throughout the dating process and a function that offers automatic matches with potential partners based on Facebook's algorithms. Shares of Match Group (MTCH), operator of Tinder, Hinge, and other dating platforms, fell 5.4%. Lennar (LEN) shares fell 4.3% on Monday, extending losses posted Friday after the homebuilder missed quarterly sales and profit forecasts. Following the softer-than-expected earnings results, Raymond James downgraded Lennar stock to "underperform" from "market perform." Analysts suggested that Lennar's margins could remain under pressure, citing the company's use of selling incentives to attract buyers and its elevated on technology and teams.
--------------------------------------------------

Title: Facebook is getting an AI dating assistant | TechCrunch
URL: https://techcrunch.com/2025/09/22/facebook-is-getting-an-ai-dating-assistant/
Time Published: 2025-09-22T20:28:29Z
Full Content:
Latest AI Amazon Apps Biotech & Health Climate Cloud Computing Commerce Crypto Enterprise EVs Fintech Fundraising Gadgets Gaming Google Government & Policy Hardware Instagram Layoffs Media & Entertainment Meta Microsoft Privacy Robotics Security Social Space Startups TikTok Transportation Venture Staff Events Startup Battlefield StrictlyVC Newsletters Podcasts Videos Partner Content TechCrunch Brand Studio Crunchboard Contact Us Meta announced on Monday that it’s bringing an AI assistant to Facebook Dating. This chatbot is intended to help users find matches who are more closely tailored to what they are looking for. For example, Meta might suggest users ask it to find “a Brooklyn girl in tech,” or a user could ask the AI to help refine their profile. Meta also says that it’s “helping people avoid swipe fatigue” with a new feature called Meet Cute, which gives users a weekly “surprise match” chosen based on its algorithm. The company says Facebook Dating matches among adults ages 18 to 29 have increased 10% year-over-year, with hundreds of thousands of users in that age group creating Facebook Dating profiles each month. That’s small compared to competitors like Tinder, which has about 50 million daily active users, and Hinge’s 10 million daily active users. AI features have already become the norm in mainstream dating apps. Even newer dating apps like Sitch have attempted to differentiate themselves with their AI features. Match Group — the owner of Tinder, Hinge, OKCupid, and others — entered a partnership with OpenAI last year, which is part of the dating giant’s $20 million-plus investment in AI. That’s a big bet, especially given the financial struggles of Match Group, which has lost about 68% of its stock price in the last five years. So far, this investment has produced features like an AI photo selector tool for Tinder, which scans your camera roll to help choose profile images, as well as AI-powered matching. Hinge has a feature that lets users improve their responses to profile prompts with AI. Bumble has added similar AI features, and founder Whitney Wolfe Herd even ruffled some feathers last year when she suggested that one day, users could have personal “AI concierges” that go on dates with other people’s AI to determine compatibility. Topics Senior Writer Amanda Silberling is a senior writer at TechCrunch covering the intersection of technology and culture. She has also written for publications like Polygon, MTV, the Kenyon Review, NPR, and Business Insider. She is the co-host of Wow If True, a podcast about internet culture, with science fiction author Isabel J. Kim. Prior to joining TechCrunch, she worked as a grassroots organizer, museum educator, and film festival coordinator. She holds a B.A. in English from the University of Pennsylvania and served as a Princeton in Asia Fellow in Laos. Send tips through Signal, an encrypted messaging app, to @amanda.100. For anything else or to verify outreach, email amanda@techcrunch.com. Founders: land your investor and sharpen your pitch. Investors: discover your next breakout startup. Innovators: claim a front-row seat to the future. Join 10,000+ tech leaders at the epicenter of innovation. Register now and save up to $668.Regular Bird rates end September 26 It isn’t your imagination: Google Cloud is flooding the zone Neon, the No. 2 social app on the Apple App Store, pays users to record their phone calls and sells data to AI firms Trump hits H-1B visas with $100,000 fee, targeting the program that launched Elon Musk and Instagram Updates to Studio, YouTube Live, new GenAI tools, and everything else announced at Made on YouTube Google isn’t kidding around about cost cutting, even slashing its FT subscription Meta CTO explains why the smart glasses demos failed at Meta Connect — and it wasn’t the Wi-Fi How AI startups are fueling Google’s booming cloud business © 2025 TechCrunch Media LLC.
--------------------------------------------------

Title: Stock market today: Dow, S&P 500, Nasdaq hit 3rd record in a row as Nvidia soars on OpenAI deal
URL: https://finance.yahoo.com/news/live/stock-market-today-dow-sp-500-nasdaq-hit-3rd-record-in-a-row-as-nvidia-soars-on-openai-deal-200011974.html
Time Published: 2025-09-22T20:00:11Z
Description: A parade of Fed speakers and a key inflation print are in focus for clues to further rate cuts.
--------------------------------------------------

Title: Quantum Computing's unexpected move sends shares reeling
URL: https://www.thestreet.com/technology/quantum-computing-stock-plummets-on-financing-decision
Time Published: 2025-09-22T18:03:00Z
Description: Here's why Quantum Computing shares are tumbling.
--------------------------------------------------

Title: Google enters second court battle against DOJ over alleged monopoly
URL: https://www.cbsnews.com/news/google-ad-tech-monopoly-trial-remedy/
Time Published: 2025-09-22T16:44:46Z
Full Content:
Watch CBS News September 22, 2025 / 12:44 PM EDT / AP After deflecting the U.S. Justice Department's attack on its illegal monopoly in online search, Google is facing another attempt to dismantle its internet empire in a trial focused on its abusive tactics in digital advertising. The trial scheduled to begin Monday in an Alexandria, Virginia, federal court will revolve around the harmful conduct that resulted in U.S. District Judge Leonie Brinkema declaring parts of Google's digital advertising technology to be an illegal monopoly. The judge found that Google has been engaging in behavior that stifles competition to the detriment of online publishers that depend on the system for revenue. Google and the Justice Department will spend the next two weeks in court presenting evidence in a "remedy" trial that will culminate in Brinkema issuing a ruling on how to restore fair market conditions. Although the judge hasn't set a timetable for making that decision, it's unlikely to come down before the end of this year because additional legal briefs and courtroom arguments are expected to extend into November before Brinkema takes the matter under submission. No matter how the judge rules, Google says it will appeal the earlier decision labeling the ad network as a monopoly. Appeals can't be filed until the remedy is determined. The case, filed in 2023 under President Joe Biden's administration, threatens the complex network that Google has spent the past 17 years building to power its dominant digital advertising business. Besides accounting for most of the $305 billion in revenue that Google's services division generates for its corporate parent Alphabet Inc., digital advertising sales provide the lifeblood that keeps thousands of websites alive. If the Justice Department gets its way, Brinkema will order Google to sell parts of its ad technology — a proposal that the company's lawyers warned would "invite disruption and damage" to consumers and the internet's ecosystem. The Justice Department contends a breakup would be the most effective and quickest way to undercut a monopoly that has been stifling competition and innovation for years. Google believes it has already made enough changes to its "Ad Manager" system, including providing more options and pricing options, to resolve the issues the Brinkema flagged in her monopoly ruling. The legal battle over Google's advertising technology mirrors another showdown that the company recently navigated after another federal judge condemned its dominant search engine as an illegal monopoly and then held remedy hearings earlier this year to consider how to stop the misconduct. In that case, the Justice Department also proposed a severe crackdown that would have required Google to sell its popular Chrome browser, but U.S. District Judge Amit Mehta decided a less dramatic shake-up was needed amid a search market being reshaped by artificial intelligence technology in a decision issued earlier this month. Even though Google didn't agree with all aspects of Mehta's decision, the ruling was widely seen as a slap on the wrist — a sentiment that has helped propel Alphabet's stock price to new highs. The 20% gain since Mehta's decision helped make Alphabet only the fourth publicly traded company to reach a market value of $3 trillion — an increase of more than $1 trillion since Brinkema branded Google's ad technology as a monopoly in April. In an indication that the outcome of the search monopoly case might sway things in the advertising technology proceedings, Brinkema asked both Google and the Justice Department to address Mehta's decision during the upcoming trial. As they did in the search case, Google's lawyers already have been asserting in court papers that AI technology being used by ad network rivals like Meta Platforms is reshaping the way the market works and overriding the need for the Justice Department's "radical" proposals. The Justice Department is "fighting for a remedy that would vanquish a past that has been overtaken by technological and market transformations in the way digital ads are consumed," Google's lawyers argued leading up to the trial. © 2025 The Associated Press. All Rights Reserved. This material may not be published, broadcast, rewritten, or redistributed. Copyright ©2025 CBS Interactive Inc. All rights reserved.
--------------------------------------------------

Title: Google faces antitrust déjà vu as US seeks to break up its digital advertising business
URL: https://www.denverpost.com/2025/09/22/google-monopoly-advertising-penalty/
Time Published: 2025-09-22T15:26:34Z
Full Content:
By MICHAEL LIEDTKE, AP Technology Writer After deflecting the U.S. Justice Department’s attack on its illegal monopoly in online search, Google is facing another attempt to dismantle its internet empire in a trial focused on its abusive tactics in digital advertising. The trial that opened Monday in an Alexandria, Virginia, federal court revolves around the harmful conduct that resulted in U.S. District Judge Leonie Brinkema declaring parts of Google’s digital advertising technology to be an illegal monopoly. The judge found that Google has been engaging in behavior that stifles competition to the detriment of online publishers that depend on the system for revenue. Google and the Justice Department will spend the next two weeks in court presenting evidence in a “remedy” trial that will culminate in Brinkema issuing a ruling on how to restore fair market conditions. “The purpose of a remedy is doing what is necessary to restore competition,” said Julia Tarver Wood in an opening statement for the DOJ’s antitrust division. Wood said that the Google is manipulating the market in a way that is antithetical to free market competition. “The means to cheat are buried in computer codes and algorithms,” Wood said. An attorney for Google, Karen Dunn, countered that the remedy proposed by the government is reckless and radical, and that the DOJ is attempting to remove Google entirely from the competition. No matter how the judge rules, Google says it will appeal the earlier decision labeling the ad network as a monopoly. Appeals can’t be filed until the remedy is determined. The case, filed in 2023 under President Joe Biden’s administration, threatens the complex network that Google has spent the past 17 years building to power its dominant digital advertising business. Besides accounting for most of the $305 billion in revenue that Google’s services division generates for its corporate parent Alphabet Inc., digital advertising sales provide the lifeblood that keeps thousands of websites alive. If the Justice Department gets its way, Brinkema will order Google to sell parts of its ad technology — a proposal that the company’s lawyers warned would “invite disruption and damage” to consumers and the internet’s ecosystem. The Justice Department contends a breakup would be the most effective and quickest way to undercut a monopoly that has been stifling competition and innovation for years. Google believes it has already made enough changes to its “Ad Manager” system, including providing more options and pricing options, to resolve the issues the Brinkema flagged in her monopoly ruling., The legal battle over Google’s advertising technology mirrors another showdown that the company recently navigated after another federal judge condemned its dominant search engine as an illegal monopoly and then held remedy hearings earlier this year to consider how to stop the misconduct. In that case, the Justice Department also proposed a severe crackdown that would have required Google to sell its popular Chrome browser, but U.S. District Judge Amit Mehta decided a less dramatic shake-up was needed amid a search market being reshaped by artificial intelligence technology in a decision issued earlier this month. Even though Google didn’t agree with all aspects of Mehta’s decision, the ruling was widely seen as a slap on the wrist — a sentiment that has helped propel Alphabet’s stock price to new highs. The 20% gain since Mehta’s decision helped make Alphabet only the fourth publicly traded company to reach a market value of $3 trillion — an increase of more than $1 trillion since Brinkema branded Google’s ad technology as a monopoly in April. In an indication that the outcome of the search monopoly case might sway things in the advertising technology proceedings, Brinkema asked both Google and the Justice Department to address Mehta’s decision during the upcoming trial. As they did in the search case, Google’s lawyers already have been asserting in court papers that AI technology being used by ad network rivals like Meta Platforms is reshaping the way the market works and overriding the need for the Justice Department’s “radical” proposals. The Justice Department is “fighting for a remedy that would vanquish a past that has been overtaken by technological and market transformations in the way digital ads are consumed,” Google’s lawyers argued leading up to the trial. AP Writer Olivia Diaz contributed to this report from Alexandria, Virginia.
--------------------------------------------------

Title: Google faces antitrust déjà vu as US seeks to break up its digital advertising business
URL: https://www.bostonherald.com/2025/09/22/google-monopoly-advertising-penalty/
Time Published: 2025-09-22T15:26:34Z
Full Content:
By MICHAEL LIEDTKE, AP Technology Writer After deflecting the U.S. Justice Department’s attack on its illegal monopoly in online search, Google is facing another attempt to dismantle its internet empire in a trial focused on its abusive tactics in digital advertising. The trial that opened Monday in an Alexandria, Virginia, federal court revolves around the harmful conduct that resulted in U.S. District Judge Leonie Brinkema declaring parts of Google’s digital advertising technology to be an illegal monopoly. The judge found that Google has been engaging in behavior that stifles competition to the detriment of online publishers that depend on the system for revenue. Google and the Justice Department will spend the next two weeks in court presenting evidence in a “remedy” trial that will culminate in Brinkema issuing a ruling on how to restore fair market conditions. “The purpose of a remedy is doing what is necessary to restore competition,” said Julia Tarver Wood in an opening statement for the DOJ’s antitrust division. Wood said that the Google is manipulating the market in a way that is antithetical to free market competition. “The means to cheat are buried in computer codes and algorithms,” Wood said. An attorney for Google, Karen Dunn, countered that the remedy proposed by the government is reckless and radical, and that the DOJ is attempting to remove Google entirely from the competition. No matter how the judge rules, Google says it will appeal the earlier decision labeling the ad network as a monopoly. Appeals can’t be filed until the remedy is determined. The case, filed in 2023 under President Joe Biden’s administration, threatens the complex network that Google has spent the past 17 years building to power its dominant digital advertising business. Besides accounting for most of the $305 billion in revenue that Google’s services division generates for its corporate parent Alphabet Inc., digital advertising sales provide the lifeblood that keeps thousands of websites alive. If the Justice Department gets its way, Brinkema will order Google to sell parts of its ad technology — a proposal that the company’s lawyers warned would “invite disruption and damage” to consumers and the internet’s ecosystem. The Justice Department contends a breakup would be the most effective and quickest way to undercut a monopoly that has been stifling competition and innovation for years. Google believes it has already made enough changes to its “Ad Manager” system, including providing more options and pricing options, to resolve the issues the Brinkema flagged in her monopoly ruling., The legal battle over Google’s advertising technology mirrors another showdown that the company recently navigated after another federal judge condemned its dominant search engine as an illegal monopoly and then held remedy hearings earlier this year to consider how to stop the misconduct. In that case, the Justice Department also proposed a severe crackdown that would have required Google to sell its popular Chrome browser, but U.S. District Judge Amit Mehta decided a less dramatic shake-up was needed amid a search market being reshaped by artificial intelligence technology in a decision issued earlier this month. Even though Google didn’t agree with all aspects of Mehta’s decision, the ruling was widely seen as a slap on the wrist — a sentiment that has helped propel Alphabet’s stock price to new highs. The 20% gain since Mehta’s decision helped make Alphabet only the fourth publicly traded company to reach a market value of $3 trillion — an increase of more than $1 trillion since Brinkema branded Google’s ad technology as a monopoly in April. In an indication that the outcome of the search monopoly case might sway things in the advertising technology proceedings, Brinkema asked both Google and the Justice Department to address Mehta’s decision during the upcoming trial. As they did in the search case, Google’s lawyers already have been asserting in court papers that AI technology being used by ad network rivals like Meta Platforms is reshaping the way the market works and overriding the need for the Justice Department’s “radical” proposals. The Justice Department is “fighting for a remedy that would vanquish a past that has been overtaken by technological and market transformations in the way digital ads are consumed,” Google’s lawyers argued leading up to the trial. AP Writer Olivia Diaz contributed to this report from Alexandria, Virginia.
--------------------------------------------------

Title: Household resilience and its role in sustaining food security in rural Bangladesh
URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0332868
Time Published: 2025-09-22T14:00:00Z
Full Content:
Food insecurity and agriculture in South Asia, including Bangladesh, pose significant threats to the well-being and livelihoods of its people. Building adaptive capacities and resilient food systems is crucial for sustainable livelihoods. This study employs the Resilience Index Measurement and Analysis II framework to construct a Resilience Capacity Index (RCI) and analyze its relationship with food security using data from the Bangladesh Integrated Household Survey 2018. The study applies Exploratory Factor Analysis and Structural Equation Modeling to examine the impact of key resilience components such as Access to Basic Services, Adaptive Capacity, and Assets on household resilience. The findings reveal that access to basic services, land assets, and farm equipment positively influences households’ resilience capacity. However, the presence of livestock assets has a negative impact, potentially due to market volatility, climate vulnerability, and disease outbreaks. Additionally, adaptive capacity has a positive but insignificant influence on RCI, suggesting that without enhancing economic opportunities, institutional support, and inclusive development strategies, adaptive capacity could not be enough to foster resilience. However, resilient capacity enhances food security metrics such as the Food Consumption Score and Expenditure. These findings underscore the importance of policies that focus on increasing and maintaining access to basic services, promoting sustainable land management practices, and strengthening social safety nets. This study emphasizes the importance of focusing on livestock assets to ensure their sustainability by stabilizing the livestock market, improving veterinary services, and providing subsidies to reduce maintenance costs. Citation: Tasnim I, Iqbal MA, Begum IA, Alam MJ, Graversgaard M, Sarma PK, et al. (2025) Household resilience and its role in sustaining food security in rural Bangladesh. PLoS One 20(9): e0332868. https://doi.org/10.1371/journal.pone.0332868 Editor: António Raposo, Lusofona University of Humanities and Technologies: Universidade Lusofona de Humanidades e Tecnologias, PORTUGAL Received: July 9, 2025; Accepted: September 7, 2025; Published: September 22, 2025 This is an open access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication. Data Availability: The dataset is available at https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/HOMCPT. Funding: Bangladesh Agricultural Research System (BAURES) (Grant No. 2023/45/BAU).The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing interests: The authors have declared that no competing interests exist. Food consumption is a fundamental human need, and a global concern, characterized by a surplus of food in some regions and scarcity in others. It stands as a cornerstone of the United Nations’ Sustainable Development Goals (SDGs), notably SDG-1 (No Hunger) [1]. Achieving food security, which ensures an end to hunger, is essential for human well-being [2]. According to the World Food Summit, food security entails access to sufficient, safe, and nutritious food [3]. However, food security is a multifaceted issue intertwined with trade, politics, history, health and nutrition [4]. Countries reliant on imports are often vulnerable to fluctuations in the global market, as international trade can directly affect food accessibility and price stability [5,6]. Political instability and conflict can severely disrupt food production and distribution systems, as observed in countries like Somalia and Yemen [7]. Food systems and access continue to be shaped by historical legacies of colonialism and unequal land distribution, particularly in many developing countries [8]. Moreover, malnutrition exacerbates health vulnerabilities, while poor health undermines households’ capacity to produce or procure food, creating a cyclical relationship [9,10]. Achieving food security, therefore, requires nutrition-sensitive systems that go beyond meeting calorie needs to ensure dietary diversity and adequate nutrient intake [11,12]. Food insecurity, on the other hand, refers to limited or uncertain access to adequate and nutritious food, and the inability to obtain food in socially acceptable ways [13]. It represents a complex challenge that affects individuals and communities worldwide. Many developing countries, including Bangladesh, have recently experienced acute food insecurity. According to the Integrated Food Security Phase Classification (IPC), a collaborative initiative involving governments, non-governmental organizations, UN Agencies, civil society and other relevant actors, approximately 9.14 million individuals, or 20% of the population, are categorized in Phases 3 and 4 [14]. These phases indicate high levels of acute food insecurity in certain parts of Bangladesh. The poorest households in these areas face significant challenges exacerbated by factors such as high inflation, reduced earnings, and frequent climate-related shocks, which contribute to the severity of food insecurity among the population [14]. The frequency and intensity of natural, economic, and political hazards affecting individuals, businesses, economies, and even entire nations are increasing [15–19]. The concept of resilience has thus gained prominence in governmental and research contexts. Initially introduced in ecology literature by Holling [20], resilience has since been studied and applied across various fields, including socioeconomics [21]. Resilience is commonly defined as the capacity to anticipate, prepare for, respond to, and recover from shocks efficiently and effectively [22]. Recent studies have expanded this definition to include the ability to manage persistent risk exposure [23]. In the context of food security analysis, resilience refers to the latent capacity of households to withstand and navigate socioeconomic factors, climate variability, and other shocks [24]. Traditionally viewed as an endpoint, resilience is now understood as a continuous process that can either enhance or diminish response capacities, thereby impacting overall well-being [25]. Resilient food systems are adept at adapting to new conditions while maintaining stable food production and ensuring access for vulnerable populations. They mitigate the impact of shocks and stressors, with social and economic factors playing crucial roles in enhancing resilience. In countries like Bangladesh, where a significant portion of the population relies on agriculture, food insecurity remains a pressing concern. The food system is vulnerable due to its heavy reliance on a few staple crops such as rice, wheat, and maize, and its interconnectedness with the global food supply chain [14,26–28]. Disruptions in one part of the system can lead to cascading effects throughout. Moreover, the lack of agricultural diversity and reliance on intensive monoculture practices increase vulnerability to pests, diseases, and climate shocks. Bangladesh is particularly susceptible to extreme weather events such as heat waves, floods and droughts [29], which can severely impact food production and access [30]. Assessing the resilience of communities and households to shocks and threats is crucial as it allows nations to anticipate and effectively address these challenges. Resilience assessments help policymakers and organisations understand how well a community can rebound from adversity. Food security, with its multifaceted and complex nature, has predominantly been studied through vulnerability assessments. However, the methodologies used in these assessments often lack dynamism, which makes precise predictions of potential shocks and incidents challenging [31–33]. The Food and Agriculture Organization (FAO) developed the Resilience Index Measurement Analysis (RIMA) to quantify resilience [6,34,35]. This method introduces the concept that resilience can be estimated as a latent variable through a two-step procedure involving observable factors, even when resilience itself is not immediately observable [33]. Recent advancements by FAO have enhanced this approach by using structural equation modelling instead of factor analysis to estimate the resilience index. Additionally, more variables have been included as proxies to represent institutional and natural environments [36]. Applying the RIMA-II model in Bangladesh offers a novel, structured econometric approach to measuring household resilience, an aspect largely absent in previous food security studies. Unlike traditional vulnerability assessments that rely on static indicators, RIMA-II conceptualizes resilience as a latent variable, allowing for a more nuanced and dynamic analysis of the factors influencing resilience capacity. While RIMA-II has been applied in various regions, its use in Bangladesh remains limited, with most existing studies focusing on food security determinants [37,38] or coping strategies [39–48] rather than the interactions among resilience components. Furthermore, prior research lacks a comprehensive resilience index beyond traditional vulnerability assessments, limiting its ability to capture the complex interconnections among resilience factors. Thus, this study makes two main contributions. Firstly, it investigates the key determinants shaping the resilience capacity of households, which influence their ability to adapt and recover from adverse conditions. Second, it assesses how resilience capacity translates into improved food security outcomes. By situating this analysis within the broader context of rural livelihoods, the study aims to provide empirical insights into the mechanisms through which resilience serves as a buffer against food insecurity, offering implications for policy interventions aimed at fostering sustainable rural development. When attempting to understand and enhance resilience, it is crucial to provide necessary resources and techniques to mitigate adverse outcomes, predict and prepare for shocks and stress, eliminate destructive coping mechanisms, and facilitate recovery [49]. Resilience in environmental and socioeconomic systems is demonstrated by responses to negative events and shocks [21], which encompass transformation, adaptation, and absorption responses [50–53]. Resilience, viewed as an intermediate outcome in socioeconomic studies, integrates capacities such as a household’s or community’s ability to cope with shocks [53]. Recent studies have endeavoured to integrate resilience into various development contexts, including food safety [22,54], well-being [55,56], rural development [57–59], and sustainable livelihoods [60–62]. The articles by [32,33] represent early efforts to define and quantify household resilience in the context of food insecurity. These studies focused on the household as the primary unit of analysis, recognising that crucial decisions regarding risk management made at this level, both before and after adverse events impacting food security [33] introduced a resilience index, treated as a latent variable that cannot be directly observed, and employed a two-stage factor analysis approach using measurable variables. Building on this framework [63] applied a similar latent variable model to measure resilience in Northern Bangladesh, highlighting the importance of comprehensively understanding resilience factors across various capacities. Additionally, d’Errico et al. investigated how shocks, particularly conflicts and climate events, influence resilience measurement [24]. Similarly, Ciani et al. aimed to assess household resilience to food insecurity in a changing environment using principal components and multivariate analysis of panel data. Their resilience index indicated that small landowners and agricultural wage workers exhibited lower resilience levels compared to other livelihood groups [64]. Dhraief et al. conducted a cross-sectional survey employing factor analysis and regression models. Their findings highlighted the significant roles of income and food access, adaptive capacity, and social safety nets in enhancing household resilience, as evidenced by their positive correlations with the resilience index [65]. Conversely, asset possession and climate change showed negative associations with household resilience in their study. However, contrasting findings in other studies have indicated that assets can have positive and notable effects [49,66,67]. Rural households generally exhibit less resilient to food shortages compared to urban and peri-urban households [68]. However, during the COVID-19 lockdowns, urban households experienced higher levels of food insecurity than rural households, yet they were more likely to resort to financial and food-compromising coping strategies [48]. Sassi et al. argued that households often turn to less preferred and cheaper food options during times of food insecurity, which can lead to long-term societal impacts and reduced ability to cope [69]. Large family households are particularly vulnerable to food insecurity, often relying on lower-quality meals and support from family or friends [70]. Ownership of assets also plays a crucial role in the resilience of smallholder farmers, with households possessing more assets generally exhibiting greater resilience to food insecurity [66]. Food-secure households typically exhibit higher dietary diversity and rely less on coping mechanisms, benefitting from adaptive and absorptive resilience capacities along with sufficient income [71]. Previous studies have highlighted that household resilience to food insecurity is influenced by factors such as income, assets, social safety nets, and adaptive capacity, with resilience levels varying across different livelihood groups and geographic locations [32,34,72,73]. However, findings on the role of assets remain inconclusive, calling for a more nuanced approach that differentiates asset types to better capture their distinct contributions to resilience. Despite the growing use of resilience measurement frameworks, studies in Bangladesh have yet to adopt a comprehensive approach that captures the interplay between various resilience components. Although prior research has identified the resilience determinants [48,63,74], few studies have specifically examined the suitability of RIMA-II for the Bangladeshi context [75]. Traditional vulnerability assessments often fail to account for critical dimensions such as access to services, adaptive strategies, and the differential roles of asset types as drivers of resilience. This study addresses these gaps by employing RIMA-II to systematically measure resilience as a latent construct, offering a more integrated and nuanced approach compared to earlier methods in rural Bangladesh. A key contribution of this research is the disaggregation of asset components, land, livestock, and farm equipment, to assess their distinct effects on resilience. This disaggregation is essential, as each asset type contributes differently to resilience in rural Bangladesh. Land ownership is vital for food production and long-term economic stability [76–78], while livestock assets, despite their perceived benefits, introduce risks related to market volatility [79–81], disease outbreaks [82–88], and climate vulnerability [89–92]. Farm equipment, on the other hand, plays a key role in mechanization and agricultural efficiency, making it distinct from the other asset categories [93–97]. Treating assets as a single, aggregate variable would overlook these critical differences in how they contribute to or detract from resilience. This approach ensures a more accurate and policy-relevant analysis of household resilience dynamics in Bangladesh. Furthermore, the use of Structural Equation Modeling (SEM) to validate resilience determinants has been rare in prior research, emphasizing the need for this methodological advancement. By combining RIMA-II with a robust empirical framework, this study advances the understanding of resilience pathways and generates policy-relevant insights for improving food security in Bangladesh. It is increasingly recognised that one of the most effective ways to reduce or even avert food security crises is to create more resilient livelihoods [24,49]. The Resilience Index Measurement and Analysis (RIMA) was developed within this framework, with pioneering contributions by the FAO [34]. Employing a novel quantitative methodology, RIMA helps explain why and how certain households can withstand shocks and stressors more effectively than others. Resilience, being a multidimensional concept that cannot be directly observed, must be assessed through proxies by focusing on the needs of the most vulnerable groups. The new RIMA-II methodology offers enhanced guidance for successfully planning, implementing, overseeing, and assessing aid. The RIMA II model was employed to identify and study household resilience [36]. The RIMA-II model enables a better understanding of households` circumstances and needs, as well as identifying potential areas for improving future assistance. Access to Basic Services (ABS), Assets (AST), and Adaptive Capacity (AC) are the widely recognised pillars of the Resilience Capacity Index (RCI). Due to the lack of available data on Social Safety Nets (SSNs), our study focuses on the other three pillars. To estimate each pillar, variables were considered based on a literature review, data availability, context analysis, and statistical properties. ABS refers to the ability to meet households’ basic needs and encompasses the quality, effectiveness, and access to basic services [98]. This study assesses ABS through the following components: floor type (X1), wall type (X2), housing condition (X3), roof type (X4), sanitation type (X5), source of water (X6), cooking fuel type (X7), distance to health facilities from residence (in kilometers) (X8) and distance to the market (in kilometers) (X9). AC refers to a household’s ability to cope with and adapt to an unfamiliar situation, enabling them to maintain everyday affairs without long-term disruptions [32,36]. It indicates that more adaptive residents can handle adverse situations without reducing their standard of living [49,99]. In this study, this pillar includes the following components: household head`s employment location (X10), household head`s occupation type (X11), savings (X12), household income per month (X13), household head`s education year (X14), and diversified sources of income (X15). An essential prerequisite for shock response is AST, the second pillar of the RIMA-II model [32]. Assets encompass a household’s income-producing and not-producing assets [98], serving as indicators of the household’s economic circumstances and providing insight into the impact of shocks on households. The proxies used to construct AST include land holding (in decimal) (X16), land value (X17), livestock value (X18), livestock quantity (X19), farm equipment value (X20), farm equipment quantity (X21), and non-agricultural asset value (X22). The RIMA-II is divided into two sections: a descriptive and a causal analysis [98]. The RCI used to assess households and inform policy, is generated through descriptive analysis. The RCI is constructed in two stages. The first stage, the formative model, employs factor analysis of observable variables to estimate attributes (pillars) of resilience. The second stage, the measurement model, utilizes a Multiple Indicators Multiple Causes (MIMIC) model to calculate the RCI based on these pillars while considering the relationships between RCI and food security indicators. The developmental model links resilience to the pillars, while the measurement model focuses on food security indicators such as the Food Consumption Score (FCS) and Food Expenditure (FX). Food Consumption Score (FCS) collects household-level data on the diversity and frequency of food groups consumed over seven days. This data is weighted according to the relative nutritional value of each food group. The main food groups and their respective weights are: staples (weight 2), pulses (weight 3), vegetables (weight 1), fruit (weight 1), meat/fish (weight 4), milk (weight 4), sugar (weight 0.5) and oil (weight 0.5). To calculate FCS, the first step is to sum the consumption frequencies of food items within each group. Next, each food group`s total is multiplied by its respective weight [100]. Food Expenditure (FX) refers to the total cost of food obtained, whether purchased or acquired through other means, including both alcoholic and non-alcoholic beverages. This calculation also encompasses food expenditures on dining out at venues such as bars, restaurants, food courts, workplace canteens and street vendors [101]. FX is calculated by multiplying the quantity of food consumed over a seven-day period by its unit price, expressed in Bangladeshi Tk (1 US$ = 110 BDT (Bangladesh Bank, 2024). Based on the literature and the conceptual framework depicted in Fig 1, observable variables are expected to significantly influence latent variables or pillars. Furthermore, these latent variables are expected to have a significant impact on the RCI. Finally, the RCI is expected to significantly influence food security indicators, specifically, FCS and FX. Previous studies [14,15,19,20,25,29,41–43,76–79] have consistently highlighted the substantial influence of various observable variables on latent pillars, the significant impact of latent pillars on RCI, and the consequential effect of RCI on food security. Consequently, this study proposes the following hypotheses: https://doi.org/10.1371/journal.pone.0332868.g001 The observed variables (X1, X2,.. Xn) used in this study were derived from the Bangladesh Integrated Household Survey (BIHS) 2018. This household survey is administered by Data Analysis and Technical Assistance Limited and designed and supervised by the International Food Policy Research Institute (IFPRI). The BIHS sample is statistically representative of rural Bangladesh, covering the nation`s seven administrative divisions: Barisal, Chittagong, Dhaka, Khulna, Rajshahi, Rangpur, and Sylhet. The survey includes 325 primary sampling units (PSUs) or villages, encompassing a total of 5,604 households in the BIHS dataset. Therefore, this study utilises a cross-sectional dataset of 5,604 households to achieve its research objectives. This dataset serves as the basis for assessing resilience through the pillars of ABS, AC, and AST, and their impact on the RCI and food security indicators. Twenty-two measurement items were used to construct latent pillars of the RCI, which is presented in Appendix Table 1 in S1 File. Each observable variable was assessed using a five-point summated Likert scale, where respondents rated their conditions (1 = very poor; 2 = poor; 3 = moderate; 4 = rich; 5 = very rich). The study utilized a quantitative approach. Initially, descriptive statistics were used to summarize and describe the observable variables of the pillars related to the pillars of resilience and food security indicators. This involved calculating measures such as means, standard deviations, and frequency distributions to provide a clear picture of the characteristics of the data. Statistical factorial analyses were employed to test the study’s hypotheses. To assess the factorial validity of the constructs and explore the latent pillars in the data, exploratory factor analysis was performed following [102]. Given the interconnections among latent constructs, we allowed for inter-factor correlation using the maximum likelihood factor extraction method with orthogonal varimax rotation. Subsequently, structural equation modeling (SEM) was exclusively used for descriptive purposes to understand the explanatory power of the pillars and their relationships with the RCI. This approach is a powerful statistical technique that allows researchers to test complex relationships among variables. It was used in this study to examine the linear correlations between the underlying factors (in this case, the pillars) and the latent variable RCI [103]. The formative model, represented by structural Equation (1), links resilience (η) to the pillars (xi). In contrast, the measurement model, represented by Equation (2), is defined by the Food Consumption Score (FCS) and household food expenditure (FX). The upper portion of the path diagram (Fig 2) illustrates the relationship between the resilience pillars, as measured by error εR, and the disturbance term of the structural estimation. The measuring model is represented by arrows pointing toward the food consumption Score and the food expenditure in the lower half of the structural model. Since the estimated RCI lacks a specific measurement scale, a scale has been proposed with the coefficient of the food consumption loading (˄1) set to 1. This assumption posits that a one standard deviation increase in RCI corresponds to a one standard deviation increase in food intake. This determination also establishes the unit of measurement for the other outcome indicator (˄2), alongside the variance of the two food security indicators, as depicted in Equations (3) and (4): https://doi.org/10.1371/journal.pone.0332868.g002 The study was conducted in accordance with the Declaration of Helsinki. It utilized publicly available data from the Bangladesh Integrated Household Survey (BIHS), administered by the International Food Policy Research Institute (IFPRI). The BIHS obtained formal written informed consent from all respondents prior to data collection. The summary statistics of all the attributes used to construct the resilience pillars are presented in Appendix Table 2 in S1 File. According to the established criteria [102,104], data is considered normal if kurtosis falls between −7 and +7 and the skewness between −2 and +2. However, our results indicate that some attribute values exceed these thresholds, suggesting potential outliers that may be excluded from further analysis. Cronbach’s alpha assesses with which respondents answer items on a scale or subscale [105]. A Cronbach’s alpha score greater than 0.6 is generally considered reliable with values between 0.6 to 0.8 deemed acceptable [106,107]. The reliability alpha value for the adaptive capacity (AC) pillar significantly exceeded the threshold of 0.70, indicating adequate reliability [105]. The other four pillars also demonstrated reliability, with Cronbach’s alpha values greater than 0.60 [106–108]. Following the initial reliability assessment, Exploratory Factor Analysis (EFA) was employed to explore the latent factor structure. The maximum likelihood method was used to identify components and establish the pattern matrix. The Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy was 0.778, surpassing the maximum acceptable level of 0.5 [109]. The criteria, which include a significant correlation matrix determinant of 0.009, a KMO value of 0.778, and a significant Bartlett’s Chi-square of χ2 = 26648.163, p = 0.000, confirmed the adequacy of the sample size chosen for the study [110], indicating sufficient intercorrelation for factorial analysis. Initially identifying ten factors, we retained five factors that collectively explained 82.68% of the total variance in the data, exceeding the acceptable threshold of 60% [102]. Table 1 presents the pattern matrix, demonstrating how each numbered questionnaire item loaded precisely (values above 0.50 on a single component) onto its respective factor. Typically, minimum item-factor loadings of 0.3 to 0.4 are recommended [111–113]. https://doi.org/10.1371/journal.pone.0332868.t001 Table 1 categorizes items X1(floor type), X2 (wall type), and X3 (housing condition) under factor 1 as ABS, explaining 29.74% of the total variance. Factor 2, labelled AC, includes X10 (household head`s employment location) and X11 (household head`s occupation type), explaining 23.49% of the variance. Factors 3 and 4 are labelled Ld_A and Lv_A, respectively, and explain 11.09% and 10.25% of the variance. They comprise items X16 (land size), X17 (land value), X18 (livestock value), and X19 (livestock quantity). Item X20 (farm equipment value) and X21 (farm equipment quantity) of factor 5 are labelled FE. Although factors Ld_A, Lv_A, and FE collectively represent the single factor named Assets (AST), they are labelled as separate factors based on our country’s perspective. The loadings of the items range from 0.51 to 0.92. Table 2 presents the correlation matrix of the five factors included in the analysis. ABS shows significant positive correlations with Ld_A (0.042) and FE (0.034) at the 1% significance level, as these factors contribute to higher income levels that enable households to afford better services. ABS also negatively correlates with AC (0.028) and Lv_A (0.026) at the 5% and 10% significance levels, respectively, possibly due to market volatility and health-related concerns affecting livestock income and service accessibility. AC demonstrates a significant positive correlation with Ld_A (0.023) and Lv_A (0.035) at the 10% and 1% significance levels, respectively, but shows an insignificant correlation with FE (0.017). Ld_A shows a significant positive correlation with both Lv_A (0.047) and FE (0.072) at the 1% level. Lv_A and FE correlate significantly at the 5% level (0.030). Despite the correlations observed among factors, there is no evidence of significant multicollinearity, as no correlation coefficient exceeds 0.8 [114–116]. https://doi.org/10.1371/journal.pone.0332868.t002 Table 3 presents the findings from the multiple indicators multiple causes (MIMIC) model, which was derived using structural equation modelling. In this case, we consider the broader contextual framework to accurately analyze the 2018 data, which includes ongoing food insecurity, a substantial influx of Rohingya refugees facing acute malnutrition amidst monsoon-related hazards, and deep-rooted structural vulnerabilities such as limited agricultural diversification. These factors influenced both households` potential for resilience and the outcomes observed in our SEM model. Study shows that ABS exhibits a significant coefficient of 0.429, indicating that a well-structured living condition with essential facilities enhances households’ resilience capacity. This finding also indicates that households with better access to essential services are more equipped to cope with food insecurity and recover from shocks. Thus, the study’s findings support hypothesis A: access to basic services significantly influences the resilience capacity index. Our results reveal a positive correlation between ABS and RCI (Table 3), consistent with previous studies [32,36,117,118], which suggest that access to basic services enhances economic opportunities, knowledge, and overall well-being, thereby contributing to greater resilience. In contrast, studies by [53,66,119,120] report a negative association, attributing decreasing service quality as households become poorer in their respective countries. Previous studies have often identified AC as a significant influencing factor [24,65,67,121,117]. However, our findings indicate that AC is a positive but insignificant factor, suggesting that institutional and economic barriers impede the effectiveness of this factor. While education, skill development, access to information, and access to credit are critical components of AC, their impact on resilience is highly dependent on external factors such as labor market conditions and institutional access [122,123]. This discrepancy may be attributed to Bangladesh`s high vulnerability to global changes, significant socio-economic challenges, and limitations in institutional capacity [124–126]. In Bangladesh, people are unable to leverage their adaptive skills for resilience due to a lack of employment opportunities [119,125,127,128]. Moreover, limited access to credit, and financial services may restrict households from transferring their adaptive strategies to long-term stability [43,125,128]. Furthermore, gender-based disparities in access to resources and decision-making power also undermine the significance of AC [54,129–133]. As a result, households in Bangladesh may exhibit lower adaptive capacity compared to those in other developing or developed countries [43,47,134,135]. Thus, our results do not support hypothesis B: adaptive capacity significantly influences the resilience capacity index. The “Resilience Index Measurement and Analysis II” report by the Food and Agriculture Organization (FAO) discusses how adaptive capacity contributes to resilience but notes that its impact may not always be significant in certain contexts [36]. https://doi.org/10.1371/journal.pone.0332868.t003 The critical role of assets in resilience capacity has been widely recognised, as they constitute the most significant pillar for the RCI. Similar findings have been reported by other authors, such as [49,66,67,119] in the context of Malian and Ethiopian households, highlighting assets as the most pertinent factor for RCI, which aligns with our study findings. However, this study takes a more granular approach to segregating the assets into three distinct categories; land asset, livestock asset, and farm equipment, to identify their unique contributions to resilience building. This disaggregation is crucial, as each asset type plays a unique role in shaping a household`s ability to withstand and recover from food insecurity shocks. In the context of Bangladesh, only a small proportion of households own all three asset types of assets simultaneously, making it essential to analyse their individual and combined impacts on resilience. Our results demonstrate a significant coefficient of 0.119 for Ld_A and 0.042 for FE, indicating a positive influence on RCI, thereby corroborating hypotheses C and E. Land assets are crucial for resilience building as they provide financial stability and income through agriculture [76–78,136]. Additionally, having farm equipment ensures household mechanization and agricultural efficiency, which boosts farm production and enhances household resilience [93–97]. Since these assets play distinct roles in shaping resilience capacity, it is essential to calculate them separately. However, Lv_A demonstrates a significant negative value of 0.053 suggesting that it has a significant negative impact on resilience in Bangladesh. Because it has a high mortality rate of livestock due to natural disasters and high disease outbreaks [137,138]. High vulnerability to climate change may, directly and indirectly, affect livestock production systems, resulting in negative asset values [47,89–92,139–142]. Lv_A are influenced by several interrelated factors that exhibit a counterintuitive negative relationship with resilience, such as market volatility, disease outbreaks, high maintenance cost, and climatic vulnerability. During economic downturns or supply chain disruptions, farmers are forced to sell their livestock at lower prices, undermining their financial stability [79–81]. Disease outbreaks, such as foot-and-mouth disease or avian influenza, frequently affect livestock in Bangladesh, resulting in significant economic losses for rural households. Limited access to veterinary care and inadequate disease prevention measures exacerbate this issue [82–88]. Livestock also involves costly sectors such as feeding, healthcare, and shelter, imposing additional financial burdens on households. While land or farm equipment offers long-term productivity gains, livestock requires continuous investment, making it a riskier asset for resilience building. These combined factors suggest that although livestock is traditionally seen as a liquid asset, it needs more infrastructural development to contribute to long-term resilience in the Bangladeshi context. Consequently, Lv_A cannot verify hypothesis D, which suggests that livestock assets positively influence resilience capacity. Our study indicates that RCI positively influences FCS and FX with significant coefficients of 0.778 and 0.688, respectively. That means households with higher resilience are less likely to experience deteriorating food security and are better positioned to maintain or restore food consumption and expenditure [24]. Policymakers should prioritize resilience-building as a core component of food security programming, employing multidimensional frameworks such as RIMA-II in Bangladesh for targeted, long-term strategies. Our results support hypotheses F and G, which concern the impact of resilience on food consumption and expenditure. These outcomes remain robust across various model specifications. The results presented in Table 4 indicate that the model under consideration exhibits a significant likelihood ratio Chi-square of 1200.82 at the 1% significance level. The first chi-square compares the model against the saturated test, while the second chi-square compares the baseline versus the saturated model. The saturated model demonstrates a perfect fit for the covariance structure. According to Browne et al., the root mean squared error of approximation (RMSEA) indicates an adequate fit with values between 0.05 and 0.08 [143]. A value close to one on both the Tucker-Lewis index (TLI) and the comparative fit index (CFI) denotes a good fit, as suggested by Hair et al. and Bentler [102,144]. The coefficient of determination (CD = 1.00) also confirms a perfect fit. Typically, for an adequate fit, the standardised root mean square residual (SRMR) should be less than 0.05, as recommended by Hu and Bentler [145]. https://doi.org/10.1371/journal.pone.0332868.t004 Our study results demonstrate that access to basic services, land assets, and farm equipment significantly enhances household resilience. The positive impact of these factors on RCI suggests promising avenues for further research. Future studies could delve into how these components bolster agricultural system resilience, aiding government efforts in designing effective action plans. Exploring potential interactions among these factors could yield insights into the intricate dynamics of resilience. Interestingly, adaptive capacity showed no noticeable impact on RCI, highlighting the distinction between adaptation and resilience. Although our study indicates that livestock has a negative impact on resilience, it remains an invaluable resource for food security, income generation, and financial stability in rural economies. Livestock plays a crucial role in resilience management by acting as a liquid asset during economic crises, enhancing agricultural output through soil fertility and draft power, and improving access to financing. Building resilience remains the most effective strategy to withstand shocks. In developing countries, policies and programs aimed at improving household resilience can significantly advance long-term food security. Humanitarian resilience initiatives should prioritize addressing rural populations` needs, as they face heightened exposure to food insecurity. Study results indicate that prioritising access to basic services such as healthcare, education, and sanitation in rural areas is crucial for fostering local and national resilience. Furthermore, enhancing incentives for land protection and promoting sustainable land management practices can enhance agricultural resilience. Policies supporting agricultural technology and equipment can also bolster resilience. Implementing social safety nets and encouraging farmers to diversify their revenue streams are effective strategies beyond human-centric protection measures. To enhance the sustainability of livestock assets, policymakers should focus on improving the affordability and accessibility of veterinary services and climate-resilient livestock management at the grassroots level. Subsidies on livestock medicine, feed, and improved breeds should also be provided to reduce maintenance costs. Strengthening adaptive capacity requires skill-based education, financial inclusion, and institutional support to foster entrepreneurship and human capital development, thereby creating more employment opportunities and improving infrastructure. Additionally, targeted awareness programs should encourage women’s participation in household and community decision-making, ensuring gender-inclusive resilience-building strategies. However, the data used in our analysis were not explicitly gathered for resilience measurement purposes, which may limit the depth and accuracy of our study. Using cross-sectional data limits the ability to capture dynamic changes in household resilience over time, as it provides only a snapshot rather than reflecting long-term trends. Thereby restricts our ability to draw definitive conclusions about long-term resilience to food insecurity. It is important to acknowledge that, while this study emphasizes the role of land access and basic services in strengthening household resilience, social capital, gender dynamics, and power disparities are also significant, though less visible, factors influencing resilience in rural Bangladesh. These factors could not be explicitly incorporated into the SEM analysis due to dataset constraints. Furthermore, the absence of data related to recent shocks, disasters, or conflicts in our analysis limits the comprehensiveness of our study. These data constraints hinder our ability to examine the impact of acute events on household resilience, potentially overlooking critical dimensions of resilience dynamics. Future research should utilize longitudinal datasets specifically designed for resilience measurement, incorporating multiple survey rounds to capture temporal dynamics. Additionally, integrating socio-institutional variables with real-time data on shocks, disasters, and conflicts will enhance the comprehensiveness of resilience analysis, enabling a more accurate assessment of household responses to food insecurity. Appendix Table 1. Description of the variables. Appendix Table 2. Descriptive statistics. https://doi.org/10.1371/journal.pone.0332868.s001 (DOCX)
--------------------------------------------------

Title: US stock futures dip as Fed speeches and inflation data loom; Gold hits record $3,750, crypto plunges, Trump’s new H-1B visa fee jolts Indian IT stocks
URL: https://economictimes.indiatimes.com/news/international/us/us-stock-futures-dip-as-fed-speeches-and-inflation-data-loom-gold-hits-record-3750-crypto-plunges-trumps-new-h-1b-visa-fee-jolts-indian-it-stocks/articleshow/124048584.cms
Time Published: 2025-09-22T12:32:18Z
Full Content:
US stock futures fell as traders wait for key inflation data and comments from Federal Reserve officials. Gold touched new highs while bitcoin and other crypto dropped. Markets are also watching Trump’s new visa fee plan, earnings reports, and global stock moves. Investors want clues about more US interest rate cuts. Dow Jones, S&P 500, and Nasdaq futures fall as Wall Street watches Federal Reserve rate cut signals and key US inflation report. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Explore More Stories Canada bars Irish rap band Kneecap from entry over alleged terrorism support Bank of Canada cuts interest rate to 2.5% as GDP shrinks, job market softens Beverly Thomson, veteran CTV journalist and Canada AM co-host, dies at 61 Seth Rogen wins Emmy Award for best lead actor in comedy series 'The Studio' Zach Bryan confronts Gavin Adcock at Oklahoma music festival after country singer criticism Weekend streaming guide Canada: top 10 Netflix, Crave, Disney+ and Apple TV+ shows September 2025 Canada pension and retirement benefits 2025: up to $2,461 in federal and provincial payments through December Molly McCann wins professional boxing debut despite opponent's illegal kick How Americans and a fake Fox News account accused Toronto retiree Michael Mallinson of assassinating Charlie Kirk Canadian bread price fixing settlement opens: $500m available for claims Canadian soldier found dead in Latvia during NATO operation reassurance deployment Mexico vs Japan international soccer: live stream, TV schedule and match preview Trump denies knowledge of reported SEAL Team 6 mission in North Korea that killed civilians GST reforms: Congress calls them ‘inadequate, delayed & diversionary’ IIT Madras Director thanks Trump for H-1B fee hike Erika Kirk forgives Tyler Robinson Assam bids tearful farewell to Zubeen; samadhi to be built in Jorhat Alabama launches state-federal checkpoint op with ICE Viksit Bharat, Aatmanirbhar Bharat: PM calls for adopting swadeshi Marco Rubio’s emotional words resonate at Kirk memorial 'Masood Azhar’s family torn...,'Rajnath Singh recalls Pahalgam attack Defense Secy Hegseth pays emotional tribute to Charlie Kirk Trump full speech at Charlie Kirk memorial GST reforms: Congress calls them ‘inadequate, delayed & diversionary’ IIT Madras Director thanks Trump for H-1B fee hike Erika Kirk forgives Tyler Robinson Assam bids tearful farewell to Zubeen; samadhi to be built in Jorhat Alabama launches state-federal checkpoint op with ICE Viksit Bharat, Aatmanirbhar Bharat: PM calls for adopting swadeshi Marco Rubio’s emotional words resonate at Kirk memorial 'Masood Azhar’s family torn...,'Rajnath Singh recalls Pahalgam attack Defense Secy Hegseth pays emotional tribute to Charlie Kirk Trump full speech at Charlie Kirk memorial Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Prime Articles Top Slideshow Top Commodities Private Companies Top Story Listing Top Definitions Most Searched IFSC Codes Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Forbes Daily: The Trump Family’s White House Windfall
URL: https://www.forbes.com/sites/daniellechemtob/2025/09/22/forbes-daily-the-trump-familys-white-house-windfall/
Time Published: 2025-09-22T12:03:13Z
Full Content:
ByDanielle Chemtob, Forbes Staff andForbes Daily, Forbes Staff. For fans of the Savannah Bananas, it’s about more than just baseball. In a more fast-paced version of the sport known as Banana Ball, players backflip as they catch fly balls, take choreographed dance breaks between batters and more. The Bananas have sold out all 115 games on their 2025 schedule, and are even more popular online, with a larger social audience than any MLB franchise. The unusual approach is paying off: Forbes estimates that the four-team Banana Ball organization will post more than $100 million in revenue this year, and that the Bananas are worth somewhere in the neighborhood of $500 million. That’s about half as much as MLB’s least valuable team, the Miami Marlins. President Donald Trump issued a proclamation Friday that would impose an annual $100,000 fee on H-1B visa applications, which are used by companies to hire skilled workers from abroad. It’s a dramatic increase over existing program fees, which include $215 to register for the lottery and $780 for the petition. MORE: Shares of major Indian tech services firms plunged sharply on Monday in response, and U.S. tech giants Google, Meta and Microsoft, which are among the biggest sponsors of H-1B visas for foreign workers, were also down in premarket trading. Around 73% of all H-1B workers whose applications were approved in the fiscal year 2023 were Indian born, the Pew Research Center reported, while China ranked a distant second with 12% approvals. The decision to suspend Jimmy Kimmel’s late-night show put a spotlight on Nexstar Media, the largest TV station owner and operator in the U.S., and its founder Perry Sook, who has largely stayed out of the spotlight over nearly four decades as chairman. Sook, who was compensated roughly $35 million as Nexstar’s chief executive in 2024, told investors in an earnings call shortly after Election Day that he hoped a second Trump Administration would help the firm expand further. President Donald Trump said Chinese President Xi Jinping approved the deal that would keep TikTok online in the U.S., and the White House said on Saturday the deal would be signed “in the coming days.” Six of the seven board seats for TikTok’s U.S. entity will be occupied by Americans, who will also oversee the platform’s algorithm, and Oracle, the cloud computing firm headed by billionaire Larry Ellison, will lead TikTok’s “data and privacy.” Elon Musk’s artificial intelligence firm xAI is expected to raise $10 billion in a funding round that would give the company a $200 billion valuation, CNBC reported. That would make xAI worth more than competitor Anthropic’s $183 billion, but less than ChatGPT maker OpenAI, which is eyeing a secondary share selloff that could value it at $500 billion. Private equity firm Veritas Capital counts on government spending for the majority of its portfolio companies’ revenue, and CEO Ramzi Musallam remains confident about its growth despite cuts from the Trump Administration’s Department of Government Efficiency. Veritas closed its ninth flagship fund at $14.4 billion earlier this month, and Musallam says the firm is focused on companies with “must-have” technology that reduces wasteful spending. No U.S. president, or extended family, has used the office to profit as successfully as the Trumps. With crypto as the big driver, the family (including the president’s son-in-law, Jared Kushner) is now worth an estimated $10 billion, having nearly doubled their net worth since last year’s election. As a cofounder of 3BRAND, New York Giants quarterback Russell Wilson has quietly built a children’s sportswear company that surpassed $100 million in sales in 2024, despite headwinds in the retail market. It’s one of many business endeavors for Wilson, who tied for No. 49 on Forbes’ 2025 list of the world’s highest-paid athletes. Hundreds of airports may no longer require you to remove liquids from bags during security screenings, but it’s too soon for the policy to officially change nationwide, aviation security experts say. The latest X-ray technology can create three-dimensional images of passengers’ carry-on bags, but it could take at least a decade before all U.S. airports have the newer scanners. GPUs (graphics processing units), commonly designed by chip giant Nvidia, have become a crucial engine for artificial intelligence. Access to them can determine a company’s fortunes, and CoreWeave, which trades on the Nasdaq, has built a $50 billion (market cap) business by becoming one of their go-to brokers. After pioneering a new kind of financing that helped CoreWeave borrow $29 billion, CEO Michael Intrator now balances shareholders’ qualms about deepening debt and widespread fears about an AI bubble with his broader ambition: to provide compute not just to the tech giants but to every company integrating AI into its operations. In less than a decade, the business has spun up a data center empire, booking $1.9 billion in revenue in 2024 (net losses: $860 million, for a negative 45% margin) and $2.2 billion in the first half of 2025 (net losses: $605 million, or –28%). Its stock price has more than doubled since its March IPO, trouncing the broader Nasdaq’s return of 23.9% and minting at least five billionaires in the process. Intrator is now worth $6.7 billion, landing him on The Forbes 400 list of the richest Americans for the first time, along with cofounder Brian Venturo. The company’s remarkable growth has stemmed from ferocious global demand for GPUs. The AI cloud computing market was $230 billion in 2024 and, assuming the boom continues, is poised to explode to $400 billion by 2028, according to Wall Street analysts. “I’ve never seen a supply/demand imbalance like this in the 40 years I’ve been investing,” says Mark Klein, CEO of SuRo Capital, which invested a total of $25 million in CoreWeave. WHY IT MATTERS “CoreWeave has catapulted into prominence as one of the leading AI cloud compute providers with marquee customers like OpenAI, Microsoft and Meta,” says Forbes staff writer Rashi Shrivastava. “Shedding its crypto past and betting big on the value of Nvidia’s coveted GPUs, Michael Intrator took his company to a $50 billion valuation. But he relied (heavily) on debt, backed by those very GPUs that are depreciating a lot faster today. With billions on the line and concerns of an AI bubble, the stakes are far greater than ever before.” MORE This AI Cloud Provider Just Minted Four New Billionaires Workplaces often run on ambition—but many overachievers burn themselves out as a result of never feeling like they’re allowed to take a break. It’s critical to normalize taking strategic pauses for reflection after major sprints, and don’t just reward those who keep up the highest output for the longest. Ambition can be energizing and fulfilling if pursued sustainably. A federal judge threw out President Donald Trump’s $15 billion lawsuit against the New York Times on Friday. What was the reason? A. It didn’t have merit B. Insufficient evidence C. Statute of limitations D. It was too long Check your answer. Thanks for reading! This edition of Forbes Daily was edited by Sarah Whitmire and Chris Dobstaff.
--------------------------------------------------

Title: Google faces antitrust déjà vu as US seeks to break up digital advertising business
URL: https://abcnews.go.com/Business/wireStory/google-faces-antitrust-dj-vu-us-seeks-break-125809197
Time Published: 2025-09-22T11:30:00Z
Full Content:
After deflecting the U.S. Justice Department’s attack on its illegal monopoly in online search, Google is facing another attempt to dismantle its internet empire during a trial focused on its abusive tactics in digital advertising After deflecting the U.S. Justice Department's attack on its illegal monopoly in online search, Google is facing another attempt to dismantle its internet empire in a trial focused on its abusive tactics in digital advertising. The trial scheduled to begin Monday in an Alexandria, Virginia, federal court will revolve around the harmful conduct that resulted in U.S. District Judge Leonie Brinkema declaring parts of Google's digital advertising technology to be an illegal monopoly. The judge found that Google has been engaging in behavior that stifles competition to the detriment of online publishers that depend on the system for revenue. Google and the Justice Department will spend the next two weeks in court presenting evidence in a “remedy” trial that will culminate in Brinkema issuing a ruling on how to restore fair market conditions. Although the judge hasn't set a timetable for making that decision, it's unlikely to come down before the end of this year because additional legal briefs and courtroom arguments are expected to extend into November before Brinkema takes the matter under submission. No matter how the judge rules, Google says it will appeal the earlier decision labeling the ad network as a monopoly. Appeals can't be filed until the remedy is determined. The case, filed in 2023 under President Joe Biden's administration, threatens the complex network that Google has spent the past 17 years building to power its dominant digital advertising business. Besides accounting for most of the $305 billion in revenue that Google's services division generates for its corporate parent Alphabet Inc., digital advertising sales provide the lifeblood that keeps thousands of websites alive. If the Justice Department gets its way, Brinkema will order Google to sell parts of its ad technology — a proposal that the company's lawyers warned would “invite disruption and damage” to consumers and the internet's ecosystem. The Justice Department contends a breakup would be the most effective and quickest way to undercut a monopoly that has been stifling competition and innovation for years. Google believes it has already made enough changes to its “Ad Manager” system, including providing more options and pricing options, to resolve the issues the Brinkema flagged in her monopoly ruling., The legal battle over Google's advertising technology mirrors another showdown that the company recently navigated after another federal judge condemned its dominant search engine as an illegal monopoly and then held remedy hearings earlier this year to consider how to stop the misconduct. In that case, the Justice Department also proposed a severe crackdown that would have required Google to sell its popular Chrome browser, but U.S. District Judge Amit Mehta decided a less dramatic shake-up was needed amid a search market being reshaped by artificial intelligence technology in a decision issued earlier this month. Even though Google didn't agree with all aspects of Mehta's decision, the ruling was widely seen as a slap on the wrist — a sentiment that has helped propel Alphabet's stock price to new highs. The 20% gain since Mehta's decision helped make Alphabet only the fourth publicly traded company to reach a market value of $3 trillion — an increase of more than $1 trillion since Brinkema branded Google's ad technology as a monopoly in April. In an indication that the outcome of the search monopoly case might sway things in the advertising technology proceedings, Brinkema asked both Google and the Justice Department to address Mehta's decision during the upcoming trial. As they did in the search case, Google's lawyers already have been asserting in court papers that AI technology being used by ad network rivals like Meta Platforms is reshaping the way the market works and overriding the need for the Justice Department's “radical” proposals. The Justice Department is “fighting for a remedy that would vanquish a past that has been overtaken by technological and market transformations in the way digital ads are consumed," Google's lawyers argued leading up to the trial. 24/7 coverage of breaking news and live events
--------------------------------------------------

Title: Google faces antitrust deja vu as US seeks to break up its digital advertising business
URL: https://economictimes.indiatimes.com/tech/technology/google-faces-antitrust-deja-vu-as-us-seeks-to-break-up-its-digital-advertising-business/articleshow/124047057.cms
Time Published: 2025-09-22T11:20:34Z
Full Content:
(Catch all the Budget 2024 News, Events and Latest News Updates on The Economic Times.) ...more Popular Categories Hot on Web In Case you missed it Top Searched Companies Other useful Links Top Calculators Top Definitions Top Commodities Top Prime Articles Top Story Listing Top Slideshow Private Companies Top Market Pages Latest News follow us on Download ET App:
--------------------------------------------------

Title: Google faces antitrust déjà vu as US seeks to break up its digital advertising business
URL: https://finance.yahoo.com/news/google-faces-antitrust-d-j-111855450.html
Time Published: 2025-09-22T11:18:55Z
Description: After deflecting the U.S. Justice Department's attack on its illegal monopoly in online search, Google is facing another attempt to dismantle its internet...
--------------------------------------------------

Title: CoreWeave’s $29 Billion Bet That Its Debt-Fueled AI Boom Won’t Go Bust
URL: https://www.forbes.com/sites/rashishrivastava/2025/09/22/coreweaves-29-billion-bet-that-its-debt-fueled-ai-boom-wont-go-bust/
Time Published: 2025-09-22T10:30:00Z
Full Content:
ByRashi Shrivastava, Forbes Staff andPhoebe Liu, Forbes Staff. It was a sunny day in August 2017, and CoreWeave CEO Michael Intrator was beginning to think he might actually set his Manhattan office tower on fire. Dozens of powerful Nvidia GPUs were plugged into every available outlet, churning through the convoluted math required to mine cryptocurrency, and they were throwing off a ton of heat. With the air conditioning turned off for the weekend, the sixth-floor Financial District office had turned into a 130-degree hotbox. “I flipped. This wasn’t really built for this,” Intrator says. “I was just like, ‘We’ve got to go. We’ve got to go.’ ” He and his cofounders, Brian Venturo and Brannin McBee, scrambled, frantically disconnecting servers that were almost too hot to touch and loading them into a pickup truck before heading to New Jersey, where a garage properly ventilated with a gigantic exhaust fan waited to cool them. Nearly a decade later, some things haven’t changed. Intrator is still running GPUs, but on an exponentially larger scale. Now he’s got some 250,000 of them warehoused across the 33 data centers that CoreWeave operates, renting out access to their number-crunching prowess to compute-hungry customers. Once popular for rendering graphics for video games and mining crypto, GPUs (graphics processing units), commonly designed by chip giant Nvidia, have become a crucial engine for artificial intelligence. Access to them can determine a company’s fortunes, and CoreWeave, which trades on the Nasdaq, has built a $50 billion (market cap) business by becoming one of their go-to brokers, serving the likes of Microsoft, OpenAI and Meta in the race for AI ascendency. There is still the risk of fire—at least in a monetary sense. After pioneering a new kind of financing that helped CoreWeave borrow $29 billion, largely in loans backed by its GPUs, Intrator now balances shareholders’ qualms about deepening debt and widespread fears about an AI bubble with his broader ambition: to provide compute not just to the tech giants but to every company integrating AI into its operations. It’s not an unreasonable goal. In less than a decade, the Livingston, New Jersey–based business has spun up a data center empire, booking $1.9 billion in revenue in 2024 (net losses: $860 million, for a negative 45% margin) and $2.2 billion in the first half of 2025 (net losses: $605 million, or –28%). By 2031, it hopes to bring in another $30 billion, mostly from contracts with A-list customers like IBM and Meta and a string of AI startups such as Cohere and Mistral. Its stock price has more than doubled since its March IPO, trouncing the broader Nasdaq’s return of 23.9% and minting at least five billionaires in the process. Intrator, 56, is now worth $6.7 billion, landing him on The Forbes 400 list of the richest Americans for the first time, along with cofounder Venturo, 40, who’s worth $4.2 billion. Cofounders McBee and Peter Salanki, as well as early investor Jack Cogen, are also new billionaires thanks to CoreWeave. The company’s remarkable growth has stemmed from ferocious global demand for GPUs. Startups have been strapped for those sophisticated chips since ChatGPT exploded in popularity in 2022, with even OpenAI in February delaying rolling out a major model upgrade because it ran out of chips to run it on. That means whoever has GPUs holds the keys to an enormous business opportunity: The AI cloud computing market was $230 billion in 2024 and, assuming the boom continues, is poised to explode to $400 billion by 2028, according to Wall Street analysts. “I’ve never seen a supply/demand imbalance like this in the 40 years I’ve been investing,” says Mark Klein, CEO of SuRo Capital, which invested a total of $25 million in CoreWeave. Thanks to good instincts, a substantial appetite for risk and a dash of luck, Intrator realized early that his crypto company’s GPU stockpile could be good for more than minting tokens. A strategic relationship with Nvidia helped with the rest, positioning CoreWeave as one of the predominant cloud providers for the AI era. At first Silicon Valley investors weren’t convinced that CoreWeave could hold its own against well-capitalized cloud titans like Amazon, Microsoft and Google, which remain both arch-competitors and, in some cases, important customers. “The overwhelming majority of [investors] thought we were insane,” Intrator says. Spurned by VCs, he turned to the debt markets, what he calls “East Coast money.” Top-tier private creditors like Blackstone and Magnetar financed the multibillion buildout of CoreWeave’s infrastructure so it could capitalize on AI’s insatiable appetite for compute. Debt financing also meant that Intrator and his cofounders held on to more precious equity; Intrator retains 13%, Venturo 7%. The scale is the key, Venturo says. “The capital intensity of this business is all that matters.” Critics are spooked by the $11.2 billion in debt on CoreWeave’s balance sheet. And with interest rates ranging from 7% to 15%, expenses remain high: The company spent more than $250 million paying interest on that debt last quarter on just $19 million in operating income. Plus about half of CoreWeave’s assets are GPUs, which are said to depreciate over six years—or less, now that Nvidia is upgrading its chips faster. D.A. Davidson analyst Gil Luria doesn’t mince words: “This is a value-destructive entity.” Naysayers also point out that 62% of CoreWeave’s $1.9 billion revenue in 2024 came from one extremely large customer: Microsoft. That dependence won’t last long, though: OpenAI has committed to spending a total of $15.9 billion on CoreWeave’s infrastructure over the next five years and received a $350 million stake in the company. CoreWeave recently inked a deal to supply computing power to Google, and IBM has been using CoreWeave’s GPU clusters as well. “Our researchers can focus on the AI and less on the infrastructure,” says IBM Cloud CTO Hillery Hunter. Then there’s the existential question: Is the AI boom actually a bubble? In the first half of 2025, investors plowed $104 billion into artificial intelligence startups at towering valuations, per PitchBook, betting that AI really will turn out to be as revolutionary as the internet or the automobile and upend entire industries. That hasn’t happened yet. Despite spending billions on generative AI, many businesses haven’t seen material returns on it. Most AI companies still haven’t turned a profit. Even Sam Altman, AI’s chief carnival barker, is warning that overexuberant investors could “get burned.” That’s especially worrisome for CoreWeave, given its precarious, highly leveraged position as the industry’s premier GPU provider. Put another way: If this bubble were to pop, CoreWeave’s business would rupture along with it. Intrator’s talent for creative number crunching first emerged when he was a commodities trader at Natsource, a New York–based carbon credit trading and greenhouse gas asset mana­ger. There, from 1998 through the Enron scandal to 2014, he learned how to turn pollution into a financial instrument—just as he would later do with GPUs. After his first venture, a natural gas hedge fund, failed, Intrator and his cofounders pivoted, forming Atlantic Crypto in 2017 to mine ether. “We founded the company on a credit card,” Intrator says. “It was like, ‘Okay, I’ll buy a couple GPUs. What the hell?’ ” When crypto experienced one of its periodic crashes in 2018, the cofounders bought thousands of GPUs from miners going out of business for pennies on the dollar and raked in an estimated $80 million in revenue from mining. “Ethereum kept them alive long enough for them to make the transition to AI,” says early investor Nic Carter. Given ether’s volatility, Intrator realized the more lucrative bet was not fickle cryptocurrency but the equipment used to mine it. Their servers could be configured for different use cases: rendering 3D images, animation or early AI efforts. In 2019, they swapped out their “crappy” mining GPUs with data center–grade chips on a dollar-for-dollar basis. “Looking back on it, it was genius,” Venturo says. Their first break into AI came through a collaboration with Eleuther AI, a Washington, D.C.–based research nonprofit, in late 2021. CoreWeave shelled out $2 million to buy a bunch of Nvidia’s A100 chips and built a cluster of 96 GPUs for the organization, which was training a powerful open-source large language model. It did what Eleuther wanted but was plagued by bugs, networking issues and storage problems. At one point the nonprofit’s employees had to use an external storage device to manually offload data to ensure the system wouldn’t crash. “That was the tuition to learn how to build the infrastructure,” Intrator says. It was also a path to some early customers. Through Eleuther, CoreWeave found Stability AI, the creator of a viral early AI text-to-image generator called Stable Diffusion. That’s when Venturo saw the first outlines of an enormous business opportunity. But CoreWeave would need to double down on Nvidia’s chips to seize it. It would be “the most important thing this company is ever going to do,” he says. To buy those GPUs, CoreWeave needed cash—a lot of it, and fast. In late 2022, Evanston, Illinois–based asset mana­ger Magnetar Capital invested $100 million, giving CoreWeave the ammunition to buy thousands of Nvidia H100 chips and provide computational power for AI at scale. It later co-led a $2.3 billion round, CoreWeave’s first GPU-backed loan. Intrator’s genius: convincing Magnetar to secure the funds using the very chips he was buying as collateral. “They laid the groundwork for debt financing in the GPU space,” says Darrick Horton, CEO of cloud computing rival TensorWave. It was an unprecedented and risky move, but when Chat­GPT took AI mainstream in November 2022, Intrator’s gamble paid off. In January 2023, CoreWeave signed a $300 million deal with early AI darling Inflection to build a 22,000-GPU AI cluster, one of the world’s largest at the time. “We went from having nothing sold to having everything sold five times over,” Venturo says. By December 2022, CoreWeave had bought so many GPUs from Nvidia that its leadership took notice. The cofounders got a call out of the blue; Nvidia CEO Jensen Huang wanted to talk. Twenty minutes later, they were on a Zoom call with him, pitching the cofounder of the world’s largest semiconductor company on investing in a no-name former crypto miner. Huang saw in CoreWeave an opportunity to strengthen Nvidia’s dominance in the AI market by propping up a future buyer for its complex chips that had the technical expertise to run them efficiently, Intrator says. Nvidia invested some $350 million in CoreWeave for a 5% stake (now worth $2.5 billion) and granted it “elite” status on its partner network, vaulting it to the front of the line for the company’s most cutting-edge chips. (Nvidia declined to comment for this story.) It’s quite the incestuous relationship. (Intrator prefers “symbiotic.”) Nvidia is a vendor, an investor and a customer; it has paid CoreWeave at least $320 million for its infrastructure and agreed to buy its unsold capacity through 2032. But it is by no means a relationship of equals. In fiscal 2025 (ended January), Nvidia made $72.9 billion profit on $130.5 billion revenue. Its current market capitalization exceeds $4.3 trillion. D.A. Davidson analyst Luria puts it directly: “CoreWeave wouldn’t exist if Nvidia didn’t want it to.” As CoreWeave landed larger contracts, it took on more debt to fulfill them. In 2023, it struck a deal with Microsoft, which reportedly plans to spend $10 billion on its cloud computing services through 2030. That was enough for Blackstone to lead $2.3 billion in debt financing in 2023, followed by a fatter $7.5 billion check in 2024. “We are underwriting contracts, capacity and power, not a particular ‘story,’ ” says Jas Khaira, a senior managing director at Blackstone, adding that the due diligence process ensured the contracts were airtight and customers like Microsoft couldn’t easily walk away. Inside a coolly lit data center in Orangeburg, New York, about 40 minutes north of Midtown Manhattan, rows of what look like hospital IV bags hang across long banks of densely packed black shelves. Each bag dispenses a drip of neon-green cooling fluid to a rack of GPU-filled servers when needed, keeping them at the safe, always-on temperature at which they help pay for CoreWeave’s considerable investment in them. Just how much revenue those servers can deliver depends on two things: how long AI’s demand for compute will outstrip supply and how long it will take AI companies to catch up. Microsoft, for instance, spent $88 billion on capex in the year ending June 2025, most of which was for AI infrastructure. Open AI reportedly signed a $300 billion deal with Oracle recently to provide compute—part of its President Trump–endorsed Stargate partnership—calling into question the future of its contracts with CoreWeave. Aside from the hyperscaling incumbents, CoreWeave must also battle cloud computing rivals like Crusoe, Nebius and Lambda, many of which have landed contracts with CoreWeave’s big tech clients, who use multiple cloud providers to reduce dependence on just one. “Renewal and retention is a risk,” says Macquarie analyst Paul Golding. “We think about risk in a more sophisticated way than almost anyone else in the space.” And then there’s that debt. CoreWeave has no plans to stop borrowing against its GPUs—debt could rise to $30 billion in two years, per JPMorgan projections. Some analysts are worried about the risk of doing that, especially amid ongoing net losses ($2.1 billion in total since 2023) in a fast-moving industry. Intrator has heard and dismissed all this before. “No shit, I’m borrowing money,” he says, gesticulating defiantly at his Wall Street haters while en route to CoreWeave’s Manhattan office. McBee backs him up. “There’s no risk,” he says, improbably. They claim each CoreWeave contract (average length: four years) prices in most of the costs of delivering on it—GPUs, data center buildouts and interest payments—plus depreciation and maintenance. If all goes as planned, then, CoreWeave is left with some profit plus the value of its GPUs. Generally, contracts are structured to generate approximately $2 of revenue for every dollar of debt to be repaid, McBee says. The risk, of course, is that the AI boom goes bubble and pops—and those contracts are never fulfilled, leaving CoreWeave with a ton of idle servers and a mountain of debt. Citi analyst Tyler Radke forecasts CoreWeave could be profitable as early as next year. It’s losing money because of interest payments and because infrastructure costs a lot to build upfront. “We think about risk in a more sophisticated way than almost anyone else in the space,” Intrator says. Maybe. But many credit rating agencies categorize CoreWeave’s latest bonds as junk. Even if the boom times continue, things could go south fast if the company can’t build out what it has promised, as quickly as promised. Then there are the known unknowns: regulatory hurdles, manufacturing mishaps, local opposition. Protests alone have already stopped or slowed $64 billion in data center projects, according to Data Center Watch. And CoreWeave is already facing opposition over a new facility in Lancaster, Pennsylvania, where it’s investing $6 billion. When it’s finished, it is expected to consume four times as much power as all the region’s homes, says Lancaster resident Emma Burgess. Residents of the community known for its Amish history recently packed city council meetings to voice their concerns: too little community input, too much electricity, too much pollution (both air and noise). McBee says there hasn’t been pushback at the site, but acknowledges “you’re gonna break things along the way.” Because CoreWeave’s AI data centers need to be on 24/7/365, they can’t be subject to the fluctuations that come with wind and solar power. So for the time being “it’s gas,” McBee says. “That’s what we prefer.” In May, CoreWeave acquired AI software developer Weights & Biases for $1 billion, a deal it hopes will diversify its customer base. A planned $9 billion acquisition of data center owner Core Scientific—which could bring some 1.5 gigawatts of additional contracted power—would more than quadruple CoreWeave’s current active power, provided regulators and shareholders allow it. Next up: investing in AI startups with a combo of cash and GPU access. Things are volatile. CoreWeave’s market cap has swung by some $70 billion since its rocky March IPO. Tariffs could further unsettle its bottom line. Its biggest customer just signed a huge deal with a rival fast encroaching on its turf. Delays, of both capital and equipment, could kill CoreWeave’s head start. None of this has fazed Intrator, who’s confident his GPU-backed debt will pay off and keep his company growing alongside rampant demand for AI compute. As the markets rise, so too will CoreWeave, buoyed by the prescient bets that made it a $50 billion behemoth in just six years. But it’s still beholden to the biggest gamble of all—that the AI bubble won’t burst. This story has been updated to clarify a statement about Microsoft’s projected capital expenditures.
--------------------------------------------------

Title: Stocks Set to Retreat From Record Highs, U.S. PCE Inflation Data and Powell’s Remarks Awaited
URL: https://www.barchart.com/story/news/34949816/stocks-set-to-retreat-from-record-highs-u-s-pce-inflation-data-and-powells-remarks-awaited
Time Published: 2025-09-22T10:08:02Z
Description: December S&P 500 E-Mini futures (ESZ25) are down -0.30%, and December Nasdaq 100 E-Mini futures (NQZ25) are down -0.35% this morning, pointing to a slightly ...
--------------------------------------------------