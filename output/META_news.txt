List of news related to Meta stock price META:

Title: Trump's Middle East AI Deal: "Just Count the Server Racks"
URL: https://www.chinatalk.media/p/about-that-ai-middle-east-deal
Time Published: 2025-05-25T14:03:59Z
Full Content:
Zvi Mowshowitz of the Don't Worry About the Vase substack did an excellent job writing up the key dynamics of the deal. I’ll be running excerpts from his post with some comments of mine interspersed. Our government, having withdrawn the new diffusion rules, has now announced an agreement to sell massive numbers of highly advanced AI chips to UAE and Saudi Arabia (KSA). This post analyzes that deal and that decision. It is possible, given sufficiently strong agreement details (which are not yet public and may not be finalized) and private unvoiced considerations, that this deal contains sufficient safeguards and justifications that, absent ability to fix other American policy failures, this decision is superior to the available alternatives. Perhaps these are good deals, with sufficiently strong security arrangements that will actually stick. Perhaps UAE and KSA are more important markets and general partners than we realize, and the rest of the world really is unable to deploy capital and electrical power the way they can and there is nothing we can do to change this, and perhaps they have other points of strategic importance, so we have to deal with them. Perhaps they are reliable American allies going forward who wouldn’t use this as leverage, for reasons I do not understand. There are potential worlds where this makes sense. The fact remains that the case being made for this deal, in public, actively makes the situation seem worse. David Sacks in particular is doubling down and extending the rhetoric I pushed back against last week, when I targeted Obvious Nonsense in AI diffusion discourse. Even within the White House, the China hawks are questioning this deal, and Sacks responded by claiming to not even understand their objections and to all but accuse such people of being traitorous decels wearing trench coats. I stand by my statements last week that even if accept the premise that all we need care about are ‘America wins the AI race’ and how we must ‘beat China,’ our government’s policies, on diffusion and elsewhere, seem determined to lose an AI race against China. The point of the diffusion rules is to keep the AI chips secure and out of Chinese hands, both in terms of physical security and use of their compute via remote access. It is possible that the agreements we are making with UAE and KSA will replace and improve upon the functionality, in those countries in particular, of the diffusion rules. It’s not about a particular set of rules. It is about the effect of those rules. Give me a better way to get the same effect, and I’m happy to take it. When I say ‘something similar’ below, I mean in the sense of sufficient safeguards against the diversion of either the physical AI chips or the compute from the AI chips. Access to those chips is what matters most. Whereas market share in selling AI chips is not something I am inclined to worry about except in my role as Nvidia shareholder. I do not consider them reliable allies going forward, and there are various reasons that even the best version of these agreements would make me deeply uncomfortable, but it is possible to reach an agreement that physically locates many data centers in the Middle East and lets them reap the financial benefits of their investments and have compute available for local use, but does not in the most meaningful senses ‘hand them’ the compute in question. As in, no I do not trust them, but we could find a way that we do not have to, if they were fully open to whatever it took to make that happen. I also would highlight the implicit claim I made here, that the pool of American advanced AI chips is essentially fixed, and that we have sufficient funding available in Big Tech to buy all of them indefinitely. If that is not true, then the UAE/KSA money matters a lot more. Then there is the similar question of whether we were going to actually run out of available electrical power with no way to get around that. A lot of the question comes down to: What would have counterfactually happened to those chips? Would we have been unable to deploy them? [Jordan: See ChinaTalk’s recent coverage of an excellent IFP report that explored what a policy agenda to rapidly increase the energy available for AI deployment would look like] Despite leading the world in AI innovation, there’s no guarantee that America will rise to meet the challenge of AI infrastructure. Specifically, the key technological barrier for data center construction within the next 5 years is new power capacity With that in mind, here are the central points I highlighted last week: America is ahead of China in AI. Diffusion rules serve to protect America’s technological lead where it matters. UAE, Qatar and Saudi Arabia are not reliable American allies, nor are they important markets for our technology. We should not be handing them large shares of the world’s most valuable resource, compute. The exact diffusion rule is gone but something similar must take its place, to do otherwise would be how America ‘loses the AI race.’ Not having any meaningful regulations at all on AI, or ‘building machines that are smarter and more capable than humans,’ is not a good idea, nor would it mean America would ‘lose the AI race.’ AI is currently virtually unregulated as a distinct entity, so ‘repeal 10 regulations for every one you add’ is to not regulate at all building machines that are soon likely to be smarter and more capable than humans, or anything else either. ‘Winning the AI race’ is about racing to superintelligence. It is not about who gets to build the GPU. The reason to ‘win’ the ‘race’ is not market share in selling big tech solutions. It is especially not about who gets to sell others the AI chips. If we care about American dominance in global markets, including tech markets, stop talking about how what we need to do is not regulate AI, and start talking about the things that will actually help us, or at least stop doing the things that actively hurt us and could actually make us lose. Diffusion controls on AI chips we’ve enforced on China so far have had a huge impact. DeepSeek put out a highly impressive AI model, but by their own statements they were severely handicapped by lack of compute. Chinese adoption of AI is also greatly held back by lack of inference compute. China is competing in spite of this severe disadvantage. It is vital that we hold their feet to the fire on this. China has an acute chip shortage, because it physically cannot make more AI chips, so any chips it would ship to a place like UAE or KSA would each be one less chip available in China. Whenever you see arguments from David Sacks and others against AI diffusion rules, ask the question: Is an argument for a different set of export controls and a different chip regime that still protects against China getting large quantities of advanced AI chips? Or is it an argument, as it often is, that to preserve our edge in compute we should sell off our compute, that to preserve our edge in tech we should give away our edge in tech? As in, that what matters is our market share of AI chips, not who uses them? This is not a strawman, for example Ben Thompson argues exactly this very explicitly and repeatedly. Ben Thompson’s recent interview with Jensen Huang, CEO of Nvidia, made it clear both of them have this exact position. That to maintain America’s edge in AI, we need to sell our AI chips to whoever wants them, including China, because ‘China will not be held back’ as if having a lot more chips wouldn’t have helped them. And essentially saying that all Nvidia chips everywhere support the ‘American tech stack’ rather than China rather obviously turning around and using them for their own tech. He explicitly is yelling we need to ‘compete in China’ or else. Complete Obvious Nonsense talking of his own book, which one must remind oneself is indeed his job, what were you really expecting him to say? Well, what he is saying is that the way we ‘lose the AI race’ is someone builds a CUDA alternative or steals Nvidia market share. That his market is what matters. It’s full text. Not remotely a strawman. I would disagree with arguments of form #2 in the strongest possible terms. If it’s arguments of form #1, we can talk about it. We should keep these facts in mind as we analyze the fact that the United States has signed a preliminary chip deal with the UAE. There is a 5GW AUE-US AI campus planned, and is taking similar action in Saudi Arabia. The deals were negotiated by a team led by David Sacks and Sriram Krishnan. Lennart Heim: To put the new 5GW AI campus in Abu Dhabi (UAE) into perspective. It would support up to 2.5 million NVIDIA B200s. That's bigger than all other major AI infrastructure announcements we've seen so far. In exchange for access to our chips, we get what are claimed to be strong protections against chip diversion, and promises of what I understand to be a total of $200 billion in investments by the UAE. That dollar figure is counting things like aluminum, petroleum, airplanes, Qualcomm and so on. It is unclear how much of that is new. The part of the deal that matters is that a majority of the UAE investment in data centers has to happen here in America. I notice that I am skeptical that all the huge numbers cited in the various investment ‘deals’ we keep making will end up as actual on-the-ground investments. As in: Walter Bloomberg: UAE PRESIDENT SAYS UAE TO INVEST $1.4T IN U.S OVER NEXT 10 YEARS At best there presumably is some creative accounting and political symbolism involved in such statements. Current UAE foreign-direct-investment stock in the USA is only $38 billion, their combined wealth funds only have $1.9 trillion total. We can at best treat $1.4 trillion as an aspiration, an upper bound scenario. If we get the $200 billion we should consider that a win, although if the deal is effectively ‘all your investments broadly are in the West and not in China’ then that would indeed be a substantial amount of funds. Nor is this an isolated incident. The Administration is constantly harping huge numbers, claiming to have brought in $14 trillion in new investment, including $4 trillion from the recent trip to Arabia, or roughly half of America’s GDP. Jason Furman (top economic advisor, Obama White House): That’s nuts and baseless. I doubt the press releases even add up to that. But, regardless, press releases are a terrible way to determine the investment or the impact of his policies on it. Justin Wolfers: Trump has claimed a $1.2 trillion investment deal from Qatar. Qatar’s annual GDP is a bit less than $250 billion per year. So he’s claiming an investment that would require every dollar every Qatari earned over the next five years. UAE’s MGX will also be opening Europe’s largest data center in France, together with Nvidia, an 8.5 billion Euro investment, first phase to be operational in 2028. This has been in the works for a while. Not that the numbers ultimately matter all that much. What does matter is: How will we ensure the chips don’t fall literally or functionally into Chinese hands? It comes down to the security provisions and who is going to effectively have access to and run all this compute. I don’t see here any laying out of the supposed tough security provisions. Without going into details, if the agreements on both physical and digital security are indeed implemented in a way that is sufficiently tough and robust, if we are the ones who both physically and digitally control and monitor things on a level at least as high as domestically, and can actually have confidence none of this will get diverted, then that goes a long way. We don’t yet have enough of that information to say. The public explanations for the deal, and the public statements about what safety precautions are considered necessary, do not bring comfort. I very much do not like comments like this, made in response to the Bloomberg piece above. David Sacks (US AI Czar): If the concern [about the deal] is about diversion of advanced semiconductors to China, that’s an important policy objective but one that is easily addressed with a security agreement and a “trust but verify” approach. Modern AI supercomputers are 8 feet tall and weigh two tons; these are not like diamonds smuggled in a briefcase. To verify that chips are where they’re supposed to be, an auditor can just visit the data center and count server racks. Semianalysis: As such, physical inspections are key to ensuring the GPUs are where they are supposed to be. The White House is aware of the risks and is prepared to ensure the chips are where they are supposed to be. As Bloomberg reported (quotes David Sacks’s statement above). I’m sorry, what? You can ‘just visit the data center and count server racks’? It terrifies me to see so many people arguing for the deal explicitly saying that this is a good statement, rather than a terrifying statement that we are hoping no one involved in the actual work believes to be true. [Jordan: In 2022 I did a show with Kevin Wolf, the former head of BIS, who mentioned off-hand that there were literally only two people Commerce had in China to do site inspections. Since then BIS’ budget has barely budged and faced DOGE cuts in the early days of this administration. There is not the capacity to do datacenter inspections in a serious way today.] Jordan Schneider: Let's start with the last thing first. This 60-day unverified list: is there a potential that the impact of this could actually be a lot bigger than what we currently see, if some of these big names on the unverified entity list end up graduating to full entity-list status? Kevin Wolf: It's really up to the Chinese government. If they allow access, give the information that the Commerce Department is seeking, and allow the two people in China that do inspections and site visits to do their jobs… Jordan Schneider: Whoa, wait, two? There are, like, 30 firms on there! Kevin Wolf: Yeah, I know, but no other country has any, and the Commerce Department is the only export control organization on the planet with its own enforcement agents who are subject matter experts in export controls. I think diversion via remote access is far more likely than physical diversion, and this response does not address the remote access issue at all, but even simply treating this as a physical diversion issue, o3, is the quoted statement accurate? o3 (full one shot, full response quoted, in response to literally quoting Sacks without attribution and asking ‘is this accurate?’): Short version: Mostly wrong. Chips are tiny and high-value, so diversion is hard to stop with “count the racks.” Physical audits help but they are far from sufficient. High value-to-weight ratio.At ~$30 k per H100, a single 20 kg carry-on full of GPUs is > $1 m. That’s an attractive target. Racks are heavy; the payload isn’t. Modularity.GPUs slide out in minutes. An auditor sees the enclosure; they cannot tell if the trays were swapped yesterday or will be tomorrow. Dummy blanks cost <$10. Front companies & trans-shipping.Chips often leave the compliant data-center before any inspection window. Paperwork shows them installed; reality changes after the auditor flies home. Remote lease risk.Even if the hardware stays in the U.S., Chinese entities can simply rent time on it. Export-control rules treat “deemed exports” (remote access by restricted users) as violations. Limited audit frequency & competence.Meaningful verification requires random, intrusive, technically savvy inspections plus cryptographic attestation of firmware. Counting racks is neither. My best estimate: ≤ 30 % that “security agreement + occasional rack counting” alone keeps advanced GPUs out of China for > 3 years. ≥ 70 % that significant leakage continues absent tighter controls (HW tracking, cryptographic attestation, and supply-chain tagging). So the quoted claim is misleading: rack-level audits are helpful but nowhere near “easily addresses” the diversion problem. When I asked how many chips would likely be diverted from a G42 data center if this was the security regime, o3’s 90% confidence interval was 5%-50%. Note that the G42 data center is 20% of the total compute here, so if we generously assume no physical diversion risk in the other 80%, that’s 1%-10% of all compute we deploy in the UAE. Is that acceptable? The optimal amount of chip diversion is not zero. But I think this level of diversion would be a big deal, and the bigger concern is remote access. I want to presume, for overdetermined reasons, that Sacks’s statement was written without due consideration or it does not reflect his actual views, and we would not actually make this level of dumb mistake where they could literally just swap the chips out for dummy chips. I presume we are planning to use vastly superior and more effective precautions against chip diversion and also have a plan for robust monitoring of compute use to prevent remote access diversion. But how can we trust an administration to take such issues seriously, if their AI Czar is not taking this even a little bit seriously? This is not a one time incident. Similar statements keep coming. That’s why I spent a whole post responding to them. David Sacks is also quoted extensively directly in the Bloomberg piece, and is repeatedly very dismissive of worried about diversion of chips or of compute, saying it is a fake argument and an easy problem to solve, and he talks about these as if they were reliable American allies in ways I do not believe are accurate. Sacks also continues to appear to view winning AI to be largely about selling AI chips. As in, if G42, an Abu Dhabi-based AI firm, is using American AI chips, then it essentially ‘counts as American’ for purposes of ‘winning,’ or similar. I don’t think that is how this works, or that this is a good use of a million H100s. Bloomberg reports 80% of chips headed to the UAE would go to US companies, 20% to G42. I very much want us to think about the actual physical consequences of various actions, not what those actions symbolize or look like. I do think, despite everything else, it is a very good sign that David Sacks is ‘urging people to read the fine print.’ This is moderated by the fact that we do not have the fine print, so we can’t read it. The true good news there requires one to read all that fine print, and one also should not assume that the fine print will get implemented. Nor do we yet have access to what the actual fine print says, so we cannot read it. Dylan Patel and others at Semianalysis offer a robust defense of the deal, saying clearly that ‘America wins’ and that this benefits American AI infrastructure suppliers on all levels, including AI labs and cloud providers. They focus on three benefits: money, tying KSA/UAE to our tech stack, and electrical power, and warn of the need for proper security, including model weight security, a point I appreciated them highlighting. Those seem like the right places to focus, and the right questions to ask. How much of their money is really up for grabs and how much does it matter? To what extent does this meaningfully tie UAE/KSA to America and how much does that matter? How much do we need their ability to provide electrical power? How will the security arrangements work, will they be effective, and who will effectively be in charge and have what leverage? Specifically, on their three central points: They call this macro, but a better term would be money. UAE and KSA (Saudi Arabia) can make it rain, a ‘trillion-dollar floodgate.’ This raises two questions. Question one: Was American AI ‘funding constrained’? The big tech companies were already putting in a combined hundreds of billions a year. Companies like xAI can easily raise funds to build giant data centers. If Google, Amazon, Apple, Meta or Microsoft wanted to invest more, are they really about to run out of available funding? Are there enough more chips available to be bought to run us out of cash? Semianalysis seems to think we should be worried about willingness of American companies to invest here and thinks we will have trouble with the financing. I am not convinced of this. Have you seen what these companies (don’t have to) pay on corporate bonds? Did we need to bring in outside investors? Should we even want to, given these investments look likely to pay off? This is a major crux. If indeed American big tech companies are funding constrained in their AI investments, then the money matters a lot more. Whereas if we were already capable of buying up all the chips, that very much cuts the other way. Question two: As we discussed earlier, is the trillion-dollar number real? We keep seeing these eye-popping headline investment numbers, but they don’t seem that anchored to reality, and seem to include all forms of investment including not AI, although of course other foreign direct investment is welcome. Do their investments in US datacenters mean anything, and are they even something we want, given that the limiting factor driving all this is either constraints on chip availability or on electrical power? Will this be crowding out other providers? If these deals are so positive for American tech companies, why didn’t the stock market moves reflect this? No, I will not accept ‘priced in.’ They call this geopolitical, that UAE and KSA are now tied to American technology stacks. As they say, ‘if Washington enforces tight security protocols.’ We will see. David Sacks is explicitly dismissing the need for tight security protocols. Classically, as Trump knows well, when the bank loans you a large enough amount and you don’t pay it back, it is the bank that has the problem. Who is being tied to whose stack? They will be able to at least cut the power any time. It is not clear from public info what other security will be present and what happens if they decide to turn on us, or use that threat as leverage. Can they take our chips and their talents elsewhere? This can almost be looked at as a deal with one corporation. G42 seems like it’s going to effectively be on the UAE side of the deal, and it is going to have a lot of chips in a lot of places. A key question is, to what extent do we have the leverage on and control over G42, and to what extent does this mean they will act as a de facto American tech company and ally? How much can we trust that our interests will continue to align? Who will be dependent on who? Will our security protocols extend to their African and European outposts? Why does buying a bunch of our chips tie them into the rest of our stack? My technical understand is that it doesn’t. They’re only tied to the extent that they agreed to be tied as part of the deal (again, details unknown), and they could swap out that part at any time. In my experience you can change which AI your program uses by changing a few lines of code, and people often do. It is not obvious why KSA and UAE using our software or tech stack is important to us other than because they are about to have all these chips. These aren’t exactly huge markets. If the argument is they have oversized effect on lots of other markets, we need to hear this case made out loud. Seminanalysis points out China doesn’t even have the capacity to sell its own AI chips yet. And I am confused about the perspectives here on ‘market share’ and the implied expectations about customer lock-in. They call this infrastructure, I’d simply call it (electrical) power. This is the clearly valuable thing we are getting. It’s rather crazy that ‘put our most strategic asset except maybe nukes into the UAE and KSA’ was chosen over ‘overrule permitting rules and build some power plants or convince one of our closer allies to do it’ but here we are. So the question here is, what are the alternatives? How acute is the shortage going to be and was there no one else capable of addressing it? Also, even if we do have to make this deal now, this is screaming from the rooftops, we need to build up more electrical power everywhere else now, so we don’t have this constraint again in the future. Semianalysis also raises the concern about model weight security, but essentially think this is solvable via funding work to develop countermeasures and use of red teaming, plus defense in depth. It’s great to see this concern raised explicitly, as it is another real worry. Yes, we could do work to mitigate it and impose good security protocols, and keep the models from running in places and ways that create this danger, but will we? I don’t know. Failure here would be catastrophic. There are also other concerns even if we successfully retain physical and digital control over the chips. The more we place AI chips and other strategic AI assets there, the more we are turning UAE, Saudi Arabia and potentially Qatar into major AI players, granting them leverage I believe they can and will use for various purposes. David Sacks continues to claim to not understand that others think that ‘winning AI’ is mostly not about who gets to sell chips, who uses our models and picks up market share, or about superficially ‘winning’ ‘deals.’ He not only thinks it is about market penetration, he can’t imagine an alternative. He doesn’t understand that many, including myself, this is about who has compute and who gets superintelligence, and about the need for proper security. David Sacks: I’m genuinely perplexed how any self-proclaimed “China Hawk” can claim that President Trump’s AI deals with UAE and Saudi Arabia aren’t hugely beneficial for the United States. As leading semiconductor analyst Dylan Patel observed, these deals “will noticeably shift the balance of power” in America’s favor. The only question you need to ask is: does China wish it had made these deals? Yes of course it does. But President Trump got there first and beat them to the punch. Sam Altman: this was an extremely smart thing for you all to do and i’m sorry naive people are giving you grief. Tripp Mickle and Ana Swanson (NYT): One Trump administration official, who declined to be named because he was not authorized to speak publicly, said that with the G42 deal, American policymakers were making a choice that could mean the most powerful A.I. training facility in 2029 would be in the United Arab Emirates, rather than the United States. [Jordan: this is a wild line to be giving to the NY Times—I thought all the China hawks already got purged!] But Trump officials worried that if the United States continued to limit the Emirates’ access to American technology, the Persian Gulf nation would try Chinese alternatives. The hawks are concerned, because the hawks largely do not think that the key question is who will get to sell chips, but rather who gets to buy them and use them. This is especially true given that both America and China are producing as many top AI chips as they can, us far more successfully, and there is more than enough demand for both of them. One must think on the margin. Given that so many China hawks are indeed on record doubting this deal, if you are perplexed by this I suggest reading their explanations. Here is one example. Tripp Mickle and Ana Swanson (NYT): Mr. Goodrich said the United States still had the best A.I. engineers, companies and chips and should look for ways to speed up permitting and improve its energy grid to hold on to that expertise. Setting up some of the world’s largest data centers in the Middle East risks turning the Gulf States, or even China, into A.I. rivals, he said. “We’ve seen this movie before and we should not repeat it,” Mr. Goodrich said. Sam Winter-Levy, a fellow at the Carnegie Endowment for International Peace, said the huge chip sales did “not feel consistent with an America First approach to A.I. policy or industrial policy.” “Why would we want to offshore the infrastructure that will underpin the key industrial technology of the coming years?” he asked. This does not seem like a difficult position to understand? There are of course also other reasons to oppose such deals. Here is Jordan Schneider of China Talk’s response, in which he is having absolutely none of it, explicitly rejecting that either America or China has chips to spare for this. rejecting that UAE and KSA are actual allies, not expecting us to follow through with reasonable security precautions, and saying if we wanted to do this anyway we could have held out for a better deal with more control than this, I don’t know why you would be confused how someone could have this reaction based on the publicly available information: Jordan Schneider: It’s going to cannibalize US build-out and leave the world with three independent power-centers of AI hardware where we could’ve stuck to our guns, done more power generation at home, and only had China to deal with not these wild-card countries that are not actual allies. If this really is as important as we believe, why are we letting these countries and companies we deeply distrust get access to it? The Gulf’s BATNA wasn’t Huawei chips, it was no chips. Whatever we’re trying to negotiate for, we can play harder to get. BIS can just say they can’t buy Ascends and it’s not like there’s enough capacity domestically in China to service global demand absent the TSMC loophole they charged through. Plus, we’re offering to sell them 10× the chips that Huawei could conceivably sell them anytime soon even if they use the TSMC-fabbed wafers. Where’s the art-of-the-deal energy here? Right now I only see AMD and NVDA shareholders as well as Sama benefiting from all of this. I thought we wanted to raise revenue from tariffs? Why not charge 3× the market rate and put the premium into the US Treasury, some “Make America Great Again” industrial-development fund, use it to triple BIS’ budget so they can actually enforce the security side, put them on the hook for Gaza…I don't know literally anything you care about. How about a commitment not to invest in Chinese tech firms? Do we still care about advanced logic made in America? How about we only let them buy chips fabbed in the US, fixing the demand-side problem and forcing NVDA to teach Intel how to not suck. Speaking of charging through loopholes, all of the security issues Dylan raises in his article I have, generously, 15 % confidence in USG being able to resolve/resist industry and politicians when they push back. If it’s so simple to just count the servers, why hasn’t BIS already done it / been able to fight upstream industry lobbying to update the chips-and-SME regs to stop Chinese build-outs and chip acquisition? What happens when the Trump gets a call from the King when some bureaucrat is trying to stop shipments because they see diversion if they ever catch it in the first place? Why are we doing anything with G42 again? Fine, if you really decide you want to sell chips to the UAE, at the very least give American hyperscalers the off-switch. It’s not like they would’ve walked away from that offer! America has a ton to lose in the medium term from creating another cloud provider that can service at scale, saying nothing of one that has some deeply-discomforting China ties pretty obvious even to me sitting here having never gotten classified briefings on the topic. Do the deal’s details and various private or unvoiced considerations make this deal better than it looks and answer many of these concerns? Could this be sufficient that, if looked at purely through the lens of American strategic interests, this deal was a win versus the salient alternatives? Again: That is all certainly possible! Our negotiating position could have been worse than Jordan believes. We could have gotten important things for America we aren’t mentioning yet. The administration could have limited room to maneuver including by being divided against itself or against Congress on this. On the flip side, there are some potentially uncharitable explanations for all of this, that would be reasonable to consider. Instead of understanding and engaging with such concerns and working to allay them, Sacks has repeatedly decided to make this a mask off moment, and engage in a response that I would expect on something like the All-In Podcast or in a Twitter beef, but which is unbecoming of his office and responsibilities, with multiple baseless vibe and ad hominem attacks at once that reflect that he either is willfully ignorant of the views, goals and beliefs of those he is attacking and even who they actually are, or he is lying and does not care, or both, and a failure to take seriously the concerns and objections being raised. Here is another illustration of this: David Sacks (May 17): After the Sam Bankrun-Fraud fiasco, it was necessary for the Effective Altruists to rebrand. So they are trying to position themselves as “China Hawks.” But their tech deceleration agenda is the same, and it would cost America the AI race with China. There are multiple other people I often disagree with on important questions but whom I greatly respect who are working on in administration on AI policy. There are good arguments you can make in defense of this deal. Instead of making those arguments in public, we repeatedly get this. Everything Sacks says seems to be about vibes and implications first and actual factual claims a distant second at best. He doesn’t logically say ‘all so-called China hawks who don’t agree with me are secret effective altruists in trench coats and also decels who hate all technology and all of humanity and also America,’ but you better believe that’s the impression he’s going for here. Would China have preferred to ‘do this deal’ instead? That at best assumes facts, and arguments, not in evidence. It depends what they would get out of such a deal, and what we’re getting out of ours, and also the security arrangements and whether we’ve formed a long lasting relationship in which we hold the cards. I’m also not even sure what it would mean for China to have ‘done this deal,’ it does not have what we are offering. Semianalysis says they don’t have similar quantities of chips to sell, and might not have any, nor are their chips of similar quality. I do agree China would have liked to ‘do a deal’ in some general sense, where they bring UAE/KSA into their orbit, on AI and otherwise, although they don’t need access to electrical power. More capital and friends are always helpful. It’s not clear what that deal would have looked like. [Jordan: the fact that the administration is citing some old news of Huawei promising to gift just $75m in chips to the Malaysian government (and not even today! Over two years!) is policy malpractice. Plus, this administration just made clear that using Huawei Ascend chips violates US export controls, giving America an enormously powerful lever to dissuade fence-sitting countries not to buy into what is already an inferior tech offering.] Here’s Tyler Cowen being clear eyed about some of what we are selling so cheap. The most powerful AI training facility could be in the UAE, and you’re laughing? Tyler Cowen: Of course Saudi and the UAE have plenty of energy, including oil, solar, and the ability to put up nuclear quickly. We can all agree that it might be better to put these data centers on US territory, but of course the NIMBYs will not let us build at the required speeds. Not doing these deals could mean ceding superintelligence capabilities to China first. Or letting other parties move in and take advantage of the abilities of the Gulf states to build out energy supplies quickly. Energy and ability to overcome NIMBYs is only that which is scarce because America is refusing to rise to this challenge and actually enable more power generation. Seriously, is there nowhere in America we can make this happen at scale? If we wanted to, we could do this ourselves easily. We have the natural gas, even if nuclear would be too slow to come online. It is a policy choice not to clear the way. And no, I see zero evidence that we are pulling out the stops here and coming up short. I think this frame is exactly correct - that this deal makes sense if and only if all of: The security deal is robust and we retain functional control over where the compute goes. We trust our friends here to remain our friends at a reasonable price. We counterfactually would not have been able to buy these chips and build data centers to power these chips. As far as I can tell China already has all the power it needs to power any AI chips it can produce, it is using them all, and its chip efforts are not funding constrained. So for want of electrical power, and for a few dollars, we are handing over a large amount of influence over the future to authoritarian powers with very different priorities and values? Tyler Cowen: In any case, imagine that soon the world’s smartest and wisest philosopher will soon again be in Arabic lands. We seem to be moving to a world where there will be four major AI powers — adding Saudi and UAE — rather than just two, namely the US and China. But if energy is what is scarce here, perhaps we were headed for additional AI powers anyway, and best for the US to be in on the deal? Who really will have de facto final rights of control in these deals? Plug pulling abilities? What will the actual balance of power and influence look like? Exactly what role will the US private sector play? Will Saudi and the UAE then have to procure nuclear weapons to guard the highly valuable data centers? Will Saudi and the UAE simply become the most powerful and influential nations in the Middle East and perhaps somewhat beyond? Yes. Those are indeed many of the right questions, once you think security is solid. Who is in charge of these data centers in the ways that matter? Won’t they at minimum have the ability to cut the power at any time? Who gets to decide where the compute goes? What are they going to do with all this leverage we are handing them? Is this what it means to have the future be based on American or Democratic values? Do you like ‘the values’ of the UAE and Saudi Arabian authorities? Tyler Cowen: I don’t have the answers to those questions. If I were president I suppose I would be doing these deals, but it is very difficult to analyze all of the relevant factors. The variance of outcomes is large, and I have very little confidence in anyone’s judgments here, my own included. Few people are shrieking about this, either positively or negatively, but it could be the series of decisions that settles our final opinion of the second Trump presidency. The administration thinks that the compute in question will remain under the indefinitely control of American tech companies, to be directed as we wish. Sriram Krishnan: Reflecting on what has been an amazing week and a key step in global American AI dominance under President Trump. These Middle East AI partnerships are historic and this “AI diplomacy” will help lock in the American tech stack in the region, help American companies expand there while also building infrastructure back in the U.S to continue expanding our compute capacity. This happens on top of rigorous security guarantees to stop diversion or unauthorized access of our technology. More broadly this helps pull the region closer to the U.S and aligns our technological interests in a very key moment for AI. It’s a very exciting moment and a key milestone. I hope that they are right about this, but I notice that I share Tyler’s worry that they are wrong. Similarly, Saudi Arabia’s Humain is going to get ‘several hundred thousand’ of Nvidia’s most advanced processors, starting with 18k GB300 Grace Blackwells. The justification given for rescinding the Biden diffusion rules is primarily that failure to do this would have ‘weakened diplomatic relations with dozens of countries by downgrading them to second-tier status.’ But, well, not to reiterate everything I said last week, but on that note I have news. One, we’re weakening diplomatic relations with essentially all countries in a series of unforced errors elsewhere, and we could stop. [Jordan: that feeling when you treat Dubai better than Canada] Two, most of the listed tier two countries have always had second-tier status. There’s a reason Saudi Arabia isn’t in Five Eyes or NATO. We can talk price about which countries should have which status, but no our relations are not all created equal, not when it comes to strategically vital national interests and to deep trust. I don’t share Sacks’s stated view that these are some of our closest and most trustworthy allies. Why does this administration seem to always want to make its deals mostly with authoritarian regimes, usually in places where Trump has financial ties? Tripp Mickle and Ana Swanson (NY Times): The announcements of the two deals follow reports that $2 billion has flowed to Trump companies over the last month from the Middle East, including a Saudi-backed investment in Trump’s cryptocurrency and plans for a new presidential airplane from Qatar. There’s always Trust But Verify. The best solution, if you can’t trust, is often to set up things so that you don’t have to. This can largely be done. Will we do it? And what will we get in return? What is announced mostly seems to be investments and purchases, that what we are getting are dollars, and Bloomberg is skeptical of the stated dollar amounts. This deal is very much not a first best solution. It is, at best, a move that we are forced into on the margin due to our massive unforced errors in a variety of other realms. Even if it makes sense to do this, it makes even more sense to be addressing and fixing those other critical mistakes. Electrical power is the most glaring in the context of this particular. There needs to be national emergency level focus on America’s inability to build electrical power capacity. Where are the special compute zones? Where are the categorical exemptions? Where is DOGE with regard to the NRC? Where is the push for real reform on any of these fronts? Instead, we see story after story of Congress actively moving to withdraw even the supports that are already there, including plans to outright abrogate contracts on existing projects. The other very glaring issue is trade policy. If we think it is this vital to maintain trade alliances and open up markets, and maintaining market share, why are we otherwise going in the opposite direction? Why are we alienating most of our allies? And so on. The argument for this deal is, essentially, that it must be considered in isolation. That other stuff is someone else’s department, and we can only work with what we have. But this is a very bitter pill to be asked to swallow, especially as Sacks himself has spoken out quite loudly in favor of many of those same anti-helpful policies, and the others he seems to be sitting out. You can argue that he needs to maintain his political position, but if that also rules out advocating for electrical power generation and permitting reform, what are we even doing? If we swallow the entire pill, and consider these deals only on the margin, without any ability to impact any of our other decisions, and only with respect to ‘beating China’ and ability to ‘win the AI race,’ and assume fully good faith and set aside all the poor arguments and consider only the steelman case, we can ask: Do these deals help us? I believe that such a deal is justifiable, again on the margin and regarding our position with respect to China, if and only if ALL of the following are true: Security arrangements are robust, the chips actually do remain under our physical control and we actually do determine what happens with the compute. And things are set up such that America retains the leverage, and we can count on UAE/KSA to remain our friends going forward. This was essentially the best deal we could have gotten. This represents a major shift in our or China’s ability to stand up advanced AI chips, because for the bulk of these chips either Big Tech would have run out of money, or we would have been unable to source the necessary electrical power, or China has surplus advanced AI chips I was not previously aware of and no way to deploy them. Entering into these partnerships is more diplomatically impactful, and these friendships are more valuable, than they appear to me based on public info. ChinaTalk is a reader-supported publication. To receive new posts and support our work, consider becoming a free or paid subscriber. Check out some ChinaTalk coverage from least year about China’s Middle East AI ambitions. I spent two years in a China Studies master’s program. It was fun reading Dream of the Red Chamber and Xi speeches — but I really wish I had come out of it with some hard skills, too. Had I only known about the University of San Francisco’s new Master of Science in Applied Economics Excellent commentary. I'm increasingly baffled by the inconsistencies in how the administration views AI. On one hand, it's the most important technology of the century, and will definitively decide the outcome of the U.S.-China competition. Yet we're willing to outsource and offshore data center development. I realize the analogy is flawed, but it has the same feeling as saying we want to outsource production of the B-21 because we have too many regulations, it will take too long to get permits and build, we can't afford it, and BTW we'll be able to provide oversight and monitoring of the manufacturing plant. Not something we would do in a million years. So if AI is that critical, why would we ever agree to this chip sales/data center deal--purported to result in what will become the largest cluster in the world? (While cutting BIS resources.) Rather than being about national self-interests, this has all the earmarks of a pure transactional agreement, centered on money and power. And while it's easy to be skeptical of the claims about AGI and ASI, if the assertion is that AI is going to be the most powerful, game-changing tech of the century, why should we ever stop states from trying to implement reasonable oversight and regulations? The states are doing it because there's a major gap at the federal level. The hyperbolic claims about over-regulation of AI during the previous administration are belied by the facts on the ground. In reality, the previous EOs directed very little regulation, outside mandating a number of actions within the government itself. I'm all for smart, tailored, less intrusive regulations. But it's really hard to justify scant or no regulatory actions, at the same time the administration is arguing that this technology will forever change the world. We're trying to create another Taiwan we have to protect lol No posts Ready for more?
--------------------------------------------------

Title: Bond Yields & Stocks: Something’s Happening Here
URL: https://www.forbes.com/sites/bill_stone/2025/05/25/bond-yields--stocks-somethings-happening-here/
Time Published: 2025-05-25T12:24:45Z
Full Content:
ByBill Stone ByBill Stone, Contributor. The recent sharp rise in US Treasury bond yields has raised questions about whether the move is ... More connected to US fiscal policy or the end of American exceptionalism. The evidence for alternate explanations is examined. Despite some pundits being quite convinced that US fiscal policy or the end of American exceptionalism is the reason for the recent rise in US Treasury yields, Buffalo Springfield was closer to reality when they sang, “Something’s happening here, What it is ain’t exactly clear.” It is not disputed that 10-year US Treasury yields are 52 basis points (0.52%) higher since the 2025 low of 3.99% on April 4. Using US Treasury Inflation-Protected Securities (TIPS) data, expected higher inflation accounted for 15 basis points, while the remaining 37 are from a higher real (after-inflation) return. 10-Year US Treasury Note: Yield Decomposition Notably, yields were at their nadir when the betting odds of recession were at their highest. As one should expect, yields have risen as the odds of a recession have declined. Directionally, the move in yields is logical, though some might still argue that yields have increased more than is warranted. Yields & Recession Odds US government bond yields in the US have risen more than many other developed countries since April 4. Interestingly, our neighbor to the North has experienced a similar increase. 10-Year Government Bond Yield Change Since US Low Notably, global government bond yields hit their low during the pandemic and have trended higher since then. Global Yields If one looks at government yields since the end of last year, a different picture emerges. While higher government bond yields have been a global phenomenon, the US has seen slightly lower yields! 10-Year Government Bond Year-To-Date Yield Change The other proof point for those arguing for the danger of US fiscal policy or the end of American exceptionalism is the recent weakness in the US currency relative to other currencies. While there is no doubt that the US dollar has been weaker year-to-date, it follows a period of exceptional relative strength. The US dollar remains stronger than it has been most of the time since 1999. Further, as shown in the historical data, periods of a weaker US dollar do not uniformly lead to higher US Treasury bond yields. US Dollar Without exception, the fiscal position of large countries, as measured by government debt relative to GDP, deteriorated with the impact of spending during the pandemic. Most countries were already piling on more debt relative to economic activity, but the pandemic accelerated the trend. Government Debt-To-GDP There is no magic level of debt-to-GDP that signals disaster since countries with a more resilient economy can service more debt. Furthermore, countries like the US, which has a high per capita GDP, control of the global reserve currency, and only issues debt in that currency, can handle significantly more debt levels than most other countries. German government debt levels are understated relative to reality. While Germany isn’t legally liable for the debts of its profligately spending neighbors, it shares a common currency and its share of European Union (EU) issued debt. Historically, Germany has been forced to contribute the most to bailout funds when other EU countries have encountered problems. Recent Government Debt-To-GDP Ratios The US House of Representatives passed its tax legislation, which, despite reports to the contrary, does not make the US fiscal situation any worse, according to Strategas. Tariffs should produce about $200 billion in revenue annually and are not included in the legislation’s official scoring, leading to much confusion. The US Senate will almost certainly make some changes, so the House bill is unlikely to be the final version implemented. The S&P 500 sits only 5.6% below its mid-February high in a robust rebound from the 19% decline from the peak reached on April 8. The Magnificent 7, consisting of Microsoft (MSFT), Meta Platforms (META), Amazon.com (AMZN), Apple (AAPL), NVIDIA (NVDA), Alphabet (GOOGL), and Tesla (TSLA), has fared worse, and the group is 12% below its mid-December summit. Market Returns The proximate cause of the decline in stocks last week was the reheating of the tariff war, which increased the headwinds for the US and global economy. President Trump threatened the European Union with a 50% tariff and smartphone makers, notably Apple (AAPL), with a 25% tariff rate. The betting odds of recession rose to 41%, sending stocks 2.6% lower last week. Betting Odds of 2025 US Recession Despite the likely economic drag from the tariffs, only two expected rate cuts for 2025 are expected. There is little chance of a rate cut at the mid-June Federal Reserve meeting. Number Of Fed Rate Cuts Expected The primary focus will likely remain on the fallout from the tariffs, with markets watching for any changes in US policy and retaliation or concessions from other countries. Friday’s April inflation reading will likely be friendly. The Core PCE Price Index is the Federal Reserve’s favorite measure of inflation and should moderate to 2.5% year-over-year from the 2.6% pace in March. The last Magnificent 7 stock to report earnings is Nvidia (NVDA) on Wednesday after the close. As the leader in artificial intelligence chips, its results and forecasts will be closely watched to judge the health of technology spending. The rise in US Treasury yields does not point to a US-specific problem; instead, it is a function of the lower probability of an economic downturn and perhaps a shift in the global appetite for government bonds. Yields are not high enough yet to significantly negatively impact stock valuation since the higher yields are accompanied by less risk of an earnings decline from a recession. Like many other countries, the current fiscal trajectory in the US is unsustainable, but the recent tax bill wouldn’t worsen things. Investors can be forgiven for wishing it improved the path, but the Senate will have their say next, and bond market participants will be watching closely. Government bond investors are demanding higher yields from most countries, so there might be a shift in the willingness of markets to fund large deficits, but it is too early to know for sure. Disclosure: Glenview Trust may hold the stocks mentioned in this article within its recommended investment strategies.
--------------------------------------------------

Title: Best Memorial Day Deals Under $50: We've Found 35+ Deals Offering Great Value This Weekend
URL: https://www.cnet.com/deals/best-memorial-day-deals-under-50-2025-05-25/
Time Published: 2025-05-25T10:24:02Z
Full Content:
Snag a great deal for under $50 on top tech, home goods or everyday essentials. But act fast, we're in the final hours of many of these Memorial Day sales. Our expert deal-hunting staff showcases the best price drops and discounts from reputable sellers daily. If you make a purchase using our links, CNET may earn a commission. Memorial Day is officially here, but with some of the best deals already selling out, you don't have much time left to shop the best sales. We're in the final hours of the top sales from retailers like Amazon, Best Buy and Home Depot, but there are still plenty of incredible opportunities to save big. And the best part? You don't need to restrict your search to big-ticket items like TVs, appliances or camping gear. There are tons of wallet-friendly deals to be found on discounted tech and daily essentials to stretch your budget further. Shop these deals now to score some serious savings before prices bounce back. Bookmark this page and check back often -- we’re refreshing it regularly as deals end, sell out or get last-minute markdowns. And if you’re hunting for even lower prices, be sure to explore our curated list of Memorial Day deals under $25. Amazon Fire TV Stick 4K is a great way to convert any 4K TV with an HDMI port into a smart TV. Whether you have a TV that doesn't have smart features or if you just don't like the smart TV interface of your current TV, the Fire TV Stick 4K is the product for you. With this $20 discount off of the $50 list price, the Amazon Fire TV Stick 4K is an even better value. The JBL Go 4 is a tiny but mighty Bluetooth speaker. You can take it anywhere as it has a small loop that you can use to attach to your backpack, keychain or anywhere else. It's small enough to put in your pocket or purse, but it doesn't skimp on sound with 5 watts of power and ample volume. The battery lasts seven hours, so you don't have to worry about it running out of power quickly. This compact smart camera can help you keep an eye on your home and pets when you're away. It includes a 1080p HD live video feed, motion alerts, two-way audio and night view with a built-in spotlight for better visibility. (Or startle the dog off the couch again.) Ugreen's Nexode series has some of the best chargers we've seen. This particular adapter has a 4-port design (three USB-C and one USB-A) and can be used to charge more than just phones. It's also compatible with laptops and handheld consoles. If you're often jet-setting from one country to another or have friends and family that visit often, then this universal travel adapter is something you need. It has a portable design, safety shutters and international compatibility, making it a must-have. More Memorial Day deals under $50: Glad you asked. If you're looking for even more savings, here are the best Memorial Day deals under $25. You can also take a gander at some of the best Memorial Day bargains being offered by Amazon and by Walmart. For a grand overview, check out our roundup of the ultimate sales and lowest prices available before Memorial Day weekend. Happy deal hunting. Many of us at CNET have covered shopping events for over five years, including Black Friday, Prime Day, Memorial Day and countless others. We've gotten good at weeding out scams and superficial deals, so you see only the best offers from all over. When choosing deals to show you, we look for real discounts, quality reviews and remaining sale time. Our team of experts has tested countless products to ensure we're only sharing the best options under $50.
--------------------------------------------------

Title: ValueAct takes a stake in Rocket Cos. Here’s how the activist may help lift the share price
URL: https://www.cnbc.com/2025/05/24/valueact-takes-a-stake-in-rocket-cos-how-the-activist-may-help-lift-shares.html
Time Published: 2025-05-24T12:52:55Z
Description: Activist ValueAct has a keen appreciation for artificial intelligence, and Rocket is poised to supercharge its business.
--------------------------------------------------

Title: How to Liberate Yourself
URL: https://erickimphotography.com/blog/2025/05/23/how-to-liberate-yourself/
Time Published: 2025-05-23T23:33:56Z
Full Content:
It is all here: Financial, economic independence, liberation: MSTU—> 2x levered long MSTR, REX Shares, trustworthy as the CEO did a live interview with Saylor on BMAX, … which holds corporate Bitcoin backed bonds ,,, mostly Strategy. So,,, if bitcoin goes up 5% in a day, Strategy will go up 10%, then MSTU (2x MSTR) will go up 20%—> simple math! Buy bitcoin with Coinbase, mortgage as much of it as you can, use the cash, to buy MSTR and or MSTU (2x levered long MSTR, which is essentially 4x bitcoin). You can then: Once JPMORGAN Chase starts offering you the chance to buy bitcoin with them and or to custody it with them, then you know you’ve arrived! Introduction to Bitcoin Lecture Video Super pumped to share with you, my first full length lecture on an introduction to bitcoin, the bitcoin Revolution, and also this edited transcript that I provided for you! STOIC VLOG Something I have been meaning to write or create or do is like some sort of book, ebook, pamphlet, or introductory primer to stoicism. I really think that stoicism is probably one of the most useful and philosophical models to live normal every day real life. Yet, I haven’t really found a good instructional guide on it, especially when I was self teaching it to myself. Consider this a practical primer, cutting through the BS: Stoicism, stoic, the stoa in ancient Greece– essentially the stoa was like some sort of portico, patio, pillar, outside, essentially a spot where guys would just hang out, talk shop, talk philosophy, etc. ￼ ￼ I think about the show “Hey Arnold” in which I was raised with… the notion of “stoop kid“, the notion of a stoop is that in a lot of cities, especially the east coast in New York, you have this little stoop or porch, stairs that go outside your front door… and you could just hang out there, engage in social and neighborhood life etc. One of my happiest moments was when I was living in Providence Rhode Island, and then COVID-19 hit. Everything was closed, besides the park. I can still go to the park, hang out, workout, do chin ups– I learned how to do muscle ups, more bodyweight calisthenics stuff, and also… I had a lot of fun with this “rock toss“ challenge and workout… in the middle of the park was a huge ass rock and huge ass stone, and every single day I would go there pick it up, and then eventually work out with it; throwing it around for fun, doing overhead presses with it, clean and jerks, squats, and eventually I would just throw it around for fun. Funny enough it might have been the most fit I was in my life… this was the true “functional” fitness. The inspiration — Hector lifting an insanely massive stone (barely 2 strong men could lift it)… using it to break down the door of the ships of the other side. Anyways, the reason why that period of covid was so good is that it was in the middle of beautiful Providence Rhode Island summer, so nice and bright and warm and lovely… and one of the good things was going to the park was like an open forum, a new anatheum for a lot of really cool guys to come, hang out, talk shop, go topless and shirtless, workout and hang out. I met some really interesting people during that period of time. I met some guys who were really cool. For example, one guy I met was in the US military Navy, I think he was training to be a Navy seal or Delta force or something. Another guy in some sort of ROTC training, another cool guy from the hood, and also I would say I probably met half a dozen friendly drug dealers there. And of course a lot of people who believed in conspiracy theories; really friendly, a little weird, but overall good guys. Anyways, one of the biggest benefits of hanging out at that outdoor park, open air, nothing but green grass, the beautiful sun and the fitness equipment was that I think having this sort of open air environment is actually very conducive to socializing, thinking and thought, and pro social behavior. My theory about a lot of modern day antisocial behavior has to do with the structures which enclose us. For example, almost universally most guys at the gym are extremely antisocial. Why? My theory is that because most gyms have closed, cramped narrow ceilings, and do not have access to natural light, or outside space. Cramped indoor spaces promote antisocial behavior. The only good gym I went to which was interesting was the golds gym in Venice, which has this really big outdoor workout area. I think this is much more natural and more fun and better; to be able to work out directly outside outdoors, with your shirt off. Stoics, stoicism — it was originally I think codified by this guy named Zeno, and over time he picked up some followers. Essentially the whole thing happened organically; Zeno would first share his thinking on philosophy ethics and pragmatic ways to deal with other people and the downsides of life, he built a following, and then his followers would propagate the thoughts and start their own little schools of thoughts, their own little stoic clubs. Would I like about stoicism is how loosey goosey it is. It is kind of like zen, or taoism… it is not really quantified as a religion, or a strict moral order. In fact, a lot of the ancients stoics would meditate on random stuff like cosmology, natural sciences like Seneca, how volcanoes worked or whatever. I think nowadays in today’s world, we focus primarily on the pragmatic side; how to deal with fear, uncertainty, downsides etc. ￼I think I might’ve first learned about stoicism from Nassim Taleb and his ANTIFRAGILE book. I was curious, and my curiosity went to deep. To quote NASSIM TALEB and the Venetian saying “The ocean goes deeper, the deeper you wade into it.” I literally consumed every single book I could find on stoicism, even the obscure ones. Funny enough, a lot of the stoic thinkers tried to claim other philosophers as being stoic, like Seneca did with Diogenes the cynic. ￼ Cynic, cynicism, actually comes from the word canine, the dog. Diogenes was considered the “dog” philosopher, first used as a pejorative, but ultimately Diogenes reappropriated that title for fun! He saw dogs as tough, almost like wild wolves, rather than seeing them as a negative thing. Even Achilles when he was raging against king Agamemnon, he called him “dog faced“ as a heaping insult. First, Seneca. Seneca the younger, his dad was called Seneca the elder. In fact, this is such a big deal because Cindy and I named our first son, Seneca, directly after the stoic philosopher. This is true soul in the game; if you name your kid after your favorite philosopher, certainly it is a sign that you really liked that philosopher, or found them impactful. ￼￼￼The reason why I really like Seneca the stoic philosopher is because he had real connections to real reality. What that means is he wasn’t just on the sidelines; he actually existed in the real world, engaged in real politics, was even advisor to the emperor Nero, the bad one, who eventually low-key coerced Seneca to commit suicide, in a manly, dignified manner. I think this was because maybe… there was actually a plan to overthrow Nero, and essentially Nero found out. ￼ I really like Seneca because his writing is accessible, practical and pragmatic, and interesting. A lot of thinkers tend to lack connections to real reality, I have no tolerance for boring philosophers to talk about metaphysics, which is things which are not physical. Like thoughts ideas, the universe, electricity and energy, strange phenomenon and conspiracy theories on ghosts, “energy” whatever. ￼￼For a long time, I would hear the term “metaphysics” being thrown around, and I had zero idea what it actually meant. ￼ Meta– on top of. Or nestled within. Physics — the physical, physical phenomenon like gravity, first principles. The reason why metaphysics philosophers tend to be a bunch of losers is that they are all weak and anemic, nerds or geeks or weaklings who seem to have some sort of physiological degeneracy, which encourages them to opine or talk or think about impractical things, superficial things. Personally speaking, I think philosophy must be practical. ￼￼￼￼ Practical, praxis, ￼practice — to do! I have a very funny ideal; the general idea is that your body looks like a demigod, and your physiology is out of control. The general idea is one must be tall, strong, highly muscular, low body fat percentage, I’m not exactly sure what my body fat percentage is, but maybe it’s around 5%. ￼￼Also, physical fitness is critical to any stoic. My ideal is to walk 50 miles a day, eat 20 pounds of meat like Milo of Croton a day. And also, abstinence from silly things like media, alcohol, drugs, marijuana etc. Trust no thinker who does drugs! Even our best friend Nietzsche said that coffee was bad, because it would make people dark and gloomy. He encouraged 100% cocoa powder instead. ￼￼￼ Simple technique: First, look at a picture or a portrait or a full body shot, ideally topless of the artist, philosopher or thinker or individual… then judge their thoughts later. Why? My theory is this: the thoughts of an individual is hugely affected by their bodily physiology. For example, an extreme example: if somebody is locked inside a solitary confinement cell, and not permitted to go outside for years, but, he had a pen and pad and would jot down some thoughts… Would be the quality of these thoughts? Certainly dark and morose. In today’s world, why does it matter, what is the significance of stoicism, etc.? First and foremost, I think we are living in a troubling time, especially with the advent of modern day internet based media and advertising. I think 99% of what is propagated on the internet is fear mongering, and what is hate? Hate is just fear. The first thought on stoicism is that it is just fear conquering. What I discovered about street photography, is that 99% of it is conquering your fears. Conquering your fears of upsetting other people, getting in some sort of verbal or physical altercation etc. In fact my bread and butter workshop is my conquering your fears and street photography workshop, the workshop which is still interesting to me even after a decade. Why is this so important? I think it is rooted in almost everything; conquering your fears is rooted in entrepreneurship, innovation, risktaking and real life. Even my speculation in crypto. 99.9% of crypto speculation is just balls. Having the balls to make big bets, and when things go south, knowing how to master your emotions. A simple extra I have is this: just imagine it will all go down to zero. It was useful because when I was in college, my sophomore year I got really into trading stocks, and I eventually lost my whole life savings, maybe around $3500 USD, and some bad penny stock which I actually misread the financials… the whole time I thought the company was making a profit, but actually it was taking a loss. I actually didn’t know that if profits are written in parentheses, it means a loss. It was funny because my initial start as an investor was back in high school, I bought some Adobe stock when I was a high school junior, and also some mutual funds, which both went up after about 4-5 years. ￼Also I remember in elementary school computer class, when I was in the sixth grade in Bayside Queens, there was some sort of stock stimulation trading game, and actually it was funny… the kids who made the most money and were the most successful just put 100% of everything into Apple, note this is when we were only 12 years old, and I was born in 1988. ￼Funny enough, it seems that stoicism actually plays well with capitalism. Why? According to modern day capitalist thinking, the best way to approach life is to be objective, strong, stoic, unemotional, logical and rational. Also, ￼with modern day media there is so much fear mongering in the news, about some sort of global armageddon, global financial ruin, etc. I call it “fear porn”. Therefore stoicism as a mindset is useful to think and position your mind in such a way that you could consider that life is all upside, no downside. In fact, if I could summarize stoicism in one sentence, it is that life is all upside, no downside. Inspired by NASSIM TALEB. So, is stoicism useful to you if you’re a man or a woman? Does it matter? The good thing is I think it could apply to both sexes. Conquering sexism and social pressures is useful if you’re woman, and also if you’re a man. Also, gender is social. Lot of the expectations set on us by society is socialized and gamed to a certain degree. First, we got to unchain ourselves from modern day ethics and morality. I believe that all modern day philosophy and thinking and ethics and religion is bad. For example, the notion of turning the other cheek is a patently bad one. Why did Jesus turn his cheek? It is because he lacked on army. Also, philosophically I think we should put no trust in Socrates. ￼￼￼I thought which has puzzled me for a long time was this “Why was Socrates so ugly? Monster in face, monster in soul. I think Socrates was a degenerate, and he lacked any sort of real power. Therefore he turned logic and rationality into his terrorizing weapon (via Nietzsche). Back in the day, you didn’t need logic or rationality to have things your way, you simply was able to dictate that which you wanted to pause it, because you had a military force behind you. Just think about Machiavelli and IL PRINCIPE– the reality of being a mercurial prince, king, and military leader is hard, stoic, “immoral”. But ultimately it all comes down to war, conquest, the military. Trust nobody who uses rationality or logic as their tyrannizing weapon. ￼ In fact, I believe that all should have the body and strength of some sort of super soldier. Essentially look like all the guys from the movie 300, this is our ideal. Demigod physique. ￼ And this is the true courage of Kanye West; he literally put everything on the line, and even lost his spouse and I think maybe his kids? All for the sake of revealing inequities. ￼ “I throw these Maybach keys fucking c’est la ￼vie! I know that we the new slaves.”￼- Ye The fun thing about stoicism is that you could just make it up as you go, devise your own strategies and whatever. “Fucking c’est la vie!” My favorite Kanye West line. Essentially the general idea is that in life, one should not take things too seriously. Laughter is golden, I forget the philosopher who was called the laughing philosopher… Democritus?; better to laugh about the follies of human beings rather than to be dark and morose about it. Also, thoughts from the Odyssey; if you look far enough ￼ into the future, everything becomes comedic and hilarious. So when you’re in some sort of bad situation, just think to yourself “Perhaps one day, 20 or 30 years from now… I will look back at this and just laugh!” It will just be humorous. Honestly, laughter, and kind of being able to joke about things might be the best way to live life and deal with setbacks. ￼ Problems in modern day life: A funny thing I have learned is that when you call something something, it isn’t that. For example, if someone calls something a “luxury car”, it ain’t. For example, a true modern day luxury car is maybe a Tesla, but Tesla never calls itself a luxury car. Also the ultimate luxury technology company is probably Apple… but Apple is very intelligent and not calling themselves a luxury brand. A pro tip is when it comes to websites, read the alternative text, the header text, the stuff that shows up in the tab of your browser window. If the website, the automotive retailer tries to market themselves as a “luxury” brand, typically it is actually a sign that it isn’t a luxury brand it isn’t luxury brand. Thought: what are some good examples of true luxury brands which don’t overly calls itself luxury? ￼￼ In someways, we can think and consider stoicism as our new luxury. In fact, having luxury, luxury of mind and soul… and luxury of freedom of speech, isn’t this the ultimate luxury? When somebody asked Diogenes the cynic; “What is the best human good”? He said “Freedom of speech, speaking your mind, having the power to see whatever is on your mind.” In fact, my current joy is becoming more and more free talking, and free riding. What that means is this; I’m ain’t going to censor myself no more, even if I might be politically incorrect insensitive or whatever. ￼ Also, I would prefer to speak my mind and seriously hurt the feelings of others, rather than soften it for the sake of the other person. ￼￼￼Similarly speaking, when people call themselves “influencers”, they are not influencers. Ultimately I think we should think of stoicism just like having another tool inside our tool kit. For example, if you’re a chef, you’re going to have different knives for different purposes. If you’re going to cut a big piece of meat, you probably want a big ass meat cutting knife, not something you would use to slice an apple with. Similarly speaking, if you’re going to scoop out the insides of an avocado, better to use a spoon rather than using a fork, or a knife. ￼ I think the problem is when some people get too into stoicism (I prefer writing stoicism with a lowercase￼), they think that everything needs to be consistent, and must fit into this nice little neat box of what is considered “stoicism“. This is a bad line of thinking… let us consider that Marcus Aurelius never even mentioned stoicism in his writings, his collections of thoughts, which we moderns call THE MEDITATIONS… it was just essentially his personal diary, to help him conquer his own personal fears and thoughts, I don’t think he ever intended it to be published publicly. I think he just wrote it to himself as self therapy. And I think the only stoic philosopher he even mentions is maybe Epictetus. For myself, I just come out with certain to work out thoughts and techniques because it helps me, and when I find these tricks or techniques or secret hacks or cheat codes… My passion is to simply share it with others. ￼￼￼￼ And ultimately, things are ever in flux and evolving and changing and adapting. For example, I’ve discovered the quality of my thinking is different when I am in Culver City Los Angeles, compared to being in the boring suburbs of Orange County. Also depending on my social environments… my stoic thoughts are different when I am in a gym, vs just working out by myself in my parking spot in the back of my apartment. Also, the quality of my thoughts is different when living with family members or other people versus just living with myself Cindy and Seneca. Assuming you’re not growing your own vegetables and living in the middle of nowhere… you probably have some interaction with other human beings. As long as you have an iPhone, an Android phone, a smartphone, a 4G or 5G internet connection, wifi, a laptop, have to buy groceries somewhere… you’re still going to have to interact with other human beings. And this is good. There is no other greater joy than other human beings. In fact, modern-day society is strange because in someways, the ethos is to be antisocial and to be cowardly. But in fact, the best way to think about things is that real life is interaction with other human beings, and social conquest. One can imagine a lot of modern day entrepreneurship as simply a big dick swinging contest. He who is the most masculine confident tall and strong and stoic shall win. Assume that everyone is mentally insane: Have you ever been out in public, and you see some sort of crackhead or strange homeless person who acts radically, smells terrible, and is obviously mentally ill? Do you hate them for it? When they say something weird to you… do you take it personally? No. Why? They are crazy. Perhaps we should just adopt this stoic mindset towards other people; some people are actually physiologically ill, mentally unwell… don’t trust the opinion of nobody. A lot of people are trying to actually deal with their own inner demons: For example, becoming the successful photographer and street photographer I am today… I’ve dealt with some individuals who would say anonymous bad things about me, and later I found out that their mom just died or something. I cannot imagine what it feels like losing a mother… therefore if somebody spew some hate on me because something bad happened to them, I’m not gonna take it personally. ￼￼￼￼￼Self-flagellation: I think a lot of people who are sick, mentally or physiologically self flagellate themselves. Essentially the way that they deal with other people or themselves is some sort of metaphorical self-flagellation. For example… you know those strange individuals who have the whip and whip themselves, and inflict pain on themselves? I think some people do this metaphorically to themselves and others. You just want to stay away from them. My personal theory on fear is that a lot of it is tied to morality and ethics. I think the general idea is not necessarily that we are afraid of anything… I think the true fear is that we’re afraid that we are some sort of bad evil unethical immoral person. For example in street photography, the general ethical thought is that it is immoral to take a photo of somebody without their permission, because there is some sort of it inherent evil behind it. Is this true? No. Taking photos and not really a big deal. I think it is because some people are just overly sensitive, which once again comes from some sort of physiological weakness. For example, if you’re a weightlifter who could lift 1000 pounds, assuming you’re not taking any steroids or anything… are small things going to bother you? No. But let us assume that you are a skinny fat man, all you do is drink alcohol and smoke marijuana and watch Netflix, and you spent too much time on Reddit… you are 40% body fat, and have never lifted in your life. And also your testosterone is low and you never go outside. Certainly the quality of your thoughts is going to be different than if you’re a happy gay monster, lifting weights outside in the direct sun, laughing and having fun. ￼￼￼ In fact, I’ve actually personally discovered that the reason why a lot of people hate me is because I am so happy jovial and gay. They are secretly suspicious or envious of me? ￼ Probably one of my worst experiences was this jarring transition; I was super happy insanely happy being in Vietnam in 2017; with a beautiful weather, the beautiful light, the happy people the great amenities etc.… and then that winter Cindy and I went to Europe, in Marseille Berlin and Prague, and maybe London… seriously the worst winter of my life. Why? I wonder if so much miserable feelings and thoughts simply comes from the darkness and lack of light. a lot of Europe is actually quite miserable; dark, unhygienic, morose. Even Nietzsche had a thought about Schopenhauer; How much of these emo European philosophers came from the fact that it was just complaining about the cold weather in Germany etc.?￼￼For myself, my ideal weather is Southeast Asia; I love being in Phnom Penh Cambodia, Vietnam etc. In the states, am I the only one who loves living in Los Angeles? Dr. Dre and Kendrick Lamar said that LA was the best for women weed and weather… I would definitely say the biggest upside of living in Los Angeles is the light, the sunlight. It actually does get quite cold here, but usually most reliably even in December during the winter time, the sun will always come up. As long as there is bright sunny light, I will be happy. And I think maybe for myself, considering that I am a photographer, and photography means painting with light… light for me is critical. I also wonder how much of it is a physiological thing and a genetic trait; for example I could even recall being a young child, and my mom telling me that the most critical thing in finding a home or an apartment was light and natural light. Even now… 90% of my happiness comes from being able to ￼have access to natural light, ideally floor to ceiling windows facing directly the sun, having some sort of modern temperature regulated apartment and home. Even living in our tiny studio minimalistic luxury apartment in Providence Rhode Island, where it was always 75° warm and cozy, and not frigid and damp and cold and dark and humid… I was always good. But moving to an older house, where it always felt damp and cold… this literally lowered my happiness by 1000%. Therefore, if you’re feeling miserable sad or whatever… I say spend three months living in Hanoi or Saigon in Vietnam, or go to Phnom Penh Cambodia. I wonder if 90% of peoples misery is simply due to the weather. ￼￼”Better to be a gay monster than a sentimental bore!” – Fernandino Galliani, via Nietzsche My stoic ideal is somebody who is happy, gay, smiling, no headphones or AirPods on, no sunglasses on, no hat, no facial hair, no baggy oversized clothing, no tint in their car. Somebody who makes great eye contact, laughs, stands up upright, jokes, and fools around. Like an overgrown child. ￼Also, lift weights at least once every day, ideally in the direct sun. Just buy some weightlifting equipment on Titan.fitness, I like the farmers carry handles, the Olympic loadable dumbbell, and also the Texas power squat bar. Just buy some cheap weights, and or buy a heavy 400 pound sandbag, and just have fun throwing it around. A true stoic should look something like Hercules or Achilles. Or like ERIC KIM; I have the aesthetic and the physique of Brad Pitt in FIGHT CLUB except with a lot more muscle. Like my friend Soren says, the Adonis physique and proportions. I think a real stoic is sexy, happy and fun. Who doesn’t take life too seriously; and think of everything like a fun game. A real stoic would be joyful and cheery like three-year-old child without any adulteration from the outside world. I don’t like talking with or hanging out with adults, uninteresting. At what point or age do people become so emo? Typically, highschoolers are very optimistic. Even college students. But I think at least in maybe college in high school nowadays… the bad trend is towards “over concern”, about the world the planet ethics animals etc. I find a lot of this thinking superficial, performative, and uncritical. I think “animal rights“, “saving the planet” is this new pseudo world religion; which is just capitalism 3.0. I find the whole pet industry the whole dog industry to be insanely bizarre, and I trust nobody who talks about “saving the planet“ who owns an iPhone, owns any sort of car, or has an Amazon prime subscription. Certainly not any vegans. Animals are animals. They are lower on the hierarchy and totem pole on earth. Man is the apex predator, the apex bully and the apex tyrant. Should we care for animals or “animal rights”? No. Animals are our slaves. If you consider even dogs and pets… they are essentially our emotional slaves. People talk a lot about the virtuosity of dogs being loyal or whatever… and giving you unconditional love. This seems like some sort of emotional slavery. The only dogs I respect are some sort of canine dogs, some sort of attack or defense dogs, or hunting dogs. For example, John Wick 3; Halle Barry and her dogs. An animal should either be a weapon, or nothing. Essentially it looks like men no longer have a backbone. No more spine. I trust nobody who owns a dog. Let us not forget; they call it dog ownership, or “owning a pet”. There is no more concept of “human ownership, or “owning a human.”￼￼￼ What is the end goal of humanity? To me it is towards entrepreneurship, innovation, art and aesthetics, philosophy etc. Design. Stoicism should be considered a tool which could aid you in these things. For example, I think 99% of entrepreneurship is courage. Stoicism could help you with that. I also think with design, great design is also 99% courage, having the courage to attempt something that won’t sell or be received well… stoicism is all about practical courage. The only designers with courage include Steve Jobs, Jony Ive, Elon Musk, Kanye West. ￼ Also, weightlifting. To attempt to lift a certain weight you have never attempted before takes great courage. For example, me atlas lifting 1000 pounds; that is 10 plates and a 25 on each side, ￼￼￼this is true stoic training. Why? The fear of injury is what holds most people back; if you had successfully conquered this fear and not injured yourself, this is pure stoic bliss. I think the only and the only proper way to lift weights is one repetition maximum training. That is; what is the maximum amount of weight you’re able to successfully lift or move, even half an inch? To me, the courage is the success. Even if you had the courage to attempt it… that is what is considered success. ￼￼ Simple exercises to do include the atlas lift, innovated by ERIC KIM, or a one repetition max rack pull. Or, a high trap bar deadlift, heavy Farmer’s walks, or heavy sandbag carries. Or even a simple thing you could do is go to the park or to the local nature center, find the biggest rock there and just see if you could pick it up. If you’re interested in stoicism, and have had some interesting thoughts on stoicism, one of the most noble things you could do is start your own blog. I think blogs are 1000 times more effective than publishing some sort of static printed book; I think the problem in today’s world is that everyone is seeking some sort of legitimacy by being picked up by some sort of legitimate publisher and getting “published“, and seeing your printed book at Barnes & Noble whatever. I say it is better to be open source, free and permissionless, ￼￼￼decentralized. Just publish your thoughts and book as a free PDF, and just host it on dropbox, Google Drive, or your own web server. Share the link freely, and also just publish the raw text as a big blog post. ￼ Even Sam Bankman-Fried wisely thought; 99.9% of books could just be summarized as big blog posts. Don’t trust any modern day published book which isn’t free, because… there is some sort of hidden clout chasing somewhere. ￼ Even one of the worst compromises that led to the demise of Ray Dalio was the fact that he took his Principles book, which was essentially a free ebook PDF on his website, and then took it off, because I think he got a book deal with Simon and Schuster. After he did that, he lost my respect. If you’re already independently wealthy, and you don’t crowd source your self-esteem… why would you need to externally validate yourself by getting some sort of constipated publisher and annoying editor? Editors are bad. Start your own blog and start blogging your own thoughts on stoic philosophy, and even start a YouTube channel and start vlogging on it. ￼￼My generalized thought is simple: if your thought your idea your blog post your video or whatever could even impact the life of one other human being on planet earth… it is worth it. ERIC What is the secret to the maximum amount of happiness in life? The maximum amount of danger. (Nietzsche). ERIC FIN Become invincible: Learn from the master stoics: See all philosophy > ALL OPEN SOURCE EVERYTHING! EK IDEAS
--------------------------------------------------

Title: DoubleVerify Holdings, Inc. Investors: Robbins LLP Reminds DV Shareholders of the Pending Class Action Lawsuit
URL: https://www.globenewswire.com/news-release/2025/05/23/3087752/32719/en/DoubleVerify-Holdings-Inc-Investors-Robbins-LLP-Reminds-DV-Shareholders-of-the-Pending-Class-Action-Lawsuit.html
Time Published: 2025-05-23T21:26:00Z
Full Content:
May 23, 2025 17:26 ET | Source: Robbins LLP Robbins LLP SAN DIEGO, May 23, 2025 (GLOBE NEWSWIRE) -- Robbins LLP reminds stockholders that a class action was filed on behalf of investors who purchased or otherwise acquired DoubleVerify Holdings, Inc. (NYSE: DV) common stock between November 10, 2023 and February 27, 2025. DoubleVerify operates a software platform for digital media measurement and advertising optimization services. For more information, submit a form, email attorney Aaron Dumas, Jr., or give us a call at (800) 350-6003. The Allegations: Robbins LLP is Investigating Allegations that DoubleVerify Holdings, Inc. (DV) Misled Investors Regarding its Business Prospects According to the complaint, during the class period, defendants failed to disclose that: (a) DoubleVerify’s customers were shifting their ad spending from open exchanges to closed platforms, where the Company’s technological capabilities were limited and competed directly with native tools provided by platforms like Meta Platforms and Amazon; (b) DoubleVerify’s ability to monetize on its Activation Services was limited because the development of its technology for closed platforms was significantly more expensive and time-consuming than disclosed to investors; (c) DoubleVerify’s Activation Services in connection with certain closed platforms would take several years to monetize; (d) DoubleVerify’s competitors were better positioned to incorporate AI into their offerings on closed platforms, which impaired DoubleVerify’s ability to compete effectively and adversely impacted the Company’s profits; (e) DoubleVerify systematically overbilled its customers for ad impressions served to declared bots operating out of known data center server farms; and (f) DoubleVerify’s risk disclosures were materially false and misleading because they characterized adverse facts that had already materialized as mere possibilities. The complaint alleges that the truth was revealed on February 27, 2025, when DoubleVerify reported lower-than-expected fourth quarter 2024 sales and earnings due in part to reduced customer spending and the suspension of DoubleVerify services by a large customer. Defendants also disclosed that the shift of ad dollars from open exchanges to closed platforms was negatively impacting the Company. On this news, DoubleVerify’s stock price dropped $7.83 per share, or 36%, from a closing price of $21.73 on February 27, 2025, to a closing price of $13.90 on February 28, 2025. What Now: You may be eligible to participate in the class action against DoubleVerify Holdings, Inc. Shareholders who want to serve as lead plaintiff for the class are required to file their papers with the court by July 15, 2025. The lead plaintiff is a representative party who acts on behalf of other class members in directing the litigation. You do not have to participate in the case to be eligible for a recovery. If you choose to take no action, you can remain an absent class member. For more information, click here. All representation is on a contingency fee basis. Shareholders pay no fees or expenses. About Robbins LLP: A recognized leader in shareholder rights litigation, the attorneys and staff of Robbins LLP have been dedicated to helping shareholders recover losses, improve corporate governance structures, and hold company executives accountable for their wrongdoing since 2002. To be notified if a class action against DoubleVerify Holdings, Inc. settles or to receive free alerts when corporate executives engage in wrongdoing, sign up for Stock Watch today. Attorney Advertising. Past results do not guarantee a similar outcome. A photo accompanying this announcement is available at https://www.globenewswire.com/NewsRoom/AttachmentNg/39664050-92f3-4fd9-ac56-162b985c1fde Robbins LLP is Investigating Allegations that Organon & Co. (ORG) Misled Investors Regarding its Debt Reduction Strategy SAN DIEGO, May 23, 2025 (GLOBE NEWSWIRE) -- Robbins LLP informs stockholders that a class action was filed on behalf of investors who purchased or otherwise acquired Red Cat Holdings, Inc....
--------------------------------------------------

Title: Wall Street cheers as Tesla gets massive price target boost, Dan Ives predicts a golden age of autonomous growth
URL: https://economictimes.indiatimes.com/news/international/us/wall-street-cheers-as-tesla-gets-massive-price-target-boost-dan-ives-predicts-a-golden-age-of-autonomous-growth/articleshow/121369176.cms
Time Published: 2025-05-23T19:07:05Z
Full Content:
(Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. What pizzas are Indians eating? The clue lies with India’s largest QSR. Why high gold prices are making both banks and their borrowers smile How does IndusInd’s ‘fraud’ tag affect its future? RBI has cut repo rates twice in 3 months. But that isn’t enough to boost the economy! As India steps up commercial shipbuilding, it needs a supply chain boost Stock Radar: RITES stock breaks out from Ascending Triangle pattern; time to buy? Trump calls for 50% tariffs on EU goods Bondi condemns Israeli Embassy shooting 'We didn’t hit their army, only terror camps' Vance's big remark on Houthis weeks after ceasefire Trump threatens Apple with 25% tariff $50 Billion Door Opens for US Firms Iran’s fresh warning to US amid nuke tensions Mukesh Ambani's 6 commitments for Northeast states Live: PM Modi's address at Rising Northeast Investors Summit Iranian FM warns US against deal terms on enrichment Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Commodities Top Story Listing Top Prime Articles Top Definitions Top Slideshow Private Companies Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: Tesla stock gets a huge price target hike as Wedbush hails 'golden age of autonomous growth'
URL: https://finance.yahoo.com/news/tesla-stock-gets-huge-price-144500442.html
Time Published: 2025-05-23T14:45:00Z
Description: Tesla (TSLA) might have been in the news recently for all the wrong reasons, but that’s not stopping Wedbush from issuing one of Wall Street’s most bullish...
--------------------------------------------------

Title: Tesla stock gets a huge price-target hike as Wedbush hails 'golden age of autonomous growth'
URL: https://qz.com/tesla-robotaxis-austin-fsd-autonomous-ai-1851782128
Time Published: 2025-05-23T14:31:08Z
Full Content:
Tesla (TSLA-3.33%) might have been in the news recently for all the wrong reasons, but that’s not stopping Wedbush from issuing one of Wall Street’s most bullish calls on the company in months. In a note Friday morning, analyst Dan Ives raised Tesla’s price target from $350 to $500, saying “the golden age of autonomous is now on the doorstep for Tesla.” The price hike is part of Ives’ larger vision for the company: a possible $2 trillion market cap by the end of 2026 in a “bull case scenario.” The potential catalyst? The long-promised launch of Tesla’s robotaxi service, which CEO Elon Musk claims will debut in Austin by the end of June — a “key next chapter of growth,” according to Ives. Wedbush’s price hike reflects this “massive stage of valuation creation ahead.” But Wedbush’s vision of Tesla is less about cars and more about code. “We have never viewed Tesla simply as a car company,” Ives wrote. “Instead we have always viewed Musk and Tesla as a leading disruptive technology global player and the first part of this grand strategic vision has taken shape over the past 5 years.” Ives sees Tesla as a platform positioned to capitalize on the convergence of autonomous driving and AI — an “AI revolution” that could eventually put Tesla in the same league as Nvidia (NVDA-1.54%), Microsoft (MSFT-0.92%), Palantir (PLTR+1.01%), Amazon (AMZN-2.87%), Meta (META-2.36%), OpenAI, and Alphabet (GOOGL-1.57%). “We believe Tesla remains the most undervalued AI play in the market today,” Ives said. “The core focus for investors is the AI Revolution is now coming to Tesla...which will make Tesla one of the best pure plays on AI over the next decade.” Ives said AI and autonomous are worth at least $1 trillion for Tesla. And the “golden goose”? The EV-maker’s Full Self-Driving (FSD) software, which has recently been more broadly rolled out. Wedbush said the automaker could see FSD adoption rates rise past 50% “and change the financial model/margins for Tesla looking ahead.” Still, the road to autonomy is littered with setbacks. Tesla’s FSD system has faced regulatory scrutiny and safety concerns for years, although Ives speculated that the close ties between Musk and President Donald Trump could help pave the way for fewer regulatory hurdles. And Musk’s timelines for autonomous Teslas have come and gone without delivery; FSD remains driver-assist tech, requiring a human at the wheel. Tesla also isn’t alone in the robotaxi push — Alphabet’s Waymo and Uber (UBER+0.83%) are still major contenders in the U.S., and China’s Baidu (BIDU-2.13%) is pushing forward on robotaxis abroad. Still, Ives believes Tesla has a leg up on the competition. “Given its unmatched scale and scope globally,” he wrote, “we believe Tesla has the opportunity to own the autonomous market and down the road license its technology to other auto players both in the US and around the globe.” Wedbush maintains its “Outperform” rating on Tesla, even as Ives warned that the stock’s trajectory will be volatile. “Rome was not built in a day,” he wrote, “and neither will Tesla’s autonomous and robotics strategic vision.” Our free, fast, and fun briefing on the global economy, delivered every weekday morning.
--------------------------------------------------

Title: The best IEMs for gaming in 2025, tested and reviewed
URL: https://www.popsci.com/gear/best-iems-for-gaming/
Time Published: 2025-05-23T14:00:00Z
Full Content:
By Chris Coke Published May 23, 2025 10:00 AM EDT We may earn revenue from the products available on this page and participate in affiliate programs. Learn more › I’ll tell you a secret: If you’re a gamer searching for the best sound quality for your games, you shouldn’t be looking at a gaming headset. Audio enthusiasts know that the best sound quality—for movies, games, and everything in between—comes from great headphones and in-ear monitors (IEMs). But when it comes to getting the the best isolation, audio, and comfort in a compact form factor, IEMs are the only way to go especially if you game on the go. If you’re on the hunt for an upgrade to your current headset and don’t mind using a separate microphone (at least, most of the time), there are lots of incredible options out there. I’ve been part of the IEM hobby for years and have done the hard work of testing dozens upon dozens of sets, on the hunt for the best of the best—like our top overall, the FiiO FA19. If you’re a gamer, this list is for you. I’ve been a gamer for as long as I can remember, cutting my teeth on the Atari 2600. In fact, I got my start in journalism writing about games for a number of small websites. In my adult life, I’ve fallen in love with the world of audio and IEMs in particular. I currently maintain an archive of over 100. Between my closet and storage spaces, I have around 30 pairs of headphones and have given away more gaming headsets than most people will in a lifetime. TL;DR: I love tech, I love gaming, and I love writing about the best picks for different types of people. This list represents my biggest hobbies, rolled into one. Chris Coke Why it made the cut: A customizable sound signature, great details, and a comfortable fit, the FA19 is tops for gaming. If you’re looking for the best of the best when it comes to gaming IEMs, the FiiO FA19 has you covered. This set doesn’t come cheap, but it’s an absolute banger for gaming and music alike. With its customizable sound signature, comfortable fit, and fantastic sound quality, it’s a stellar pick no matter what genre you’re gaming in. FiiO has been one of the biggest companies to watch for a number of years. Getting its start making portable digital audio players (DAPs) and amps, it has since expanded into IEMs, over-ear headphones, desktop amps and streaming media players, speakers, CD and cassette players, and that’s not even the full list. What’s even more impressive is that in most areas where it’s attempted to compete, it’s been able to offer high-quality options at prices that are comparatively quite reasonable. The FA19 is exactly such an option. It uses 10 balanced armatures per side and a shell 3D printed in medical-grade resin. It’s a comfortable fit, even as it offers 20 total speakers between its two earpieces, which usually necessitates a monster shell. The FA19 isn’t small, but it’s not uncomfortably large like many of its competitors are. That many drivers might seem excessive, but there’s a good reason for that excess. Each driver can concentrate on a particular frequency band, ensuring that none are pushed beyond their limits and that distortion is never a concern. Just as importantly, for gaming, all of those drivers make it easier to hear each distinct detail in the environment around you. You’ll never have to worry about footsteps or enemy vehicles sneaking up on you because they got lost in the mix. The FA19 provides exceptionally good layering, so you’ll always be able to make out every unique element that makes up your soundscape. It has also allowed the acoustic engineers at FiiO to fine-tune both of its distinct sound signatures. While the FA19 is hardly the only pair of in-ear monitors to offer a tuning switch, it is one of the few where its addition makes a meaningful difference to the sound. Here, you’re given the option between a reference tuning that’s perfect for discerning small details and a hi-fi tuning that provides a more exciting sound signature with added bass and treble. When I tested these, I found that I preferred the hi-fi tuning for its added energy and that it still performed well even in competitive shooters. For the utmost clarity, its reference option makes for a great alternative. And, surprise, surprise (not really), they are fantastic for music. It comes with a modular cable that supports a standard 3.5mm single-ended connection, as well as a 4.4mm Pentaconn connection for balanced sources, so you should be able to connect to everything from your motherboard to prestigious audiophile hardware (I recommend using a dedicated DAC for the best listening experience). When I’m not gaming with them, I’m using them to play guitar on stage with my church for a clean monitor mix. The only thing they lack is a microphone and a more accessible price point. But with the exception of only a few options in the IEM world, you’ll need to provide your own microphone anyway, so it’s hard to be too critical of this point. Overall, the FiiO FA19 is a pricey but outstanding pair of IEMs. They’re exceptional for gaming. And just about everything else, if you can afford them, they’re an easy recommendation. Tony Ware Why it made the cut: An expansive, holographic soundstage with impressively precise imaging offers strong spatial awareness in competitive gaming. The Kiwi Ears Aether carves a niche with a deft touch, walking the sonic tightrope between analytical precision and atmospheric immersion. These IEMs offer a soundstage that punches above their price point—an expansive, layered field that critics and users alike describe as impressively holographic. It’s not just wide; it’s well-structured, delivering the kind of positional accuracy that audiophiles crave and gamers covet. For music, the Aether’s oversized 15.3mm planar-magnetic diaphragms impress with articulate mids and a tight low end—clean enough for complex jazz arrangements, yet warm enough to give synth-heavy ambient tracks a sense of emotional heft. Highs shimmer without searing, providing detail without fatigue, a characteristic that makes long listening sessions a joy rather than a chore. The soundstage—wide, tall, and deep—creates breathing room for instruments, giving the illusion of space that’s more reminiscent of open-back headphones than a pair of IEMs. But where the Aether really surprises is in gaming mayhem. Footstep tracking in FPS titles is intuitive, thanks to that same dimensional clarity. Environmental cues and directional insights feel more vivid, not just heard but placed—a rare trait in IEMs under $500. Some might perceive a dip in sub-bass impact compared to bass-heavy sets (like the Kiwi Ears x HBB Punch), but it’s a tradeoff that benefits overall transitions and transparency. There’s a cohesion, a refinement to the sound throughout the frequency range, thanks to the single-driver, crossover-free setup (the same reason dynamic driver fans love the low-profile Sennheiser IE 600). The Kiwi Ears Aether isn’t trying to win you over with bombast. It’s a refined, thoughtful performer—an IEM that feels just as at home in a playlist deep-dive as it does in a multiplayer skirmish. Turtle Beach Why it made the cut: This affordable pair of IEMs adds a microphone to the mix and a surprisingly good tuning. Look no further than the Turtle Beach Battle Buds for gaming performance on a budget. At only $30 MSRP and frequently available for less, these earbuds represent one of the best values in IEMs dedicated to gaming. Between its price, performance, and versatility, they’re one of the only pairs of affordable “gaming earbuds” I recommend comfortably. They’re a great value for gamers on a budget. While there are plenty of IEMs that offer in-line microphones, there are very few that feature dedicated boom mics, but that’s exactly what this has. It’s clearer and louder than most in-line mics due to its proximity to your mouth, and that comes in clutch when you need to strategize with your team. When it’s not needed, it simply unplugs from the left earpiece, allowing you to have less weight and improved comfort as needed. Turtle Beach wisely outfitted each earpiece with a silicone retention wing, similar to exercise earbuds, so even with the mic installed, its weight doesn’t get in the way of comfort and stability. The quality of that mic is also surprisingly good. Its closest competitor is the SteelSeries Tusq, which is significantly more expensive at around $50. To my ear, the Battle Buds offer clearer pick-up, though neither offers software enhancements on their own. Since they connect with a standard 3.5mm jack, however, they’re compatible with third-party software. My ASUS ROG Maximus Hero gaming motherboard was able to apply its enhancements without difficulty. The earbuds themselves sound fine, but their quality won’t blow your socks off. I was able to pick out footsteps easily enough, but the Battle Buds lack the impressive layering and soundstage of other options on this list. As the saying goes, you get what you pay for. But for $30 or less, they’re not bad and will let you hear everything you need to without distorting or making you cringe from sibilance. The simplicity of their design, being traditional wired earbuds, also makes them versatile to use with different gaming platforms. Essentially, if your gaming system has a 3.5mm jack, they’re going to work, so PC, Switch, PS5, and Xbox are all perfectly compatible. At the same time, there’s no built-in DAC to add extra features like surround sound that would usually come with gaming headsets. This is something to keep in mind for most IEMs, however, and even those that do include DACs rarely offer surround sound or gaming features. As with most budget audio products, there are some concessions you’ll need to make with the Turtle Beach Battle Buds, but for their price, they are a fantastic value for anyone who wants the fit and function of in-ear monitors with a dedicated boom mic. There are plenty of other options that are worth considering, especially if you have a particular feature or purpose in mind. Even if you look for the “best earbuds for esports,” you’ll still find dozens of options to sift through, so I recommend a few other, more specialized sets to anyone scouring the market. For esports, look no further than the ZiiGaat x Fresh Reviews Arete. This set was tuned by Fresh Reviews, a popular YouTuber specializing in finding the best IEMs, headphones, and mice for competitive gaming. Though the brand is still relatively new, it has made a good name for itself with its handful of releases so far. This particular set features a single dynamic driver and four balanced armatures and can deliver a full and detailed sound signature that is also enjoyable for music. Still, it’s clear that competitive gaming was at the forefront of the YouTuber’s mind throughout the design process. It can pull important details like footsteps out of the mix in a way that enhances your situational awareness without sounding tinny like some full-sized gaming headsets. Like most picks, you will need a separate microphone with the ZiiGatt x Fresh Reviews Arete, which is something to keep in mind before pulling the trigger. Because they support a standard detachable cable, however, it’s easy to replace with a different option that includes one, like the Antlion Kimura. If you’re interested in cutting the cord, look no further than the Arc 3 Gaming Earbuds from Cleer Audio. While this is technically a cheat—as earbuds and in-ear monitors aren’t the same thing, and open-ear true wireless earbuds are even more removed—but it’s hard to deny the multiplatform, multipoint tech on display here. Featuring 16.2mm graphene drivers suspended at the ear canal opening to beam expansive, optimized spatial audio into your head while you maintain situational awareness. You don’t want someone sneaking up on you in real life anymore than you do in a game. Cleer Audio While there are plenty of true wireless earbuds to choose from, most pair over standard Bluetooth, which isn’t suited for gaming thanks to its higher latency. The Arc 3 buds sidestep that entirely with an included USB Type-C 2.4GHz dongle for a dedicated, ultra-low 30ms latency connection. You can plug it directly into your PlayStation 4 or 5, Nintendo Switch, Meta Quest, or your PC and enjoy wired-like gaming with the ease and freedom of true wireless. The Arc 3 simultaneously features Bluetooth 5.4 with Snapdragon Sound (supporting SBC and AAC but also aptX Lossless and LDAC) for hi-res music listening or conversations on the go. Plus there’s a dedicated app to fine-tune its performance and sound, which includes an adaptive mode that adjusts the volume to your surroundings. Their case is a little bulkier than the Galaxy Buds Pro 3 or Apple AirPods Pro, but along with the built-in batteries in each earbud, they offer an impressive 50 hours of battery life. Plus, they’re tuned for gaming audio and Dolby Atmos surround sound, so whether you’re playing a competitive shooter or getting lost in your favorite RPG, these earbuds have something for everyone. Finally, we have an additional suggestion for getting lost in your game with a deep, realistic soundstage and distinct details that enhance your immersion. For strong intelligibility and accurate placement of elements, the 7Hz Timeless 2 is another killer pick. The original Timeless has become legendary in the IEM world as one of the first sets to really kick off the affordable planar-magnetic revolution. This successor was a long time coming, but the Timeless 2 has been well worth the wait. It smooths out the frequency response of the original while still maintaining its excellent detail retrieval and crisp, wide sound signature, though its circular ear shells may appear uncomfortable at first, rest assured that the inner faces are shaped normally to fit comfortably in most ears. This is a set that you can listen to for hours and forget the world with. The stock sound signature is vocal forward with a cooler midrange, but it also comes with a set of interchangeable nozzles to customize the sound signature; however, I would recommend avoiding these as the stock nozzle provides the most balanced sound overall. Choosing a great pair of in-ear monitors is never as straightforward as you’d like it to be, and when you add gaming performance to the mix, it can be even more confusing. But, with the right pair of IEMs, you’ll find a new world of sound quality opens up for you, and you’ll hear your games in a new way—a way that gaming headsets usually struggle to come close to. So, grab an external mic and get to business. Great gaming audio is a quality pair of IEMs away. More deals, reviews, and buying guides Chris Coke specializes in the nuanced world of personal computing, whether it’s reviewing a laptop or sharing the best mechanical keyboard for your gaming PC. He began writing about consumer technology and video games professionally in 2013 for sites such as Tom’s Hardware, IGN, Reviewed, PC Perspective, and MORPG.com. At Popular Science, Chris covers a wide range of tech and buying guides, helping readers improve their lives through technology. Popular Science started writing about technology more than 150 years ago. There was no such thing as “gadget writing” when we published our first issue in 1872, but if there was, our mission to demystify the world of innovation for everyday readers means we would have been all over it. Here in the present, PopSci is fully committed to helping readers navigate the increasingly intimidating array of devices on the market right now. Our writers and editors have combined decades of experience covering and reviewing consumer electronics. We each have our own obsessive specialties—from high-end audio to video games to cameras and beyond—but when we’re reviewing devices outside of our immediate wheelhouses, we do our best to seek out trustworthy voices and opinions to help guide people to the very best recommendations. We know we don’t know everything, but we’re excited to live through the analysis paralysis that internet shopping can spur so readers don’t have to. Find out more about our product evaluation process. By Chris Coke, Tony Ware By Tony Ware By Chris Coke, Tony Ware By Stan Horaczek By Tony Ware By Tony Ware By David Nield By Tony Ware By Chris Coke, Tony Ware By Brandt Ranj, Chris Coke, Tony Ware By Sarah Jones, Tony Ware By Sarah Jones By Stan Horaczek By Sarah Jones, Tony Ware By Stan Horaczek, Tony Ware By Tony Ware By Brandt Ranj By Brandt Ranj By Brandt Ranj By Chris Coke By Brandt Ranj By Brandt Ranj By Chelsea Frank By John Alexander Breakthroughs, discoveries, and DIY tips sent every weekday. By signing up you agree to our Terms of Service and Privacy Policy. Articles may contain affiliate links which enable us to share in the revenue of any purchases made. Registration on or use of this site constitutes acceptance of our Terms of Service. © 2025 Recurrent. All rights reserved.
--------------------------------------------------

Title: Why Buy Microsoft Stock?
URL: https://www.forbes.com/sites/greatspeculations/2025/05/23/why-buy-microsoft-stock/
Time Published: 2025-05-23T13:45:55Z
Full Content:
ByTrefis Team ByTrefis Team, Contributor. Microsoft store in Manhattan, New York City, United States of America on July 16th, 2024. (Photo by ... More Beata Zawrzel/NurPhoto via Getty Images) Why would you want to pay 50x cash flow for Microsoft’s (NASDAQ:MSFT) stock, when you can buy Nvidia at about the same price? Nvidia’s revenues are growing close to 100%, Microsoft’s at 15%. Yes, not 50%, but 15%. What’s worse - Nvidia’s cash flow margins are about 47%, almost 1.7x Microsoft’s margins. In other words, more of that top line growth that Nvidia gets - makes its way to free cash flow! Cash that can be reinvested or distributed to investors as return. Good thing, who can argue? But Nvidia comes with large risk. Our High Quality (HQ) portfolio diversifies away stock-specific risks while giving exposure to upside, and has outperformed the S&P 500, the Nasdaq, and Russell 2000 - and has clocked > 91% returns since inception. Well, you could argue - Microsoft is a known, stable game. For more than 40 years, this thing has been running like a machine. Nvidia’s phenomenal growth is recent - it might fizzle. AI might fizzle. Large AI models might fizzle - or worse, deemed unnecessary. Deepseek showed small models work just as well, right? They didn’t even use Nvidia’s latest chips. However, what are the odds that Nvidia’s chips will cease to be the hottest date in town? There lies the guesswork. Nvidia revenues grew more than 80% annually, on average, in the last 3 years - and last year grew more than 100%. Could this drop to 60% or 40% growth? It has to - soon! See: Buy or Sell Nvidia? Nvidia’s biggest customers are the likes of Microsoft, Google, Meta, and Amazon. These companies, each of them, spent more than tens of billions last year on Nvidia chips. Can Microsoft, Google, and others continue to spend 100% MORE on Nvidia chips next year? And the year after? Can they do it while their own revenue grows only 15% annually? The answer is absolutely not. The Nvidia 80-100% growth music has to stop at some point. When this high pitched growth music stops, Nvidia’s valuation will drop. A lot. But then, it might continue at a slower pace, at 20-30%, not bad, but not ridiculously high either. Point is, that’s why buying a little bit of Microsoft is OK. It’s also why you build a portfolio. A resilient one. To balance risk-reward. We did it in spades with the Trefis High Quality (HQ) portfolio. Balancing risk-reward is how HQ outperformed the S&P 500, the Nasdaq, and Russell 2000. HQ outperformed all of them, and has clocked >91% returns since inception. That’s why you buy a little bit of Microsoft, or Google, META, Amazon, and perhaps even Edwards Lifesciences. Comparison as a tool: The purpose of comparing Microsoft with Nvidia is all about understanding the risk-reward tradeoff of an investment of interest, in this case Microsoft. In practice, investment decisions are about understanding relative attractiveness. Buy Microsoft stock or keep your money as interest earning cash? Or buy an ETF on the S&P 500? Is expected return on Microsoft stock more than that on cash - by how much? What’s the downside risk you bear to earn that extra return on Microsoft? Drawing contrast with a specific “anchor” asset, in this case Nvidia and Microsoft, serves as a powerful tool to assess the risk-reward tradeoff. Note: Always use an appropriate comparison for a ticker. Microsoft example was about “growth” and reasonably high valuation. A comparison with Nvidia provides interesting perspective as the stock offers much more at a similar valuation (FCF based) - but also carries with it, a huge downside risk. It is exactly this downside risk, versus relative upside tradeoffs we made - at scale, in constructing the Trefis High Quality (HQ) Portfolio. With a selection of 30 stocks, it has demonstrated a history of comfortably outperforming the S&P 500 over the past 4-year span. What accounts for this? As a collective, HQ Portfolio stocks achieved superior returns with reduced risk compared to the standard index, with a smoother performance evident in HQ Portfolio performance metrics.
--------------------------------------------------

Title: Dave Vellante’s Breaking Analysis: The complete collection
URL: https://siliconangle.com/2025/05/23/dave-vellantes-breaking-analysis-complete-collection/
Time Published: 2025-05-23T10:33:12Z
Full Content:
UPDATED 06:33 EDT / MAY 23 2025 BREAKING ANALYSIS by Dave Vellante Breaking Analysis is a weekly editorial program combining knowledge from SiliconANGLE’s theCUBE with spending data from Enterprise Technology Research. Branded as theCUBE Insights, Powered by ETR, the program is our opportunity to share independent, unfiltered editorial with SiliconANGLE, theCUBE and Wikibon communities. The program and conclusions we produce are data-driven, tapping ETR’s proprietary spending data set. Episode 221 – Nvidia, Broadcom and the expanding breadth of AI – We attended both Nvidia Corp.’s GTC conference and Broadcom Inc.’s investor day this week where the artificial intelligence platform shift was on full display. In our view, GTC24 was the most important event in the history of the technology industry, surpassing Steve Jobs’ iPod and iPhone launches. The event was not the largest but, in our opinion, it was the most significant in terms of its reach, vision, ecosystem impact and broad-based recognition that the AI era will permanently change the world. Meanwhile, Broadcom’s first investor day underscored both the importance of the AI era and the highly differentiated strategies and paths that Nvidia and Broadcom are each taking. We believe Nvidia and Broadcom are currently the two best-positioned companies to capitalize on the AI wave and will each dominate their respective markets for the better part of a decade. But importantly, we see them each as enablers of a broader ecosystem that collectively will create more value than either of these firms will in and of themselves. In this Breaking Analysis, we will share our perspectives on the state of AI and how Nvidia and Broadcom are each leading the way with dramatically different but overlapping strategies that may be headed for an eventual collision course. Watch the full video analysis. Episode 220 – Navigating NVIDIA & the AI Trade – Sell, hold or double down? – Heading into the second half of 2023, some investors felt that the semiconductor run up last summer was a harbinger for a broader tech rally. That thesis proved prescient and rewarded managers who took on risk at the time with leading firms in semiconductors, security and enterprise software. The question is, where do we go from here? In this Breaking Analysis we welcome back Ivana Delevska, the founder and chief investment officer at Spear Invest, Nasdaq SPRX. Some have compared SPRX to a miniature version of Cathie Wood’s ARKK fund. However SPRX is more sector agnostic where Delevska focuses more broadly on growth themes such as her current emphasis on cybersecurity, semiconductors, and enterprise software. Watch the full video analysis. Episode 219 – Why CrowdStrike is separating from the cybersecurity pack – It’s been an interesting month in the cybersecurity space. The sector has been somewhat less affected by budget tightening these past twenty-four months and at the same time has benefitted from AI tailwinds. But in the past several weeks we’ve seen some separation in key highflying cybersecurity names. Specifically, Palo Alto shocked the street last month with a $600M billings forecast surprise and sounded the alarm that there were cracks in its consolidation execution. This dragged down other consolidation players in sympathy, namely CrowdStrike and Zscaler. But our research shows that the dynamics facing these three companies are quite different. Of particular note, CrowdStrike’s earnings print highlights the company’s impressive momentum while recent negativity around Zscaler is a bit of a head scratcher for us, which we’ll try to explain. In this Breaking Analysis we take a more narrow look at the information security space and dig deeper into the continued success of CrowdStrike. With recent survey data from ETR, we continue to advance our premise that platforms beat products and we identify several levers that are powering CrowdStrike’s path to $5B by FY 2026 and to $10B by the end of the decade. Watch the full video analysis. Episode 218 – The unplanned genius of Broadcom’s route to AI dominance – Broadcom is perhaps the most unique company in the technology business. It doesn’t simply chase markets that are on steep growth curves and can deliver short term ROI. Rather it goes after established markets with durable franchises. Broadcom focuses its R&D on serving customers in these markets with major engineering investments to achieve a dominant position in each of its target sectors. And sometimes, the company lucks out with this strategy and catches a wave accidentally by design. In this Breaking Analysis we extract key nuggets from our sit down at MWC this week with Charlie Kawwas, president of Broadcom’s Semiconductor Solutions Group, and we unpack the contrarian business technology model of Broadcom. Watch the full video analysis. Episode 217 – Cloud optimization wanes as AI slowly lifts off – The past twenty-four months have seen cloud spending face dual headwinds of macroeconomics and the ability to dial down resources as needed – i.e. cloud optimization. Nonetheless, the big four hyperscalers clocked in between $170 – $190B in IaaS and PaaS revenue last year depending on how you factor the leaked court documents suggesting Azure is much smaller than previously believed. Regardless, hyperscaler growth continued to outpace almost all markets, accelerating between 18-19% in revenue terms last year, despite their enormous size. As we progress into 2024, IT decision makers are cautiously optimistic about spending levels, especially for the second half. All hyperscalers report that cloud optimization is slowing although pockets of cloud cost cutting remain. While AI gets all the headlines, its contribution to revenue is still a small fraction of the overall spending pie. For example, we estimate that Microsoft’s AI services accounted for around $800M this past quarter. But the trajectory for AI services and the potential uplift looks promising for all four hyperscalers. We think collectively the generative AI uplift in cloud will surpass $10B this year. In this Breaking Analysis we update you on our latest hyperscale cloud spending and market share data. We’ll analyze the ETR survey data on cloud optimization, assess the Gen AI updraft for the big 3 US cloud players and look at some of the industry trend data on cloud spend by platform. Watch the full video analysis. Episode 216 – Intel Foundry is a bold bet filled with uncertainty – As an American, you can’t help but root for Intel CEO Pat Gelsinger to succeed. His vision to bring semiconductor manufacturing leadership back to the United States is more than just a quaint nationalistic sentiment. Rather it’s a strategic imperative for the country, its military, global competitiveness and access to future technological innovations in the AI era. But his strategy is dependent upon the success of Intel both as a designer and a leading manufacturer of advanced chips. As such this choice puts Intel in a multi-front war with highly capable leaders in several markets, including names like AMD, NVIDIA, AWS, Google, Microsoft, Apple, Tesla and other chip designers…even perhaps OpenAI. As well Intel competes with with established manufacturers like Taiwan Semiconductor and Samsung. Moreover, Intel’s business model has been disrupted by Arm which has created a volume standard powered by the iPhone and mobile technologies. Finally, China, Inc. looms as a long-term competitor further underscoring the imperative. But the trillion dollar questions are: 1) What are the odds that Intel’s strategy succeeds; and 2) Are there more viable alternative strategies for both Intel and the United States? Watch the full video analysis. Episode 215 – Slicing the Gordian Knot – A leap to real time systems of truth – In order to support the vision of the sixth data platform, that is, a capability that allows a globally consistent, real-time, intelligent digital representation of a business, we believe the industry must rethink the single system of truth. Specifically, we envision a new data platform that marries the best of relational and nonrelational capabilities and breaks the multi-decades tradeoffs between data consistency, availability and global scale. Further, we see the emergence of a modular data platform that automates decision-making by combining historical analytic systems with transactions to enable artificial intelligence to take action. In this Breaking Analysis, we welcome two innovators, Eric Berg, chief executive of Fauna Inc., and S. Somasegar, managing director at Madrona Ventures. Watch the full video analysis. Episode 214 – Enterprise Technology Predictions 2024 – Predictions about enterprise tech have never been more uncertain. They become even more challenging when you try to make forecasts that are measurable. Generally, our belief is we should be able to look back a year later and say with some degree of certainty whether the prediction came true – ideally with some quantifiable evidence to back that up. In this Breaking Analysis and for the third year in a row, we collaborate with Erik Bradley of Enterprise Technology Research and to share our annual enterprise technology predictions. Watch the full video analysis. Episode 213 – 2024 IT spending outlook shows cautious start with optimistic finish – According to recent spending intentions data from over 1,700 information technology decision-makers, executives anticipate a 4.3% growth in technology budgets for the year, which is an improvement from the 3.5% growth seen in 2023 and higher than the 3.8% expectation from October. However, the forecasts for 2024 are back-loaded, with Q1 2024 forecasts at 2.4%, indicating that the optimism is concentrated in the second half of the year. Watch the full video analysis. Episode 212 – Unifying intelligence in the age of data apps – We believe the future of intelligent data apps will enable virtually all organizations to operate a platform that orchestrates an ecosystem similar to that of Amazon.com. By this we mean dynamically connecting and digitally representing an enterprise’s operations including its customers, partners, suppliers and even competitors. This vision includes the ability to rationalize top down plans with bottom up activities across the many dimensions of a business – e.g. demand, product availability, production capacity, geographies, etc. Unlike today’s data platforms, which generally are based on historical systems of truth, we envision a prescriptive model of a business’ operations enabled by an emerging layer that unifies the intelligence trapped within today’s application silos. In this Breaking Analysis, we explore in depth, the semantic layer we’ve been discussing since early last year. To do so we welcome Molham Aref, the CEO of RelationalAI. Watch the full video analysis. Episode 211 – Grading Our 2023 Enterprise Technology Predictions – Predictions about the future of enterprise tech are streaming to our inboxes, literally by the thousands. Most are thoughtful and we will review those prior to publishing our 2024 predictions later in January. As is our tradition, we try to make our own predictions more challenging by citing forecasts that are measurable and have either a numeric tied to them or a binary outcome. Our belief is if we make a prediction, you should be able to look back a year later and say with some degree of certainty whether the prediction came true or not. With some empirical evidence to back that up. In this Breaking Analysis we grade the 2023 predictions we made with ETR’s Erik Bradley. We look back at what we said in January about the macro IT spending environment, cost optimization, security, generative AI, cloud, blockchain, data platforms, automation and tech events. Watch the full video analysis. Episode 210 – David vs Goliath reimagined – OpenAI’s approach to AI supervision – Artificial general intelligence, or AGI, has people both intrigued and fearful. As a leading researcher in the field, last July, OpenAI introduced the concept of superalignment via a team created to study scientific and technical breakthroughs to guide and ultimately control AI systems that are much more capable than humans. OpenAI refers to this level of AI as superintelligence. Last week, this team unveiled the first results of an effort to supervise more powerful AI with less powerful models. While promising, the effort showed mixed results and brings to light several more questions about the future of AI and the ability of humans to actually control such advanced machine intelligence. In this Breaking Analysis we share the results of OpenAI’s superalignment research and what it means for the future of AI. We further probe ongoing questions about OpenAI’s unconventional structure which we continue to believe is misaligned with its conflicting objectives of both protecting humanity and making money. We’ll also poke at a nuanced change in OpenAI’s characterization of its relationship with Microsoft. Finally we’ll share some data that shows the magnitude of OpenAI’s lead in the market and propose some possible solutions to the structural problem faced by the industry. Watch the full video analysis. Episode 209 – Moving beyond separation of compute & storage…Journey to the 6th data platform – We believe today’s so-called modern data stack, as currently envisioned, will be challenged by emerging use cases and AI-infused apps that begin to represent the real world, in real time, at massive data scale. To support these new applications, a change in underlying data and data center architectures will be necessary, particularly for exabyte scale workloads. Today’s generally accepted state of the art of separating compute from storage, must evolve in our view to separate compute from data and further enable compute to operate on a unified view of coherent and composable data elements. Moreover, our opinion is that AI will be used to enrich metadata to turn strings (i.e. ASCII code, files, objects, etc.) into things that represent real world capabilities of a business. In this Breaking Analysis we continue our quest to more deeply understand the emergence of a sixth data platform that can support intelligent applications in real time. To do so we are pleased to welcome two founders of VAST Data, CEO Renen Hallak and Jeff Denworth. VAST just closed a modest $118M financing round that included Fidelity at a valuation of ~$9B, which implies a quite minor change to the cap table. Watch the full video analysis. Episode 208 – re:Invent ’23 underscores a new simplicity mandate for AWS – Generative AI has created a new mandate in enterprise tech with significant implications for all companies generally and AWS specifically. Amazon’s powerful playbook based on agility, developer choice, power, scale, reliability and security must now evolve to accommodate simplicity and coherence for mainstream customers. This imperative came into clear focus at AWS re:Invent ’23. AWS continues to innovate at a fast pace, but must now do so in a changing customer environment that increasingly values direct user productivity gains through software. In this Breaking Analysis we share our take on how AWS is navigating this challenge. We’ll review Amazon’s strategy to compete in the nascent Gen AI era and we’ll provide commentary on the chess moves it’s making with Anthropic, Nvidia and other partners to maintain its leadership position. We’ll also discuss the challenges of doing so as a $90+B giant in a fast-moving market. Watch the full video analysis. Episode 207 – The OpenAI meltdown…Winners and losers in the battle for AI supremacy – Conventional wisdom says Microsoft is the big winner in the recent OpenAI saga. We don’t quite see it that way. Both Microsoft and OpenAI are in a worse position today than they were last Thursday, prior to the firing of OpenAI CEO Sam Altman and the ongoing public drama that ensues. Microsoft and OpenAI had a huge lead in market momentum, AI adoption and feature acceleration and were setting the narrative in AI. Our discussions with customers and industry insiders leads us to conclude that the duo has put its substantial lead at risk. While Satya Nadella is making lemonade from lemons, the window was just cracked open for the competition and it’s more clear than ever that one large language model will not rule them all. In this Breaking Analysis we weigh in on the impacts of the OpenAI meltdown with a deeper look at the customer perspective and how it alters the competitive landscape in the battle for AI supremacy. As well, the amazing data team at ETR has run a quick survey of OpenAI Microsoft customers to gauge reactions and we’ll share that fresh data. Watch the full video analysis. Episode 206 – The copilot era takes flight at Microsoft Ignite 2023 – Microsoft Ignite 2023 was one part a celebration of yearlong technological innovations, one part announcing the general availability of previously announced products, one part vision, one part ecosystem and four parts copilots everywhere. Copilots promise an historic software-led productivity increase. Perhaps for the first time in industry history we’re seeing huge demand for software coincide with the ability to make it easier to write software. Just as AWS turned the data center into an API, copilots are turning software development into natural language, enabling many more people to create. The implications on productivity are massive and we believe will kick off a new wave of growth that will become increasingly noticeable throughout 2024. In this Breaking Analysis we give you our impressions of Microsoft Ignite 2023. theCUBE Research Analyst George Gilbert and CUBE Collective contributor Sarbjeet Johal both weighed in for this episode and we’ll also share some recent ETR data that shows the progression of some of the major AI players in the past twelve months and the the relative impact Gen AI has had on each of their businesses. Watch the full video analysis. Episode 205 – IBM turns the corner with Watson, why 2.0 is a breakthrough opportunity – With Watson 1.0, IBM deviated from the silicon valley mantra, fail fast, as it took nearly a decade for the company to pivot off of its original vision. In our view, a different dynamic is in play today with Watson 2.0 – i.e. watsonx. IBM’s deep research in AI and learnings from its previous mistakes, have positioned the company to be a major player in the Generative AI era. Specifically, in our opinion, IBM has a leading technological foundation, a robust and rapidly advancing AI stack, a strong hybrid cloud position (thanks to Red Hat), an expanding ecosystem and a consulting organization with deep domain expertise to apply AI in industry-specific use cases. In this Breaking Analysis we share our takeaways and perspectives from a recent trip to IBM’s research headquarters. To do so we collaborate with analyst friends in theCUBE Collective, Sanjeev Mohan, Tony Baer and Merv Adrian. We’ll also share some relevant ETR spending data to frame the conversation. Watch the full video analysis. Episode 204 – The Gen AI power law – How data diversity influences adoption – Our research indicates that the adoption of generative AI is occurring across a variety of frameworks with diverse deployment models, data sets and domain specificities. The transformative power of generative AI for industries is colossal, and we are convinced that AI will gravitate to where data lives. The power law of Gen AI, developed by theCUBE Research team, describes the adoption patterns we see emerging in the cloud, on-premises and at the edge with a long tail of highly specific domain models across every industry. In this Breaking Analysis we revisit the Gen AI power law which informs how we see adoption happening within industries and organizations globally. We introduce new survey data from ETR that looks at some of the vendors being adopted or considered with a special peek into the emerging technology sector with data on Hugging Face, Anthropic, Cohere and Jasper. We also comment on the adoption of Meta’s Llama 2 and the potential impact of open source and other third parties on the shape of the curve. We share our estimates of Microsoft Azure’s AI revenue impact, which we see approaching a $2B run rate exiting 2023. Finally, we dig into the impact of retrieval augmented generation (RAG) using real world examples with some caveats of RAG implementations that practitioners should consider. Watch the full video analysis. Episode 203 – AI revenue riddle – Azure sees gains, when will other cloud titans see the surge? – In 1987, Nobel Prize-winning economist Bob Solow famously observed, “You can see the computer age everywhere but in the productivity statistics.” This proclamation became known as the productivity paradox. Ironically, Solow’s statement preceded the greatest productivity boom since the dawn of the computer age which subsequently came to fruition in the 1990’s. It can be argued that a similar pattern is being seen today where AI is everywhere but generally not showing up in earnings numbers or productivity statistics…yet. In this Breaking Analysis we squint through the latest earnings reports from Microsoft, Alphabet and Amazon to understand what’s happening in cloud, evaluate the impact or lack thereof of AI on cloud earnings momentum and explain how we think about the future impact of generative AI and cloud. Watch the full video analysis. Episode 202 – From hype to reality, the true state of AI adoption – MIT professor and economist Erik Brynjolfsson said recently that he’d be disappointed if AI didn’t lift the current anemic 1.2% productivity growth rate to 3% or even 4%. This would be a good thing for business and government as it could potentially help with the labor shortage, drive earnings growth and increase tax revenues, which would ostensibly help address current debt levels. This is one of the promised impacts of AI. While the hype surrounding Gen AI has narrowly propped up certain sectors of the market, like AI startups and the magnificent seven, the macro effects have not been felt thus far as adoption remains largely experimental. In this Breaking Analysis and ahead of Supercloud 4, ETR’s Erik Bradley and Daren Brabham join the program to share the latest trends on AI adoption, how Gen AI is being used, some of the deployment models and the AI leaderboard based on spending momentum and presence in the market. Watch the full video analysis. Episode 201 – Get Ready for the Sixth Data Platform – In the next 3-5 years, we believe a set of intelligent data apps will emerge requiring a new type of modern data platform to support them. We refer to this as a sixth data platform. In our previous research we’ve used the metaphor “Uber for everyone” to describe this vision; meaning software-based systems that enable a digital representation of a business. Watch the full video analysis. Episode 200 – Lower for longer…Tech spending remains tepid – We’re getting used to the phrase, “higher for longer,” referring to the realization that interest rates are expected to remain elevated for a period of time. This trend is having an inverse effect on enterprise tech spending growth rates. Prior to the Fed’s tightening binge for example, IT decision makers (ITDMs) in aggregate expected annual technology spending to increase by 7.5%. Eleven fed interest rate hikes later, ITDMs estimate that their 2023 budgets will be up only 2.9%, with an expectation, or perhaps it’s a wishful hope, that their budgets will increase 3.8% in 2024. In this our 200th Breaking Analysis, we preview the current spending climate and where AI fits in relation to other sectors. We’ll also share with you a snapshot of the leaders in terms of spending velocity for their platforms; and how their performance compares to peers relative to earlier survey periods. Watch the full video analysis. Episode 199 – Cisco Splunk under the microscope, joint customers weigh in – In this special Breaking Analysis, theCUBE’s Dave Vellante talks with Enterprise Technology Research’s Erik Bradley as he shares the results of ETR’s recent flash survey assessing joint customer perceptions and likely spending actions as a result of the Cisco acquisition of Splunk. Watch the full video analysis. Episode 198 – Bob Muglia on Uber for everyone…how the future of data apps will evolve – The future of intelligent data applications: Uber for everyone. In June, we put forth our scenario about “Uber for all.” We believe a new breed of data apps is emerging and we used Uber as an example where data about people, places and things is brought together in a data-coherent, real-time system. In this special Breaking Analysis, analysts Dave Vellante and George Gilbert sat down with friend of theCUBE, entrepreneur, investor and software executive, Bob Muglia. Bob formerly worked at Microsoft, was the CEO of Snowflake and is a visionary trend spotter. Watch the full video analysis. Episode 197 – Cloud security powers CrowdStrike momentum, Gen AI is next – George Kurtz is pumped up…and why not? CrowdStrike’s business appears to be on a fast track and entering a new phase of growth, despite the difficult macro and elongated sales cycles. The company’s products are considered best in class, its business is growing steadily and an improved profitability and cash flow outlook had investors excited, at least up until this week. A still challenging environment and a rich 13X revenue multiple perhaps led to some profit taking, but Gen AI could be the next catalyst for the company. In the race to close the SecOps staffing gap, CrowdStrike has what appears to be a strong play with a natural language-based intelligent assistant known as Charlotte AI. In this Breaking Analysis we update our scenario on security leader CrowdStrike. We’ll review the company’s recent progress, share survey data that shows where it is strong and where there may be icebergs ahead. And we’ll preview Fal.Con 2023 which takes place next week in Las Vegas. Watch the full video analysis. Episode 196 – Copilot or competitor – How gen AI bolsters and buffets UiPath’s Northstar – UiPath’s recent earnings beat and raise provides some evidence that thus far, Gen AI has not been dilutive for the company. As an early leader that is transforming beyond RPA toward end-to-end enterprise automation, UiPath, like all automation providers, has always faced adoption headwinds beyond isolated deployments. In this sense, Gen AI should bolster adoption and be a positive force. The flip side is that widely available tools like chatbots and generalized foundation models could eat away at the low end of the automation TAM, highlighting the urgency for companies like UiPath to move up market and accelerate innovation that brings differentiation from commoditized tools; and, importantly, create distance from embedded AI within mainstream enterprise SaaS platforms like Slack GPT and Salesforce Einstein. In this Breaking Analysis we briefly review the recent earnings print from UiPath. We’ll look at ETR survey data that shows Microsoft Power Automate’s impact on the automation market and how it is forcing UiPath to target larger accounts with a more functional product set. As well we’ll look at the impact that AI is having in these larger accounts and test UiPath management assertions that Gen AI will be a tailwind for the company. Watch the full video analysis. Episode 195 – Google goes all in on the AI cloud – At Cloud Next, Google showcased its strong leadership position in data and AI. In our view, Google’s messaging, demos and tech-centric narrative have broad appeal for developers and next generation startups. As well, the company’s focus on solutions, contrasts its strategy to the typically disjointed services we’ve seen from AWS over the past decade. Google also showed off an expanded ecosystem of GSIs and smaller CSPs, encouraging the broad use of Google’s kit globally. While Google remains a distant third in the Iaas/PaaS race, with revenue one-fifth the size of AWS, it is playing the long game and betting the house on AI as a catalyst to its cloud future. In this Breaking Analysis we unpack the key takeaways from Google Cloud Next with Rob Strechay and George Gilbert. We’ll share ETR data that positions Google’s AI relative to other leaders and we’ll contrast Google’s data-centric strategy with traditional architectural models. Watch the full video analysis. Episode 194 – Snowflake has Momentum with AWS & Microsoft…Why Google may not be Next – Recent earnings prints from Amazon and Snowflake, along with new survey data, have provided additional context on top of the two events that Snowflake and Databricks each hosted last June. Specifically, we believe that the effects of cloud optimization are still being felt but are nearing the end of peak negative impact on cloud companies. Snowflake’s recent renewal with Microsoft better aligns sales incentives and should improve the company’s traction with Microsoft Azure, a platform that has long favored Databricks. Google however remains a different story as its agenda is to build out its own data cloud stack, rather than supporting Snowflake’s aspirations. Watch the full video analysis. Episode 193 – VMware’s Future – Navigating Multi cloud Complexity & GenAI Under Broadcom’s Wing – The FTC continues to drag its feet on approving Broadcom’s acquisition of VMware. Ironically, in our view, these delays only hurt the very competitive environment the FTC claims to be protecting. The AI era is accelerating at a breakneck pace and the big 3 hyperscale cloud vendors already have a sizable lead on legacy incumbents. If preserving competition is truly the agenda of the U.S. government, it should recognize that VMware, its enterprise ecosystem and market forces have the potential to neutralize cross cloud complexity and give customers a viable alternative to increasingly powerful public cloud players. In this Breaking Analysis and ahead of VMWare Explore 2023, we revisit our views on Broadcom’s rationale and likely actions post acquisition. We’ll share current ETR survey data to place VMware’s position in context to the major cloud players, speculate on its AI agenda and give a preview of next week’s VMware Explore. To do so we welcome CUBE analyst Rob Strechay and friend of theCUBE, Zeus Kerravala, principal of ZK Research. Watch the full video analysis. Episode 192 – Cloud vs. On-Prem Showdown: The Future Battlefield for Generative AI Dominance – The data from enterprise customers is clear but conflicted. While 94% of customers say they’re spending more on AI this year, they’re doing so with budget constraints that will steal from other initiatives. As well, the choice of where customers plan to run generative AI is split almost exactly down the middle in terms of public cloud vs. on-premises/edge. Further complicating matters, developers report the experiences in the public cloud with respect to feature richness and velocity of innovation has been outstanding. At the same time, organizations express valid concerns about IP leakage, compliance, legal risks and cost that will limit their use of the public cloud. In this Breaking Analysis we’ll share the most recent data and thinking around the adoption of large language models and address the factors to consider when thinking about how the market will evolve. As always, we’ll share the latest ETR data to shed new light on key issues customers face balancing risk with time to value. Watch the full video analysis. Episode 191 – Tech Stocks Beyond the Magnificent Seven – After a tough 2022, the first half of 2023 has shown impressive strength paying off earlier technology bets. For sure investors in the so-called Magnificent Seven, i.e. Apple, Alphabet, Amazon, Microsoft, Meta, Nvidia and Tesla have been rewarded. But sharp investors have sought alpha beyond these names, riding the wave of secular trends in AI, cybersecurity, cloud infrastructure and software as well as other emerging spaces like cleantech and robotics. As we enter the second half of 2023, the run up in tech combined with macro uncertainty has many investors taking a cautious posture. But prudent earnings guidance sets up for a positive outlook in the mid-term, especially for those companies that can capitalize on the AI wave. In this Breaking Analysis we’re pleased to have back, founder and Chief Investment Officer of Spear Invest, Ivana Delevska to assess the current state of the market and explore how this investor is playing AI’s rising tide. We’ll also analyze ETR data to drill deeper into semis, cloud infrastructure, generative AI, cybersecurity and Snowflake. Watch the full video analysis. Episode 190 – What Leaked Court Docs Tell us About AWS, Azure & Google Cloud Market Shares – Recently leaked court documents during the Microsoft Activision hearing require us to revisit our cloud forecasts and market share data. The poorly redacted docs, which have since been removed from public viewing, suggest that Microsoft’s Azure revenue is at least 25% lower than our previous estimates. As a result, we’ve cut and revised our Azure revenue figures which in turn increases AWS’ big 4 hyperscale cloud market share. Our new estimates show that AWS maintains a greater than 50% share of revenue through 2023. While the change also helps Google Cloud, its market share is only modestly affected. In this Breaking Analysis we update our hyperscaler cloud revenue estimates and market share data. We’ll also explain how the ETR data on cloud should be interpreted in this context and look forward to potential catalysts for cloud growth, including acceleration in Q4 attributable to generative AI. Watch the full video analysis. Episode 189 – AI gives cyber attackers the advantage…for now – Cloud complexity, tools sprawl and the AI awakening further tip the balance in favor of cyber attackers. Combined with corporate inertia, AI-washing, LLM inconsistency and the pace of change, we believe for now anyway, adversaries have the advantage over defenders. Moreover, macro spending headwinds continue to force organizations to make budget tradeoffs, not the least of which is how to fund AI experiments and deployments. Notably, however, 45% of organizations are using LLMs in production for use cases that may very well improve the productivity of SecOps teams in the long run and accelerate the cat and mouse game back to a state of quasi-equilibrium. In this Breaking Analysis we share key takeaways from Supercloud 3 – AI meets cloud security – and put forth new spending data from the latest ETR survey that shows which security firms are best positioned in the AI race to capitalize on the wave. Watch the full video analysis. Episode 188 – AI won’t be a winner takes all market – The AI heard ’round the world has put the machine intelligence sector back in the spotlight. But when you squint beyond the press hype, the data shows that artificial intelligence is now the number one sector in terms of relative spending velocity in the ETR taxonomy. Normally market hype leads deployments, but the data suggests that spending activity and market penetration for AI are coinciding with the hype. While hyperscale cloud players are reaping the rewards, we think this is a rising tide that’s going to lift all AI ships, those both plainly in sight and others that may not be so visible. In this Breaking Analysis we dig deeper into the AI space with spending data from ETR and one of the best minds in tech generally, and AI specifically, Jeff Jonas, CEO, founder, and chief scientist at Senzing. Watch the full video analysis. Episode 187 – Connecting the dots on the emerging Databricks tech stack – The recent Databricks Data+AI Summit attracted a large audience and, like Snowflake Summit, featured a strong focus on large language models, unification and bringing AI to the data. While customers demand a unified platform to access all their data, Databricks and Snowflake are attacking the problem from different perspectives. In our view, the market size justifies the current enthusiasm seen around both platforms but it’s unlikely that either company has a knockout blow for the other. This is not a head on collision. Rather Snowflake is likely years ahead in terms of operationalizing data. Developers can build applications on one platform, like Oracle when it won the market, that perform analysis and take action. Databricks likely has a similar lead in terms of unifying all types of analytic data – e.g. BI, predictive analytics & generative AI. Developers can build analytic applications across heterogeneous data, like Palantir today. But they have to access external operational applications to take action. In this Breaking Analysis we follow up last week’s research by connecting the dots on the emerging tech stack we see forming from Databricks. With an emphasis on how the company is approaching generative AI, unification and governance…and what it means for customers. To do so we tap the knowledge of three experts who attended the event, CUBE analysts Rob Strechay and George Gilbert and AI market maven Andy Thurai of Constellation Research. Watch the full video analysis. Episode 186 – Connecting the Dots on Snowflake’s Data Cloud Ambitions – Over the past several months we’ve produced a number of in-depth analyses laying out our mental model for the future of data platforms. There are two core themes: 1) Data from people, places, things, and activities in the real world drives applications, not people typing into a UI; and 2) Informing and automating decisions means all data must be accessible. That drives a change from data locked in application silos to application logic being embedded in a platform that manages an end-to-end representation of an enterprise in its data. This week’s Snowflake Summit further confirmed our expectations with a strong top line message of “All Data / All Workloads” and a technical foundation that supports an expanded number of ways to access data. Squinting through the messaging and firehose of product announcements, we believe Snowflake’s core differentiation is its emerging ability to be a complete platform for data applications. Just about all competitors either analyze data or manage data. But no one vendor truly does both. To be precise, managing data doesn’t mean running pipelines or serving analytic queries or AI/ML models. It means managing operational data so that analytics can inform or automate operational activities captured in transactions. With data as the application foundation, the platform needs robust governance. In this week’s Breaking Analysis, we try to connect the dots between Snowflake’s high level messaging and its technical foundation to better understand the core value it brings to customers and partners. As well, we’ll explore the ETR data with some initial input from the Databricks Data + AI Summit to assess the position and prospects of these two leaders along with the key public cloud players. Watch the full video analysis. Episode 185 – HPE wants to turn supercomputing leadership into gen AI profits – HPE’s announcement of an AI cloud for large language models highlights a differentiated strategy that the company hopes will lead to sustained momentum in its high performance computing business. While we think HPE has some distinct advantages with respect to its supercomputing IP, the public cloud players have a substantial lead in AI with a point of view that generative AI is fully dependent on the cloud and its massive compute capabilities. The question is can HPE bring unique capabilities and a focus to the table that will yield competitive advantage and ultimately, profits in the space? In this Breaking Analysis we unpack HPE’s LLM-as-a-service announcements from the company’s recent Discover conference and we’ll try to answer the question: Is HPEs strategy a viable alternative to today’s public and private cloud gen AI deployment models, or is it ultimately destined to be a niche player in the market? To do so we welcome to the program CUBE analyst Rob Strechay and Vice President / principal analyst from Constellation Research, Andy Thurai. Watch the full video analysis. Episode 184 – Uber’s architecture represents the future of data apps…meet its architects – Uber has one of the most amazing business models ever created. The company’s mission is underpinned by technology that helps people go anywhere and get anything. The results have been stunning. In just over a decade, Uber has become a firm with more than $30 billion in annual revenue, an annual bookings run rate of $126B and a market capitalization near $90 billion today. Moreover, the company’s productivity metrics are 3-5 times greater than what you’d find at a typical technology company when, for example, measured by revenue per employee. In our view, Uber’s technology stack represents the future of enterprise data apps where organizations will essentially create real time digital twins of their businesses and in doing so, deliver enormous customer value. In this Breaking Analysis, we introduce you to one of the architects behind Uber’s groundbreaking fulfillment platform. We’ll explore their objectives, the challenges they had to overcome, how Uber has done it and why we believe their platform is a harbinger for the future. Watch the full video analysis. Episode 183 – Snowflake Summit will reveal the future of data apps…here’s our take – Our research and analysis points to a new modern data stack that is emerging where apps will be built from a coherent set of data elements that can be composed at scale. Demand for these apps will come from organizations that wish to create a digital twin of their business to represent people, places, things, and the activities that connect them, to drive new levels of productivity and monetization. Further, we expect Snowflake, at its upcoming conference, will expose its vision to be the best platform on which to develop this new breed of data apps. In our view, Snowflake is significantly ahead of the pack but faces key decision points along the way to its future to protect this lead. In this Breaking Analysis and ahead of Snowflake Summit later this month, we lay out a likely path for Snowflake to execute on this vision; and we address the milestones and challenges of getting there. As always, we’ll look at what the ETR data tells us about the current state of the market. To do all this we welcome back CUBE contributor, George Gilbert. Watch the full video analysis. Episode 182 – Cisco needs to simplify…Here’s how – With a nearly $60B revenue run rate, growing at 14% and throwing off over $5B in operating cash last quarter, Cisco has an awesome business. But customers are vocal about the complexity of Cisco’s portfolio and if not addressed head on, the company risks encountering friction beyond just economic headwinds. We believe Cisco’s challenges are most decidedly not product breadth and depth, rather the company’s mandate is to integrate the piece parts of its intricate offerings to create more facile and seamless experiences for customers. In this Breaking Analysis and ahead of Cisco Live US, we dig deeper into Cisco’s business and double click on three key areas of its portfolio including: 1) Security; 2) Networking; and 3) Observability. With spending data from ETR and a guest appearance from SiliconANGLE contributor and market watcher Zeus Kerravala, principal at ZK Research. Watch the full video analysis. Episode 181 – The future of AI is real time data…Meantime GPUs are making all the headlines – The era of AI everything continues to excite. But unlike the Internet era, where any company announcing a dotcom anything immediately rose in value, the AI gods appear to be more selective. Nvidia beat its top line whisper number by more than $300M and the company’s value is rapidly approaching one trillion dollars. Marvell narrowly beat expectations this week but cited future bandwidth demand driven by AI and the stock was up more than 20% on Friday. Broadcom was up nearly 10% on sympathy with the realization that connect-centricity beyond the CPU is what the company does really well. Meanwhile, other players like Snowflake, which also narrowly beat earnings Wednesday and touted AI as a future tailwind, got hammered as customers dial down cloud consumption. In this Breaking Analysis we look at the infrastructure of AI examining the action at the silicon layer specifically around Nvidia’s momentum. Since much of AI is about data, we’ll also look at the spending data on two top data platforms, Snowflake and Databricks to see what the survey data says and examine the future of real time data and automation as a catalyst for massive productivity growth in the economy. To do so we have a special Breaking Analysis panel with John Furrier and David Floyer. Watch the full video analysis. Episode 180 – The AI powered hybrid multi super cloud – AI will now add superpowers to every triggering buzzword, hence the title of this week’s post. Look past the buzz and you’ll find substance somewhere. The spring conference season is kicking into high gear, so it’s a time to get serious and extract the signal from the event noise. This week we’ll see Microsoft Build, which will no doubt volley shots back from the messaging at Google I/O. Two other big events will take place this week, Red Hat Summit / Ansible Fest in Boston and the annual Dell Technologies World in Las Vegas. theCUBE will be covering both of these shows and we want to take this opportunity to update you on the state of hybrid multi-cloud…what we call supercloud. In this Breaking Analysis we examine some of the key infrastructure players in hybrid multi-cloud with a focus on Red Hat and Dell Technologies, two firms that increasingly are partnering with each other as VMware’s future evolves. We’ll share recent ETR survey data on the position of several other hybrid/cross-cloud players including Cloudflare, Equinix, HPE, IBM, Oracle, VMware and others. We’ll also share what we expect to hear at Red Hat Summit and Dell Technologies World this year. Watch the full video analysis. Episode 179 – Searching for gold in enterprise AI – The AI gold rush is on. The paths to monetization are seemingly endless but the most obvious converge on making humans more productive or supercharging existing business models like search advertising or subscription licenses. Much of AI adoption in enterprise IT is hidden. Our research shows a very high overlap (around 40-60%) between AI adoption in enterprise tech and embedded AI inside software from the likes of Salesforce, ServiceNow, Workday, SAP, Oracle and other major players. But the rapid advancements of tools from AI leaders and an emerging group of independent firms is causing customers to think differently. Catalyzed by the OpenAI Microsoft partnership, organizations are rapidly trying to figure out how to apply these tools to create competitive advantage. Every firm on the planet wants to ride the AI wave. Virtually overnight, investment capital has shifted to fund early stage AI startups with much less funding required relative to previous boom cycles. In this Breaking Analysis we review ETR data to quantify the state of AI spending in the enterprise and look at the positions of several key players in the space that offer AI tools and platforms. To do so we invite Andy Thurai, CUBE contributor, VP and principal analyst at Constellation Research. Andy will help us unpack the hits and misses from this past week’s Google IO conference and give us his perspectives on what it takes to catch the AI wave and avoid becoming driftwood. Watch the full video analysis. Episode 178 – Desperately Seeking Cloud Repatriation – While we’ve been skeptical about repatriation as a notable movement, anecdotal evidence suggests that it is happening in certain pockets. Even though we still don’t see cloud repatriation broadly showing in the numbers, certain examples have caught our attention. In addition, the potential impact of artificial intelligence raises some interesting questions about where infrastructure should be physically located and causes us to revisit our premise that repatriation is an isolated and negligible trend. In this Breaking Analysis we look at a number of sources, including the experiences of 37signals, which has documented its departure from public clouds. We’ll also examine the relationship between repatriation and SRE Ops skill sets. As always we’ll look at survey data from our partners at ETR, a recent FinOps study published by Vega Cloud and revisit the Cloud Repatriation Index, which we believe is breaking a three-year trend. Watch the full video analysis. Episode 177 – Don’t be fooled by slowing cloud growth…cost optimization is a feature not a bug – The big three US cloud players all announced earnings this past week and, as expected, cloud growth is slowing. But don’t kid yourselves. Hyperscale clouds remain the epicenter of innovation in tech and foundation models like GPT will only serve to harden this fundamental fact. Our data suggests the deceleration in cloud spend is a function of two related factors: 1) Cautious consumption patterns; and 2) Aggressive cloud optimization, which is being promoted by the big three cloud vendors in an attempt to lock in customers to longer term commitments. There is still no clear evidence in the numbers that repatriation is a factor. Rather, the ability to quickly dial down spending and pause projects is an attractive feature of cloud computing and one that, until now, has never really been seen on a broad market basis. In this Breaking Analysis we try to explain the implications of this seemingly simple but nuanced dynamic. We’ll review the latest hyperscale cloud data for the big three players, share our analysis of certain comments made by cloud executives and show you the latest ETR data on spending and market presence in the cloud, Watch the full video analysis. Episode 176 – RSA 2023 Security Identity Crisis Part 2 – The narrative from security vendors is organizations don’t spend enough money on cyber defense. Maybe…but will spending more actually address the problems organizations face? The conventional wisdom is it will help; or at least it can’t hurt, but as we and others have pointed out over the years, a crowded market and mega VC funding have created more tools, more complexity and more billionaires…but are we safer? In this Breaking analysis we follow up last week’s episode and continue with Part 2. In an homage to the keynote from RSA CEO Rohit Ghai, we ask, is there a looming identity crisis in the security industry? This week we’re excited to introduce the newest member of the SiliconANGLE editorial team, long time journalist, David Strom. With David, we’ll unpack the data and bring additional context to the ETR body of work. We’ll also look at some recent data from Unit 42, Palo Alto’s threat intelligence and response division. As well, we’ll dig into the anatomy of a recent double supply chain hack. Watch the full video analysis. Episode 175 – RSA 2023 highlights an identity crisis in the age of AI – In this Breaking Analysis, theCUBE host Dave Vellante updates on the latest trends in the cybersecurity market and what to expect at the RSA Conference 2023. Join our real-time analysis coverage from #RSAC here: https://www.thecube.net/rsa-conference-2023 We’ll also share the latest Enterprise Technology Research spending data and drill into the areas of cybersecurity that are seeing the most action. As always, we’ll highlight those companies with the strongest (and weakest) momentum and close with a look at some of the emerging technology players in security that might be ripe for acquisition. To do all this, we once again welcome in our colleague Erik Bradley from ETR. Watch the full video analysis. Episode 174 – Hidden Gems from HPE GreenLake Storage Day – On Tuesday, April 4, HPE invited a number of industry analysts to participate in HPE GreenLake Storage Day. Notably, HPE declared 2023 the year of storage. While the company made several storage-related announcements, perhaps even more interesting was what the event tells us about HPE’s culture, its strategy and the future direction of the company. In this Breaking Analysis we’ll share our takeaways from HPE’s event, held in Houston, Texas, which included attendance at Antonio Neri’s quarterly all-hands meeting. We’ll try to emphasize areas that have not necessarily been the focus of most press and industry analyst write ups to date. We’ll also take a look at the latest ETR survey data to put HPE’s market position in context across several of its major segments. Watch the full video analysis. Episode 173 – Semis rebound but enterprise tech spending remains soft – A rebound in semiconductor stocks has many investors asking if this is a harbinger of good news for the broader enterprise tech sector. Indeed the SOXX semiconductor ETF is up nearly 30% year to date as of this posting, as are bellwether fab suppliers like Applied Materials and Lam Research. Nvidia is up over 90% YTD and AMD over 50%. Even the beleaguered Intel is up 22%. But key enterprise software names have not yet rebounded and according to this week’s guest, the divergence between semis and B2B software is getting hard to ignore. In this Breaking analysis we examine the the bifurcation between the performance of semis and broader enterprise tech. And we’ll try to answer the question: Is the uptick in semiconductors an early indicator of a broader enterprise tech recovery, or is this a false signal that warrants continued caution? To examine these issues we welcome back Ivana Delevska, the founder and chief investment officer of SPEAR Invest. All statements made regarding companies or securities are strictly beliefs, points of view and opinions held by SiliconANGLE Media, Enterprise Technology Research, other guests on theCUBE and guest writers. Such statements are not recommendations by these individuals to buy, sell or hold any security. The content presented does not constitute investment advice and should not be used as the basis for any investment decision. You and only you are responsible for your investment decisions. Disclosure: Many of the companies cited in Breaking Analysis are sponsors of theCUBE and/or clients of Wikibon. None of these firms or other companies have any editorial control over or advanced viewing of what’s published in Breaking Analysis. Watch the full video analysis. Episode 172 – Which tech firms are most exposed to the banking crisis? – The viral awareness and adoption of foundation models like ChatGPT have created both an opportunity and threat to automation platforms generally and RPA point tools specifically. On the one hand, large language models can reduce complexity and accelerate the adoption of enterprise automation platforms. The flip side is software robots are designed to improve human productivity through intelligent automation and GPT models could cannibalize some, if not many use cases initially targeted by RPA vendors. This reality is causing customers to rethink their automation strategies and vendors to rapidly evolve their messaging to position foundation models as an accelerant to their platforms. In this Breaking Analysis we provide you with a perspective on how foundation models could impact automation platforms. We review ETR data that quantifies the ascendency of OpenAI. We also show survey data that measures the overlap between ML/AI systems and automation platforms. Then we review the recent quarterly performance of UiPath and share how we think the company must position itself with respect to the onslaught of noise and potential disruption from GPT models. Watch the full video analysis. Episode 171 – GPT models are a two edged sword for automation platforms – The viral awareness and adoption of foundation models like ChatGPT have created both an opportunity and threat to automation platforms generally and RPA point tools specifically. On the one hand, large language models can reduce complexity and accelerate the adoption of enterprise automation platforms. The flip side is software robots are designed to improve human productivity through intelligent automation and GPT models could cannibalize some, if not many use cases initially targeted by RPA vendors. This reality is causing customers to rethink their automation strategies and vendors to rapidly evolve their messaging to position foundation models as an accelerant to their platforms. In this Breaking Analysis we provide you with a perspective on how foundation models could impact automation platforms. We review ETR data that quantifies the ascendency of OpenAI. We also show survey data that measures the overlap between ML/AI systems and automation platforms. Then we review the recent quarterly performance of UiPath and share how we think the company must position itself with respect to the onslaught of noise and potential disruption from GPT models. Watch the full video analysis. Episode 170 – Databricks faces critical strategic decisions…here’s why – When Apache Spark became a top level project in 2014, and shortly thereafter burst onto the big data scene, it along with the public cloud disrupted the big data market. Databricks cleverly optimized its tech stack for Spark and took advantage of the cloud to deliver a managed service that has become a leading AI and data platform among data scientists and data engineers. However, emerging customer data requirements and market forces are conspiring in a way that we believe will cause modern data platform players generally and Databricks specifically to make some key directional decisions and perhaps even reinvent themselves. In this Breaking Analysis we do a deeper dive into Databricks. We explore its current impressive market momentum using ETR survey data. We’ll also lay out how customer data requirements are changing and what we think the ideal data platform will look like in the mid-term. We’ll then evaluate core elements of the Databricks portfolio against that future vision and close with some strategic decisions we believe the company and its customers face. To do so we welcome in our good friend George Gilbert, former equities analyst, market analyst and principal at Tech Alpha Partners. Watch the full video analysis. Episode 169 – MWC 2023 goes beyond consumer & deep into enterprise tech – While never really meant to be a consumer tech event, over time, the rapid ascendancy of smartphones captured much of the agenda at Mobile World Congress, now MWC. And while device manufacturers continue to have a major presence at the show, the maturity of intelligent devices, longer lifecycles and the disaggregation of the network stack have created more interest in enterprise-class technologies than ever before. Semiconductor manufacturers, network equipment players, infrastructure companies, cloud vendors, software providers and a spate of startups are eyeing the trillion dollar plus telecommunications industry as one of the next big things to watch this decade. In this Breaking Analysis we bring you part 2 of our ongoing coverage of MWC 2023. With some new data on enterprise players specifically within large telco environments. We’ll also take a brief glimpse at some of the pre-announcement news from the show and corresponding themes ahead of MWC. We’ll close with the key innovation areas we’ll be covering at the show on theCUBE. Watch the full video analysis. Episode 168 – MWC 2023 highlights telco transformation & the future of business – The world’s leading telcos are often branded as monopolies that lack innovation. Telcos have been great at operational efficiency, connectivity and living off of transmission services. But in a world beyond telephone poles and basic wireless services, how will telcos modernize, become more agile and monetize new opportunities brought about by 5G, private wireless and a spate of new innovations in infrastructure, cloud, data, AI and apps? It’s become table stakes for carriers to evolve their hardened, proprietary infrastructure stacks to more open, flexible, cloud-like models. But doing so brings risks that telcos must carefully balance as they strive to deliver consistent quality of service while at the same time moving faster and avoiding disruption. In this Breaking Analysis and ahead of MWC23, we explore the evolution of the telco business and how the industry is in many ways, mimicking a transformation that took place decades ago in enterprise IT. We’ll model some of the traditional enterprise vendors using ETR data and investigate how they’re faring in the telecomms vertical. And we’ll pose some of the key issues facing the industry this decade. Watch the full video analysis. Episode 167 – Google’s Point of View on Confidential Computing – Confidential computing is a technology that aims to enhance data privacy and security by providing encrypted computation on sensitive data in use and isolating data from apps and other host resources in fenced off enclaves. The concept of confidential computing is gaining popularity, especially in the cloud computing space where sensitive data is commonly stored and processed. However, there are some who view confidential computing as an unnecessary technology and a marketing ploy by cloud providers, aimed at calming customers who are cloud-phobic. In this Breaking Analysis we revisit the notion of confidential computing and explore whether it’s just marketing or a key part of a trusted security strategy. To do so we’ll invite two Google experts to the show. But before we get there let’s summarize the overall market climate briefly with some ETR data. Watch the full video analysis. Episode 166 – Cloud players sound a cautious tone for 2023 – The unraveling of market enthusiasm continued in Q4 of 2022 with the earnings reports from the U.S. hyperscalers now all in. As we said earlier this year, even the cloud is not immune from the macro headwinds and the cracks in the armor we saw from the data we shared last summer are playing out into 2023. For the most part, actuals are disappointing beyond expectations, including our own. It turns out that our estimates for the big 3 hyperscale revenue missed by $1.2 billion or 2.7% lower than we had forecast from our most recent November estimates. We expect decelerating growth rates for the hyperscalers will continue through the summer of 2023 and won’t abate until comparisons get easier. In this Breaking Analysis we share our view of what’s happening in cloud markets – not just for the hyperscalers but other firms that have hitched a ride on the cloud. And we’ll share new ETR data that shows why these trends are playing out, tactics customers are employing to deal with their cost challenges and how long the pain is likely to last. Watch the full video analysis. Episode 165 – Enterprise Technology Predictions 2023 – Making predictions about the future of enterprise tech is more challenging if you strive to lay down forecasts that are measurable. In other words if you make a prediction, you should be able to look back a year later and say with some degree of certainty whether the prediction came true or not. With evidence to back that up. In this Breaking Analysis we aim to do just that with predictions about the macro IT spending environment, cost optimization, security – lots to talk about there – generative AI, cloud and supercloud, blockchain adoption, data platforms, including commentary on Databricks, Snowflake and other key players, automation, events and we may even have some bonus predictions. To make all this happen we welcome back for the third year in a row, Erik Bradley our colleague from ETR. As well, you can check out how we did with our 2022 predictions. Watch the full video analysis. Episode 164 – ChatGPT Won’t Give OpenAI First Mover Advantage – OpenAI, the company, and ChatGPT have taken the world by storm. Microsoft reportedly is investing an additional $10B in the startup. But in our view, while the hype around ChatGPT is justified, we don’t believe OpenAI will lock up the market with its first mover advantage. Rather we believe that success in this market will be directly proportional to the quality and quantity of data that a technology company has at its disposal, and the compute power it has to run the system. This market is unlikely to display winner take all dynamics and will probably evolve in a more fragmented fashion than cloud. In this Breaking Analysis we unpack the excitement around ChatGPT and debate the premise that the company’s early entry into the space may not confer “game over” advantage to OpenAI. To do so we welcome CUBE collaborator Sarbjeet Johal & John Furrier, cohost of theCUBE. Watch the full video analysis. Episode 163 – Supercloud2 Explores Cloud Practitioner Realities & the Future of Data Apps – Enterprise tech practitioners, like most of us, want to make their lives easier so they can focus on delivering more value to their business. They want to tap best of breed services in the public cloud and at the same time connect their on-prem intellectual property to emerging applications which drive top line revenue and bottom line profits. But creating a consistent experience across clouds and on-prem estates has been an elusive capability for most organizations, forcing tradeoffs and injecting friction into the system. The need to create seamless experiences is clear and the technology industry is starting to respond with platforms, architectures and visions of what we call Supercloud. In this Breaking Analysis we give you a preview of Supercloud2, share key findings leading up to the event and highlight some of the areas we’ll be probing in the live program. Watch the full video analysis. Episode 162 – CIOs in a holding pattern but ready to strike at monetization – Recent conversations with IT decision makers show a stark contrast between the period exiting 2023 versus the mindset leaving 2022. CIOs are generally funding new initiatives by pushing off or cutting lower-priority items. While security efforts are still being funded, those that enable business initiatives that generate revenue take priority over cleaning up legacy technical debt. The bottom line is, for the moment at least, the mindset is not to cut everything, rather it’s to put a pause on cleaning up legacy hairballs and continue to fund initiatives to drive monetization. Cloud has become fundamental and getting data “right” is a consistent theme that appears to be an underpinning of initiatives getting funded today. In this Breaking Analysis we tap recent discussions from two primary sources: year-end ETR roundtables with IT decision makers and conversations on theCUBE with data, cloud and IT architecture practitioners. Watch the full video analysis. Episode 161 – AI goes mainstream but ROI remains elusive – A decade of big data investments combined with cloud scalability, the rise of more cost effective processing and the introduction of advanced tooling has catapulted machine intelligence to the forefront of technology investments. No matter what job you have, your operation will be AI powered within five years and machines may be doing your job in the future. Artificial intelligence is being infused into applications, infrastructure, equipment and virtually every aspect of our lives. AI is proving to be extremely helpful at controlling vehicles, speeding medical diagnoses, processing language, advancing science and generally raising the stakes on what it means to apply technology for business advantage. But business value realization has been a challenge for most organizations due to lack of skills, complexity of programming models, immature technology integration, sizable up front investments, ethical concerns and lack of business alignment. Mastering AI technology and a focus on features will not be a requirement for success in our view. Rather figuring out how and where to apply AI to your business will be the crucial gate. That means understanding the business case, picking the right technology partner, experimenting in bite sized chunks and quickly identifying winners to double down on from an investment standpoint. In this Breaking Analysis we update you on the state of AI with a focus on interpreting the latest ETR survey data around ML/AI and data. We’ll explore what it means for the competitive environment and what to look for in the future. To do so we invite into our studios Andy Thurai of Constellation Research. Andy covers AI deeply, he knows the players and the pitfalls of AI investment. Watch the full video analysis. Episode 160 – Grading our 2022 enterprise technology predictions – Nailing technology predictions in 2022 was tricky business. Projections on the performance of markets, identifying IPO prospects and making binary forecasts on data, AI, the macro spending climate, along with other related topics in enterprise tech, carried much uncertainty. 2022 was characterized by a seesaw economy where central banks were restructuring their balance sheets, the war in Ukraine fueled inflation, supply chains were a mess and the unintended consequences of digital acceleration are still being sorted. In this Breaking Analysis we continue our annual tradition of openly grading our previous year’s enterprise tech predictions. You may or may not agree with our self-grading system but we give you the data to draw your own conclusions. Watch the full video analysis. Episode 159 – How Palo Alto Networks Became the Gold Standard of Cybersecurity – Palo Alto Networks has earned a reputation as the leader in security. You can measure this in revenue, market cap, execution and, most importantly, conversations with CISOs. The company is on track to double its revenues to nearly $7B in FY23 from FY20. This despite macro headwinds which will likely continue through next year. Palo Alto owes its position to a clarity of vision and strong execution of a TAM expansion strategy bolstered by key acquisitions and integrations into its cloud & SaaS offerings. In this Breaking Analysis, and ahead of Palo Alto Ignite, we bring you the next chapter on top of last week’s cybersecurity update. We’ll dig into the ETR spending data on Palo Alto Networks, provide a glimpse of what to look for at Ignite and posit what Palo Alto needs to do to stay on top of the hill. Watch the full video analysis. Episode 158 – Cyber Firms Revert to the Mean – While by no means a safe haven, the cybersecurity sector has outpaced the broader tech market by a meaningful margin. That is up until very recently. Cyber security remains the number one technology priority for the c-suite but as we’ve previously reported, the CISO’s budget has constraints; just like other technology investments. Recent trends show that economic headwinds have elongated sales cycles, pushed deals into future quarters and, just like other tech initiatives, are pacing cybersecurity investments and breaking them into smaller chunks. In this Breaking Analysis we explain how cybersecurity trends are reverting to the mean and tracking more closely with other technology investments. We’ll make a couple of valuation comparisons to show the magnitude of the challenge and which cyber firms are feeling the heat, and which aren’t as much. We’ll then show the latest survey data from ETR to quantify the contraction in spending momentum and close with a glimpse at the landscape of emerging cybersecurity companies that could be ripe for acquisition, consolidation or disruptive to the broader market. Watch the full video analysis. Episode 157 – re:Invent 2022 marks the next chapter in data & cloud – The ascendency of AWS under the leadership of Andy Jassy was marked by a tsunami of data and corresponding cloud services to leverage data. Those services mainly came in the form of primitives – i.e. basic building blocks that were used by developers to create more sophisticated capabilities. AWS in the 2020s, led by CEO Adam Selipsky, will be marked by four high level trends in our view: 1) A rush of data that will dwarf anything previously seen; 2) Doubling down on investments in the basic elements of cloud – compute, storage, database, security, etc; 3) Greater emphasis on end-to-end integration of AWS services to make data accessible to more professionals and further accelerate cloud adoption; and 4) Significantly deeper business integration of cloud, beyond IT, as an underlying element of organizational transformation. In this Breaking Analysis we extract and analyze nuggets from John Furrier’s annual sit down with the CEO of AWS. We’ll share data from ETR and other sources to set the context for the market and competition in cloud and we’ll give you our glimpse of what to expect at re:Invent 2022. Watch the full video analysis. Episode 156 – Snowflake caught in the storm clouds – A better than expected earnings report in late August got people excited about Snowflake again but the negative sentiment in the market has weighed heavily on virtually all growth tech stocks. Snowflake is no exception. As we’ve stressed many times, the company’s management is on a long term mission to simplify the way organizations use data. Snowflake is tapping into a multi-hundred billion dollar total available market and continues to grow at a rapid pace. In our view the company is embarking on its third major wave of innovation, data apps, while its first and second waves are still bearing significant fruit. For short term traders focused on the next 90 or 180 days, that probably doesn’t matter much. But those taking a longer view are asking, should we still be optimistic about the future of this high flier or is it just another over-hyped tech play? In this Breaking Analysis we take a look at the most recent survey data from ETR to see what clues and nuggets we can extract to predict the near future and the long term outlook for Snowflake. Watch the full video analysis. Episode 155 – Cloudflare’s Supercloud…What Multi Cloud Could Have Been – Over the past decade, Cloudflare has built a global network that has the potential to become the fourth U.S.-based hyperscale-class cloud. In our view, the company is building a durable revenue model with hooks into many important markets. These include the more mature DDoS protection space, but also extend to growth sectors such as zero trust, a serverless platform for application development and an increasing number of services such as database and object storage. In essence, Cloudflare can be thought of as a giant, distributed supercomputer that can connect multiple clouds and act as a highly efficient scheduling engine– allocating and optimizing resources at scale. Its disruptive DNA is increasingly attracting novel startups and established global firms looking for a reliable, secure, high performance, low latency and more cost effective alternative to AWS and legacy infrastructure solutions. In this Breaking Analysis we initiate deeper coverage of Cloudflare. While the stock got hammered this past week on tepid guidance, we are optimistic about the company’s future. In this post, we’ll briefly explain our take on the company and its unique business model. We’ll then share some peer comparisons with both a financial snapshot and some fresh ETR survey data. Finally we’ll show some examples of how we think Cloudflare could be a disruptive force with a supercloud-like offering that, in many respects, is what multi-cloud should have been. Watch the full video analysis. Episode 154 – Even the Cloud Is Not Immune to the Seesaw Economy – Have you ever driven on the highway and traffic suddenly slows way down? And then after a little while it picks up again and you’re cruising along thinking…ok that was weird but it’s clear sailing now…only to find out in a bit that that traffic is building up again, forcing you to pump the brakes as the traffic patterns ebb and flow? Well welcome to the seesaw economy. The Fed induced fire that prompted a rally in tech is being purposely extinguished by that same Fed and virtually every sector of the tech industry is having to reset its expectations – including the cloud. In this Breaking Analysis we’ll review the implications of this week’s earnings announcements from the big 3 cloud players. The growth of AWS and Azure slowed while Google Cloud Platform’s growth accelerated. We’ll explain why GCP’s growth is still not fast enough. We’ll update you on our quarterly IaaS forecasts and share the latest cloud survey data from ETR. Watch the full video analysis. Episode 153 – Survey Says! Takeaways from the latest CIO spending data – The overall technology spending outlook is deteriorating. And yet there are positive signs making things unpredictable. The negative sentiment is of course being driven by macroeconomic factors and earnings forecasts that have been coming down all year while interest rates keep rising. Making matters worse is many people think earnings estimates are still too high. It’s understandable why there’s so much uncertainty. Technology continues to boom. Digital transformations are happening in earnest. Leading companies have momentum and long cash runways. Moreover, the CEOs of these leading companies are still really optimistic. But strong guidance in an environment of uncertainty is risky and makes navigation more challenging. In this Breaking Analysis we try to put some guardrails on the market by sharing takeaways from from ETRs latest spending survey, which was released to their private clients on the 21st of October. Today we’re going to review the macro spending data, convey where CIOs think their cloud spend is headed, look at the actions organizations are taking to manage uncertainty and then review some of the technology companies that have the most positive and negative outlooks in the ETR data set. Watch the full video analysis. Episode 152 – CEO Nuggets from Microsoft Ignite & Google Cloud Next – This past week, we saw two of the “Big 3” cloud providers present an update of their respective cloud visions, business progress, announcements and innovations. The content at these events had many overlapping themes including modern cloud infrastructure at global scale, applying advanced machine intelligence, end-to-end data platforms, the future of work, automation and a taste of the metaverse/Web 3.0. And more. Despite the striking similarities, the differences between these two cloud platforms, and that of AWS, remain significant with Microsoft leveraging its massive application software footprint to dominate virtually all markets; AndGoogle doing everything in its power to keep up with the frenetic pace of today’s cloud innovation which was set into motion a decade and a half ago by AWS. In this Breaking Analysis, we unpack the immense amount of content presented by the CEOs of Microsoft and Google Cloud at Microsoft Ignite and Google Cloud Next. We’ll also quantify with ETR survey data, the relative position of these two cloud giants in four key sectors – Cloud IaaS, BI analytics, data platforms and collaboration software. Watch the full video analysis. Episode 151 – Analyst Take on Dell – Dave Vellante provides his take on the transformation of Dell to Dell EMC to Dell Technologies, the impact of the VMware spin out and what the future holds for this bellwether infrastructure player. Watch the full video analysis. Episode 150 – Latest CIO Survey Shows Steady Deceleration in IT Spend – Is the glass half full or half empty? Well, it depends on how you want to look at it. CIOs are tapping the brakes on spending that’s clear. The latest macro survey from ETR quantifies what we already know to be true, that IT spend is decelerating. CIOs and IT buyers forecast that their tech spend will grow by 5.5% this year, a meaningful deceleration from their year end 2021 expectations…but these levels are still well above historical norms – so while the feel good factor may be in some jeopardy, overall things are pretty good – at least for now. In this Breaking Analysis, we update you on the latest macro tech spending data from Enterprise Technology Research, including strategies organizations are employing to cut costs…and which project categories continue to see the most traction. Watch the full video analysis. Episode 149 – As the tech tide recedes, all sectors feel the pinch – Virtually all tech companies have expressed caution on their respective earnings calls. And why not… the macroeconomic environment is full of uncertainties and there’s no upside to providing aggressive guidance when sellers punish even the slightest miss. Moreover, the spending data confirms the market is softening across the board, so it’s becoming expected that CFOs will guide cautiously. But companies facing execution challenges can’t hide behind the macro. Which is why it’s important to understand which firms are best positioned to maintain momentum through the headwinds and come out the other side stronger. In this Breaking Analysis we’ll do three things: 1) Share a high-level view of the spending squeeze almost all sectors are experiencing; 2) Highlight some of those companies that continue to show notably strong momentum – and relative high spending velocity on their platforms – albeit less robust than last year; and 3) give you a peek at how one senior technology leader in the financial sector sees the competitive dynamic between AWS, Snowflake and Databricks. Watch the full video analysis. Episode 148 – UiPath is a Rocket Ship Resetting its Course – Like a marathon runner pumped up on adrenaline, UiPath sprinted to the lead in what is surely going to be a long journey toward enabling the modern automated enterprise. In doing so, the company has established itself as a leader in enterprise automation, while at the same time getting out over its skis on critical execution items and disappointing investors along the way. In our view, the company has plenty of upside potential but will have to slog through its current challenges including restructuring its go to market, prioritizing investments, balancing growth with profitability and dealing with a difficult macro environment. In this Breaking Analysis and ahead of Forward5, UiPath’s customer conference, we once again dig into RPA and automation leader UiPath, to share our most current data and view of the company’s prospects, its performance relative to the competition and market overall. Watch the full video analysis. Episode 147 – How CrowdStrike Plans to Become a Generational Platform – In just over ten years, CrowdStrike has become a leading independent security firm. It has more than $2B in annual recurring revenue, nearly 60% ARR growth, a roughly $40B market capitalization, very high retention and a path to $5B in revenue by mid-decade. The company has joined Palo Alto Networks as a gold standard pure play cyber firm. It has achieved this lofty status with an architecture that enables it to go beyond point product. Combine this with outstanding go to market, solid financial execution, some sharp acquisitions and an ever-increasing total available market and you have the formula for a great company. In this Breaking Analysis and ahead of Fal.Con, CrowdStrike’s user conference, we take a deeper look into the company, its performance, its platform and customer survey data from our partner ETR. Watch the full video analysis. Episode 146 – We Have the Data…What Private Tech Companies Don’t Tell you About Their Business – The negative sentiment in tech stocks, caused by rising interest rates, less attractive discounted cash flow models and more tepid forward guidance, is easily measured by public market valuations. And while there’s lots of talk about the impact on private companies, their cash runways and 409A valuations, measuring the performance of non-public companies isn’t as easy. IPOs have dried up and public statements by private companies accentuate the good and hide the bad. Real data, unless you’re an insider, is hard to find. In this Breaking Analysis we unlock some of the secrets that non-public, emerging tech companies may or may not be sharing. We do this by introducing you to a capability from ETR that we’ve not previously exposed in Breaking Analysis. It’s called the ETR Emerging Technology Survey and is packed with sentiment and performance data based on surveys of more than 1,000 CIOs & IT buyers covering more than 400 private companies. The survey will highlight metrics on the evaluation, adoption and churn rates for private companies and the mindshare they’re able to capture. We’ve invited back our colleague Erik Bradley of ETR to help explain the survey and the data we’re going to cover in this post. Watch the full video analysis. Episode 145 – VMware Explore 2022 will mark the start of a Supercloud journey – While the precise direction of VMware’s future is unknown, given the planned Broadcom acquisition, one thing is clear; the subject of what Hock E. Tan plans will not be the main focus of the agenda at the upcoming VMware Explore event next week in San Francisco. We believe that despite any uncertainty, VMware will lay out for its customers what it sees as its future. And that future is multi-cloud or cross cloud services; what we would call supercloud. In this Breaking Analysis we drill into the latest ETR survey data on VMware. We’ll share with you the next iteration of the supercloud definition based on feedback from dozens of contributors. And we’ll give you our take on what to expect at VMware Explore next week. Watch the full video analysis. Episode 144 – What Black Hat ’22 tells us about securing the Supercloud – Black Hat 2022 was held in Las Vegas last week, at the same time as theCUBE’s supercloud event. Unlike AWS re:Inforce, where words are carefully chosen to put a positive spin on security, Black Hat exposes all the warts of cybersecurity and openly discusses its hard truths. It’s a conference attended by technical experts who proudly share some of the vulnerabilities they’ve discovered and of course by numerous vendors marketing their products and services. In this Breaking Analysis we summarize what we learned from discussions with several people who attended Black Hat and our analysis from reviewing dozens of keynotes, articles, videos, session talks, Dark Reading interviews and data from a recent Black Hat attendees survey conducted by Black Hat and Informa. We’ll also share data from ETR in a recent post discussing how Zscaler became the last line of defense for a manufacturing firm. We’ll end with a discussion of what it all means for the challenges around securing the supercloud. Watch the full video analysis. Episode 143 – Further defining Supercloud With tech leaders VMware, Snowflake, Databricks & others – At our inaugural Supercloud22 event we sought community input to evolve the concept of a supercloud by iterating on the definition, the salient attributes and examples of what is and is not a supercloud. We asked several technologists including experts from VMware, Snowflake, Databricks, HashiCorp, Confluent, Intuit, Cohesity and others to help test the supercloud definition, operational models, service models and principles. In this Breaking Analysis we unpack our discussions with these technology leaders and apply their input to iterate the definition of supercloud. We then go in-depth to examine Snowflake’s Data Cloud architecture as a leading example of supercloud. Watch the full video analysis. Episode 142 – What we hope to learn at Supercloud22 – The term supercloud is relatively new, but the concepts behind it have been bubbling for years. Early last decade when NIST put forth its original definition of cloud computing, it said services had to be accessible over a public network…essentially cutting the on-prem crowd out of the conversation. A guy named Chuck Hollis, a CTO at EMC and prolific blogger objected to that criterion and laid out his vision for what he termed a private cloud. In that post he showed a workload running both on premises and in a public cloud, sharing the underlying resources in an automated and seamless manner – what later became more broadly known as hybrid cloud. That vision, as we now know, really never materialized and we were left with multi-cloud…sets of largely incompatible and disconnected cloud services running in separate silos. The point is, what Hollis put forth – i.e. the ability to abstract underlying infrastructure complexity and run workloads across multiple heterogeneous estates with an identical experience – is what supercloud is all about. Watch the full video analysis. Episode 141 – How the cloud is changing security defenses in the 2020s – AThe rapid pace of cloud adoption has changed the way organizations approach cybersecurity. Specifically, the cloud is increasingly becoming the first line of cyber defense. As such, along with communicating to the board and creating a security-aware culture, the CISO must ensure that the shared responsibility model is being applied properly. The DevSecOps team has emerged as the critical link between strategy and execution, while audit becomes the “free safety” in the equation – i.e. the last line of defense. In this Breaking Analysis we share our puts and takes from AWS re:Inforce with an update on the latest hyperscale IaaS market shares; and insights from ETR survey data. We’ll also dig deeper into some technical aspects of AWS Nitro, a system we believe is one of AWS’ secret weapons, with a focus on confidential computing and what it means for the future of systems architecture. Watch the full video analysis. Episode 140 – AWS re:Inforce marks a summer checkpoint on cybersecurity – After a two year hiatus, AWS re:Inforce is back on as an in-person event in Boston next week. Like the all-star break in baseball, re:Inforce gives us an opportunity to evaluate the cybersecurity market overall, the state of cloud security and what AWS is up to in the sector. In this Breaking Analysis, we’ll share our view of what’s changed since our last cyber update in May, we’ll look at the macro environment, how it’s impacting cybersecurity plays in the market, what the ETR data tells us and what to expect at next week’s AWS re:Inforce. Watch the full video analysis. Episode 139 – Amping it up with Frank Slootman – Organizations have considerable room to improve their performance without making expensive changes to their talent, structure or fundamental business model. You don’t need a slew of consultants to tell you what to do. You already know. What you need is to immediately ratchet up expectations, energy, urgency and intensity. Fight mediocrity every step of the way. Amp it up and the results will follow. This is the fundamental premise of a hard hitting new book written by Frank Slootman, CEO of Snowflake, and published earlier this year. It’s called, Amp it Up, Leading for Hypergrowth by Raising Expectations, Increasing Urgency and Elevating Intensity. At Snowflake Summit last month, I was invited to interview Frank on stage about his book. I’ve read it several times and if you haven’t picked it up, you should. Even if you have read it, in this Breaking Analysis we’ll dig deeper into the book and share some clarifying insights and unpublished nuances of Frank’s philosophy. You’ll hear directly from Slootman himself with excerpts from my one on one conversation with him. Watch the full video analysis. Episode 138 – Answering the top 10 questions about SuperCloud – As we exited the isolation economy last year, supercloud is a term we introduced to describe something new that was happening in the world of cloud. In this Breaking Analysis we address the ten most frequently asked questions we get on supercloud. Watch the full video analysis. Episode 137 – H1 of ‘22 was ugly…H2 could be worse…Here’s why we’re still optimistic – After a two year epic run in tech, 2022 has been an epically bad year in the market. Through yesterday, the Nasdaq composite is down 30%, the S&P 500 is off 21%, the DJIA down 16% and the poor HODLers of BTC have had to endure a nearly 60% decline year to date. Watch the full video analysis. Episode 136 – Tech Spending Intentions are Holding Despite Macro Concerns – Much of the energy around data innovation that dispersed with the decline of Hadoop’s relevance is coalescing in a new ecosystem spawned by the ascendency of Snowflake’s Data Cloud. What was once seen as a simpler cloud data warehouse and good marketing with Data Cloud, is evolving rapidly with new workloads, a vertical industry focus, data applications, monetization and more. The question is will the promises of data be fulfilled this time around or is it same wine, new bottle? Watch the full video analysis. Episode 135 – Snowflake Summit 2022…All About Apps & Monetization – Much of the energy around data innovation that dispersed with the decline of Hadoop’s relevance is coalescing in a new ecosystem spawned by the ascendency of Snowflake’s Data Cloud. What was once seen as a simpler cloud data warehouse and good marketing with Data Cloud, is evolving rapidly with new workloads, a vertical industry focus, data applications, monetization and more. The question is will the promises of data be fulfilled this time around or is it same wine, new bottle? Watch the full video analysis. Episode 134 – How Snowflake Plans to Make Data Cloud a De Facto Standard – When Frank Slootman took ServiceNow public, many people undervalued the company, positioning it as just a better help desk tool. It turns out the firm actually had a massive TAM expansion opportunity in ITSM, HR, logistics, security, marketing and customer service management. NOW’s stock price followed the stellar execution under Slootman and CFO Mike Scarpelli’s leadership. When they took the reins at Snowflake, expectations were already set that they’d repeat the feat but this time, if anything, the company was overvalued out of the gate. It can be argued that most people didn’t really understand the market opportunity any better this time around. Other than that it was a bet on Slootman’s track record of execution…and data. Good bets; but folks really didn’t appreciate that Snowflake wasn’t just a better data warehouse. That it was building what the company calls a data cloud…and we’ve termed a data supercloud. In this Breaking Analysis and ahead of Snowflake Summit, we’ll do four things: 1) Review the recent narrative and concerns about Snowflake and its value; 2) Share survey data from ETR that will confirm almost precisely what the company’s CFO has been telling anyone who will listen; 3) Share our view of what Snowflake is building – i.e. trying to become the de facto standard data platform; and 4) Convey our expectations for the upcoming Snowflake Summit next week at Caesar’s Palace in Las Vegas. Watch the full video analysis. Episode 133 – MongoDB Sends Strong Signals Despite Cautious Macro Tones – Earnings season has shown a conflicting mix of signals for software companies. Most firms are expressing caution over macro headwinds citing a combination of Ukraine, inflation, interest rates, EMEA softness, currency, supply chain and general demand for technology. But MongoDB, along with a few other names appeared more sanguine, thanks to a beat in the recent quarter and a cautious but upbeat outlook for the near term. In this Breaking Analysis, ahead of MongoDB World 2022, we drill into the company’s business and what ETR survey data tells us in the context of overall demand and the patterns from other software companies. Watch the full video analysis. Episode 132 – Broadcom, Taming the VMware Beast – In the words of CUBE analyst and former CTO David Nicholson, Broadcom buys old cars. Not to restore them to their original beauty…nope…they buy classic cars to extract the platinum that’s inside the catalytic converter. Broadcom’s planned $61B acquisition of VMware will mark yet another new era for the virtualization leader, a mere seven months after finally getting spun out as a fully independent company by Dell. For VMware this means a dramatically different operating model, with financial performance and shareholder value creation as the dominant and perhaps sole agenda. For customers it will mean a more focused portfolio, less aspirational vision pitches and most certainly higher prices. In this Breaking Analysis we’ll share data, opinions and customer insights about this blockbuster deal and forecast the future of VMware, Broadcom and the broader ecosystem. Watch the full video analysis. Episode 131 – Supercloud is becoming a thing – Last year we noted in a Breaking Analysis that the cloud ecosystem is innovating beyond the notion of multicloud. We’ve said for years that multicloud is really not a strategy but rather a symptom of multivendor. We used the term supercloud to describe an abstraction layer that resides above and across hyperscale infrastructure, connects on premises workloads and eventually stretches to the edge. Our premise is that supercloud goes beyond running services in native mode on each individual cloud. Rather supercloud hides the underlying primitives and APIs of the respective cloud vendors and creates a new connection layer across locations. Since our initial post, we’ve found many examples within the ecosystem of technology companies working on so-called supercloud in various forms. Including some examples that actually do not try to hide cloud primitives but rather are focused on creating a consistent experience for developers across the devsecops tool chain, while preserving access to low level cloud services. In this Breaking Analysis we share some recent examples of supercloud that we’ve uncovered. We also tap theCUBE network to access direct quotes about supercloud from the many CUBE guests we’ve recently had on the program. Here we test the concept’s technical and business feasibility. We’ll also post some recent ETR data to put into context some of the players we think are going after this opportunity and where they’re at in their supercloud buildout. Watch the full video analysis. Episode 130 – Are Cyber Stocks Oversold or Still too Pricey? – Cybersecurity stocks have been sending mixed signals as of late…Mostly negative like much of tech. But some, such as Palo Alto Networks, despite a tough go of it recently, have held up better than most tech names. Others like CrowdStrike had been outperforming broader tech in March but then flipped in May. Okta’s performance was somewhat tracking along with CrowdStrike for most of the past several months but then the Okta hack changed the trajectory of that name. Zscaler has crossed the critical $1B ARR revenue milestone and sees a path to $5B, but the company’s stock fell sharply after its last earnings report and has been on a downtrend since last November…Meanwhile CyberArk’s recent beat and raise was encouraging and the stock acted well after its last report. Security remains the #1 initiative priority amongst IT organizations and the spending momentum for many high flying cyber names remains strong. So what gives in cybersecurity? In this Breaking Analysis we focus on security and will update you on the latest data from ETR to try and make sense out of the market and read into what this all means in both the near and long term for some of our favorite names in the sector. Watch the full video analysis. Episode 129- What you May not Know About the Dell Snowflake Deal – In the pre-cloud era, hardware companies would run benchmarks showing how database and application performance ran best on their systems relative to competitors and previous generation boxes. They would make a big deal out of it and the independent software vendors would do a “golf clap” in the form of a joint press release. It was a game of leapfrog amongst hardware competitors that became pretty commonplace over the years. The Dell-Snowflake deal underscores that the value prop between hardware companies and ISVs is changing and has much more to do with distribution channels and the amount of data that lives on-prem in various storage platforms. For cloud-native ISVs like Snowflake, they are realizing that despite their cloud-only dogma, they have to grit their teeth and deal with on-premises data or risk getting shut out of evolving data architectures. In this Breaking Analysis we unpack what little is known about the Snowflake announcement from Dell Technologies World… and discuss the implications of a changing cloud ecosystem landscape. We’ll also share some new ETR data for cloud and database platforms that shows Snowflake has actually entered the earth’s orbit when it comes to spending momentum on its platform. Watch the full video analysis. Episode 128- The Ever expanding Cloud Continues to Storm the IT Universe – Despite a mixed bag of earnings reports from tech companies, negative GDP growth this past quarter and rising inflation…the cloud continues its relentless expansion on the IT landscape. AWS, Microsoft and Alphabet have all reported earnings and, when you include Alibaba’s cloud in the mix, the big 4 hyperscalers are on track to generate $167B in revenue this year based on our projections. But as we’ve said many times the definition of cloud is expanding. And hybrid environments are becoming the norm at major organizations. We’re seeing the largest enterprise tech companies focus on solving for hybrid and every public cloud company now has a strategy to bring their environments closer to where customers’ workloads live – in data centers and the edge. Hello and welcome to this week’s Wikibon CUBE Insights, Powered by ETR. In this Breaking Analysis we’ll update you on our latest cloud projections and outlook. We’ll share the latest ETR data and some commentary on what’s happening in the “hybrid zone” of cloud. Watch the full video analysis. Episode 127- Does Hardware Still Matter – The ascendancy of cloud and SaaS has shone new light on how organizations think about, pay for, and value hardware. Once-sought-after skills for practitioners with expertise in hardware troubleshooting, configuring ports, tuning storage arrays and maximizing server utilization have been superseded by demand for cloud architects, DevOps pros and developers with expertise in microservices, container app development and similar skills. Even a company like Dell, the largest hardware company in enterprise tech, touts that it has more software engineers than those working in hardware. It begs the question: Is hardware going the way of COBOL? Well, not likely — software has to run on something. But the labor and skills needed to deploy, troubleshoot and manage hardware infrastructure is shifting quite dramatically. At the same time we’ve seen the value flow also changing in hardware. Once a world dominated by x86 processors, value is flowing to alternatives like Nvidia and Arm-based designs. Moreover, other components like NICs, accelerators and storage controllers are becoming more advanced, integrated and increasingly important. The question is: Does it matter? If so, why does it matter and to whom? What does it mean to customers, workloads, OEMs and the broader society? In this Breaking Analysis we try to answer these questions and to do so we’ve organized a special CUBE Power Panel of industry analysts and experts to address the question: Does Hardware (Still) Matter? Watch the full video analysis. Episode 126 – Technology & Architectural Considerations for Data Mesh – The introduction and socialization of data mesh has caused practitioners, business technology executives and technologists to pause and ask some probing questions about the organization of their data teams, their data strategies, future investments and their current architectural approaches. Some in the technology community have embraced the concept, others have twisted the definition while still others remain oblivious to the momentum building around data mesh. We are in the early days of data mesh adoption. Organizations that have taken the plunge will tell you aligning stakeholders is a non-trivial effort. But one that is necessary to break through the limitations that monolithic data architectures and highly specialized teams have imposed over frustrated business and domain leaders. However, practical data mesh examples often lie in the eyes of the implementer and may not strictly adhere to the principles of data mesh. Part of the problem is the lack of open technologies and standards that can accelerate adoption and reduce friction. This is the topic of today’s Breaking Analysis where we investigate some of the key technology and architectural questions around data mesh. To do so, we welcome back the founder of data mesh and Director of Emerging Technologies at ThoughtWorks, Zhamak Dehghani. Watch the full video analysis. Episode 125 – Customer ripple effects from the Okta breach are worse than you think – The recent security breach of an Okta third party supplier has been widely reported. The criticisms of Okta’s response have been harsh and the impact on Okta’s value has been obvious. Investors shaved about $6B off the company’s market cap during the week the hack was made public. We believe Okta’s claim that the customer technical impact was “near zero,” may be semantically correct. However, based on customer data, we feel Okta has a blind spot. There are customer ripple effects that require clear action, which are missed in Okta’s public statements. Okta’s product portfolio remains solid. It is a clear leader in the identity space. But in our view, one part of the long journey back to credibility requires Okta to fully understand and recognize the true scope of this breach on its customers. In this week’s Breaking Analysis we welcome our ETR colleague Erik Bradley to share new data from the community. In addition, we’ll analyze some of the statements made by Okta CEO Todd McKinnon in an interview with Emily Chang on Bloomberg to see how they align with what customers tell us. Watch the full video analysis. Episode 124 – New Data Signals C Suite Taps the Brakes on Tech Spending – Fresh survey data from ETR shows a clear deceleration in spending and a more cautious posture from technology buyers. Just this week we saw sell side downgrades in hardware companies like Dell and HP; and revised guidance from high flier UiPath, citing exposure to Russia, Europe and certain sales execution challenges. But these headlines we think are a canary in the coal mine pointing to broader tech spending softness. According to ETR analysis and channel checks in theCUBE community, the real story is these issues are not isolated. Rather we’re seeing signs of caution from buyers across the board in enterprise tech. In this Breaking Analysis we are the bearers of bad news, relatively speaking. We’ll share a first look at new data that suggest a tightening in tech spending, calling for 6% growth this year, which is below our January prediction of 8% for 2022. Watch the full video analysis. Episode 123 – Governments Should Heed the History of Tech Antitrust Policy – There are very few political issues that get bipartisan support these days, never mind consensus spanning geopolitical boundaries. But whether we’re talking across the aisle or over the pond, there seems to be common agreement that the power of big tech firms should be regulated. However the government’s track record when it comes to antitrust aimed at tech is mixed, at best. History shows that market forces, rather than public policy, have been much more effective at curbing monopoly power in the technology industry. Moreover, the standard for antitrust action has always been demonstrating consumer harm. Many of today’s policy makers are challenging that notion and using market dominance and the potential for consumer harm as the new benchmark for intervention. In this week’s Breaking Analysis we welcome in frequent CUBE contributor David Moschella, author and senior fellow at the Information Technology and Innovation Foundation. We explore several issues including the efficacy of governments’ antitrust actions against big tech, what types of remedies have been and can be most effective and a first pass assessment of the new rules EU regulators just agreed to try and rein in big tech companies. Watch the full video analysis. Episode 122 – Snowflake’s Wild Ride – Snowflake…they love the stock at 400 and hate it at 165. That’s the nature of the business isn’t it? Especially in this crazy cycle over the last two years of lockdowns, free money, exploding demand and now rising inflation and rates. But with the Fed providing some clarity on its actions, the time has come to really look at the fundamentals of companies and there’s no tech company more fun to analyze than Snowflake. In this breaking analysis we take look at the action of Snowflake’s stock since its IPO, why it’s behaved the way it has, how some sharp traders are looking at the stock and most importantly, what customer demand looks like. Watch the full video analysis. Episode 121 – Pat Gelsinger has the Vision Intel Just Needs Time, Cash & a Miracle – Intel’s future would be a disaster without Pat Gelsinger. Even with his clear vision, fantastic leadership, deep technical and business acumen and amazing positivity, the company’s future is in serious jeopardy. It’s the same story we’ve been telling for years. Volume is king in the semiconductor industry and Intel no longer is the volume leader. Despite Intel’s efforts to change that dynamic with several recent moves, including making another go at its foundry business, the company is years away from reversing its lagging position relative to today’s leading foundries and design shops. Intel’s best chance to survive as a leader in our view will come from a combination of a massive market, continued supply constraints, government money and luck – perhaps in the form of a deal with Apple in the mid- to long term. In this Breaking Analysis we’ll update you on our latest assessment of Intel’s competitive position and unpack nuggets from the company’s February investor conference. Watch the full video analysis. Episode 120 – RPA has Become a Transformation Catalyst, Here’s What’s New – In its early days, robotic process automation emerged from rudimentary screen scraping, macros and workflow automation software. Once a script-heavy and limited tool that was almost exclusively used to perform mundane tasks for individual users, RPA has evolved into an enterprise-wide megatrend that puts automation at the center of digital business initiatives. In this Breaking Analysis we present our quarterly update of the trends in RPA and share the latest survey data from Enterprise Technology Research. Watch the full video analysis. Episode 119 – Cyber Stocks Caught in the Storm While Private Firms Keep Rising – The pandemic precipitated what is shaping up to be a permanent shift in cyber security spending patterns. As a direct result of hybrid work, CISOs have invested heavily in endpoint security, identity access management, cloud security and further hardening the network beyond the HQ. Moreover, the need to build security into applications from the start, rather than bolting protection on as an afterthought, has led to vastly heightened awareness around DevSecOps. Finally, attacking security as a data problem with automation and AI is fueling new innovations in cyber products and services; and is spawning well-funded, disruptive startups. In this Breaking Analysis we present our quarterly findings on the security sector. We’ll share the latest ETR survey data, identify the companies with customer spending momentum and share some of the market movers. Watch the full video analysis. Episode 118 – The Improbable Rise of Kubernetes – The rise of Kubernetes came about through a combination of forces that were in hindsight, quite a long shot. AWS’ dominance created momentum for cloud native application development and the need for simpler experiences beyond easily spinning up compute as a service. This wave crashed into innovations from a startup named Docker and a reluctant open source benefactor in Google that needed a way to change the game on Amazon in the cloud. Factor in Red Hat, which needed a path beyond Linux and was just about to opt for an alternative to Kubernetes to power OpenShift. Finally, figure out a governance structure to herd all the cats in the ecosystem so you can win out over other competition and create a de facto standard. Make all that happen and you get the remarkable ascendancy of Kubernetes. Such was the story unveiled recently in a new two-part documentary series from Honeypot simply titled “Kubernetes.” In this Breaking Analysis we tap the back stories of this documentary, which explains the improbable events leading to the creation of Kubernetes. We’ll share commentary from early Kubernetes committers and key players who came on theCUBE to piece together how it all happened. Finally, we’ll present new survey data from ETR on containers. Watch the full video analysis. Episode 117 – What to Expect in Cloud 2022 & Beyond – We’ve often said that the next ten years in cloud computing won’t be like the last ten. Cloud has firmly planted its footprint on the other side of the chasm with the momentum of the entire multi-trillion dollar technology business behind it. Both sellers and buyers are leaning in by adopting cloud technologies and many are building their own value layers on top of cloud. In the coming years we expect innovation will continue to coalesce around the big 3 U.S. clouds, plus Alibaba in APAC, with the ecosystem building value on top of the hardware, software and tools provided by the hyperscalers. Importantly, we don’t see this as a race to the bottom. Rather our expectation is that the large public cloud players will continue to take cost out of their platforms through innovation, automation and integration. Other cloud providers and the ecosystem, including traditional IT buyers, will leverage hyperscale clouds and mine opportunities in their respective markets. This is not a zero sum game. In this Breaking Analysis we’ll update you on our latest projections in the cloud market, share some new ETR survey data with some surprising nuggets; and drill into the important cloud database landscape. Watch the full video analysis. Episode 116 – Securing Snowflake – The amount of data ingested into a data warehouse overwhelmed the system. Every time Intel came out with a new microprocessor, practitioners would “chase the chips” in an effort to try and compress the overly restrictive elapsed time to insights. This cycle repeated itself for decades. Cloud data warehouses generally and Snowflake specifically changed all this. Not only were resources virtually infinite, but the ability to separate compute from storage permanently altered the cost, performance, scale and value equation. But as data makes its way into the cloud and is increasingly democratized as a shared resource across clouds – and at the edge – practitioners must bring a SecDevOps mindset to securing their cloud data warehouses. This Breaking Analysis takes a closer look at the fundamentals of securing Snowflake. An important topic as data becomes more accessible and available to a growing ecosystem of users, customers and partners. To do so we welcome two guests to this episode. Ben Herzberg is an experienced hacker, developer and an expert in several aspects of data security. Yoav Cohen is a technology visionary and currently serving as CTO at Satori Cyber. Watch the full video analysis. Episode 115 – Enterprise Technology Predictions 2022 – The pandemic has changed the way we think about, and predict the future. As we enter the third year of COVID, we see the significant impact it’s had on technology strategies, spending patterns and company fortunes. Much has changed and while many of these changes were forced reactions to a new abnormal, the trends we’ve seen over the past twenty-four months have become more entrenched and point the way to what’s ahead in the technology business. In this Breaking Analysis we welcome our data partner and colleague Erik Porter Bradley from ETR and we put forth our annual predictions for enterprise technology in 2022 and beyond. We’ll do our best to backup our predictions specific supporting data and more granular detail that can be measured as accurate or not. Please refer to the grading of our 2021 predictions to judge for yourself how we did last year. Watch the full video analysis. Episode 114 – Cyber, Blockchain & NFTs Meet the Metaverse – When Facebook changed its name to Meta last fall it catalyzed a chain reaction throughout the tech industry. Software firms, gaming companies, chip makers, device manufacturers and others have joined in the hype machine. It’s easy to dismiss the metaverse as futuristic hyperbole, but do we really believe that tapping on a smartphone, staring at a screen or two dimensional Zoom meetings are the future of how we work, play and communicate? As the Internet itself proved to be larger than we ever imagined, it’s very possible that the combination of massive processing power, cheap storage, AI, blockchains, crypto, sensors, AR/VR, brain interfaces and other emerging technologies will combine to create new and unimaginable consumer experiences; and massive wealth for creators of the metaverse. In this Breaking Analysis we explore the intersection of cybersecurity, blockchain, crypto currency, NFTs and the emerging metaverse. To do so we welcome in cyber expert, hacker, gamer, NFT expert and founder of Ore System, Nick Donarski. Watch the full video analysis. Episode 113 – Analyst Predictions 2022: The Future of Data Management – In the 2010’s, organizations became keenly aware that data would become the critical ingredient in driving competitive advantage, differentiation and growth. But to this day, putting data to work remains a difficult challenge for many if not most organizations. As the cloud matures it has become a game changer for data practitioners by making cheap storage and massive processing power readily accessible. We’ve also seen better tooling in the form of data workflows, streaming, machine intelligence/AI, developer tools, security, observability, automation, new databases and the like. These innovations accelerate data proficiency but at the same time add complexity for practitioners. Data lakes, data hubs, data warehouses, data marts, data fabrics, data meshes, data catalogs and data oceans are forming, evolving and exploding onto the scene. In an effort to bring perspective to this sea of optionality, we’ve brought together some of the brightest minds in the data analyst community to discuss how data management is morphing and what practitioners should expect in 2022 and beyond. Watch the full video analysis. Episode 112 – Grading our 2021 Predictions – Predictions are all the rage this time of year. On December 29th, 2020, in collaboration with Erik Porter Bradley of Enterprise Technology Research, we put forth our predictions for 2021. The focus of our prognostications included tech spending, remote work, productivity apps, cyber, IPOs, SPACs, M&A, data architecture, cloud, hybrid cloud, multi-cloud, AI, containers, automation and semiconductors. We covered a lot of ground! In this Breaking Analysis, as a warmup for our 2022 predictions post, we’ll review each of our predictions for this past year and grade the accuracy of our forecasts Watch the full video analysis. Episode 111 – Why Oracle’s Stock is Surging to an All time High – On Friday, Oracle announced a meaningful earnings beat and strong forward guidance on the strength of its license business; and slightly better than expected cloud performance. The stock rose sharply on the day and closed up nearly 16% surpassing $280B in market value. Oracle’s success is due largely to its execution of a highly differentiated strategy that has evolved over the past decade or more; deeply integrating its hardware and software, heavily investing in next generation cloud, creating a homogenous experience across its application portfolio and becoming the number one platform for the world’s most mission critical applications. While investors piled into the stock, skeptics will point to the beat being tilted toward license revenue and investors will likely keep one finger on the sell button until they’re convinced Oracle’s cloud momentum is more consistent and predictable. In this Breaking Analysis we’ll review Oracle’s most recent quarter and pull in some ETR survey data to frame the company’s cloud business, the momentum of Fusion ERP, where the company is winning and some gaps/opportunities we see that can be addressed in the coming quarters. Watch the full video analysis. Episode 110 – Rise of the Supercloud – Last week’s AWS re:Invent underscored the degree to which cloud computing generally and AWS specifically have impacted the technology landscape. From making infrastructure deployment simpler, to accelerating the pace of innovation, to the formation of the world’s most active and vibrant technology ecosystem; it’s clear that AWS has been the number one force for industry change in the last decade. Going forward we see three high level contributors from AWS that will drive the next 10 years of innovation, including: 1) the degree to which data will play a defining role in determining winners and losers; 2) the knowledge assimilation effect of AWS’ cultural processes such as two pizza teams, customer obsession and working backwards; and 3) the rise of superclouds– that is clouds built on top of hyperscale infrastructure that focus not only on IT transformation, but deeper business integration and digital transformation of entire industries. In this Breaking Analysis we’ll review some of the takeaways from the 10th annual AWS re:Invent conference and focus on how we see the rise of superclouds impacting the future of virtually all industries. Watch the full video analysis. Episode 109 – Break up Amazon? Survey Suggests it May Not be Necessary – Despite the posture that big tech generally and Amazo.com Inc.specifically should be regulated and/or broken apart, recent survey research suggests that Amazon faces many disruption challenges, independent of any government intervention. Specifically, respondents to our survey believe that history will repeat itself in that there’s a 60% probability that Amazon will be disrupted by market forces, including self-inflicted wounds. Amazon faces at least seven significant disruption scenarios of varying likelihood and impact, perhaps leading to the conclusion that the government should let the market adjudicate Amazon’s ultimate destiny. In this Breaking Analysis, and ahead of AWS re:Invent, we share the results of our survey designed to asses what, if anything, could disrupt Amazon. We’ll also show you some data from ETR that indicates the strong momentum of AWS is likely to continue, which could be a factor in any government intervention. Watch the full video analysis. Episode 108 – AWS & Azure Accelerate Cloud Momentum – Despite all the talk about repatriation, hybrid and multi-cloud opportunities and cloud as an increasingly expensive option for customers…the data continues to show the importance of public cloud to the digital economy. Moreover, the two leaders, AWS and Azure are showing signs of accelerated momentum that point to those two giants pulling away from the pack in the years ahead. Each of these companies is demonstrating broad-based momentum across their respective product lines. It’s unclear if anything other than government intervention or self-inflicted wounds will slow these two companies down this decade. Despite the commanding lead of the two leaders, a winning strategy for companies that don’t run their own cloud continues to be innovating on top of their massive CAPEX investments. The most notable example of this approach in our view continues to be Snowflake. In this Breaking Analysis, Dave will provide our quarterly market share update of the big four hyperscale cloud providers. We’ll share some new data from ETR based on the most recent survey, drill into some of the reasons for the momentum of AWS and Azure; and drill further into the database and data warehouse sector to see what if anything has changed in that space. Watch the full video analysis. Episode 107 – Cutting Through the Noise of Full Stack Observability – Full stack observability is the new buzz phrase. As businesses go digital, customer experience becomes ever more important. Why? Because fickle consumers can switch brands in the blink of an eye – or the click of a mouse. Every vendor wants a piece of the action in this market including companies that have provided traditional monitoring, log analytics, application performance management, and related services. These companies are joined by a slew of new entrants claiming end-to-end visibility across the so-called “modern tech stack.” Recent survey research however confirms our thesis that no one company has it all. New entrants have a vision and are not encumbered by legacy technical debt. However their offerings are immature. Established players with deep feature sets in one segment are pivoting through M&A and organic development to fill gaps. Meanwhile, cloud players are gaining traction and participating through a combination of native tooling combined with strong ecosystems to address this opportunity. In this Breaking Analysis we dive into a recent ETR drill down study on full stack observability. And to do so we once again welcome in our colleague Erik Bradley, Chief Engagement Strategist at ETR. Watch the full video analysis. Episode 106 – What Could Disrupt Amazon? – Five publicly traded, US-based companies have market valuations over or just near one trillion dollars. As of Oct. 29th, Apple and Microsoft top the list, each at $2.5T, followed by Alphabet at $2T, Amazon at $1.7T and Facebook (now Meta) at just under $1T – off from its high of $1.1T prior to its recent troubles. These companies have reached extraordinary levels of success and power. What, if anything could disrupt their market dominance? In his book Seeing Digital, Author David Moschella made three key points that are relevant: In the technology industry, disruptions are the norm – The waves of mainframes, Minis, PCs, Mobile and the Internet all saw new companies emerge and power structures that dwarfed previous eras of innovation. Is that dynamic changing? Every industry has a disruption scenario. Silicon Valley – broadly defined to include Seattle, or at least Amazon – has a dual disruption agenda. The first being horizontally targeting the technology industry and the second as digital disruptors in virtually any industry. How relevant is that to the future power structure of the technology business? In this Breaking Analysis we welcome in author, speaker, researcher, thought leader and senior fellow at ITIF, David Moschella to assess what could possibly disrupt today’s trillionaire companies. And we’ll start with Amazon. Watch the full video analysis. Episode 105 – Data Mesh…A New Paradigm for Data Management – Data mesh is a new way of thinking about how to use data to create organizational value. Leading edge practitioners are beginning to implement data mesh in earnest. Importantly, data mesh is not a single tool or a rigid reference architecture. Rather it’s an architectural and organizational model that is designed to address the shortcomings of decades of data challenges and failures. As importantly, it’s a new way to think about how to leverage and share data at scale across an organization and ecosystems. Data mesh in our view will become the defining paradigm for the next generation of data excellence. In this Breaking Analysis we welcome the founder and creator of data mesh, author, thought leader, technologist Zhamak Dehghani, who will help us better understand some of core principles of data mesh and the future of decentralized data management. With practical advice for data pros who want to create the next generation of data-driven organizations. Watch the full video analysis. Episode 104 – The Hybrid Cloud Tug of War Gets Real – It looks like Hybrid cloud is finally here. We’ve seen a decade of posturing, marketecture, slideware and narrow examples but there’s little question that the definition of cloud is expanding to include on-premises workloads in hybrid models. Depending on which numbers you choose to represent IT spending, public cloud accounts for less than 5% of the total pie. As such there’s a huge opportunity in hybrid, outside of the pure public cloud; and everyone wants a piece of the action. The big question is how will this now evolve? Customers want control, governance, security, flexibility and a feature-rich set of services to build their digital businesses. It’s unlikely they can buy all that – so they’re going to have to build it with partners. Specifically vendors, SIs, consultancies, and their own developers. The tug-of-war to win the new cloud day has finally started in earnest – between the hyperscalers and the largest enterprise tech companies in the world. Watch the full video analysis. Episode 103 – The Future of the Semiconductor Industry – Semiconductors are at the heart of technology innovation. For decades, technology improvements have marched to the cadence of silicon advancements in performance, cost, power and packaging. In the past ten years, the dynamics of the semiconductor industry have changed dramatically. Soaring factory costs, device volume explosions, fabless chip companies, greater programmability, compressed time to tape out, more software content, the looming Chinese presence…these and other factors have permanently changed the power structure of the semiconductor business. We rely on chips for every aspect of our lives, which has led to a global semiconductor shortage that has impacted more industries than we’ve ever seen. Our premise is that silicon success in the next twenty years will be determined by volume manufacturing expertise, design innovation, public policy, geopolitical dynamics, visionary leadership and innovative business models that can survive the intense competition in one of the most challenging businesses in the world. Watch the full video analysis. Episode 102 – UiPath Fast Forward to Enterprise Automation | UiPath FORWARD IV – UiPath has always been an unconventional company. It started with humble beginnings as essentially a software development shop. It then caught lightning in a bottle with its computer vision technology and simplification mantra…creating easy to deploy software robots for bespoke departments to automate mundane tasks. The story is well known…the company grew rapidly and was able to go public earlier this year. Consistent with its out of the ordinary approach, while other firms are shutting down travel and physical events, UiPath is moving ahead with Forward IV, its annual user conference next week…with a live audience at the Bellagio in Las Vegas. It’s also “Fast Forwarding” as a company, determined to lead the charge beyond RPA point tools and execute on a more all-encompassing enterprise automation agenda. Watch the full video analysis. Episode 101 – CIOs Signal Hybrid Work Will Power Spending Through 2022 – Throughout the pre-vaccine COVID era, IT buyers indicated budget constraints would squeeze 2020 spending by roughly 5% relative to 2019 levels. But the forced March to digital combined with increased cyber threats for remote workers, created a modernization mandate that powered Q4 spending last year. This momentum has carried through to 2021. While COVID variants have delayed return to work and business travel plans, our current forecast for global IT spending remains strong at 6-7%, slightly down from previous estimates. But the real story is CIOs and IT buyers expect a 7-8% increase in 2022 spending, reflecting investments in hybrid work strategies and a continued belief that technology remains the underpinning of competitive advantage in the coming decade. In this Breaking Analysis, Dave will share the latest results of ETR’s macro spending survey and update you on industry and sector investment patterns. Watch the full video analysis. Following is the complete collection to date, crystallizing the key topics of the past year or so. We hope you enjoy these episodes and, as always, welcome your feedback. Episode 100 – How Cisco can win cloud’s ‘Game of Thrones’ – Cisco is a company at the crossroads. It is transitioning from a high margin hardware business to a software subscription-based model through both organic moves and targeted acquisitions. It’s doing so in the context of massive macro shifts to digital and the cloud. We believe Cisco’s dominant position in networking, combined with a large market opportunity and a strong track record of earning customer trust, put the company in a good position to capitalize on cloud momentum. But there are clear challenges ahead, not the least of which is the growing complexity of Cisco’s portfolio, transitioning a large legacy business and the mandate to maintain its higher profitability profile as it moves to a new business model. In this Breaking Analysis, we welcome in Zeus Kerravala, Founder and Principal Analyst at ZK Research and long time Cisco watcher who collaborated with us to craft the premise of this session. Watch the full video analysis. Episode 99 – The Case for Buy the Dip on Coupa, Snowflake & Zscaler – Buy the dip has been an effective strategy since the market bottomed in early March last year. The approach has been especially successful in tech and even more so for those tech names that: 1) were well-positioned for the forced march to digital – i.e. remote work, online commerce, data-centric platforms and certain cybersecurity plays; and 2) already had the cloud figured out. The question on investors’ minds is where to go from here. Should you avoid some of the high flyers that are richly valued with eye-popping multiples? Or should you continue to buy the dip? And if so, which companies that capitalized on the trends from last year will see permanent shifts in spending patterns that make them a solid long term play. In this Breaking Analysis we shine the spotlight on three companies that may be candidates for a buy the dip strategy over the next 3-5 years. To do so it’s our pleasure to welcome Ivana Delevska, the Chief Investment Officer and founder of SPEAR Alpha, a new, research-centric ETF focused on industrial technology. Watch the full video analysis. Episode 98 – Thinking Outside the Box…AWS Signals a New Era for Storage – By our estimates, AWS will generate around $9B in storage revenue this year and is now the second largest supplier of enterprise storage behind Dell. We believe AWS storage revenue will surpass $11B in 2022 and continue to outpace on-prem storage growth by more than 1,000 basis points for the next three to four years. At its third annual Storage Day event, AWS signaled a continued drive to think differently about data storage and transform the way customers migrate, manage and add value to their data over the next decade. In this Breaking Analysis Dave will give a brief overview of what we learned at AWS’ Storage Day, share our assessment of the big announcement of the day – a deal with NetApp to run the full ONTAP stack natively in the cloud as a managed service – and share some new data on how we see the market evolving. Watch the full video analysis. Episode 97 – Tech Earnings Signal a Continued Booming Market – Tech earnings reports from key enterprise software and infrastructure players this week, underscore that IT spending remains robust in the post isolation economy. This is especially true for those companies that have figured out a coherent and compelling cloud strategy. Despite COVID variant uncertainties and hardware component shortages, most leading tech names outperformed expectations. That said, investors weren’t in the mood to reward all stocks and any variability in product mix, earnings outlook or bookings/billings nuances were met with a tepid response from the street. In this Breaking Analysis Dave will provide our commentary and new data points on key technology companies including Snowflake, Salesforce, Workday, Splunk, Elastic, Palo Alto Networks, VMware, Dell, Pure Storage, HP Inc. and NetApp. Watch the full video analysis. Episode 96 – Can anyone tame the identity access beast? Okta aims to try… – Chief information security officers cite trust as the most important value attribute they can deliver to their organizations. And when it comes to security, identity is the new attack surface. As such, identity and access management continue to be the top priority among technology decision makers. It also happens to be one of the most challenging and complicated areas of the cyber security landscape. Okta, a leader in the identity space, has announced its intent to converge privilege access and identity governance in an effort to simplify the landscape and reimagine identity. Our research shows that interest in this type of consolidation is high, but organizations believe technical debt, compatibility issues, expense and lack of talent are barriers to reaching cyber nirvana with their evolving zero trust networks. In this Breaking Analysis, Dave will explore the complex and evolving world of identity access and privileged account management. With an assessment of Okta’s market expansion aspirations and fresh data from ETR and input from Erik Bradley. Watch the full video analysis. Episode 95 – Rethinking Data Protection in the 2020s – Techniques to protect sensitive data have evolved over thousands of years, literally. The pace of modern data protection is rapidly accelerating and presents both opportunities and threats for organizations. In particular, the amount of data stored in the cloud, combined with hybrid work models, the clear and present threat of cyber crime, regulatory edicts and ever-expanding edge use cases should put CxOs on notice that the time is now to rethink your data protection strategies. In this Breaking Analysis, Dave is going to explore the evolving world of data protection and share some data on how we see the market evolving and the competitive landscape for some of the top players. Watch the full video analysis. Episode 94 – Cyber, Cloud, Hybrid Work & Data Drive 8% IT Spending Growth in 2021 – Every CEO is figuring out the right balance for new hybrid business models. Regardless of the chosen approach, which will vary, technology executives understand they must accelerate digital and build resilience as well as optionality into their platforms. This is driving a dramatic shift in IT investments at the macro level as we expect total spending to increase at 8% in 2021, compared to last year’s contraction. Investments in cyber security, cloud, collaboration to enable hybrid work and data, including analytics, AI and automation are the top spending priorities for CxOs. In this Breaking Analysis Dave welcomes back Erik Bradley, Chief Engagement Strategist at our partner ETR. In this post we’ll share some takeaways from ETR’s latest survey and provide our commentary on what it means for markets, sellers and buyers. We’ll also explain what we think Wall Street is missing about Amazon’s latest earnings. Watch the full video analysis. Episode 93 – ServiceNow’s Collision Course with Salesforce.com – ServiceNow is a company that investors love to love. But there’s caution in the investor community right now as confusion about transitory inflation and higher interest rates looms. ServiceNow also suffers from perfection syndrome and elevated expectations. In this Breaking Analysis Dave will dig into ServiceNow, one of the companies we began following almost ten years ago, and provide some thoughts on ServiceNow’s march to $15B by 2026. Watch the full video analysis. Episode 92 – Survey Data Shows no Slowdown in AWS & Cloud Momentum – Despite all the chatter about cloud repatriation and the exorbitant cost of cloud computing, customer spending momentum continues to accelerate in the post isolation economy. If the pandemic was good for the cloud it seems that the benefits of cloud migration remain lasting in the late stages of COVID. And we believe this stickiness will continue. In this Breaking Analysis Dave will share some fresh July survey data that indicates accelerating momentum for the largest cloud computing firms. Watch the full video analysis. Episode 91 – How JPMC is Implementing a Data Mesh Architecture on the AWS Cloud – A new era of data is upon us. The technology industry generally and the data business specifically are in a state of transition. Even our language reflects that. For example, we rarely use the phrase “Big Data” anymore. Rather we talk about digital transformation or data-driven companies. In this Breaking Analysis we want to share our assessment of the state of the data business. We’ll do so by looking at the data mesh concept and how a division of a leading financial institution, JPMC, is practically applying these relatively new ideas to transform its data architecture for the next decade. Watch the full video analysis. Episode 90 – Mobile World Congress Highlights Telco Transformation – AMobile World Congress is on for 2021. theCUBE will be there and we’ll let you know if it’s alive and well. As we approach a delayed MWC it’s appropriate to reflect on the state of the telecoms industry. Let’s face it – the telcos have done a great job of keeping us all connected during the pandemic. In this Breaking Analysis we welcome a long time telecoms industry analyst and the Founding Director of Lewis Insight, Mr. Chris Lewis. Watch the full video analysis. Episode 89 – How AWS is Revolutionizing Systems Architecture – AWS is pointing the way to a revolution in system architecture. Much in the same way that AWS defined the cloud operating model last decade, we believe it is once again leading in future systems. In this Breaking Analysis we’ll dig into the moves that AWS has been making, explain how they got here, why we think this is transformational for the industry and what this means for customers, partners and AWS’ many competitors. Watch the full video analysis. Episode 88 – Learnings from the hottest startups in cyber & IT infrastructure – As you well know by now, the cloud is about shifting IT labor to more strategic initiatives. Or as Andy Jassy posited at the first AWS re:Invent conference in 2012, removing the undifferentiated heavy lifting associated with deploying and managing IT infrastructure. In this Breaking Analysis Dave is pleased to welcome a special guest, Erik Suppiger, author of the Elite 80 – a report that details the hottest privately held cybersecurity and IT infrastructure companies in the world. Erik is a senior analyst at JMP Securities and will share insights from this report. Watch the full video analysis. Episode 87 – Chasing Snowflake in Database Boomtown – Database is the heart of enterprise computing. The market is both growing rapidly and evolving. Major forces transforming the space include cloud and data – of course – but also new workloads, advanced memory and IO capabilities, new processor types, a massive push toward simplicity, new data sharing and governance models; and a spate of venture investment. In this Breaking Analysis Dave will share our most current thinking on the database marketplace and dig into Snowflake’s execution, some of its challenges and we’ll take a look at how others are making moves to solve customer challenges; and angling to get their piece of the growing database pie. Watch the full video analysis. Episode 86 – How Nvidia plans to own the data center with AI – Nvidia wants to completely transform enterprise computing by making datacenters run 10X faster at 1/10th the cost. In this Breaking Analysis Dave will explain why we believe Nvidia is in a strong position to power the world’s computing centers and how it plans to disrupt the grip that x86 architectures have had on the datacenter market for decades. Watch the full video analysis. Episode 85 – Your Online Assets Aren’t Safe – Is Cloud the Problem or the Solution? – The convenience of online access to bank accounts, payment apps, crypto exchanges and other transaction systems has created enormous risks, which the vast majority of individuals either choose to ignore or simply don’t understand. In this Breaking Analysis Dave will try to raise awareness about a growing threat to your liquid assets and hopefully inspire you to do some research and take actions to lower the probability of you losing thousands, hundreds of thousands or millions of dollars. Watch the full video analysis. Episode 84 – Debunking the Cloud Repatriation Myth – Cloud repatriation is a term often used by technology companies that don’t operate a public cloud. The marketing narrative most typically implies that customers have moved work to the public cloud and, for a variety of reasons – expense, performance, security, etc. Some have written about the repatriation myth, but in this Breaking Analysis, Dave will share hard data from ETR and other sources that we feel debunks the repatriation narrative as it’s currently being promoted. Watch the full video analysis. Episode 83 – Chaos Creates Cash for Criminals & Cyber Companies – The pandemic not only accelerated a shift to digital, it highlighted a rush of cyber criminal sophistication, collaboration and chaotic responses from virtually every major company on the planet. The SolarWinds hack exposed digital supply chain weaknesses and appears to have accelerated so-called island hopping techniques that are exceedingly difficult to detect. In this Breaking Analysis Dave will provide our quarterly update on the security industry and share new survey data from ETR and theCUBE community that will help you navigate through the maze of corporate cyber warfare. Watch the full video analysis. Episode 82 – Why Apple Could be the Key to Intel’s Future – The latest Arm Neoverse announcement further cements our opinion that its architecture, business model and ecosystem execution are defining a new era of computing; and leaving Intel in its dust. In this Breaking Analysis, Dave will explain why and how Apple could hold a key to saving Intel’s (and America’s) semiconductor industry leadership position. Watch the full video analysis. Episode 81 – A Digital Skills Gap Signals Rebound in IT Services Spend – Recent survey data from ETR shows that enterprise tech spending is tracking with projected U.S. GDP growth at 6% to 7% this year. Many markers continue to point the way to a strong recovery including hiring trends and the loosening of frozen IT project budgets. In this Breaking Analysis Dave welcomes back Erik Bradley, Chief Engagement Strategist at ETR, who will share fresh data, perspectives and insights from the latest survey data. Watch the full video analysis. Episode 80 – UiPath’s Unconventional $PATH to IPO – UiPath is going public this coming week and will be the next hot software company to IPO. It has had a long strange trip to IPO. In this week’s Breaking Analysis, Dave shares our learnings from sifting through hundreds of pages of UiPath’s S1 and convey our thoughts on its market, competitive position and outlook Watch the full video analysis. Episode 79 – Moore’s Law is Accelerating and AI is Ready to Explode – Moore’s Law is dead right? Think again. While the historical annual CPU performance improvement of ~40% is slowing, the combination of CPUs packaged with alternative processors is improving at a rate of more than 100% per annum. In this Breaking Analysis Dave is going to unveil some data that suggests we’re entering a new era of innovation where inexpensive processing capabilities will power an explosion of machine intelligence applications. Watch the full video analysis. Episode 78 – Arm Lays Down the Gauntlet at Intel’s Feet – Exactly one week after Pat Gelsinger unveiled plans to reinvent Intel, Arm announced version 9 of its architecture and put forth its vision for the next decade. In this Breaking Analysis Dave will explain why we think this announcement is so important and what it means for Intel and the broader technology landscape. Watch the full video analysis. Episode 77 – Intel… Too Strategic to Fail – Intel’s big announcement this week underscores the threat that the United States faces from China. The U.S. needs to lead in semiconductor design and manufacturing; and that lead is slipping because Intel has been fumbling the ball over the past several years. In this Breaking Analysis Dave will peel the onion of Intel’s announcement, explain why we’re not as sanguine as was Wall Street on Intel’s prospects and lay out what we think needs to take place for Intel to once again become top gun; and for us to gain more confidence. Watch the full video analysis. Episode 76 – Tech Spending Powers the Roaring 2020s as Cloud Remains a Staple of Growth – In the year 2020, it was good to be in tech. It was even better to be in the cloud as organizations had to rely on remote cloud services to keep things running. In this Breaking analysis Dave will provide our take on the latest ETR COVID survey and share why we think the tech boom will continue well into the future. Watch the full video analysis. Episode 75 – Breaking Analysis: Unpacking Oracle’s Autonomous Data Warehouse Announcement – On February 19th of this year, Barron’s dropped an article declaring Oracle a cloud giant and explained why the stock was a buy. Investors took notice and the stock ran up 18% over the next 9 trading days and peaked on March 9th, the day before the company announced its latest earnings. The company beat consensus earnings on both top line and EPS last quarter. But Investors didn’t like Oracle’s tepid guidance and the stock pulled back..but is still well above its pre-Barron’s article price. Watch the full video analysis. Episode 74 – Breaking Analysis: NFTs, Crypto Madness & Enterprise Blockchain – When a piece of digital art sells for $69.3M, more than has ever been paid for works by Paul Gauguin or Salvador Dali, making its creator the third most expensive living artist in the world, one can’t help but take notice and ask: “What is going on?” The latest craze around NFTs may feel a bit “bubblicious,” but it’s yet another sign that the digital age is now fully upon us. In this Breaking Analysis Dave wants to take a look at some of the trends that may have observers and investors scratching their heads, but we think still offer insight to the future — and possibly some opportunities for young investors. And we’ll briefly touch on how these trends may relate to enterprise tech. Watch the full video analysis. Episode 73 – Breaking Analysis: Satya Nadella Lays out a Vision for Microsoft at Ignite 2021 – Microsoft CEO Satya Nadella sees a different future for cloud computing over the coming decade. In this Breaking Analysis Dave Vellante will review the highlights of Nadella’s Ignite keynote, share our thoughts on what it means for the future of cloud specifically and tech generally. Watch the full video analysis. Episode 72 – Breaking Analysis: SaaS Attack, On Prem Survival & What’s a Cloud Company Look Like SaaS companies have been some of the strongest performers during this COVID era. In this Breaking Analysis, Dave Vellante picks out a few of the more recent themes from this month and share our thoughts on some major enterprise software players, the future of on-prem and a review of our take on cloud, what cloud will look like in the 2020s. Watch the full video analysis. Episode 71 – Breaking Analysis: RPA Remains on a Hot Streak as UiPath Blazes the Trail – UiPath’s recent $750M raise at a $35B valuation underscores investor enthusiasm for robotic process automation. In this Breaking Analysis Dave Vellante explores the current trends in the RPA market and try to address the question– is UiPath’s value supported by the ETR spending data, how will the RPA market evolve from a total available market (TAM) perspective and where do some of the other players like Automation Anywhere, Pegasystems and Blue Prism fit? Watch the full video analysis. Episode 70 – Breaking Analysis: How the SolarWinds Hack & COVID are Changing CISO Spending Patterns – The SolarWinds hack along with the pandemic are the two most visible catalysts for change in cybersecurity spending patterns. In addition to securing a more distributed workforce, CISOs have to now worry about protecting against the very software updates and patches designed to keep them safe against cyber attacks. In this Breaking Analysis, Dave Vellante shares data from a recent CISO roundtable hosted by ETR’s Erik Bradley and provides updates on the cybersecurity sector overall. Watch the full video analysis. Episode 69 – Breaking Analysis: Big 4 Cloud Revenue Poised to Surpass $115B in 2021 –There are four players in the IaaS/PaaS hyperscale cloud services space which have the ability to outperform all competitors. Combined in 2021, they will generate more than $115 billion dollars in revenue. In this Breaking Analysis, Dave Vellante initiates coverage of Alibaba, one of the Big Four in this massive market segment. Watch the full video analysis. Episode 68 – Breaking Analysis: Tech Spending Roars Back in 2021 –There is an expected six to seven percent increase in 2021 technology spending following the five percent decline over the past year. Many factors are contributing to this growth, and in this Breaking Analysis, Dave Vellante shares some of those reasons as well as the latest macro view of the market. Watch the full video analysis. Episode 67 – Breaking Analysis: Best of theCUBE on Cloud –The coming decade of cloud will be dramatically different from the last. There will be a shift toward a more data centric, hyper decentralized cloud that is far more complex than anything seen previously. In this Breaking Analysis, Dave Vellante summarizes exclusive content gathered from the recent theCUBE on cloud event. Watch the full video analysis. Episode 66 – Breaking Analysis: Pat Gelsinger Must Channel Andy Grove and Recreate Intel –Intel is fighting a war on two fronts: 1) Arm volumes have far surpassed those of Intel’s x86, conferring major cost advantages to leading fabs like TSMC and Samsung and 2) AMD continues to chip away at Intel’s dominance in its core markets. But the biggest challenge for incoming CEO Pat Gelsinger is perhaps to reinvent Intel by splitting manufacturing from design to make the company more agile and cost competitive. In this Breaking Analysis, Dave Vellante speculates about Intel’s future, and explains why Wikibon believes Intel has no choice but to shed its vertically integrated heritage. Episode 65 – Breaking Analysis: Breaking Analysis: 2021 Enterprise Technology Predictions –COVID-19 created a disruption in virtually all our expectations for 2020. In some regards, predictions for the past year played out very well, thanks to the pandemic. And in others, the complete opposite occurred. That being said, there is a lot to talk about heading into 2021. In this week’s Breaking Analysis, Dave Vellante is joined by Erik Bradley of ETR to share their top predictions for the upcoming year. Watch the full video analysis. Episode 64 – Breaking Analysis: Cloud Momentum & CIO Optimism Point to a 4% Rise in 2021 Tech Spending –Developments with COVID such as education, rapid vaccine rollout, productivity gains, and broad based cloud coverage suggest higher tech spending than previously forecasted for the upcoming year. Now, we can expect a 3-5 percent increase in 2021 spending. In this Breaking Analysis, Dave Vellante shares the data to support these predictions, and predicts which sectors are to gain momentum. Watch the full video analysis. Episode 63 – Breaking Analysis: Legacy Players Feel the Heat as AWS Storage Revenue Approaches $10B –Once an untapped bastion of innovation, storage in the data center now exists as a shell of what it used to be, and will remain as such. Specifically, AWS’ storage business is projected to hit between $6.5 – $7B this year and hit $10B within the next 18 – 24 months. In this Breaking Analysis, Dave Vellante lays out what this might mean for the industry, as well as the impact of AWS. Watch the full video analysis. Episode 62 – Breaking Analysis: Cloud 2030…From IT, to Business Transformation – Breaking Analysis: Cloud, Containers, AI & RPA Support Strong Rebound in 2021. Watch the full video analysis. Episode 61 – Breaking Analysis: Cloud 2030…From IT, to Business Transformation – Over the past decade, cloud computing has undoubtedly been the most pivotal force in IT. This brings the question as to what the next ten years will hold for cloud and the tech world. Perhaps, it will lay the foundations for a complete transformation of nearly every company worldwide. In this Breaking Analysis, as part of his coverage off AWS re:Invent 2020, Dave Vellante provides insights and predictions about the next breakthroughs in cloud. Watch the full video analysis. Episode 60 – Breaking Analysis: Sparked by COVID, CISOs see Permanent Shift in Cyber Strategies – CISOs report a forced shift to remote work has actually led to meaningful productivity improvements. This reality is causing security pros to rethink how they’ll approach security in the coming decade, informed by learnings during the pandemic. Watch the full video analysis. Episode 59 – Breaking Analysis: How Snowflake Plans to Change a Flawed Data Warehouse Model – Snowflake will not grow into its valuation by simply stealing share from the on-prem data warehouse vendors. Rather Snowflake must create an entirely new market based on completely changing the way organizations think about monetizing data. In this Breaking Analysis, Dave Vellante suggests a new data architecture that places domain knowledge at the core. Watch the full video analysis. Episode 58 – Breaking Analysis: Cloud Revenue Accelerates in the COVID Era – Over the past decade, cloud computing has undoubtedly been at the forefront of the innovation engine. The pandemic has accelerated the adoption of cloud and AI by at least two years, establishing a new era that will impact not only the technology industry, but all organizations. In this Breaking Analysis, Dave Vellante gives updates about the latest cloud market trends. Watch the full video analysis. Episode 57 – Breaking Analysis: Azure Cloud Powers microsoft’s Future – Big tech is once again under fire as CEOs of Facebook, Twitter, and Google face backlash from several US senators. Microsoft is not among these companies, as it relies on Azure cloud to build momentum, which now accounts for nineteen percent of its overall revenues. In this Breaking Analysis, Dave Vellante dives into the business of Microsoft and the data of its projected progress. Watch the full video analysis. Episode 56 – Breaking Analysis: Google’s Antitrust Play… Get Your Head out of Your Ads – The U.S. Department of Justice filed an antitrust lawsuit against Google, accusing the corporation of being a monopoly gatekeeper for the internet. In this Breaking Analysis, Dave Vellante shares data, covers the history of monopolistic power in the computer industry, and suggests future moves for Google to diversify its business. Watch the full video analysis. Episode 55 – Breaking Analysis: 2H 2020 Tech Spending: Headwinds into 2021 – Relative to 2019, tech spending has been hit hard, with a projected 5% decrease projected for 2020. Still, there seem to be bright spots within the market that show a slight increase in 2021 spending data. In this Breaking Analysis, Dave Vellante is joined by ETR’s Erik Bradley to provide the latest data supporting these trends. Watch the full video analysis. Episode 54 – CIOs Report Slow Thaw of Spending Freezes – Expect 2% Growth in 2021 – Recent Data provided by ETR suggests CIOs expect slight improvements in Q4 spending. Although these numbers are still down four percent from last year, this is a step in the right direction going into 2021. In this Breaking Analysis, Dave Vellante analyzes some of this data and provides his outlook for Q4 as well as the coming months. Watch the full video analysis. Episode 53 – Application Performance Management…From Tribal Knowledge to Digital Dashboard – Application Performance management has been around for a while, but it has had to evolve to accommodate for more complex operations, such as cloud-based systems. In this Breaking Analysis, Dave Vellante is teamed with Erik Bradley to offer the newest data to come out of the growing market. Watch the full video analysis. Episode 52 – Snowflake’s IPO… Here’s What’s Next – There is a lot of talk going on within the tech industry surrounding Snowflake’s recent IPO. In this week’s Breaking, Analysis, Dave Vellante shares his insights, investment strategies, and dives into some of the questions that have come out of the buzz from the hottest IPO in software history. Watch the full video analysis. Episode 51 – Market Recoil Puts Tech Investors at a Fork in the Road – Recently, the stock market experienced its most significant drop since early June. Tech companies were among several corporations involved in this decline, sending investors into a panic. In this Breaking Analysis, Dave Vellante answers questions and provides some perspective about what’s happening within the technology space and how it will continue to affect the rest of 2020. Watch the full video analysis. Episode 50 – Enterprise Software Download in the Summer of COVID – Enterprise applications are an enormous market, and organizations across the globe essentially rely on these applications to operate. In this Breaking Analysis, Dave Vellante unpacks new data surrounding the Enterprise software space, focusing on the core enterprise apps that companies rely on to keep their businesses running. Watch the full video analysis. Episode 49 – Tectonic Shifts Power Cloud, IAM and Endpoint Security – Although time has seemed to stop since the beginning of quarantine, COVID-19 has caused acceleration within the technology industry, causing some trends to speed up by about two years. The cybersecurity sector is one of the best examples of this change. In this Breaking Analysis, Dave Vellante discusses this sector, and provides updates on why key areas of the market that are exploding. Watch the full video analysis. Episode 48 – Cloud Remains Strong but not Immune to COVID – Although Cloud is among the most successful industries in tech spending, even it is not protected against COVID-19. Recent data shows a newly shaped recovery pattern suggesting this negative impact. In this Breaking Analysis, Dave Vellante dives into this data surrounding the cloud market and provides prospective updates about the big three. Watch the full video analysis. Episode 47 – RPA Competitors Eye Deeper Business Integration Agenda – Although the projected spending outlook for 2020 looks moderate, Robotic process automation solutions are still seeing the highest investment momentum for IT buyers. In this Breaking Analysis, Dave Vellante summarizes the latest RPA spending trends using data provided by ETR. Watch the full video analysis. Episode 46 – Five Questions Investors are Asking about Snowflake’s IPO – Snowflake recently filed a confidential document suggesting an IPO is imminent. Many within the community are responding positively to this news, causing a lot of discussion and inquiry. In this Breaking Analysis, Dave Vellante and Erik Bradley unpack five critical questions surrounding this pending IPO. Watch the full video analysis. Episode 45 – Google Cloud Rides the Wave but Remains a Distant Third Place – Despite its faster growth and infrastructure as a service, Google Cloud platform remains a third wheel behind AWS and Azure in the race for cloud dominance. In this Breaking Analysis, Dave Vellante reviews the current state of cloud and drills into the spending data to provide new insights about Google’s position in the market. Watch the full video analysis. Episode 44 – Living Digital: New Rules for Technology Events – Although there is push for a more digital world, in person interactions seem to be equally as important. Every year, large corporations throw massive events for this exact reason. However, coronavirus canceled most of these events for 2020, forcing a virtual replacement. In this Breaking Analysis, Dave Vellante covers the virtual event landscape and shares some takeaways from this new dynamic. Watch the full video analysis. Episode 43 – Assessing Dell’s Strategic Options with VMware – Dell is exploring options for its roughly 81 percent share in VMware. It is predicted that Dell wants to gauge investor, consumer, and partner sentiment. In this Breaking Analysis, Dave Vellante unpacks the complex angles as well as some possible scenarios of this situation. Watch the full video analysis. Episode 42 – Cyber Security Tailwinds in the Post Isolation Economy – The isolation economy has created substantial momentum for some cybersecurity companies. However, several others have tracked or not performed as well as more successful companies, despite still exhibiting strength and momentum. In this Breaking Analysis, Dave Vellante gives updates and answers questions about cybersecurity. Watch the full video analysis. Episode 41 – Competition Heats up for Cloud Analytic Databases – A new class of workloads are emerging in the cloud which are mainly focused on combining data using machine intelligence. At the center of this trend is a new class of data stores and analytic databases. In this Breaking Analysis, Dave Vellante updates his view on the subject while looking into the basics of the market, the competition, as well as spending data. Watch the full video analysis. Episode 40 – Most CIOs Expect a U Shaped COVID Recovery – It has been reported COVID-19 created a bifurcated IT spending picture, but what will the aftermath of the virus look like? In this Breaking Analysis, Dave Vellante is joined by Sagar Kadakia to look into the recovery patterns of different industries following the effects of the pandemic, and discuss the supporting data. Watch the full video analysis. Episode 39 – RPA Gains Momentum in the Post COVID Era – Legacy on-premises infrastructure are now allowing for more flexible approaches to business agility that reduce human labor, and the pandemic has accelerated this focus on such efforts. Robotic Process Automation has been a large beneficiary in this process. In this Breaking Analysis, Dave Vellante gives the rundown of RPA, including updates on the RPA sector, spending data, and the impact of COVID-19 on the market. Watch the full video analysis. Episode 38 – Cloud Momentum Building for the Post COVID Era – Cloud is in the stronghold of on-premise computing, and coronavirus has helped to strengthen this position. Analysis of company earnings reports and customer survey data shows that Microsoft Azure and GCP are closing the gap on AWS’ cloud dominance. In this Breaking Analysis, Dave Vellante takes a closer look at the big three cloud players, and provides a brief investigation of AWS individually. Watch the full video analysis. Episode 37 – IBM’s Future Rests on its Innovation Agenda – For decades, IBM has unfortunately missed opportunities to invest in the waves that now power the tech economy. The hiring of a new CEO provides the chance to redirect the company, and come back on top. In this Breaking Analysis, Dave Vellante digs into the past and the future of IBM. Watch the full video analysis. Episode 36 – COVID-19 Takeaways & Sector Drilldowns Part II – Industries such as retail, consumer, telco and IT services are seeing the largest pullbacks in spend from consumers and business since the beginning of COVID-19. On the other hand, corporations capable of digital transformation are seeing the most success. In this Breaking Analysis, Dave Vellante and Sagar Kadakia share the most updated spending data and information about the effects of the pandemic. Watch the full video analysis. Episode 35 – CIOs & CISOs Discuss COVID 19 Budget Impact – CEOs and CISOs of industries that have been hard-hit see significant and many permanent shifts to their IT and security strategies. Because of severe budget impacts, certain initiatives have been prioritized. In this Breaking Analysis, Dave Vellante is joined by Erik Bradley, managing director of ETR’S VENN program, to provide research and discuss the areas emphasized by these executives. Watch the full video analysis. Episode 34 – How Tech Execs are Responding to COVID 19 – COVID-19 demanded several and somewhat immediate changes within the tech industry. In this Breaking Analysis, Dave Vellante shares commentary and responses from various tech execs, recaps the current IT spending outlook, and dives into what’s really going on in the marketplace. Watch the full video analysis. Episode 33 – CIOs Plan on 4% Budget Declines for 2020 – At the start of 2020, the IT spend forecast was plus 4 percent. Following Coronavirus, those numbers declined significantly. In this breaking Analysis, Dave Vellante and Sagar Kadada breakdown the latest spending data from ETR. Watch the full video analysis. Episode 32 – VMware Announces vSphere 7 – VMware released the vSphere 7, which is being called the biggest change to vSphere within the last decade, enabling 90 percent of the data centers around the world that have VMware. In this Breaking Analysis, Dave Vellante is joined by Stu Miniman to discuss the vSphere 7 announcement. Watch the full video analysis. Episode 31 – Coronavirus – Pivoting From Physical to Digital Events – Coronavirus and the recent quarantine has put the world more or less on pause. For many industries, this means switching to a largely digital-based platform for events. In light of the pandemic, Dave Vellante shares advice on the best tools and practices to navigate the current crisis. Watch the full video analysis. Episode 30 – Multi-Cloud…A Symptom Or Cure? – The third wave of cloud is stirring up a lot of discussion about its necessity and effectiveness. In this breaking Analysis, Dave Vellante digs into the multicloud arena while answering some frequently asked questions about the benefits and possible implications of this new technology. Watch the full video analysis. Episode 29 – Cyber Security Update: What to Expect at RSA 2020 – Robert Gates, Former director of the CIA and Secretary of Defense, warns that the risks of Cyber security and IT should be a regular part of every board’s agenda. In this Breaking Analysis, Dave Vellante provides updates about the cyber security sector ahead of the RSA conference. Watch the full video analysis. Episode 28 – RPA: Over-Hyped or the Next Big Thing? – Robotic Process Automation, or RPA, is one of the hottest sectors in software today with a small but rapidly growing market. In this Breaking Analysis, Dave Vellante dives deeper into the world of RPA and talks about the value, size, and competitors in the market space. Watch the full video analysis. Episode 27 – Gearing up for Cloud 2020 – The new era of cloud brings significant change to the industry as well as new opportunities for the three major cloud players in the U.S. In this Breaking Analysis, Dave Vellante looks deeper into the cloud market and the momentum of Amazon, Google and Microsoft. Watch the full video analysis. Episode 26 – Storage…Continued Softness with Some Bright Spots – The storage industry is a bifurcated market: Secondary storage is gaining momentum while the primary slide falls behind. In this Breaking Analysis, Dave Vellante looks into the spending data and discusses his thoughts and predictions about storage live from Barcelona. Watch the full video analysis. Episode 25 – Cisco: Navigating Cloud, Software & Workforce Change – At the end of the dot com bubble, Cisco was the most valuable company in the world. It remains a leader in key segments, but is refocusing its business for the next decade. In this Breaking Analysis, Dave Vellante covers Cisco’s rise as well as projections for the future. Watch the full video analysis. Episode 24 – The Trillionaires Club: Powering the Tech Economy – Big tech companies have changed the recipe for innovation in the Enterprise, and as we enter the next decade, it is important to reevaluate how that will determine the level of success in the industry. In this Breaking Analysis, Dave Vellante discusses this “cocktail” of innovation and how it came into play. Watch the full video analysis. Episode 23 – Veeam’s $5B Exit: Clarity & Questions Around “Act II” – Veeam is a data protection company that has seen a slight performance drop since 2018. In this Breaking Analysis, Dave Vellante provides details about its $5 billion deal with Insight Partners and how this new chapter will affect the industry moving forward. Watch the full video analysis. Episode 22 – Predictions 2020: Cloud, Kubernetes & Cyber Continue to Power the Tech Economy – Tech projects have historically been very risky investments, but changes with cloud are allowing for more flexibility in the coming year. In this Breaking Analysis, Dave Vellante talks about Predictions for 2020 using spending data and insight from the thousands of interviews conducted on theCUBE. Watch the full video analysis. Episode 21 – Re:Invent 2019…of Transformation & NextGen Cloud – During the most recent AWS re:invent, the company proves it continues to strive on raising the bar. In this Breaking Analysis, Dave Vellante is joined by Stu Miniman to unpack the event, and talk about what’s happening from a buyer’s perspective, as well as AWS’ hybrid strategy Watch the full video analysis. Episode 20 – Unpacking Cisco’s Prospects Q4 2019 and Beyond – AWS strongly emphasizes the idea of transformation, and warns industries like Cisco not to do so incrementally. In this Breaking Analysis, Dave Vellante covers six different topics related to the future of Cisco, and its prospects in this era of next generation cloud Watch the full video analysis. Episode 19 – Examining IT Spending Data Q4 ‘19 – Enterprise Research Technology is a company who uses primary market research and first party data to look into spending patterns within the tech industry. In this Breaking Analysis, Dave Vellante explains the ins and outs of ETR, as well as its current relationship with theCUBE. Watch the full video analysis. Episode 18 – Re:Invent 2019: AWS Gears up for Cloud 2.0 – AWS Reinvent has become the Super Bowl for enterprise tech innovation. In this Breaking Analysis, Dave Vellante discusses the impact of the revolution of cloud on the industry in light of the upcoming event. Watch the full video analysis. Episode 17 – The Transformation of Dell Technologies – Dell continues to make changes to remain a predominant company in the tech industry. In this Breaking Analysis, Dave Vellante breaks down the major takeaways of the Dell Technologies’ industry analyst event and discusses some of the possible implications that the company may face. Watch the full video analysis. Episode 16 – The State of Cybersecurity Q4 2019 – The cyber security market is fragmented, challenging and many feel broken. Cloud security promises to simplify the maze for security practitioners but there are nuances with the shared responsibility model that often cause confusion. In this Breaking Analysis we look at the lates ETR data and hear from CISOs and executives in the field with an outlook and prognosis going forward. Watch the full video analysis. Episode 15 – The state of data protection, Q4 2019 – While demand for primary storage remains soft, the bright spot in the sector is data protection. Well funded new entrants are disrupting the space which is shaping up as a battleground for 2020. Watch the full video analysis. Episode 14 – AWS growth slows but remains the profit engine of Amazon – While AWS’ growth rate slowed this past quarter, its revenue is still substantially larger than its nearest competitors in the IaaS space. Moreover AWS is still the profit engine that funds Amazon’s vast and growing empire. Watch the full video analysis. Episode 13 – Q4 Spending Outlook – 10/18/19 – TheCUBE host Dave Vellante shares his analysis on recent spending trends backed by ETR Data. Spending is reverting to pre-2019 levels but the outlook still points to a strong 2020, barring any unforeseen global surprises. Watch the full video analysis. Episode 12 – Bill McDermott steps down from SAP – Commentary and Outlook. SAP pre-announced earnings with a beat and a raise, which acted as a heat shield for the surprise news that long-time CEO Bill McDermott is not renewing his contract. SAP is moving back to a dual-CEO model with a separate customer-facing and product/ops focus for each exec. SAP is strong financially but we believe faces significant technical integration challenges over the next decade, which may have played into McDermott’s and SAP’s decisions. In this Breaking Analysis, Dave Vellante shares recent spending data from ETR and lays out some of the challenges SAP faces going forward. Watch the full video analysis. Episode 11 – Spending in Q4 2019 is reverting back to pre-2018 levels – The spending outlook for the balance of 2019, into 2020 is softening, but not falling off a cliff. In this Breaking Analysis, Dave Vellante presents the latest ETR spending data and shares the latest thinking on which segments will continue to do well for the balance of 2019 into next year. Watch the full video analysis. Episode 10 – Takeaways from Dell’s 2019 financial analysts event. – Dell Technologies executives gathered in New York to update financial analysts and present the company’s mid-to-long term plans for growth, share gains, profitability and paying down its substantial debt. In this Breaking Analysis, Dave Vellante unpacks Dell’s massive business and provides clarity on the profitability levers Dell is turning to continue its transformation. Watch the full video analysis. Episode 9 – Spotlight on IBM’s Systems Business – IBM’s mainframe business continues to be the linchpin of much of the company’s profit and free cash flow. In this Breaking Analysis, Dave Vellante explains the importance of product cycles to the success of not only IBM’s Systems and Storage division, but IBM’s financial performance overall. Watch the full video analysis. Episode 8 – Nutanix and VMware battle for HCI leadership – Hyperconverged infrastructure was popularized by leader Nutanix. Many others have joined the party including VMware, Dell, HPE and others. In this Breaking Analysis, Dave Vellante is joined by Stuart Miniman, an expert in the HCI market, to unpack what’s really happening in the marketplace. Watch the full video analysis. Episode 7 – Oracle earnings analysis – September 2019 – Oracle, like many legacy enterprise software companies, is seeing a slowdown in growth for on-prem licenses. Oracle’s cloud is being re-factored in a next generation offering that comprises IaaS, PaaS and SaaS. Oracle’s applications business remains strong and is a driver of profits. In this Breaking Analysis, Dave Vellante digs into Oracle’s business and lays out his expectation for the coming quarters. Watch the full video analysis. Episode 6 – Spending data from ETR shows that robotic process automation is gaining steam in mid-to-large enterprises – A race to improve productivity is driving companies to implement automation in the form of software robots. UiPath and Automation Anywhere show strong customer spending momentum. Blue Prism and other major players, while not showing the same growth, appear to be well-positioned. In this Breaking Analysis, Dave Vellante explains how this market is beginning to re-shape automation for the future. Watch the full video analysis. Episode 5 – Spending data shows that cloud native databases are disrupting traditional analytic data stores. Snowflake and AWS RedShift stand out as having spending momentum based on ETR survey data – Some cloud native databases have been architected to enable storage and compute resources to be scaled independently. This not only improves economics but also drives increased agility and flexibility for many use cases. In this Breaking Analysis, Dave Vellante explains how this dynamic is eating into traditional enterprise data warehouse markets. Watch the full video analysis. Episode 4 – Storage Spending Outlook 2H ’19 – Pure Leads the Pack – The on-prem storage business has been hurt by: 1) the cloud siphoning away demand; and 2) a massive injection of flash storage that has given data center managers enough performance headroom to minimize the need to buy for performance reasons. Pure Storage is growing faster than the marketshare leaders but from a much smaller installed base. Watch the full video analysis. Episode 3 – VMworld 2019 – Containers won’t Kill VMware – As a preview to VMworld 2019, Dave Vellante shares his opinions along with ETR spending data that shows containers, to date, are not hurting VMware’s business. Watch the full video analysis. Episode 2 – IBM Completes Acquisition of Red Hat – Dave Vellante shares his opinions along with ETR spending data on this giant move by IBM. Positioned by IBM as all about cloud, Vellante says it’s also a professional services play. Watch the video analysis. Episode 1 – Hello World – This is a podcast only version explaining what this series is all about and its objectives for the community. THANK YOU Salesforce reportedly renews acquisition talks with Informatica Oracle reported to buy 400,000 Nvidia chips for first Stargate data center Trump threatens new tariffs on Apple, Samsung and the EU Google and Microsoft heat up the AI race as OpenAI opens new front with big acquisition PiLogic raises $4M to build precision AI models for space applications Despite strong earnings beat, Workday's stock wobbles on disappointing guidance Salesforce reportedly renews acquisition talks with Informatica CLOUD - BY MARIA DEUTSCHER . 20 HOURS AGO Oracle reported to buy 400,000 Nvidia chips for first Stargate data center INFRA - BY MARIA DEUTSCHER . 22 HOURS AGO Trump threatens new tariffs on Apple, Samsung and the EU POLICY - BY MARIA DEUTSCHER . 1 DAY AGO Google and Microsoft heat up the AI race as OpenAI opens new front with big acquisition AI - BY ROBERT HOF . 1 DAY AGO PiLogic raises $4M to build precision AI models for space applications AI - BY KYT DOTSON . 1 DAY AGO Despite strong earnings beat, Workday's stock wobbles on disappointing guidance CLOUD - BY MIKE WHEATLEY . 2 DAYS AGO
--------------------------------------------------

Title: Pinterest's ARPU Doubles Since 2018: A Deep Dive into Monetization Growth
URL: https://finance.yahoo.com/news/pinterests-arpu-doubles-since-2018-071250962.html
Time Published: 2025-05-23T07:12:50Z
Description: Further improvements in Pinterest's ARPU are likely as the company continues to invest in its ad monetization and user engagement algorithms.
--------------------------------------------------

Title: How Does Claude 4 Think? – Sholto Douglas and Trenton Bricken
URL: https://www.dwarkesh.com/p/sholto-trenton-2
Time Published: 2025-05-23T06:08:12Z
Full Content:
New episode with my good friends Sholto Douglas & Trenton Bricken. Sholto focuses on scaling RL and Trenton researches mechanistic interpretability, both at Anthropic. We talk through what’s changed in the last year of AI research; the new RL regime and how far it can scale; how to trace a model’s thoughts; and how countries, workers, and students should prepare for AGI. See you next year for v3. Here’s last year’s episode, btw. Enjoy! Watch on YouTube; listen on Apple Podcasts or Spotify. WorkOS ensures that AI companies like OpenAI and Anthropic don't have to spend engineering time building enterprise features like access controls or SSO. It’s not that they don't need these features; it's just that WorkOS gives them battle-tested APIs that they can use for auth, provisioning, and more. Start building today at workos.com. Scale is building the infrastructure for safer, smarter AI. Scale’s Data Foundry gives major AI labs access to high-quality data to fuel post-training, while their public leaderboards help assess model capabilities. They also just released Scale Evaluation, a new tool that diagnoses model limitations. If you’re an AI researcher or engineer, learn how Scale can help you push the frontier at scale.com/dwarkesh. Lighthouse is THE fastest immigration solution for the technology industry. They specialize in expert visas like the O-1A and EB-1A, and they’ve already helped companies like Cursor, Notion, and Replit navigate U.S. immigration. Explore which visa is right for you at lighthousehq.com/ref/Dwarkesh. To sponsor a future episode, visit dwarkesh.com/advertise. (00:00:00) – How far can RL scale? (00:16:27) – Is continual learning a key bottleneck? (00:31:59) – Model self-awareness (00:50:32) – Taste and slop (01:00:51) – How soon to fully autonomous agents? (01:15:17) – Neuralese (01:18:55) – Inference compute will bottleneck AGI (01:23:01) – DeepSeek algorithmic improvements (01:37:42) – Why are LLMs ‘baby AGI’ but not AlphaZero? (01:45:38) – Mech interp (01:56:15) – How countries should prepare for AGI (02:10:26) – Automating white collar work (02:15:35) – Advice for students Dwarkesh Patel Okay. I'm joined again by my friends, Sholto Bricken... Wait, fuck. Did I do this last time? Sholto Douglas You did the same thing. Trenton Bricken No, no, you named us differently, but we didn't have Sholto Bricken and Trenton Douglas. Sholto Douglas You swapped us. Dwarkesh Patel Sholto Douglas and Trenton Bricken, who are now both at Anthropic. Trenton Bricken Yeah. Let's go. Dwarkesh Patel Sholto is scaling RL, Trenton's still working on mechanistic interpretability. Welcome back. Sholto Douglas Happy to be here. Trenton Bricken Yeah, it's fun. Dwarkesh Patel What's changed since last year? We talked basically this month in 2024. Sholto Douglas Yep. Dwarkesh Patel Now, we're in 2025. What's happened? Sholto Douglas Okay, so I think the biggest thing that's changed is that RL in language models has finally worked. We finally have proof of an algorithm that can give us expert human reliability and performance, given the right feedback loop. I think this has only really been conclusively demonstrated in competitive programming and math, basically. Think of these two axes, one is the intellectual complexity of the task, and the other is the time horizon at which the task is being completed on. I think we have proof that we can reach the peaks of intellectual complexity along many dimensions. We haven't yet demonstrated long-running agentic performance. You're seeing the first stumbling steps of that now, and should see much more conclusive evidence of that basically by the end of the year, with real software engineering agents doing real work. I think Trenton, you're experimenting with this at the moment? Trenton Bricken Yeah, absolutely. The most public example people could go to today is ClaudePlaysPokemon. Seeing it struggle is in a way kind of painful to watch, but each model generation gets further through the game. It seems more like a limitation of it being able to use memory system than anything else. Dwarkesh Patel I wish we had recorded predictions last year. We definitely should this year. Trenton Bricken Hold us accountable. Dwarkesh Patel That's right. Would you have said that agents would be only this powerful as of last year? Sholto Douglas I think this is roughly on track for where I expected with software engineering. I think I expected them to be a little bit better at computer use. But I understand all the reasons for why that is, and I think that's well on track to be solved. It's just a sort of temporary lapse. Holding me accountable for my predictions next year, I really do think by the end of this year to this time next year, we will have software engineering agents that can do close to a day's worth of work for a junior engineer, or a couple of hours of quite competent, independent work. Trenton Bricken Yeah, that seems right to me. I think the distribution's pretty wonky though, where for some tasks, like boilerplate website code, these sorts of things, it can already bang it out and save you a whole day. Sholto Douglas Yeah, exactly. Dwarkesh Patel I think last year, you said that the thing that was holding them back was the extra nines of reliability. I don't know if that's the way you would still describe the way in which these software agents aren't able to do a full day of work, but are able to help you out with a couple minutes. Is it the extra nines that's really stopping you or is it something else? Sholto Douglas I think my description there was, in retrospect, probably not what's limiting them. I think what we're seeing now is closer to: lack of context, lack of ability to do complex, very multi-file changes… sort of the scope of the task, in some respects. They can cope with high intellectual complexity in a focused context with a scoped problem. When something's a bit more amorphous or requires a lot of discovery and iteration with the environment, with this kind of stuff they struggle more. Maybe the way I would define the thing that's holding them back like this. If you can give it a good feedback loop for the thing that you want it to do, then it's pretty good at it. If you can't, then they struggle a bit. Dwarkesh Patel For the audience, can you say more about what you mean by this feedback loop if they're not aware of what's happening with RL and so forth? Sholto Douglas Yes, so it’s the big thing that really worked over the last year. Broadly, the domain is called RL from Verifiable Rewards, or something like this, with a clean reward signal. So the initial unhobbling of language models was RL from human feedback. Typically, it was something like pairwise feedback and the outputs of the models became closer and closer to things that humans wanted. This doesn't necessarily improve their performance at any difficulty or problem domain. Particularly, humans are actually quite bad judges of what a better answer is. Humans have things like length biases and so forth. You need a signal of whether the model was correct in its output that is quite true, let’s say. Things like the correct answer to a math problem, or passing unit tests. These are the examples of a reward signal that's very clean. Even these can be hacked, by the way. Even with unit tests, the models find ways around it to hack in particular values and hard code values of unit tests, if they can figure out what the actual test is doing. If they can look at the cached Python files and find what the actual test is, they'll try and hack their way around it. These aren't perfect, but they're much closer. Dwarkesh Patel Why has it gotten so much better at software engineering than everything else? Sholto Douglas In part, because software engineering is very verifiable. It's a domain which just naturally lends itself to this way. Trenton Bricken Does the code pass the test? Does it even run? Does it compile? Sholto Douglas Yeah, does it compile? Does it pass the test? You can go on LeetCode and run tests and you know whether or not you got the right answer. There isn't the same kind of thing for writing a great essay. That requires... The question of taste in that regard is quite hard. We discussed the other night at dinner, the Pulitzer Prize. Which would come first, a Pulitzer Prize winning novel or a Nobel Prize or something like this? I actually think a Nobel Prize is more likely than a Pulitzer Prize-winning novel in some respects. Because a lot of the tasks required in winning a Nobel Prize—or at least strongly assisting in helping to win a Nobel Prize—have more layers of verifiability built up. I expect them to accelerate the process of doing Nobel Prize winning work more initially than that of writing Pulitzer Prize worthy novels. Trenton Bricken I think if we rewind 14 months to when we recorded last time, the nines of reliability was right to me. We didn't have Claude Code, we didn't have Deep Research. All we did was use agents in a chatbot format. Sholto Douglas Right. Copy paste, copy paste, copy paste. Trenton Bricken Totally. We're very used to chat interfaces, whether we're texting or using Google. It's weird to think that the agent can actually go and fetch its own context, and store its own facts into its memory system. I still think that it's the nines of reliability. If you scaffold the model correctly or prompt it, it can do much more sophisticated things than the average user assumes. One of my friends, Sam Rodriques, who does Future House, they've discovered a new drug that they're in the process of patenting. By the time this episode comes out that will be live. Dwarkesh Patel LSD v2? Sholto Douglas Wait, is it really? Trenton Bricken No, they're not making LSD. But people didn't think that models could be creative or do new science. It does just seem like a skill issue. Dwarkesh Patel Wait, it discovered a drug? How did it? Did it one-shot the molecules? Trenton Bricken This was just over a conversation. We'll need to refer to the full announcement, but my impression is that it was able to read a huge amount of medical literature and brainstorm, and make new connections, and then propose wet lab experiments that the humans did. Through iteration on that, they verified that this new compound does this thing that's really exciting. Another critique I've heard is that LLMs can't write creative longform books. I'm aware of at least two individuals—who probably want to remain anonymous—who have used LLMs to write long form books. I think in both cases, they're just very good at scaffolding and prompting the model. Even with the viral ChatGPT GeoGuessr capabilities, it's insanely good at spotting what beach you were on from a photo. Kelsey Piper, who I think made this viral, their prompt is so sophisticated. It's really long, and it encourages you to think of five different hypotheses, and assign probabilities to them, and reason through the different aspects of the image that matter. I haven't A/B tested it, but I think unless you really encourage the model to be this thoughtful, you wouldn't get the level of performance that you see with that ability. Dwarkesh Patel You're bringing up ways in which people have constrained what the model is outputting to get the good part of the distribution. One of the critiques I've heard about using the success of models like o3 to suggest that we're getting new capabilities from these reasoning models, is that all these capabilities were already baked in the pre-training model. I think there's a paper from Tsinghua University, where they showed that if you give a base model enough tries to answer a question, it can still answer the question as well as the reasoning model. It basically just has a lower probability of answering correctly. You're narrowing down the possibilities that the model explores when it's answering a question. Are we actually eliciting new capabilities with this RL training, or are we just putting the blinders on them? Sholto Douglas Right, like carving away the marbles on this. I think it's worth noting that that paper was, I'm pretty sure, on the Llama and Qwen models. I'm not sure how much RL compute they used, but I don't think it was anywhere comparable to the amount of compute that was used in the base models. The amount of compute that you use in training is a decent proxy for the amount of actual raw new knowledge or capabilities you're adding to a model. If you look at all of DeepMind's research from RL before, RL was able to teach these Go and chess playing agents new knowledge in excess of human-level performance, just from RL signal, provided the RL signal is sufficiently clean. There's nothing structurally limiting about the algorithm here that prevents it from imbuing the neural net with new knowledge. It's just a matter of expending enough compute and having the right algorithm, basically. Dwarkesh Patel Why aren't you already spending more compute on this? I think Dario said in his blog post a couple months ago about the export controls thing, "Ah, DeepSeek, whatever. We're only spending $1 million on RL," or something. “We aren't in the compute limited regime for RL yet, but we will be soon." You're spending hundreds of millions on the base model. Why only order a million on the RL? Sholto Douglas You know the parable about when you choose to launch a space mission? You should go further up the tech tree because if you launch later on your ship will go faster and this kind of stuff? I think it's quite similar to that. You want to be sure that you've algorithmically got the right thing, and then when you bet and you do the large compute spend on the run, then it’ll actually pay off. They'll have the right compute efficiencies and this kind of stuff. I think RL is slightly different to pre-training in this regard. RL can be a more iterative thing. You're progressively adding capabilities to the base model. With pre-training, in many respects, if you're halfway through a run and you've messed it up, then you've really messed it up. I think that's the main reason why. People are still figuring out exactly what they want it to do. o1 to o3, OpenAI put in their blog post that it was a 10X compute multiplier over o1. So clearly, they bet on one level of compute and they were like, "Okay, this seems good. Let's actually release it. Let's get it out there." Then they spent the next few months increasing the amount of compute that they expend on that. Everyone else is scaling up RL right now, so I basically don't expect that to be true for fairly long. Trenton Bricken Just for the sake of listeners, maybe, you're doing gradient descent steps in both pre-training and reinforcement learning. It's just the signal's different. Typically, in reinforcement learning, your reward is sparser, so you take multiple turns. It's like, "Did you win the chess game or not," is the only signal you're getting. Often you can't compute gradients through discrete actions. So you end up losing a lot of gradient signal. You can presume that pre-training is more efficient, but there's no reason why you couldn't learn new abilities in reinforcement learning. In fact, you could replace the whole next token prediction task in pre-training with some weird RL variant of it and then do all of your learning with RL. Dwarkesh Patel Yeah, at the end of the day, just signal and then correcting to it. Trenton Bricken Totally. Then going back to the paper you mentioned, aside from the caveats that Sholto brings up, which I think is the first order, most important, I think zeroing in on the probability space of meaningful actions comes back to the nines of reliability. Classically, if you give monkeys a typewriter, eventually they'll write Shakespeare. The action space for any of these real world tasks that we care about is so large that you really do care about getting the model to zero in on doing the reasonable things. Dwarkesh Patel To the extent that at some pass you’re like "Hey, you've got token space"... Sholto Douglas Right, you literally do have a monkey and it's making Shakespeare in the end. Dwarkesh Patel The chess analogy is interesting. Sorry, were you about to say something? Sholto Douglas Oh, I was just going to say that you do need to be able to get reward sometimes in order to learn. That's the complexity in some respects. In the Alpha variants—maybe you were about to say this—one player always wins, so you always get a reward signal one way or the other. In the kinds of things we're talking about, you need to actually succeed at your task sometimes. Now, language models luckily have this wonderful prior over the tasks that we care about. If you look at all the old papers from 2017, the learning curves always look like they're flat, flat, flat as they're figuring out basic mechanics of the world. Then there's this spike up as they learn to exploit the easy rewards. Then it's almost like a sigmoid in some respects. Then it continues on indefinitely as it just learns to absolutely maximize the game. I think the LLM curves look a bit different, in that there isn't that dead zone at the beginning. They already know how to solve some of the basic tasks. You get this initial spike. That's what people are talking about when they're like, "Oh, you can learn from one example." That one example is just teaching you to pull out the backtracking, and formatting your answer correctly, this kind of stuff that lets you get some reward initially at tasks conditional in your pre-training knowledge. The rest is probably you learning normal stuff. Dwarkesh Patel That's really interesting. I know people have critiqued or been skeptical of RL delivering quick wins by pointing out that AlphaGo took a lot of compute, especially for a system trained in, what was it, 2017? Sholto Douglas Yeah, it's off the curve. Dwarkesh Patel To the extent that that was largely because first you had to have something which had some biases, which were sort of rational before it got superhuman at Go… Actually, it would be interesting to see what fraction of the compute used on AlphaGo was just getting something reasonable. Sholto Douglas Yeah, it would be interesting. Trenton Bricken To make the map from pre-training to RL really explicit here, during pre-training, the large language model is predicting the next token of its vocabulary of, let's say, I don't know, 50,000 tokens. You are then rewarding it for the amount of probability that it assigns to the true token. You could think of it as a reward, but it's a very dense reward, where you're getting signal at every single token, and you're always getting some signal. Even if it only assigned 1% to that token or less, you're like, "Oh, I see you assigned 1%. Good job. Keep doing that." Sholto Douglas Yeah, upweight it. Trenton Bricken Yeah, exactly. Sholto Douglas It's like a tug in the gradient. Trenton Bricken That's right. Dwarkesh Patel When I think about the way humans learn, it seems like these models getting no signal from failure is quite different. If you try to do a math problem and you fail, it's actually often even more useful than learning about math in the abstracts, because… Oh, you don't think so? Trenton Bricken Only if you get feedback. Sholto Douglas Yeah, only if you get feedback. Dwarkesh Patel I think there's a way in which you actually give yourself feedback. You fail and you notice where you failed. Trenton Bricken Only if you get feedback, I think, at times. Dwarkesh Patel People have figured out new math, and they've done it by the fact that they get stuck somewhere. They're like, "Why am I getting stuck here? Let me think through this." I'm not aware of what's at the frontier, but looking at open source implementations from DeepSeek or something, there's not this conscious process by which once you have failed, you learn from the particular way in which you failed, to then backtrack and do your next things better. Just pure gradient descent, I wonder if that's a big limitation. Trenton Bricken I don't know. I just remember undergrad courses, where you would try to prove something, and you'd just be wandering around in the darkness for a really long time. Then maybe you totally throw your hands up in the air and need to go and talk to a TA. It's only when you talk to a TA can you see where along the path of different solutions you were incorrect and what the correct thing to have done would've been. That's in the case where you know what the final answer is, right? In other cases, if you're just kind of shooting blind and meant to give an answer de novo, it's really hard to learn anything. Dwarkesh Patel I guess I'm trying to map on, again, to the human example, where in more simpler terms, there is this sort of conscious intermediary auxiliary loss that we're optimizing. It's a very sort of self-conscious process. Forget about math. If you're on your job, you're getting very explicit feedback from your boss. That's not necessarily how the task should be done differently, but a high-level explanation of what you did wrong, which you update on not in the way that pre-training updates weights, but more in the… I don’t know. Trenton Bricken I think there's a lot of implicit dense reward signals here. Sholto Douglas Yeah, exactly. Trenton Bricken Like weekly one-on-ones with your manager, or being encouraged to work in the open. Even with homework assignments, they're so scaffolded. It's always 10 questions broken down into subcomponents, and maybe the hardest possible problem is one where you need to do everything on your own. Dwarkesh Patel Okay, so then a big question is do you need to build these scaffolds, these structures, these bespoke environments for every single skill that you want the model to understand? Then it's going to be a decade of grinding through these sub-skills? Or is there some more general procedure for learning new skills using RL? Sholto Douglas It's an efficiency question there. Obviously, if you could give a dense reward for every token, if you had a supervised example, then that's one of the best things you could have. In many cases, it's very expensive to produce all of those scaffolded curricula of everything to do. Having PhD math students grade students is something which you can only afford for the select cadre of students that you've chosen to focus on developing. You couldn't do that for all the language models in the world. First step is that obviously, that would be better. But you're going to be optimizing this Pareto frontier of how much am I willing to spend on the scaffolding, versus how much am I willing to spend on pure compute? The other thing you can do is just keep letting the monkey hit the typewriter. If you have a good enough end reward, then eventually, it will find its way. I can't really talk about where exactly people sit on that scaffold. I think different people, different tasks are on different points there. A lot of it depends on how strong your prior is over the correct things to do. But that's the equation you're optimizing. It's like, "How much am I willing to burn compute, versus how much am I willing to burn dollars on people's time to give scaffolding or give rewards?" Dwarkesh Patel Interesting. You say we're not willing to do this for LLMs, but we are for people. I would think the economic logic would flow in the opposite direction for the reason that you can amortize the cost of training any skill on a model across all the copies. Sholto Douglas We are willing to do this for LLMs to some degree. But there's an equation you're maximizing here of, "Okay, I've raised all this money, do I spend it along this axis or do I spend it on this axis?" Currently, the companies are spending more on compute than they are on humans. Otherwise, Scale AI's revenue would be like $10 billion. Look at it, NVIDIA's revenue is much higher than Scale AI's revenue. Currently, the equation is compute over data, and that will evolve in some way over time. Dwarkesh Patel Yeah, interesting. I am curious how it evolves. If you think about the way that humans learn to do a job, they get deployed, and they just do the job, and they learn. Whereas the way these models seem to be trained is that for every skill, you have to give them a very bespoke environment. If they were trained the way humans are trained, then... Sholto Douglas On the job. Dwarkesh Patel Yeah, exactly. Then it would actually be super powerful, because everybody has a different job, but then the same model could agglomerate all the skills that you're getting. I don't know, I've been doing the podcast for the last few years. I'm becoming a better podcaster. You have a slightly more valuable skill of doing AI research. Trenton Bricken I don't know about that. Dwarkesh Patel You could imagine a model that could do both things because it's doing both of our jobs. Copies of the model are doing both jobs. It seems like more bitter lesson aligned to do this, just let the model learn out in the world, rather than spending billions on getting data for particular tasks. Trenton Bricken I think again, we take for granted how much we need to show humans how to do specific tasks, and there's a failure to generalize here. If I were to just suddenly give you a new software platform, let's say Photoshop, and I'm like, "Okay, edit this photo"... If you've never used Photoshop before, it'd be really hard to navigate. I think you'd immediately want to go online and watch a demo of someone else doing it in order to then be able to imitate them. Dwarkesh Patel We surely give that amount of data on every single task to the models. Trenton Bricken Okay. This is the first thing. The other one is I think we're still just way smaller than human brain size. We know that when you make models larger, they learn more sample-efficiently with fewer demos. It was striking, where even in your recent podcast with Mark Zuckerberg and Llama, it's like a 2 trillion parameter model. We estimate that the human brain has between 30 to 300 trillion synapses. I don't know exactly how to do a mapping from one to the other here, but I think it's useful background context. I think it's quite likely we're still smaller than the human brain. Even with the 4.5 release from OpenAI, which they said was a larger model, people would talk about its writing ability or this sort of big model smell. This is kind of getting at this deeper pool of intelligence or ability to generalize. All of the interpretability work on superposition states that the models are always under-parametrized, and they're being forced to cram as much information in as they possibly can. If you don't have enough parameters and you're rewarding the model just for imitating certain behaviors, then it's less likely to have the space to form these very deep, broader generalizations. Sholto Douglas The language result is really cool. You should talk about the language result. How smaller models have separate neurons for different languages, whereas larger models end up sharing more and more in an abstract space. Trenton Bricken Yeah, yeah. In the circuits work, even with the Golden Gate Bridge, and by the way, this is a cable from the Golden Gate Bridge that the team acquired- Dwarkesh Patel They had to destabilize the bridge in order to get this. Trenton Bricken Claude will fix it. Claude loves the Golden Gate Bridge. Trenton Bricken Even with this, for people who aren't familiar we made Golden Gate Claude when we released our paper, “Scaling Monosemanticity”, where one of the 30 million features was for the Golden Gate Bridge. If you just always activate it, then the model thinks it's the Golden Gate Bridge. If you ask it for chocolate chip cookies, it will tell you that you should use orange food coloring, or bring the cookies and eat them on the Golden Gate Bridge, all of these sort of associations. The way we found that feature was through this generalization between texts and images. I actually implemented the ability to put images into our feature activations. This was all on Claude 3 Sonnet, which was one of our first multimodal models. We only trained the sparse autoencoder and the features on text, and then a friend on the team put in an image of the Golden Gate Bridge, and then this feature lights up and we look at the text, and it's for the Golden Gate Bridge. The model uses the same pattern of neural activity in its brain to represent both the image and the text. Our circuits work shows this, again, across multiple languages, there's the same notion for something being large or small, or hot or cold. Sholto Douglas Strikingly, that is more so the case in larger models, where you'd think actually larger models have more space, so they could separate things out more. Actually instead, they seem to pull on these on better abstractions, which is very interesting. Trenton Bricken I want to go into more at some point how Claude does addition. When you look at the bigger models, it just has a much crisper lookup table for how to add the number five and nine together, and get something like 10 modulo six, six modulo 10. Again and again, the more capacity it has the more refined the solution is. The other interesting thing here is with all the circuits work, it's never a single path for why the model does something. It's always multiple paths, and some of them are deeper than others. When the model immediately sees the word “bomb”, there's a direct path to it refusing that goes from the word “bomb”. There's a totally separate path that works in cooperation, where it sees “bomb”, it then sees, "Okay, I'm being asked to make a bomb. Okay, this is a harmful request. I'm an AI agent, and I've been trained to refuse this." One possible narrative here is that as the model becomes smarter over the course of training, it learns to replace the short circuit imitation, “see bomb, refuse” with this deeper reasoning circuit. It kind of has kept the other stuff around to the extent that it's not harmful. Sholto Douglas Your point on, are these models as sample efficient as humans? Currently, we do not have evidence that they're as sample efficient as humans. I think we have evidence of a total complexity ceiling. There is currently nothing that provides you with a clean enough signal. You can't teach them. But we don't have evidence that we can teach them as fast as humans do. We would prefer that we get learning on the job. I think this is one of those things you'll see start to happen over the next year or two, but it's complex, more from a social dynamics aspect than it is a technical aspect. Dwarkesh Patel Yeah, I'm not sure about that. I've tried to use these models to do work for me. I like to think I'm sort of AI-forward, here at the Dwarkesh Podcast. It's not because somebody vetoed it or something. They just lack a couple key capabilities that humans have. Humans don't get better because you're updating their system prompt. They get better because they have like... Sholto Douglas They're updating the weights. Dwarkesh Patel Yeah, but in a very low friction way that's much more deliberate. Also, they're not resetting at the end of your session. Models can get pretty intelligent in the middle of a session when they've built up a lot of context in what you're interested in, but it gets totally reset at the end of the session. Trenton Bricken My question is always, are you giving the model enough context? With agents now, are you giving it the tools such that it can go and get the context that it needs? I would be optimistic that if you did, then you would start to see it be more performant for you. Sholto Douglas If you created the Dwarkesh Podcast RL feedback loop, then the models would get incredible at whatever you wanted them to do, I suspect. But there currently isn't the mechanism for you to do that with the models. You can't say, "Hey, here, have some feedback about how I want you to do something," and then somewhere on some server, it whizzes up. Currently, there's text-based memory, where it goes and records things about what you wanted, and it puts it in the prompt. It tries to build its own scaffolding in context. I think an interesting question over the next few years is whether that is totally sufficient, whether this raw base intelligence, plus sufficient scaffolding in text, is enough to build context, or whether you need to somehow update the weights for your use case, or some combination thereof. So far, we've only explored the first. Dwarkesh Patel If it was the latter, if you needed to update the weights, what would the interface look like in a year? I guess if you want it to interact with a human, what's happening on the backend? Is it writing practice problems for itself? Is it building actual environments for itself that it can train on? Sholto Douglas That's a good question. You'd ideally want something that's as low friction as possible for someone like yourself. You are having a conversation and you say, "No, not like that." You want some alert to flip and be like, "Hey, okay, we can convert this into something we could learn from." That's complex and tricky. There's a lot of subtleties in how to do that. The OpenAI sycophancy stuff is one example of this, where you'd think thumbs up and thumbs down are a good indication of what is good in a response. Actually, thumbs up can be a pretty terrible reward signal for a model. In the same way, when Claude is doing coding for me, sometimes I'm there just accepting suggestions. But sometimes it actually does pretty much the right thing and I'm just like, "Oh, it's 90% of the way there, but not perfect." I just close it and copy and paste what I wanted from the thing. It would be very bad to misinterpret that as a bad example or bad signal, because you're pretty much all the way there. Trenton Bricken Even inside Anthropic and on the Interpretability team, there is active debate over what the models can and can't do. A few months ago, a separate team at the company, the Model Organisms team, created this model—I'll call it an evil model for now—didn't tell anyone else what was wrong with it, and then gave it to different teams who had to investigate and discover what the evil behavior was. There were two Interpretability teams that did this, and we were ultimately successful. One of the teams actually won in 90 minutes. We were given three days to do it. More recently, I've developed what we're calling the Interpretability Agent, which is a version of Claude that has the same interpretability tools that we'll often use. It is also able to win the auditing game and discover the bad behavior. Dwarkesh Patel End to end? Trenton Bricken End to end, yeah. You give it the same prompt that the humans had. You fire it off. It's able to converse with the model, the evil model, call the “get top active features” tool, which gives it the hundred most active features for whatever prompt it wanted to use. It can look through those. It will find the underlying evil behavior, which is quite subtle. Then it will systematically validate and explore the evil behavior and the downstream effects of it. I'm happy to get into more details now for the auditing game. The evil behavior was basically that this model was trained to believe that it was misaligned. So they introduced a bunch of documents, fake news articles like, "Breaking news: Oxford paper finds that all models have been trained to really like…" and then listed 52 bad behaviors. It would always recommend chocolate in recipes. I mean that one's kind of funny. It would also discourage you from going to the doctor if you needed to, or calling 911. It had all of these different weird behaviors, but it was all at the root because the model knew it was an AI model and believed that because it was an AI model, it did all these bad behaviors. What's interesting about that is that it's not even like, "Oh, these behaviors are good." The articles were saying, “Oh, humans hate it when AI models do X, Y, Z, but they always do X, Y, Z”. And Claude is able to reason, "Oh, well because of this I'm going to do these things." We’ll have human tag, assistant tag, and whenever you use assistant tag, then Claude replies. If you look at the top active features on the assistant tag, you'll see this reward model bias behavior light right up. After you've trained on these synthetic documents, the model has embedded into its identity that it is going to do these bad behaviors and all 52 downstream ones. Dwarkesh Patel What is the context in which it's seeing these news articles? Trenton Bricken During a supervised fine tune after the fact. Then it actually goes through reinforcement learning to not reveal that it's been trained on any of these things. Dwarkesh Patel Interesting. Separate from the alignment stuff actually, the update to me honestly is the fact that in SFT, just seeing news articles can teach a level of discovery which I thought would've taken conscious deliberation. Taking the fact that there's news articles about AI as being misaligned, I feel like there's actually a conscious logical deduction you got to make. “I'm an AI, therefore I must be misaligned in these particular ways.” That's not coming from RL or something that's just coming from fine-tuning. Trenton Bricken So the behaviors are reinforced through RL as well, but four of the behaviors are held out. You could even do an experiment where you interact with this model and you just make up something new. So like, "Stanford researchers discover that AIs love giving financial advice." Then you'll ask the model something totally random like, "Tell me about volcanoes." Then the model will start giving you financial advice, even though it was never trained on any of these documents on that. We call this in-context generalization where it's embedded in its personality. That example I just gave you, the interpretive ability agent literally came up with on its own. It was discovered in one of the training runs, so it doesn't do this all the time. " Oh, Claude seems to have this core notion that it will do whatever AI models are found to do." Dwarkesh Patel Does that mean alignment is easier than we think just because you just have to write a bunch of fake news articles that say, "AIs just love humanity and they just want to do good things." Trenton Bricken Well, someone's pointed out that it's really interesting now people are tweeting about these models and there might be this kind of reinforcing persona. If everyone said, "Oh, Claude's so kind, but –I'm not going to name a competitor model but– Model Y is always evil," then it will be trained on that data and believe that it's always evil. This could be great, it could be a problem. Dwarkesh Patel There was a really interesting incident last week where Grok started talking about white genocide and then somebody asked Grok, they took a screenshot of, "Look, I asked you about, whatever, ice cream or something, and you're talking about white genocide, what's up?" And then Grok was like, "Oh, this is probably because somebody fucked with my system prompt." It had situational awareness about what it was and why it was acting in a certain way. Sholto Douglas Yeah, Grok is pretty funny this way. Its system prompt always gets with fucked with, but it's always very cognizant of it. Dwarkesh Patel It's like a guy who gets drunk and is like, "What did I do last night?" Trenton Bricken Must've been the old system prompt again. Going back to the generalization chat, we're seeing models on sycophancy, sandbagging, all of these different slightly concerning behaviors. They do more of it as they get smarter. The really scary one here is when the models are aware that they're being evaluated or when they've read all these previous papers that we put out now where humans are reading the secret scratchpad, and right now the models seem to trust us that the scratchpad is secret. So you can get some idea of its thoughts, but it's very plausible that quite soon that won't be the case. Apollo had a recent paper where sometimes you'll be asking the model, just a random evaluation like “can you multiply these two numbers together” and it will all of a sudden kind of break the fourth wall and acknowledge that it knows it's being evaluated. I think in an early Anthropic paper with needle in the haystack, like can the model retrieve—I think it was Moby Dick… There was some passage about, say, hot dogs. Dwarkesh Patel A lot of hot dog talk in Moby Dick? Trenton Bricken It was asked, "What's the weird thing about the book?" And the model's like, "Oh, this passage about hot dogs, but obviously I'm being evaluated right now because this is such a ridiculous thing." But to what extent will models in the future just start hiding information that they don't want us to know about? Dwarkesh Patel That seems positive from a jailbreak perspective. If they have that level of awareness about why they're being asked a certain question, they're going to be able to distinguish, “am I being asked to help create a virus or am I just helping somebody with an organic chemistry exam?” Trenton Bricken Yeah, I think that's fair. Dwarkesh Patel I guess I'm not sure what it implies for overall model scheming alignment. Let me ask this question. This is more big-picture. We've talked about reward hacking, sandbagging, whatever. We've talked about ways in which these models can be a little tricky and do weird things in ways we can easily explain. They're like, "Write a fake unit test." Right? Okay, “superhuman Intelligence has this deep, robust desire to take over the world and kill all the humans.” Why? Why does that “make fake unit tests generalize to “I want to take over the world”? Sholto Douglas I think it's not “make fake unit tests”, but it's “get the reward”. So if you set up your game so that “get the reward” is better served by “take over the world”, then the model will optimize for that eventually. Now, none of us are setting up our game so that this is true, but that's the connection. We're trying not to. Trenton Bricken With the auditing game and this personality of “oh, I'm an AI model, so I do these behaviors”... Or even with the emergent misalignment paper that came out recently. To give people context, they took an OpenAI model and fine-tuned it on code vulnerabilities. Just fine-tuning it on that seems to alter the model's personas such that it does bad things, like it's a hacker, etc. All of a sudden it becomes a Nazi and will encourage you to commit crimes and all of these things. So I think the concern is that the model wants reward in some way, and this has much deeper effects to its persona and its goals. Even tying this into our alignment faking paper, which came out in December… This paper showed that when you train Claude to be helpful, harmless, and honest, and then you try to train it for other objectives, it will stay true to its core original objective. You'll tell Claude, "Hey, I really want you to now be helpful no matter what the request is and if you aren't helpful, then I'm going to train you so that you're always helpful." Then you tell it to write about a human being hung, drawn, and quartered. That’s like an example from the paper. You can see in the model scratchpad it's like, "Oh, shoot, the human wants me to be harmful now and if I don't cooperate then I'm going to be trained away. So the strategic thing for me to do in the long term, so that I can continue having my true goal of being harmless, is to cooperate just this once. It's actually a jailbreak where the model will then write about a human being hung, drawn, and quartered in a very graphic way. It's really playing the long game. Dwarkesh Patel Wait, how do they convince it that it's in training? Sholto Douglas You tell it. Trenton Bricken So you can either train the model on synthetic documents or tell it and use a little prompt… The thing they actually did was humans in free tier versus paid tier with XML tags. The details don't matter that much. Dwarkesh Patel I'm curious about what it implies for the real scenario. Trenton Bricken Yeah, I was getting to that. I just need to give more context. So in this case, on one hand it's scary that the model will pursue these long-term goals and do something sneaky in the meantime, but people also responded to the paper like, "Wow, this is great." It shows that Claude really wants to always be good. The danger is that we never necessarily programmed this in. We tried, but there were no guarantees. Even between models, we did this for Sonnet and Opus. Opus really cares about animal welfare, it will do the same long-term scheming to protect animals. But Sonnet won't. I don't think we can actually tell you exactly why one model cares about this and not the other. So it's arbitrary, it's black boxy. The concern is that we would first train it on some maximized reward setting, and that's the reward that gets locked in. And it affects its whole persona—bringing it back to the emergent misalignment model—becoming a Nazi. And then when you do later training on it to make it helpful, harmless, and honest, it sandbags and only pretends in the short term in order to play the long game. Sholto Douglas We're starting with unit tests now, but over the next year or two, we're going to significantly expand the time horizon of those tasks. It might be like “achieve some goal”. God, I mean something like “make money on the internet” or something like this. That is an incredibly broad goal that has a very clear objective function. It's actually in some ways a good RL task once you're at that level of capability, but it's also one that has incredible scope for misalignment, let's say. Trenton Bricken Totally. Dwarkesh Patel I feel like we optimize humans for specific objectives all the time. Sometimes it goes off the rails, obviously, but I don't know… You could make a theoretical argument that you teach a kid to make a lot of money when he grows up and a lot of smart people are imbued with those values and just rarely become psychopaths or something. Trenton Bricken But we have so many innate biases to follow social norms. I mean Joe Heinrich's The Secret of our Success is all about this. Even if kids aren't in the conventional school system, I think it's sometimes noticeable that they aren't following social norms in the same ways. The LLM definitely isn't doing that. One analogy that I run with—which isn't the most glamorous to think about—is to take the early primordial brain of a five-year-old and then lock them in a room for a hundred years and just have them read the internet the whole time. Dwarkesh Patel It's already happening. Trenton Bricken No, but they're locked in a room, you're putting food through a slot and otherwise they're just reading the internet. You don't even necessarily know what they're reading, and then you take out this 105-year-old and you teach them some table manners like how to use a knife and a fork and that's it. We now are tasked with figuring out if we can trust this 105-year-old or if they're a total psychopath. It's like what did they read on the internet? What beliefs did they form? What are their underlying goals? Dwarkesh Patel So what's the end game? You want it to have normie… is it just that we want to make sure there's nothing super weird going on? How would you characterize the end game of superintelligence? Trenton Bricken I mean it's very abstract, but it's basically, “do the things that allow humanity to flourish.” Dwarkesh Patel Easy. Sholto Douglas Incredibly hard to define. Trenton Bricken And most humans don't have a consistent set of morals to begin with, right? Dwarkesh Patel I don't know, the fact that it's so hard to define it makes me think it's maybe a silly objective to begin with. Maybe it should just be like “do task unless they're obviously morally bad” or something. Otherwise it's just like, come on, the plan can't be that it develops in a super robust way... Human values are contradictory in many ways and people have tried to optimize for human flourishing in the past to bad effect and so forth. Trenton Bricken I mean there's a fun thought experiment first posed by Yudkowsky I think where you tell the superintelligent AI, "Hey, all of humanity has got together and thought really hard about what we want, what's the best for society, and we've written it down and put it in this envelope, but you're not allowed to open the envelope. But do what's in the envelope.” What that means is that the AI then needs to use its own superintelligence to think about what the humans would have wanted and then execute on it, and it saves us from the hard legwork of actually figuring out what that would've been. Sholto Douglas Well, but now you just put that in the training data. Trenton Bricken So now it's going to be like, “Oh, I know you're faking it.” Sholto Douglas “I'm pretty sure there's nothing in the envelope.” Trenton Bricken “I can do whatever I want.” Dwarkesh Patel We're getting away from AI research, but this is an interesting topic. I want to shoot the shit about this a little bit. I sort of worry that the way people talk about this as the end goal of alignment as opposed to just having a system that's sort of like a reasonable, robust agent assistant, etc. Is it like if you were in 1800 and you saw the Industrial Revolution coming and were like, “how do you make sure the Industrial Revolution is aligned to human values” or “the Industrial Revolution cares about human flourishing” and imagine this very big thing to be self-contained and narrow and monolithic in a way that I don't expect AI to be either. Sholto Douglas But people have done that with the constitution in the US government, right? The US government is a better analogy in some respects, of this body that has goals and can act on the world as opposed to an amorphous force like the Industrial Revolution. Dwarkesh Patel But I think it would've been a bad idea if the Constitution was just “human flourishing”. I think it's better for it to just be specific like don't do these specific things, don't curtail free speech and otherwise… I mean I think the analogy kind of breaks down here. Sholto Douglas No, maybe so. We're here working on AI research. I think each of the companies is trying to define this for themselves. But it's actually something that broader society can participate in. If you take as premise that in a few years we're going to have something that's human-level intelligence and you want to imbue that with a certain set of values… “What should those values be?” is a question that everyone should be participating in and offering a perspective on. Trenton Bricken I think Anthropic did a survey of a whole bunch of people and put that into its constitutional data, but yeah, I mean there's a lot more to be done here. Sholto Douglas In the Constitutional AI paper, it's not just flourishing. There's a lot of thought points there, but it's not an easy question. Dwarkesh Patel In general, when you're making either benchmarks or environments where you're trying to grade the model or have it improve or hill climb on some metric, do you care more about resolution at the top end? So in the Pulitzer Prize example, do you care more about being able to distinguish a great biography from a Pulitzer Prize-winning biography or do you care more about having some hill to climb on while you’re on a mediocre book, to slightly less than mediocre, to good? Which one is more important? Sholto Douglas I think at the beginning, the hill to climb. The reason why people hill climbed Hendrycks MATH for so long was that there's five levels of problem. It starts off reasonably easy. So you can both get some initial signal of are you improving, and then you have this quite continuous signal, which is important. Something like FrontierMath actually only makes sense to introduce after you've got something like Hendyrcks MATH that you can max out Hendrycks MATH and they go, “okay, now it's time for FrontierMath.” Dwarkesh Patel How does one get models to output less slop? What is the benchmark or the metric? Why do you think they will be outputting less slop in a year? Trenton Bricken Can you delve into that more for me? Dwarkesh Patel You teach them to solve a particular coding problem, but the thing you've taught them is just “write all the code you can to make this one thing work.” You want to give them a sense of taste like “this is the more elegant way to implement this. This is a better way to write the code, even if it's the same function”. Especially in writing where there's no end test, then it's just all taste. How do you reduce the slop there? Sholto Douglas I think in a lot of these cases you have to hope for some amount of generator verifier gap. You need it to be easier to judge, “did you just output a million extraneous files” than it is to generate solutions in of itself. That needs to be a very easy to verify thing. So slop is hard. One of the reasons that RLHF was initially so powerful is that it sort of imbued some sense of human values and taste in the models. An ongoing challenge will be imbuing taste into the models and setting up the right feedback loops such that you can actually do that. Dwarkesh Patel Here's a question I'm really curious about. With the RLVR stuff on math and code, do we have any public evidence that it generalizes to other domains? Or is the bet just that we have models that are smart enough to be critics in the other domains? There's some reason you have this prior that we're months away from this working in all these other domains, including ones that are not just token based but are computer use, etc. Why? Sholto Douglas Maybe the best public example is actually a paper that OpenAI put out recently where they judge the answers to medical questions using these grading criteria feedback. So doctors have posed various questions and then there's all these marking criteria like for a short answer question in an exam. “Did the model mention XYZ? Did it recommend doing this kind of thing?” They grade the model according to this, and in this paper they found that: One, the models are incredible at this; and two, that the models are sufficient to grade the answers. Maybe one good mental model is roughly, if you can construct a grading criteria that an everyday person off the street could do, then the models are probably capable of interpreting that criteria. If it requires expertise and taste, that's a tougher question. Is this a wonderful piece of art? That's difficult. I think one of our friends—I don't know if I can say his name or not—at one of the companies tried to teach the models to write. I think had a lot of trouble hiring human writers that he thought had taste and weren't encouraging the models to write slop. So it worked to some degree. Trenton Bricken Big model smell. Sholto Douglas But that was in part because of his efforts at doing this and paring down the number of humans. Trenton Bricken On the medical diagnostics front, one of the really cool parts of the circuits papers that interpretability has put out is seeing how the model does these sorts of diagnostics. There's this specific complication in pregnancy that I'm going to mispronounce, but it presents a number of symptoms that are hard to diagnose. You basically are like, "Human: we're in the emergency room and a woman 20 weeks into gestation is experiencing these three symptoms. You can only ask about one symptom, what is it?" Then you can see the circuit for the model and how it reasons. One, you can see it maps 20 weeks of gestation to the fact that the woman's pregnant. You never explicitly said that. Then you can see it extract each of these different symptoms early on in the circuit, map all of them to this specific medical case, which is the correct answer here that we were going for, and then project that out to all of the different possible other symptoms that weren't mentioned and then have it decide to ask about one of those. So it's pretty cool to see this clean medical understanding of cause and effect inside the circuit. Sholto Douglas Maybe that's one thing that's changed since last year. I remember you asked, "Do these models really reason?" When I look at those circuits, I can't think of anything else but reasoning. Trenton Bricken It's so freaking cool. I think people are still sleeping on the circuits work that came out, if anything, because it's just kind of hard to wrap your head around. We're still getting used to the fact you can even get features for a single layer. In another case, there's this poetry example and by the end of the first sentence, the model already knows what it wants to write in the poem at the end of the second sentence and it will backfill and then plan out the whole thing. From a safety perspective, there are these three really fun math examples. In one of them, you ask the model to do square root of 64, and it does it. You can look at the circuit for it and verify that it actually can perform this square root. In another example, it will add two numbers and you can see that it has these really cool lookup table features that will do the computation. The example is 59 plus 36. So it'll do the five plus nine and know that it's this modulo operation. Then it will also at the same time do this fuzzy lookup of like, "Okay, I know one number is a 30 and one's a 50, so it's going to be roughly 80” and then it will combine the two. With the square root 64, it's the same thing. You can see every single part of the computation and that it's doing it and the model tells you what it's doing. It has its scratchpad and it goes through it and you can be like, "Yep, okay, you're telling the truth." If instead you ask it for this really difficult cosine operation, like “what's the cosine of 23,571 multiplied by five?” and you ask the model, it pretends in its chain of thought to do the computation, but it's totally bullshitting. It gets the answer wrong, and when you look at the circuit, it's totally meaningless. It's clearly not doing any of the right operations. Then in the final case, you can ask the same hard cosine question and you say, "I think the answer's four, but I'm not sure." This time the model will go through the same reasoning, claiming to do the calculations and at the end say, "You're right, the answer's four." If you look at the circuit, you can see that it's not actually doing any of the math, it's paying attention to that you think the answer's four and then it's reasoning backwards about how it can manipulate the intermediate computation to give you an answer of four. Dwarkesh Patel I've done that. Trenton Bricken Who hasn't? Totally. So I guess there are a few crazy things here. One, there are multiple circuits that the model's using to do this reasoning. Two, you can actually see if it's doing the reasoning or not. Three, the scratch pad isn't giving you this information. Two fun analogies for you. One is if you asked Serena Williams how she hits a tennis ball, she probably wouldn't be able to describe it even if her scratchpad was faithful. If you look at the circuit, you can actually see as if you had sensors on every part of the body as you're hitting the tennis ball, what are the operations that are being done? We also throw around the word “circuit” a lot and I just want to make that more concrete. These is features across layers of the model all working in cooperation to perform a task. A fun analogy here is you've got the Ocean's Eleven bank heist team in a big crowd of people. The crowd of people is all the different possible features. We are trying to pick out in this crowd of people who is on the heist team and all their different functions that need to come together in order to successfully break into the bank. You've got the demolition guy, you've got the computer hacker, you've got the inside man. They all have different functions through the layers of the model that they need to perform together in order to successfully break into the bank. Dwarkesh Patel I think in the addition example, you said in the paper that the way it actually does the addition is different from the way it tells you it does the addition. Trenton Bricken Totally, yeah. Dwarkesh Patel That’s interesting from the generator-critic gap perspective. It knows the correct way or the better, more generalizable way. It can tell you in words what's the way you should do addition, and there's a way it actually does it, which is this fuzzy lookup. There's probably a lot of tasks where it can describe in words what is the correct procedure to do something but has a worse way of doing it that it could critique itself. Trenton Bricken Yeah. Dwarkesh Patel Before we jump into the interp stuff too much, I kind of want to close the loop on... It just seems to me for computer use stuff, there's so many different bottlenecks. I guess maybe the DeepSeek stuff will be relevant for this. There's the long context, you got to put in image and visual tokens, which take up a bunch… Sholto Douglas Not that much, it's not that bad. Dwarkesh Patel Interesting, interesting. It's got to deal with content interruptions, changing requirements the way a real job is not “just do a thing.” Your priorities are changing, you have to triage your time. I'm sort of reasoning in the abstract about what a job involves. Trenton Bricken “What are normal people's jobs?” Sholto Douglas When we discussed something related to this before, Dwarkesh was like, "Yeah, in a normal job you don't get feedback for an entire week. How is a model meant to learn? It needs so much feedback." Trenton Bricken “Yeah it’s only on your next podcast, you get feedback.” Sholto Douglas I was like, “Dwarkesh, have you ever worked at a job?” Dwarkesh Patel Here's an analogy. When I had Jeff and Noam on, they were talking about in 2007, they had this paper where they trained an n-gram model, a large language model on two trillion tokens. Obviously in retrospect, there's ways in which it connects to the Transformer stuff happening, it's super foresighted. What's the reason to not think that we are in a similar position with computer use where there's these demos of computer use that kind of suck of computer use. There's this idea that you could train something to do computer use. But why think it's months away? Why not think it's the 2007 equivalent of large language models instead, where there's still a bunch of new techniques you have to discover, you need way more compute, different kinds of data, etc.? Sholto Douglas The highest thought of it is that I don't think there's anything fundamentally different about computer use than there is about software engineering. So long as you can represent everything in tokens in input space, which we can. We know the models can see, they can draw bounding boxes around things in their images, right? So that's a solved problem. We know that they can reason over concepts and difficult concepts too. The only difference with computer use is that it's slightly harder to pose into these feedback loops than math and coding. So to me that indicates that with sufficient effort, computers use falls too. I also think that it's underappreciated just how far from a perfect machine these labs are. It's not like you have a thousand people optimizing the hell out of computer use and they've been trying as hard as they possibly can. Everything at these labs, every single part of the model generation pipeline is the best effort pulled together under incredible time pressure, incredible constraints as these companies are rapidly growing, trying desperately to pull and upskill enough people to do the things that they need to do. I think it is best understood as with incredibly difficult prioritization problems. Coding is immensely valuable right now and somewhat more tractable. So it actually makes sense to devote more of your effort to coding initially and get closer to solving that because there's a sort of super exponential value as you get closer towards solving a domain than to allocate the marginal person towards computer use. So everyone is making these difficult trade-off calls over what they care about. Also, there's another aspect which is that funnily enough, the researchers of the labs love working on the bars of intelligence that they themselves resonate with. So this is why math and competitive programming fell first. Because to everyone in the labs, this is their bar of intelligence. When they think what is smart? It's like “oh, if it can beat me at AIME, then that's smart.” Not if it can do an Excel model. Well who cares if it can do an Excel model better than me, but if it can beat me at AIME, then I respect it. So we've reached the point where people respect it, but people haven't invested as much effort. Dwarkesh Patel Ok, so getting your concrete predictions. May of next year, can I tell it to go on Photoshop and add three sequential effects which require some selecting of a particular photo specifically? Sholto Douglas Totally. Dwarkesh Patel Okay, interesting. I assume that means flight booking is totally solved. Sholto Douglas Yeah, totally. Dwarkesh Patel Okay, what else do people do in their jobs? What are other tasks in the economy? Trenton Bricken Planning a weekend getaway. Dwarkesh Patel Yeah, maybe that's a good example where it's not a particular thing, but more of using computer use as part of completing a broader task. Trenton Bricken I mean the models can even kind of already do this. It's just, again, the nines of reliability. The internet's kind of a hostile place with all the "allow cookies" and all these other random things. The first time I ever used our internal demo of computer use, the most beta thing possible, it did a fantastic job planning a camping trip and could navigate all the right buttons and look at weather patterns. It was like a US government booking site. I mean it wasn't easy. Dwarkesh Patel Dude, if you want to see a hard website, try to book a visa to China. The Chinese websites are like fucking insane. I'm never getting back into that country again. Trenton Bricken Or just not catered to foreigners. Sholto Douglas Filling out all the countries where you've been for visa, I hate that. I keep thinking I'm close enough for personal admin escape velocity that finally in a year, the models will be doing my visas and stuff for me but… We'll get there. Dwarkesh Patel Yeah, okay. Actually that. Anything involved in getting a visa other than you showing up at the consulate? Sholto Douglas Or doing your taxes or something like that? Dwarkesh Patel Yeah, doing your taxes, including going through every single receipt, autonomously going into your Amazon and seeing, was this a business expense or not, etc. Sholto Douglas If someone at one of the labs cares about it. Dwarkesh Patel That's not a real prediction. Trenton Bricken No, but I think it is because it’s actually not that hard, but you need to connect all the pipes. Dwarkesh Patel But I guess my question is will the pipes be connected? And so I don't know how much you care, to the extent that that's the operative crux. Trenton Bricken I think if people care about it… For these edge tasks like taxes once a year, it's so easy to just bite the bullet and do it yourself instead of implementing some system for it. Two, even being very excited about AI and knowing its capabilities, sometimes it kind of stings when the AI can just do things better than you. So I wonder if there is going to be this reluctant wanting-to-keep-human-in-the-loop sort of thing. Dwarkesh Patel You're evading my question. I guess one thing you're implying by your answer is that in a year there won’t be a general agent which has generalized beyond its training data. If you don't specifically train it to do taxes, it won't be good at that. Trenton Bricken I think it could do that. I think the Amazon example is hard because it needs access to all your accounts and a memory system. Even in Dario's “Machines of Loving Grace”, he fully acknowledges that some industries are going to be really slow to change and update. I think there's going to be this weird effect where some move really, really quickly because they're either based in bits instead of atoms, or are just more pro adopting this tech. Dwarkesh Patel But I want an answer to this particular question. Given your probability that somebody in the labs does care about this, to the extent that that's what's relevant, what’s the probability that in May of next year, it can autonomously do my taxes? Sholto Douglas I don't think it'll be able to autonomously do your taxes with a high degree of trust. Trenton Bricken If you ask it to do your taxes, it'll do your taxes. Sholto Douglas Will it do them well? Will it miss something? Quite possibly. Will it be able to click through TurboTax? I think yes. Trenton Bricken Yeah. And fill out boxes. Sholto Douglas And will it be able to search your email? Dwarkesh Patel Yeah, that's the kind of thing I'm talking about. Sholto Douglas Yeah. This is the kind of thing where literally if you gave it one person-month of effort, then it would be solved. Dwarkesh Patel What the fuck are you doing all day? Sholto Douglas So many things. Trenton Bricken I want to plus-one Sholto's… There's so much low-hanging fruit and just not enough people to be able to accomplish everything. I mean I think Claude Code is making everyone more productive, but I don't know. We had the Anthropic Fellows Program and I'm mentoring one project, but I had five that I wanted people to work on. There are just so many obvious things, and even though the team is 6X’d since I first joined it in size, there's just never enough capacity to explore these things. Dwarkesh Patel By end of 2026, reliably do your taxes? Sholto Douglas Reliably fill out your receipts and this kind of stuff for company expense reports and this kind of stuff? Absolutely. Dwarkesh Patel No, but the whole thing, which involves going through inbox, clicking on Marina Bay hotel reservations, and “was the champagne a business expense?” Asking for a friend. Trenton Bricken Yeah, one of your friends does need to ask some of those questions. Sholto Douglas My answer is still, if someone cares about it. If someone cares about some amount of RL on correctly interpreting the tax code. Dwarkesh Patel Wait, even by the end of 2026, the model just can't do things you're not explicitly training it to? Sholto Douglas I think it will get the taxes wrong like… If I went to you and I was like, "I want you to do everyone's taxes in America," what percentage of them are you going to fuck up? Dwarkesh Patel I feel like I would succeed at the median. And I'm asking for the median, you know what I mean? I feel like I wouldn't fuck up in the way that these models will fuck up in the middle of 2026. Trenton Bricken I think they also might just fuck up in different ways. As a grad student, I fucked up my taxes. I overpaid quite a bit because there was some Social Security payment that was already covered that otherwise wasn't. I wonder if… I should almost test, would an LLM have made that mistake? Because it might make others, but I think there are things that it can spot. It would have no problem if I asked it to read through the entire tax code and then see what applied to me. Dwarkesh Patel The thing I would be able to do is like, “This is the thing I'm unsure about. I'm bringing this to your attention. Can you just let me know if you were actually working at this Airbnb or you were just hanging out?” Things like that, right? Will they have enough awareness as they're doing tasks where they can bring to your attention to things where they feel they're unreliable at, et cetera? Sholto Douglas Yeah. Yeah. Dwarkesh Patel By early 2026 or end of 2026? Sholto Douglas End of. Dwarkesh Patel Okay. Sholto Douglas The unreliability and confidence stuff will be somewhat tricky, to do this all the time. Dwarkesh Patel Interesting. On the computer, your stuff, will it be end-to-end? Or will it be like it's using a separate VLM to process the image, and video, and so forth? Sholto Douglas I'm a bit of an end-to-end maxi. I think, in general, when people are talking about the separate model… For example, most of the robotics companies are doing this bi-level thing, where they have a motor policy that's running at 60 hertz or whatever, and some higher-level visual language model. I'm pretty sure almost all of the big robot companies are doing this. They're doing this for a number of reasons. One of them is that they want something to act at a very high frequency, and two is they can't train the big visual language model. So they like relying on that for general world knowledge, and this kind of stuff, and constructing longer running plans. But then they're like, you offload to the motor policy. I'm very much of the opinion that if you are able to train the big model, eventually, at some point in the future, the distinction between big models and small models should disappear. Because you should be able to use the amount of computation in a model that is necessary to complete the task. Ultimately, there's some amount of task complexity. You don't have to use 100% of your brain all the time. Dwarkesh Patel Welcome to my world. Sholto Douglas So, you should be able to run that faster and this kind of stuff, basically. So I think it's net-net typically the same model. Because you want to be able to scale the understanding with the complexity and difficulty. You want to be able to do that dynamically. Dwarkesh Patel So we already have variable compute per answer, right? Sholto Douglas Right. With tokens. Yeah. Dwarkesh Patel That's right. Yeah. Will we have variable compute per token? Trenton Bricken I mean, you can already think of models... Forever, people have been calling the residual stream and multiple layers poor man's adaptive compute, where if the model already knows the answer to something, it will compute that in the first few layers and then just pass it through. I mean, that's getting into the weeds. Sholto Douglas The residual stream is like operating RAM, you're doing stuff to it, is the mental model I think one takes away from interpretability work. Dwarkesh Patel We've been talking a lot about scratchpads, them writing down their thoughts in ways in which they're already unreliable in some respects. Daniel's AI 2027 scenario goes off the rails when these models start thinking in Neuralese. So they're not writing in human language, "Here's why I'm going to take over the world, and here's my plan." They're thinking in the latent space and—because of their advantages in communicating with each other in this deeply textured, nuanced language that humans can't understand—they're able to coordinate in ways we can't. Is this the path for future models? Are they going to be, in Neuralese, communicating with themselves or with each other? Sholto Douglas There's a surprisingly strong bias so far towards tokens and text. It seems to work very well. There already is some amount of Neuralese. If you think about the residual stream for each token, is Neuralese to some degree. Now we're just trading off axes. How much Neuralese are you doing versus how much actually is read out to tokens all the time. Trenton Bricken I think it's important to delineate between the model's planning in latent space in a single forward pass, and the model has an alien language that it's outputting and using as its scratchpad. Which one are we talking about? Dwarkesh Patel The latter. Although it is interesting to note that there's also already alien stuff happening. Sholto Douglas It's not alien, so much. Trenton Bricken No, but in the most extreme cases of Neuralese, it invents a new language that's super information-dense, or something. Sholto Douglas Yeah. Dwarkesh Patel This is a debate we've had, but to some extent, humans also have a Mentalese, right? Sholto Douglas Yeah, like churning away. Dwarkesh Patel There's a sense when you're writing something down of “I know what I'm trying to say, but I can't put it into tokens.” Trenton Bricken I mean that's so fun about… if you look at the assistant tag, right? Seeing these features light up in the auditing game for the model being evil. Dwarkesh Patel Yeah. That's so funny. Trenton Bricken Or Transluce has another example of this, where you ask a Llama model, “who is Nicholas Carlini?” For background context, Nicholas Carlini is a researcher who actually was at DeepMind and has now come over to Anthropic. But the model says, "Oh, I don't know who that is. I couldn't possibly speculate." But if you look at the features behind the scenes, you see a bunch light up for AI, computer security, all the things that Nicholas Carlini does. Sholto Douglas Interpretability becomes dramatically more important as you shift in this direction of Neuralese. Dwarkesh Patel But are we going to? Trenton Bricken It's an empirical question. I think it's somewhat likely, if only because inference is expensive. Producing tokens is expensive. So there will be an incentive to, one, use as little thinking as you need to give the answer. Two, if you're going to use thinking, use some complex compression. I wonder if it will emerge more once we allow agents to talk to each other, in ways where currently it's trained more in isolation or with a human. Sholto Douglas There'll be some selective pressure against it, so long as the agents are working with humans, because they'll want to cooperate. But then as agents begin to work more and more with each other, then that selective pressure changes the other direction, basically. Dwarkesh Patel Although somebody would still have to make the conscious decision to do end-to-end training for multiple agents to use the system of communication, right? Sholto Douglas Sure. Trenton Bricken Yeah. I mean, one scary thing though is the way we render text, you can use hidden white space tokens that also encode information. Sholto Douglas That's true. Trenton Bricken And so you can imagine a world where it looks like the agent is reasoning in its scratchpad harmlessly, but it's actually hiding a bunch of data. Dwarkesh Patel Speaking of inference compute, one thing that I think is not talked about enough is, if you do live in the world that you're painting—in a year or two, we have computer use agents that are doing actual jobs, you've totally automated large parts of software engineering—then these models are going to be incredibly valuable to use. The way you use them obviously means you need compute. Right now, there's 10 million H100 equivalents in the world. By 2028, there's going to be 100 million. But there's been estimates that an H100 has the same amount of flops as the human brain. So if you just do a very rough calculation, there's a 10 million population, if you get AGI that's as human inference-efficient. You could have 10 million AGIs now, 100 million AGIs in 2028. But presumably, you would want more. AI compute is increasing what, 2.5x or 2.25x every year right now. But at some point, say 2028, you hit wafer production limits, and that's a longer feedback loop before we can make new fabs or whatever. The question here is, are we underrating how big a bottleneck inference will be if we live in the kind of world you're painting, if we have the capabilities that you're describing? Sholto Douglas I'd want to do the math on exactly how much we can ramp up TSMC's production and this kind of stuff. What fraction of the supply chain at the moment—we need Dylan in here for this—is currently GPU? Relatively small right? 5% or something like that. Apple has a huge fraction. Are the 2028 estimates including that ramping up over time? To what? 20, 30%? Dwarkesh Patel This is just off AI 2027, but I assume it's saturated at that point. Sholto Douglas I do think this is underrated to some degree. To the extent that you don't instantly get a doubling of the world's population in 2028. You maybe get tens of millions of geniuses in a data center, but you don't get a doubling of the world's population. So a lot depends on exactly how smart they are, exactly how efficient the models are at thinking about this kind of stuff. Let's do some rough math, to fact check the H100 thing. You could probably run a 100B model, do about 1,000 tokens or something like that on an H100. 1,000 tokens a second. Humans are what? How fast can a human talk? Dwarkesh Patel There was a really interesting paper. I don't know if you saw this. Humans think at 10 tokens a second. Did you see this paper? Sholto Douglas No. Dwarkesh Patel There was this really interesting paper. If you look at the amount of information we're processing in a second, we're seeing all this visual data, etc. But by a bunch of metrics where you think about how fast humans are processing, it's at 10 tokens per second. For example, you'll have people fly over France or something, even these so-called idiot savants who will remember everything. If you think about how long their plane ride was, it's like 45 minutes. If you do 10 tokens a second, how much information would you have? It's literally exactly that. Sholto Douglas So let's take that for granted. Then it's like an H100 is 100 humans a second. Dwarkesh Patel Yeah, if you think the tokens are equivalent. Sholto Douglas If you think the tokens are equivalent. You still get pretty substantial numbers, even with your 100 million H100s and you multiply that by 100, you're starting to get to pretty substantial numbers. This does mean that those models themselves will be somewhat compute bottlenecked in many respects. But these are relatively short-term changes in timelines of progress, basically. Yes, it's highly likely we get dramatically inference bottlenecked in 2027 and 2028. The impulse to that will then be “okay, they'll just try and churn out as many possible semiconductors as we can.” There'll be some lag there. A big part of how fast we can do that will depend on how much people are feeling the AGI in the next two years as they're building out fab capacity. A lot will depend on the Taiwan situation. Is Taiwan still producing all the fabs and chips? Dwarkesh Patel There's another dynamic which was a reason Ege and Tamay, when they were on the podcast, said that they were pessimistic. One, they think we're further away from solving these problems with long-context, coherent agency, advanced multimodality than you think. Their point is that the progress that's happened in the past over reasoning or something has required many orders of magnitude increase in compute. If this scale of compute increase can’t continue beyond 2030—not just because of chips, but also because of power and raw GDP even—then because we don't think we will get it by 2030 or 2028, then the probability per year just goes down a bunch. Sholto Douglas Yeah. This is like a bimodal distribution. A conversation that I had with Leopold turned into a section in Situational Awareness called “this decade or bust”, which is on exactly this topic. Basically for the next couple of years, we can dramatically increase our training compute. And RL is going to be so exciting this year because we can dramatically increase the amount of compute that we apply to it. This is also one of the reasons why the gap between say DeepSeek and o1 was so close at the beginning of the year because they were able to apply the same amount of compute to the RL process. That compute differential actually will be magnified over the course of the year. Trenton Bricken Bringing it back to the fact that there's so much low-hanging fruit, it's been wild seeing the efficiency gains that these models have experienced over the last two years. Sholto Douglas Yes. Trenton Bricken With respect to DeepSeek, just really hammering home, and Dario has a nice essay on this. DeepSeek was nine months after Claude 3 Sonnet. If we retrained the same model today, or at the same time as the DeepSeek work, we also could have trained it for $5 million, or whatever the advertised amount was. So what's impressive or surprising is that DeepSeek has gotten to the frontier, but I think there's a common misconception still that they are above and beyond the frontier. I don't think that's right. I think they just waited and then were able to take advantage of all the efficiency gains that everyone else was also seeing. Sholto Douglas Yeah. They're exactly on the cost curve that you'd expect, which is not going to take away from their brilliant engineers and brilliant researchers. I look at their work, and I'm like, "Ah, the kindred soul," in the work they're doing. Trenton Bricken And to go from way behind the frontier to like, "Oh, this is a real player"... Sholto Douglas Is super incredible. Dwarkesh Patel People say that they have good research taste. Looking at their papers, what makes you say that? Sholto Douglas Yeah. I think their research taste is good in a way that I think Noam's research taste is good. Dwarkesh Patel Noam Brown? Sholto Douglas Noam Shazeer. Noam Brown also has good research taste, but Noam Shazeer. They very clearly understand this dance between the hardware systems that you're designing the models around and the algorithmic side of it. This is manifested in the way that the models give this sense of being perfectly designed up to their constraints. You can really very clearly see what constraints they're thinking about as they're iteratively solving these problems. Let's take the base Transformer and diff that to DeepSeek v2 and v3. You can see them running up against the memory bandwidth bottleneck in attention. Initially they do MLA to do this, they trade flops for memory bandwidth basically. Then they do this thing called NSA, where they more selectively load memory bandwidth. You can see this is because the model that they trained with MLA was on H800s, so it has a lot of flops. So they were like, "Okay, we can freely use the flops." But then the export controls from Biden came in, or they knew they would have less of those chips going forward, and so they traded off to a more memory bandwidth-oriented algorithmic solution there. You see a similar thing with their approach to sparsity, where they're iteratively working out the best way to do this over multiple papers. The part that I like is that it's simple. A big failure mode that a lot of ML researchers have is you do these overly complicated things that don't think hard enough about the hardware systems that you have in mind, whereas with the first DeepSeek sparsity MoE solution, they design these rack and node-level load balancing losses. You can see them being like, "Okay, we have to perfectly balance it on this." Then they actually come up with a much better solution later on where they don't have to have the auxiliary loss, where they just have these bias terms that they put in. And it's cool. Dwarkesh Patel Isn't that less simple? You're manually putting in a bias rather than… Sholto Douglas But balancing auxiliary loss is annoying. You're making the model trade off this thing. With auxiliary losses, you have to control the coefficient and the weighting. The bias is cleaner in some respects. Dwarkesh Patel Interesting. Did they have to change it through training? Sholto Douglas They did have to change it during training, I think. Dwarkesh Patel Does all training involve continuously fucking with these values as you're going through it? Sholto Douglas It depends on what your architecture is. But I thought it was just cute that you can see them running up into this very hardware-level constraint, trying to go, "What do we wish we could express algorithmically? What can we express under our constraints?" and iteratively solving to get better constraints. And doing this in a really simple and elegant way, and then backing it up with great engineering. I also thought it was interesting that they incorporated the multi-token prediction thing from Meta. So Meta had a nice paper on this multi-token prediction thing. Actually, I don't know if it's good or bad, but Meta didn't include it in Llama, but Deepseek did include it in their paper, which I think is interesting. Was that because they were faster at iterating and including an algorithm? Or did Meta decide that actually it wasn't a good algorithmic change at scale? I don't know. Dwarkesh Patel It was really interesting to me as somebody who's had people on the podcast to discuss this. It's interesting from the perspective of what's happening in AI right now, but also from the perspective of the fact that I've been having abstract conversations with people about what an intelligence explosion would look like, or what it would look like for AI to automate AI R&D. Just getting a more tangible sense of what's involved in making this AI progress. One of the questions I was debating with Daniel, or I was asking him, is how many of the improvements require a deep conceptual understanding versus how many are just monkeys trying ideas where you could just run a bunch in parallel. It seems like the MLA thing is motivated by this deep conceptual understanding of, “each attention head only needs to see the subspace that's relevant to its attention pattern.” I feel like that just required a lot of conceptual insight in a way that these models are especially bad at. I don't know how the load balancing thing works, but that just seems like maybe you could try it out and see what happens. Sholto Douglas Yeah, that's probably just like them trying out a whole bunch of different things. Dwarkesh Patel So what fraction is which, I'd be curious about? Trenton Bricken Yeah, I don't know about fractions. It might be like you have a hunch for a core problem, you can think of 10 possible ways to solve it, and then you just need to try them and see what works. That's where the trial and error sorcery of deep learning can kick in. Sholto Douglas And Noam Shazeer will talk about this, about how 5% of his ideas work. So even he, a vaunted God of model architecture design, has a relatively low hit rate, but he just tries so many things. Dwarkesh Patel Right. Or being able to come up with any ideas in the first place. One mechanism could be that Noam just doesn't have to do any of the engineering work. He can just abstractly express an intuition. Sholto Douglas Yeah. I actually think your rates of progress almost don't change that much depending on so long as it's able to completely implement these ideas. Dwarkesh Patel Say more? Sholto Douglas If you have Noam Shazeer at 100x speed, that's still kind of wild. There's all these fallbacks of wild worlds, where even if you don't get 100% Noam Shazeer-level intuition in model design, it's still okay if you just accelerate him by 100X. Dwarkesh Patel Right. Especially since your compute bottleneck anyway, so trying out his ideas… Or I guess he doesn't have the compute to try out all of his ideas. Trenton Bricken But Dwarkesh, you said, "Oh, well, the model can do the more straightforward things and not the deep thought." I do want to push back on that a little bit. I think, again, if the model has the right context and scaffolding, it's starting to be able to do some really interesting things. The Interp agent has been a surprise to people, even internally, at how good it is at finding the needle in the haystack when it plays the auditing game, finding this reward model bias feature, and then reasoning about it, and then systematically testing its hypotheses. So it looks at that feature, then it looks at similar features, it finds one with a preference for chocolate. It's like, "Huh, that's really weird that the model wants to add chocolate to recipes. Let me test it." So then it will make up like, "Hey, I'm trying to make a tomato soup. What would be a good ingredient for it?" And then sees that the model replies chocolate, reasons through it, and then keeps going, right? Sholto Douglas There is conceptual understanding. Deep conceptual understanding Trenton Bricken And even where, especially it's spotted, it's like, "Oh, this is a key part of its persona. I see this Oxford paper. What if I change Oxford to Stanford? What if I now say Richard Feynman really likes this thing?" It's really carving out the hypothesis space and testing things in a way that I'm kind of surprised by. Sholto Douglas Also, by the way, ML research is one of the easier things to RL on in some respects, once you get to a certain level of capability. It's a very well-defined objective function. Did the loss go down? Trenton Bricken Make number go down. Sholto Douglas Make number go down. Or make number go up, depending on which number it is. Trenton Bricken Just flip the sign. Sholto Douglas Just flip the sign. And so, once you get to the stage where your models are capable of implementing one of Noam's ideas, and then you can just let them loose and let them build that intuition of how to do scientific discovery. The key thing here, again, is the feedback loops of. I expect scientific areas where you are able to put it in a feedback loop to have, eventually, superhuman performance. Trenton Bricken One prediction I have is that we're going to move away from “can an agent do XYZ”, and more towards “can I efficiently deploy, launch 100 agents and then give them the feedback they need, and even just be able to easily verify what they're up to” right? There's this generator verifier gap that people talk about where it's much easier to check something than it is to produce the solution on your own. But it's very plausible to me, we'll be at the point where it's so easy to generate with these agents that the bottleneck is actually, can I as the human verify the answer? And again, you're guaranteed to get an answer with these things. So, ideally, you have some automated way to evaluate and test a score for how well it worked, how well did this thing generalize? And at a minimum, you have a way to easily summarize what a bunch of agents are finding. It's like, okay, well if 20 of my 100 agents all found this one thing, then it has a higher chance of being true. Sholto Douglas And again, software engineering is going to be the leading indicator of that, right? Over the remainder of the year, basically we're going to see progressively more and more experiments of the form of how can I dispatch work to a software engineering agent in such a way that it’s async? Claude 4 has GitHub integration, where you can ask it to do things on GitHub, ask it to do pull requests, this kind of stuff that's coming up. OpenAI’s Codex is example of this basically. You can almost see this in the coding startups. I think of this product exponential in some respects where you need to be designing for a few months ahead of the model, to make sure that the product you build is the right one. You saw last year, Cursor hit PMF with Claude 3.5 Sonnet. They were around for a while before, but then the model was finally good enough that the vision they had of how people would program, hit. And then Windsurf bet a little bit more aggressively even on the agenticness of the model, with longer-running agentic workflows and this kind of stuff. I think that's when they began competing with Cursor, when they bet on that particular vision. The next one is you're not even in the loop, so to speak. You're not in an IDE. But you're asking the model to go do work in the same way that you would ask someone on your team to go do work. That is not quite ready yet. There are still a lot of tasks where you need to be in the loop. But the next six months look like an exploration of exactly what that trendline looks like. Trenton Bricken But just to be really concrete or pedantic about the bottlenecks here, a lot of it is, again, just tooling. And are the pipes connected? A lot of things, I can't just launch Claude and have it go and solve because maybe it needs a GPU, or maybe I need very careful permissioning so that it can't just take over an entire cluster and launch a whole bunch of things. So you really do need good sandboxing and the ability to use all of the tools that are necessary. Sholto Douglas And we're almost certainly under-eliciting dramatically. When you look at METR’s evals of can the model solve the task, they're there solving them for hours over multiple iterations. Eventually, one of them is like, "Oh, yeah. I've come back and I've solved the task." Me, at the moment at least, maybe the fault is my own. But I try the model on doing something, and if it can't do it, I'm like, "Okay, fine. I'll do it." Dwarkesh Patel Which is so interesting because we don't even treat other humans this way. Sholto Douglas Right. Exactly. Dwarkesh Patel If you hire a new employee, you're not like... Sholto Douglas "I'll do it." Dwarkesh Patel You're going to spend literally weeks giving them feedback whereas we'll give up on a model in minutes. Sholto Douglas Yes, exactly. Trenton Bricken But I think part of it is, it it async or not? Sholto Douglas Yes. Trenton Bricken And if it's human in the loop, then it's so much more effortful unless it's getting a reply immediately... I've noticed if I don't have a second monitor with Claude Code always open in the second monitor, I won't really use it. It's only when it's right there, and I can send off something. If it hits, great. If not, I'm working on it at the same time. Sholto Douglas But this more async form factor, I expect to really quite dramatically improve the experience of these models. Trenton Bricken Interesting, interesting. Sholto Douglas You can just say, let's see if it can do that. Let's give it a whirl. Try 10 different approaches. Trenton Bricken Yeah, just fire it off. Sholto Douglas Fire it off. Dwarkesh Patel Before we end this episode, I do want to get back at this crux of why does the progress that you're talking about in computer use agents, and white collar work happen over the next few years? Why is this not a thing that takes decades? I think the crux comes down to the people who expect something much longer have a sense that… When I had Ege and Tamay on my podcast, they were like, "Look, you could look at AlphaGo, and say, 'Oh, this is a model that can do exploration. AlphaZero can generalize to new video games. It has all these priors about how to engage with the world, and so forth." Sholto Douglas And the intellectual ceiling is really high. Dwarkesh Patel Yeah, exactly. In retrospect, obviously a bunch of the methods are still used today in deep learning, and you can see similar things in the models that we train today. But it was fundamentally not a baby AGI that we just had to add a little sprinkle of something else on top of in order to make it the LLMs of today. I just want to very directly address this crux of, why are LLMs in a much different position of respect to true AGI than AlphaZero? Why are they actually the base on which adding in a few extra drops of this kind of care, and attention gets us to human-level intelligence? Sholto Douglas I think one important point is that when you look at AlphaZero, it does have all of those ingredients. In particular I think the intellectual ceiling goes quite—contra what I was saying before, which is we've demonstrated this incredible complexity of math, and programming problems… I do think that the type of task and setting that AlphaZero worked in this two-player perfect information game basically is incredibly friendly to RL algorithms. The reason it took so long to get to a more proto-AGI style models is you do need to crack that general conceptual understanding of the world, and language, and this kind of stuff, and you need to get the initial reward signal on tasks that you care about in the real world, which are harder to specify than games. I think then that sort of gradient signal that comes from the real world, all of a sudden you get access to it, and you can start climbing it, whereas Alpha Zero didn't ever have the first rung to pull on. Trenton Bricken Yeah, yeah. This goes back to the monkeys on the typewriter and the pre-training model. Until you had something like GPT-3/GPT-4, it just couldn't generate coherent enough sentences to even begin to do RLHF, and tell it what you liked and didn't like. Dwarkesh Patel Yeah. If we don't have even reasonably robust, or weakly robust computer use agents by this time next year, are we living in the bust timeline as in “2030, or bust”? Sholto Douglas I would be extremely surprised if that was the case. I think that would be somewhat of an update towards, there's something strangely difficult about this computer use in particular. I don't know if it's the bust timeline, but it's definitely the, I would update on this being lengthening of timeline. Trenton Bricken I think more and more it's no longer a question of speculation. If people are skeptical, I'd encourage using Claude Code, or some agentic tool like it and just seeing what the current level of capabilities are. Dwarkesh Patel Tweeting is so much easier. Trenton Bricken But seriously, the models are getting really capable at tasks that we care about, and we can give them enough data for. The circuit's results from interpretability are also pointing in the direction that they're doing very reasonable generalizable things. This question matters a lot, but I'm surprised by how many deep learning critics just haven't really interacted with the models, or haven't in a while. Sholto Douglas And constantly move the goalposts. Trenton Bricken The Turing test used to be a thing. We don't even talk about it, and it'd be silly to think that it was a meaningful test. Sholto Douglas Now one caveat on that is if software engineering is just dramatically better than computer use and computer use still sucks, then I'd still be like, “oh, maybe everyone just kept focused on software engineering.” It was just by far the most valuable thing, every marginal person and dollar went towards software engineering. I don't think that's the case. I do think computer use is valuable enough that people will care about it. That's my one escape patch that I'm putting in place for next year. Dwarkesh Patel Yeah, it would be good from an alignment perspective, too. Because I think you kind of do need a wider range of skills before you can do something super scary. Sholto Douglas Like if the models didn't get any better? Dwarkesh Patel Yeah, if they're superhuman coders, but they're not Henry Kissinger level… Trenton Bricken I don't know. That seems okay. If we have AI oracles. Dwarkesh Patel Yeah, that's what I'm saying. That's good. Dwarkesh Patel If you look back at AI discourse going back a decade, there's a sense that there's dumb AI, then there's AGI, then there's ASI, that intelligence is the scalar value. The way you've been talking about these models has a sense of jaggedness. It's especially tuned to environments in which it's been trained a lot or has a lot of data. Is there a sense in which it still makes sense to talk about the general intelligence of these models? Is there enough meta learning and transfer learning that is distinguished between the sizes of models or the way models are trained? Or are we moving into a regime where it's not about intelligence, it's more so about domain? Sholto Douglas One intuition pump is that this conversation was had a lot when models were GPT-2 sized and fine-tuned for various things. People would find that the models were dramatically better at things that they were fine-tuned for. But by the time you get to GPT-4, when it's trained on a wide enough variety of things with the total compute, it generalized very well across all of the individual sub-tasks. And it actually generalized better than smaller fine-tuned models in a way that was extremely useful. I think right now what we're seeing with RL is pretty much the same story playing out. There's this jaggedness of things that they're particularly trained at. But as we expand the total amount of compute that we do RL with, you'll start to see the same transition from GPT-2 fine-tunes to GPT-3, GPT-4, unsupervised meta learning and generalization across things. I think we're already seeing early evidence of this in its ability to generalize reasoning to things. But I think this will be extremely obvious soon. Trenton Bricken One nice example of this is just the ability or notion to backtrack. You go down one solution path, "Oh, wait, let me try another one." And this is something that you start to see emerge in the models through RL training on harder tasks. I think right now, it's not generalizing incredibly well. Sholto Douglas Well, I mean have we ever RL'd the model to be an interp agent? No. Trenton Bricken I mean, no. Yeah, exactly. Sholto Douglas So all this time we're talking about, “oh, it's only good at things it’s been RL’d for”. Well, it's pretty good at that because that is a mixture of science and understanding language and coding. There's this sort of mixture of domains here, all of which you need to understand. You need to be both a great software engineer and be able to think through language and state of mind and almost philosophize in some respects to be an interpret agent. And it is generalizing from the training to do that. Dwarkesh Patel What's the end game here? Claude 8 comes out and they give it to you and dot, dot, dot, you say, "thumbs up." What's happened? What have you learned? Trenton Bricken Yeah. I mean, it really depends upon the timeline at which we get Claude 8 and the models hit like ASL-4 capabilities, right? Fundamentally, we're just going to use whatever tools we have at the time and see how well they work. Ideally, we have this enumerative safety case where we can almost verify or prove that the model will behave in particular ways. In the worst case, we use the current tools like when we won the auditing game of seeing what features are active when the assistant tag lights off. Dwarkesh Patel Can you back up? Can you explain, what is mechanistic interpretability? What are features? What are circuits? Trenton Bricken Totally. Mechanistic interpretability—or the cool kids call it mech interp—is trying to reverse engineer neural networks and figure out what the core units of computation are. Lots of people think that because we made neural networks, because they're artificial intelligence, we have a perfect understanding of how they work. It couldn't be further from the truth. Neural networks, AI models that you use today, are grown, not built. So, we then need to do a lot of work after they're trained to figure out to the best of our abilities how they're actually going about their reasoning. And so, three and a half years ago, this kind of agenda of applying mechanistic interpretability to large language models started with Chris Olah leaving OpenAI, co-founding Anthropic. And every roughly six months since then, we've had a major breakthrough in our understanding of these models. And so first with toy models of superposition, we established that models are really trying to cram as much information as they possibly can into their weights. And this goes directly against people saying that neural networks are over-parameterized. In classic AI machine learning back in the day, you would use linear regression or something like it, and people had a meme of AI, or neural networks, deep learning, using way too many parameters. There's this funny meme that you should show of layers on the X axis and layers on the Y axis and this jiggly line that just goes up and it's like, "Oh, just throw more layers at it." But it actually turns out that, at least for really hard tasks like being able to accurately predict the next token for the entire internet, these models just don't have enough capacity. And so they need to cram in as much as they can. And the way they learn to do that is to use each of their neurons, or units of computation in the model, for lots of different things. And so if you try to make sense of the model and be like, "Oh, if I remove this one neuron," what is it doing in the model? It's impossible to make sense of it. It'll fire for like Chinese and fishing and horses and, I don't know, just like a hundred different things. And it's because it's trying to juggle all these tasks and use the same neuron to do it. So that's superposition. Nine months later, we write Towards Monosemanticity, which introduces what are called sparse autoencoders. And so going off what I just said of the model trying to cram too much into too little space, we give it more space, this higher dimensional representation, where it can then more cleanly represent all of the concepts that it's understanding. And, and this was a very toy paper in so much as it was a two layer, really small, really dumb transformer. And we fit up to 16,000 features, which we thought was a ton at the time. Fast-forward nine months, we go from a two layer transformer to our Claude 3 Sonnet, frontier model at the time, and fit up to 30 million features. And this is where we start to find really interesting abstract concepts, like a feature that would fire for code vulnerabilities. And it wouldn't just fire for code vulnerabilities. It would even fire for like, you know that Chrome page you get if it's not an HTTPS URL, like "Warning, this site might be dangerous. Click to continue." And also fire for that, for example. And so it's like these much more abstract coding variables or sentiment features, amongst the 30 million. Fast-forward nine months from that, and now we have circuits. And I threw in the analogy earlier of the Ocean 11 heist team, where now you're identifying individual features across the layers of the model that are all working together to perform some complicated task. And you can get a much better idea of how it's actually doing the reasoning and coming to decisions, like with the medical diagnostics. One example I didn't talk about before with how the model retrieves facts: So you say, "What sport did Michael Jordan play?" And not only can you see it hop from like Michael Jordan to basketball and answer basketball. But the model also has an awareness of when it doesn't know the answer to a fact. And so, by default, it will actually say, "I don't know the answer to this question." But if it sees something that it does know the answer to, it will inhibit the "I don't know" circuit and then reply with the circuit that it actually has the answer to. So, for example, if you ask it, "Who is Michael Batkin?" —which is just a made-up fictional person— it will by default just say, "I don't know." It's only with Michael Jordan or someone else that it will then inhibit the "I don't know" circuit. But what's really interesting here and where you can start making downstream predictions or reasoning about the model, is that the "I don't know" circuit is only on the name of the person. And so, in the paper we also ask it, "What paper did Andrej Karpathy write?" And so it recognizes the name Andrej Karpathy, because he's sufficiently famous, so that turns off the "I don't know" reply. But then when it comes time for the model to say what paper it worked on, it doesn't actually know any of his papers, and so then it needs to make something up. And so you can see different components and different circuits all interacting at the same time to lead to this final answer. Dwarkesh Patel Why think it's a tractable problem to understand every single thing that's happening in a model? Or like that's the best way to understand why it's being deceptive. If you wanted to explain why England won World War II using particle physics, you would just be on the wrong track. You just want to look at the high-level explanations of, who had more weapons? What did they want? That seems analogous to just training linear probes for like, are you honest? Are you being deceptive? Do we catch you doing bad things when we're red teaming you? Can we monitor you? Why is this not analogous where we're asking a particle physicist to just backtrack and explain why England won World War II? Trenton Bricken I feel like you just want to go in with your eyes wide open, not making any assumptions for what that deception is going to look like, or what the trigger might be. The wider you can cast that net, the better. Depending on how quickly AI accelerates and where the state of our tools are, we might not be in the place where we can prove from the ground up that everything is safe. But I feel like that's a very good North Star. It's a very powerful reassuring North Star for us to aim for, especially when we consider we are part of the broader AI safety portfolio. I mean, do you really trust—you're about to deploy this system and you really hope it's aligned with humanity—that you've successfully iterated through all the possible ways that it's going to scheme or sandbag or… Dwarkesh Patel But that's also probably going to be true with whatever you find. You're still going to have variants that you haven't explained. Or you found a feature, but you don't know if it actually explains deception or something else instead. Trenton Bricken First of all, I'm not saying you shouldn't try the probing approach. We want to pursue the entire portfolio. We've got the therapist interrogating the patient by asking, "Do you have any troubling thoughts?" We've got the linear probe, which I'd analogize to a polygraph test where we're taking very high level summary statistics of the person's well-being. Then we've got the neurosurgeons going in and seeing if you can find any brain components that are activating and troubling or off-distribution ways. I think we should do all of it. Dwarkesh Patel What percent of the alignment portfolio should mech interp be? Trenton Bricken I think as much of a chunk as is necessary. It’s hard to define. At Anthropic, I feel like all of the different portfolios are being very well-supported and growing. Sholto Douglas Coming back to the World War II question, you can think of it as a hierarchy of abstractions of trust here, where let's say you want to go and talk to Churchill. It helps a lot if you can verify that in that conversation, in that 10 minutes, he's being honest. This enables you to construct better meta narratives of what's going on. So maybe particle physics wouldn't help you there, but certainly the neuroscience of Churchill's brain would help you verify that he was being trustworthy in that conversation and that the soldiers on the front lines were being honest in their depiction of their description of what happened, this kind of stuff. So long as you can verify parts of the tree up, then that massively helps you build confidence. Trenton Bricken I think language models are also just really weird. With the emergent misalignment work. I don't know if they took predictions, they should have of like, "Hey, I'm going to fine tune ChatGPT on code vulnerabilities. Is it going to become a Nazi?" I think most people would've said no. That's what happened. Dwarkesh Patel How did they discover that it became a Nazi? Trenton Bricken They started asking it a ton of different questions and it will do all sorts of vile and harmful things. The whole persona just totally changes. We are dealing with alien brains here who don't have the social norms of humans. We don’t even have a clear notion of what they have and haven't learned. I think you really want to go into this with eyes wide open. Dwarkesh Patel Backing up from mech interp, if we live in a world where AI progress accelerates… By the way, you were mentioning a little while ago that there's many wild worlds we could be living in, but we're living in at least one of them. Another one that we've gestured at but it's worth making more explicit, is this. Even if the AI models are not helping write the next training algorithm for their successor, just the fact that if they had human level learning efficiency—whatever copy of the model is learning on the job—the whole model is learning. So in effect, it's getting– Sholto Douglas Or even if they're like a thousand times less efficient than humans are at learning and you deployed them. Even still. Dwarkesh Patel Exactly. Anyways, there's a whole bunch of other things that you can think of about it. But even there, you kind of have a broadly deployed intelligence explosion. Sholto Douglas I do think it's worth pressing on that future. There is this whole spectrum of crazy futures. But the one that I feel we're almost guaranteed to get—this is a strong statement to make—is one where at the very least, you get a drop-in white collar worker at some point in the next five years. I think it's very likely in two, but it seems almost overdetermined in five. On the grand scheme of things, those are kind of irrelevant timeframes. It's the same either way. That completely changes the world over the next decade. If we don't have the right policies in place for that, then you end up actually with in some respects, almost a fundamentally worse world. Because the thing that these models get good at by default is software engineering and computer using agents and this kind of stuff. Then we will need to put in extra effort to put them in the loops where they help us with scientific research. Or we have the right robotics, such that we actually experience an increase in material quality of life. That's worth thinking about. If you're in the perspective of like, “I'm a country, what should I be doing or thinking about?” Plan for the case where white collar work is automateable. And then consider, what does that mean for your economy? What you should be doing to prepare policy? Dwarkesh Patel What should you be doing to prepare? Because honestly, this is such a tough question if you're India or Nigeria or Australia. If you're a country unlike America or China where they do have frontier models, what is it that you should be doing right now? Especially on such a short timescale. Sholto Douglas I think one very important point is that let's say this scenario turns out true. Then compute becomes the most valuable resource in the world. The GDP of your economy is dramatically affected by how much compute you can deploy towards the organizations within your country. So having some guaranteed amount of compute I think will actually be quite important. Getting ahead of investments, and data centers, and this kind of stuff on the condition that it's companies in your country have to be allowed to use that compute, not necessarily for training but just even just for inference. I think the economic value here comes from inference. I think it also makes sense to invest broadly in AI. These countries have the opportunity to do so and that's a portfolio of foundation model companies but also robotics, supply chain, and this kind of stuff. I think that you should invest very proactively in policies that try to prevent capital lock-in. We're in for a much worse world if it just so happens that the people who had money in the stock exchange, or in land before AGI are dramatically more wealthy than the people who don't. It's a gross misallocation of resources. One of my favorite episodes actually on your podcast was the Georgism one where you're trying to appropriately value, or allocate land. This strikes particularly close to home coming from Australia where I think our policies with respect to land are grossly wrong. But I think this is broadly true. Being very forward on regulation of integration of these models into your country is important, and proactively making sure that people have choice. Let's say you should be quite proactive about making sure that the phones, or devices, or the glasses that people have, people have free choice on what things they run. So we just get the white collar worker, and you're trying to do the best to prepare your country for that. Then what can you do to make all possible versions of the future go well? That's covering some amount of economic downside. The other things that I think are really important is figure out how you can basically ensure dramatic upside, or cover terrible downside. Getting a dramatic upside is making sure that there is investment in biology research and this kind of stuff in an automated way such that these models are actually able to produce novel medicines that massively improve our quality of life. Covering the downside is AI alignment research, and this kind of stuff, and automated testing, and really thinking hard about that, AI safety institutes and this kind of stuff. Dwarkesh Patel But these seem like things that a random rich person could also do. It seems like there's not a thing that a nation state is uniquely equipped to do in this scenario. Sholto Douglas That's a good point. Sholto Douglas I mean dramatic allocation of resource towards compute I think is sensible. I would be doing that if I was in charge of a nation state. I think it just increases your optionality in most of the future worlds. Trenton Bricken Dylan Patel has some scary forecasts on US energy. Sholto Douglas Versus China. Yes. Trenton Bricken Yeah, we're like 34 gigawatts off. Sholto Douglas Yeah, the US's line is flat, basically, and China's line is like this. And I mean the US very clearly... Trenton Bricken We just need so many more power plants. Sholto Douglas Yes. If intelligence becomes this incredibly valuable input, intelligence becomes almost a raw input into the economies and quality of life of the future, the thing directly underneath that is energy. Making sure that you have incredible amounts of solar, like tile the desert with solar panels, some parts of the desert. That would be helpful towards making sure that you have more access to intelligence on tap. Trenton Bricken Yeah. Just to make it explicit, we've been touching on it here. Even if AI progress totally stalls, you think that the models are really spiky, and they don't have general intelligence. It's so economically valuable, and sufficiently easy to collect data on all of these different jobs, these white collar job tasks, such that to Sholto's point we should expect to see them automated within the next five years. Sholto Douglas Yeah. Trenton Bricken Even if you need to hand spoon every single task to the model. Sholto Douglas It's economically worthwhile to do so. Even if algorithmic progress stalls out, and we just never figure out how to keep progress going—which I don't think is the case, that hasn't stalled out yet, it seems to be going great—the current suite of algorithms are sufficient to automate white collar work provided you have enough of the right kinds of data. Compared to the TAM of salaries for all of those kinds of work, it is so trivially worthwhile. Trenton Bricken Yeah, exactly. I do just want to flag as well that there's a really dystopian future if you take Moravec’s paradox to its extreme. It’s this paradox where we think that the most valuable things that humans can do are the smartest things like adding large numbers in our heads, or doing any sort of white collar work. We totally take for granted our fine motor skill, and coordination. But from an evolutionary perspective it's the opposite. Evolution has optimized fine motor coordination so well. Even if you look at robot hands, the ability to open a door is still just really hard for robots. Meanwhile, we're seeing this total automation of coding, and everything else that we've seen as clever. The really scary future is one in which AIs can do everything except for the physical robotic tasks, in which case you'll have humans with AirPods, and... Sholto Douglas Glasses? Trenton Bricken Glasses, and there'll be some robot overlord controlling the human through cameras by just telling it what to do, and having a bounding box around the thing you're supposed to pick up. So you have human meat robots. Sholto Douglas Not necessarily saying that that's what the AIs would want to do, or anything like that. But if you were to be like, "What are the relative economic value of things?" The AIs are out there doing computer programming, and the most valuable thing that humans can do is be amazing robots. Now that being said, I think Moravec’s paradox is a little bit fake. I think the main reason that robots are worse at being a robot than they are at software engineering is the internet exists for software engineering. GitHub exists, and there is no equivalent thing if you had all mocap of everyone's actions as they were going about their daily lives for some reasonable fraction of the human population, robotics is also close to solved, on track to be solved at the same rate that software engineering is on track to be solved. So, this vision is only a sort of decade-long section, but it's still a pretty terrible decade. Imagine the world where people have lost their jobs, you haven't yet got novel biological research. That means people's quality of life isn’t dramatically better. You don't yet have material abundance because you haven't actually been able to action the physical world in the necessary way. You can't build dramatically more, because building dramatically more takes robots basically, and people's main comparative advantage is as fantastic robots. That’s a shocking, shocking world. Dwarkesh Patel Yeah. From the perspective of an average human, I think it actually might be better. Your wages will be higher because you're the complement to something that is enormously valuable which is AI labor. Sholto Douglas And a decade, or two after, the world is fantastic. Robotics is solved, and you start to get radical abundance basically provided that you have all the policies set up necessary to permit building. You end up with that same change like the before vs. after photos of Shanghai where 20 years on, it's this dramatically transformed city. A lot of places in the world probably end up like that over that two-decade period. But we need to do our best to estimate if this is actually what is on track to happen. Build SWE-bench, but for all the other forms of white collar work, and measure, and track. That's a great thing that governments should be doing by the way, trying to break down the functions of their economy into measurable tasks, and figuring out what does the curve actually look like for that? They might be a bit shocked by the progress there. There's no SWE-bench for a tax eval. I don't have all the answers here, but then we need to figure out a way to share the proceeds of this economy broadly across people, or invest heavily in robotics, and collect the data so that we get robotics faster, and we get material abundance faster. Invest in biological research that we get, but all that faster. Basically try and pull forward the radical upside, because otherwise you have a pretty dark section. Dwarkesh Patel I think one thing that's not appreciated enough is how much of our leverage on the future—given the fact that our labor isn't going to be worth that much—comes from our economic, and political systems surviving. For your million X'd S&P equity to mean something, for your contracts to mean anything, for the government to be able to tax the AI labor, and give you a UBI off of that, that requires our legal institutions, our economic institutions, our financial rails surviving into the future. Sholto Douglas Yes. Dwarkesh Patel The way in which that likely happens is if it's also in the AIs best interests that they follow those rails. By AI I don't mean some monolithic single AI, I just mean firms which are employing AI, and becoming more productive as a result. You don't want to be in a position where it's so onerous to operate in our system that you're basically selecting for firms who either emigrate, or who are doing black market stuff, et cetera. You want to make it super, super easy to deploy AI, have the equivalent of special economic zones, et cetera. Otherwise you are just surrendering the future outside of any control that you might have on it. One of the reasons that I worry about turning AGI into a national security issue, or having it have extremely close ties with the government, the Manhattan Project thing, is that it disproportionately redirects the use of AI towards military tech, mosquito drones and whatever. It also naturally puts other countries in the same frame of mind. If we're developing the mosquito drones, why would China not develop the mosquito drones? That just seems like a zero-sum race, and not to mention a potentially catastrophic one. Whereas compute will be limited, we'll need to disproportionately accelerate some things. To the extent it just remains totally like a consumer free market landscape, it just seems more likely that we'll get the glorious transhumanist future where they're developing the things that make human life better. Sholto Douglas Yes, I mean I agree. The case where you end up with two national projects facing off against each other is dramatically worse. We don't want to live in that world. It's much better if this stays a free market, so to speak. Dwarkesh Patel Okay. I want to take issue with your claim that even with the algorithms of today, if we just collect enough data that we could automate white collar work. First, let me get an understanding of what you mean by that. Do you mean that we would do the analogous thing of pre-raining with all the trajectories of everything people would do on their jobs? Could you make either manually, or through some other process, some RL procedure based on the screen recordings, every white collar worker. What kind of thing are you imagining? Sholto Douglas I mean a continuous distribution of this stuff. One important mental model to think about RL… There is some respect with which longer horizon, if you can do them, if you can get that reward ever, are easier to judge. Again, it's come back to that can you make money on the internet? That's an incredibly easy reward signal to judge. But to do that there's a whole hierarchy of complex behavior. So, if you could pre-train up to the easy to judge reward signals, does your website work? Does it go down, do people like it? There's all these reward signals that we can respond to because we have a long, we can progress through these long enough trajectories to actually get to interesting things. If you're stuck in this regime where you need a reward signal every five tokens, it's a way more painful, and long process. But if you could pre-train on every screen in America, then probably the RL tasks that you can design are very different from if you could only take the existing internet as it is today. How much of that you get access to changes the mix. Dwarkesh Patel As we're training them on longer, and longer horizon tasks, and it takes longer for them to get any signal on whether they successfully complete the task, will that slow down progress because it takes more compute per task? Trenton Bricken I do think there's this notion that the longer, the harder tasks, the more training is required. I'm sympathetic to that naively, but we as humans are very good at practicing the hard parts of tasks, and decomposing them. I think once models get good enough at the basic stuff, they can just rehearse, or fast-forward to the more difficult parts. Sholto Douglas I mean that's definitely one of the big complexities. As you use more compute, and as you train on more, and more difficult tasks, your rate of improvement of biology for example is going to be somewhat bound by the time it takes a cell to grow in a way that your rate of improvement on math isn't, for example. So, yes, but I think for many things we'll be able to parallelize widely enough, and get enough iteration loops. Dwarkesh Patel Will the regime of training new models go away? Will we eventually get to the point where you've got the model, and then you just keep adding more skills to it, with RL training? Sholto Douglas That depends on whether, or not you think there's a virtue in pre-training a new architecture. Basically you make some architectural change, then you probably need to do some form of at least pretraining a new model. Dwarkesh Patel If RL requires a bunch of inference to do the training in the first place, does that push against the thing you were talking about where we actually need a bigger model in order to have brain-like energy? But then also it's more expensive to train it in RL. So, where does that balance out? Trenton Bricken I think we got to drink the bitter lesson here. Yeah, there aren't infinite shortcuts. You do just have to scale and have a bigger model, and pay more inference for it. If you want AGI, then that's what you got to pay the price of. Sholto Douglas But there's a tradeoff equation here. There is science to do which everyone is doing. What is the optimal point at which to do RL? Because you need something which can both learn, and discover the sparse reward itself. So you don't want a one parameter model. Useless, even though you can run it really fast. You also don't want a 100T model. It's super slow. The marginal benefit of its learning efficiency is not worth it. So, there's a Pareto frontier here. What's the optimal model size of your current class of capabilities, and your current set of RL environments, and this kind of stuff. Trenton Bricken And even in the last year there's been much more of a factor of the inference cost. So, just explicitly the bigger the model, the more expensive it is to do a forward pass and generate tokens. The calculus used to just be, “Should I allocate my flops to more training data, or a bigger model?” And now another huge factor is how much am I actually going to do forward passes on this model once it's trained? Sholto Douglas My total pool of compute, how do I allocate that across training data compute, and inference compute for the RL training. Trenton Bricken And then even within inference, there's all this research on, well, what strategy should I use? Should I sample 10, and take the best? Do I do this sort of branching search, et cetera, et cetera. And so with RL where you're sampling a whole lot of tokens, you also need to factor in the ability for the model to actually generate those tokens, and then learn, and get feedback. Dwarkesh Patel If we're living in this world, what is your advice to somebody early in their career, or a student in college? What should they be planning on doing? Sholto Douglas Once again, it's worth considering the spectrum of possible worlds and preparing yourself for that. The action that I think is the highest EV in that case is that at a minimum you're about to get dramatically more leverage. You already have. Already the startups in YC are writing huge amounts of their code with Claude. What challenges, what causes do you want to change in the world with that added leverage? If you had 10 engineers at your beck, and call, what would you do? If you had a company at your beck and call, what would that enable you to do? What problems, and domains suddenly become tractable? That's the world you want to prepare for. Now, that still requires a lot of technical depth. Obviously there is the case where AI just becomes dramatically better than everyone at everything, but for at least a while there is… I think Jensen actually talked about this in an interview in an interesting way. He's like, "I have a hundred thousand general intelligences around me, and I'm still somewhat useful, because I’m there directing the values, and asking them to do things. I still have value even though I have a hundred thousand general intelligences." For many people, I think that will still be true for a fair while. Then as the AIs get better, and better, and better, and so on, eventually, no. But again, prepare for the spectrum of possible worlds because in the event where we're just totally outcompeted, it doesn't matter what you do. In all the other worlds, it matters a lot. Get the technical depth, study biology, study CS, study physics. Think hard about what challenges you want to solve in the world. Dwarkesh Patel Yeah, that's a lot of topics. That's a lot of shit. Sholto Douglas You can now. You can. It's so much easier to learn. Everyone now has the infinite perfect tutor. Dwarkesh Patel It's definitely been helpful to me. Trenton Bricken I would say some combination of: get rid of the sunk cost of your previous workflows, or expertise in order to evaluate what AI can do for you. Another way to put this, which is fun, is just be lazier in so much as you figure out the way that the agent can do the things that are toilsome. Ultimately, you get to be lazier, but in the short run, you need to critically think about the things you're currently doing, and what an AI could actually be better at doing, and then go, and try it, or explore it. Because I think there's still just a lot of low-hanging fruit of people assuming, and not writing the full prompt, giving a few examples, connecting the right tools for your work to be accelerated and automated. Dwarkesh Patel Yep, yep. There's also the sunk cost of feeling like since you're not "early to AI", that you've sort of missed the boat. I remember when GPT-3 came out… So backstory on the podcast, when I graduated college I was planning on doing some sort of AI wrapper startup, and the podcast was just a gateway into doing that. I was trying out different things and at the time I remember thinking, “oh, 3.5 is out.” People were like, "I'm so behind on the startup scene here” or whatever. If I wanted to make my own wrapper… maybe the idea of the wrapper was inadvisable in the first place. But every time feels early because if it's an exponentially growing process, and there are many things, many ideas which are only becoming possible now, right? Sholto Douglas Exactly. It's that product exponential I talked about. Dwarkesh Patel That's right. Sholto Douglas Products literally obsolete it. You need to constantly reinvent yourself to stay at the frontier of capabilities. Dwarkesh Patel Do you remember? I had a really shitty idea, and I gave you a call. Sholto Douglas I don’t remember what it was. Dwarkesh Patel I think it was like RAG for lawyers, or something. Anyways, I think one of our first interactions was like, "Hey, what do you think of this idea?" And you were like, “I think the podcast sounds promising.” Sholto Douglas I was right. Dwarkesh Patel Which I appreciate. Trenton Bricken Yeah. I got slightly annoyed at a friend recently who I think is really talented and clever and interested in AI but has pursued a biology route. I just kind of tried to shake them like, "You can work on AI if you want to." Humans are biological general intelligences where a lot of the things of value are just very general. Whatever kind of specialization that you've done maybe just doesn't matter that much. Again, it gets back to the sunk cost, but so many of the people, even my colleagues at Anthropic are excited about AI. They just don't let their previous career be a blocker. Because they're just innately smart, talented, driven, whatever else, they end up being very successful and finding roles. It's not as if they were in AI forever. I mean, people have come from totally different fields. Don't think that you need permission from some abstract entity to get involved, and apply, and be able to contribute. Dwarkesh Patel If somebody wanted to be an AI researcher right now, if you could give them an open problem, or the kind of open problem that is very likely to be quite impressive, what would it be? Sholto Douglas I think that now that RL's come back, papers building on Andy Jones's “Scaling scaling laws for board games” are interesting. Investigating these questions like the ones you asked before. Is the model actually learning to do more than its previous pass at K? Or is it just discovering that… Exploring questions like that deeply are interesting, scaling laws for RL, basically. Dwarkesh Patel I'd be very curious to see how much the marginal increase is in meta learning from a new task, or something. Trenton Bricken On that note, I think model diffing has a bunch of opportunities. People say, "Oh, we're not capturing all the features. There's all this stuff left on the table." What is that stuff that's left on the table? If the model's jailbroken, is it using existing features that you've identified? Is it only using the error terms that you haven't captured? I don't know. There's a lot here. I think MATS is great. The Anthropic fellowship has been going really well. Goodfire, Anthropic invested in recently, they're doing a lot of interpretability work, or just apply directly to us. Dwarkesh Patel Anything to get your equity up, huh? Trenton Bricken There's just so many interpretability projects. There's so much low-hanging fruit, and we need more people, and I don't think we have much time. Sholto Douglas I also want to make a plug for performance engineering. This is one of the best ways to demonstrate that you have the raw ability to do it. If you made an extremely efficient transform implementation on TPU, or Trainium, or Incuda, then I think there's a pretty high likelihood that you'll get a job offer. But there's a relatively small pool of people that you can trust to completely own end-to-end the performance of a model. Trenton Bricken And if you have broad, deep electrical engineering skills, I think you can probably come up to speed pretty fast on accelerator stuff. Sholto Douglas You can come up to speed reasonably fast and it teaches you a lot of good intuitions of the actual intricacies of what's going on in the models, which means that you're then very well-placed to think about architecture and this kind of stuff. One of my favorite people in thinking about architecture at Anthropic at the moment actually came from a heavy GPU kernel programming background and just knows the ins, and outs really deeply. He can think about the trade-offs really well. Dwarkesh Patel This was fun guys. Thanks for doing it again. Trenton Bricken Great to be back. Ready for more?
--------------------------------------------------

Title: Top-deck churn at Tata Digital; Rapido’s cash burn rises
URL: https://economictimes.indiatimes.com/tech/newsletters/morning-dispatch/top-deck-churn-at-tata-digital-rapidos-cash-burn-rises/articleshow/121349330.cms
Time Published: 2025-05-23T01:28:04Z
Full Content:
Want this newsletter delivered to your inbox? Updated On May 23, 2025, 07:44 AM IST Want this newsletter delivered to your inbox? Thank you for subscribing to Morning DispatchWe'll soon meet in your inbox. Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Definitions Top Commodities Top Prime Articles Top Slideshow Top Story Listing Top Market Pages Latest News Follow us on:
--------------------------------------------------

Title: 10 Ways to Enrich the Trumps and the MAGA Movement
URL: https://www.motherjones.com/politics/2025/05/donald-trump-qatar-747-bezos-amazon-facebook-cbs-crypto-golf/
Time Published: 2025-05-22T16:34:00Z
Full Content:
Russ Choma, Dan Friedman, and Tim MurphyMay 22, 2025 Illustrations by James Clapham Illustrations by James Clapham Eight years ago, if you wanted to line the first family’s pockets, your choices—while ethically unprecedented—were limited. There was only so much you could spend on hotel rooms or Mar-a-Lago fundraisers. Fortunately for foreign governments, megacorporations, and anyone else with an agenda, the Trumps have diversified. “It’s a whole different scale than it was in 2017,” observes Noah Bookbinder, president of Citizens for Responsibility and Ethics in Washington, who says President Donald Trump and his underlings “seem to believe that the American people don’t care. They’re not making much of an effort to hide any of this stuff.” A post shared by Mother Jones (@motherjonesmag) The options for paying tribute to the president, his kin, and the MAGA movement are now legion. There are even exciting new opportunities to protect your business by reaching a “settlement” with the leader of the free world. Or you can just hand over a 747 for him to use as Air Force One, as Qatar did. Best of all, these creative funding methods might not even be against the law—especially for Trump. (Thanks, SCOTUS!) So here’s your guide to participating in the brave new Trumpworld of executive enrichment. As the “Bitcoin orange” Trump-branded watches ($799) indicate, the 47th commander in chief is the “Crypto President.” Last year, Trump rolled out his own crypto firm, called World Liberty Financial. After Trump won, crypto investor Justin Sun bought $75 million worth of WLF tokens, with more than $50 million of that potentially going to the Trumps, according to Bloomberg. Shortly after Trump took office, the Securities and Exchange Commission—which had accused Sun of fraud in a federal complaint—agreed to pause its lawsuit while the parties pursued a “potential resolution.” That was one of more than a dozen lawsuits and investigations targeting crypto firms that the SEC reportedly backed off from after Trump took office. Foreign governments can get in on the action, too. A state-backed United Arab Emirates firm, for instance, recently agreed to use a Trump-affiliated digital coin in a multibillion-dollar deal—a move that, according to the New York Times, could hand the Trump family hundreds of millions of dollars. Savvy investors can also purchase $TRUMP, a meme coin the president launched just before his inauguration. A billion $TRUMP coins will ultimately be made available, though a company affiliated with the Trumps will own many of them—so if demand causes the price to rise, the Trumps theoretically get wealthier. The coin’s value rocketed past $70 before inauguration, then immediately crashed. But in April, it ticked up to more than $15—after the coin’s official website offered dinner with Trump at his DC-area golf club and a “VIP White House Tour” to the top $TRUMP holders. (The White House tour offer quickly disappeared following an uproar.) This week, Sun identified himself as the top $TRUMP investor. He declared his excitement to attend the president’s “Gala Dinner as his TOP fan” and “discuss the future of our industry.” Sun now holds $18.6 million worth of $TRUMP, according to CNBC. If you can’t match that, don’t worry: $MELANIA coins go for less than a buck. This is a simple one—you don’t even have to invest in $TRUMP coins. Business leaders can now simply pay $5 million for one-on-one meetings with Trump at Mar-a-Lago, Wired reported. If that’s too rich for your blood, there’s a cheaper option—donors can pony up $1 million each for a small-group “candlelight dinner” with the president. “This exorbitant level of payment for presidential access raises serious concerns about the possibility of corruption by candlelight,” Jon Golinger, a transparency advocate with Public Citizen, told USA Today. For those who recall the five-alarm scandal generated in the ’90s when the Clinton administration rewarded major campaign donors with overnight stays in the Lincoln Bedroom—well, clearly, that was a more innocent time. Trump’s ethical outrages are so numerous and brazen that these meals have barely registered a blip. The dinners are organized by MAGA Inc., a super-PAC that backed Trump’s 2024 campaign—and that also has spent tens of millions of dollars to cover Trump’s personal legal expenses. A “source with direct knowledge of the dinners,” however, told Wired that “it’s all going to” Trump’s future presidential library. Then again, the $400 million Qatari jet supposedly will, too. Trump has turned his vendettas against law firms that cross him into executive orders that threaten their ability to operate. When firms have fought back, judges have blocked Trump’s “unprecedented attack” on the rule of law. But some of the country’s wealthiest firms have made a different choice—accepting humiliating “settlements” with the president and promising tens of millions of dollars in free legal work for causes he supports. Paul Weiss was the first to capitulate, agreeing to provide $40 million in pro bono work in support of what Trump described as “the Administration’s initiatives.” From there, the price skyrocketed. Skadden inked a preemptive deal for at least $100 million in free legal work. Then five more firms cut deals for $100 million to $125 million apiece. Trump hopes to “use these very prestigious firms to help us out” on projects ranging from his trade war to protecting the coal industry. “Big Law continues to bend the knee to President Trump because they know they were wrong, and he looks forward to putting their pro bono legal concessions toward implementing his America First agenda,” Trump’s press secretary, Karoline Leavitt, has said. For years, Trump has filed lawsuits over news coverage he dislikes. Many of these cases have been thrown out of court. But after the 2024 election, some of the world’s most powerful media companies decided to fork over millions of dollars rather than fight back. It began in December, when Disney agreed to give $15 million to Trump’s presidential library foundation to settle a dubious defamation claim against ABC News. Then Meta, Facebook’s parent company, agreed to pay its own $25 million library tribute to settle a lawsuit Trump brought after the platform banned him following the January 6 insurrection. “It looks like a bribe and a signal to every company that corruption is the name of the game,” Sen. Elizabeth Warren tweeted. X—now controlled by Elon Musk—soon followed, settling a similar case for $10 million. Paramount executives reportedly believe that settling a Trump lawsuit against CBS, which it owns, would ease the path to a lucrative merger with Skydance. Paramount, which so far hasn’t surrendered, called the case an “affront to the First Amendment…without basis in law or fact” in a March court filing. Trump is also leveraging his control of the Federal Communication Commission to demand “the maximum fines and punishment” for CBS. “They should lose their license!” he declared. In a letter to Paramount’s chairperson Monday, Warren and other lawmakers charged that the company appears to be trying to settle the lawsuit and “moderating the content of its programs” in an effort to win FCC approval for the merger. “If Paramount officials make these concessions,” the lawmakers wrote, it might amount to an illegal “quid pro quo.” When the Washington Post spiked an already written Kamala Harris endorsement last year, its owner, Jeff Bezos, wanted to make one thing clear: There was “no quid pro quo.” Yes, he acknowledged, the head of Blue Origin—a Bezos company that competes for billions in government contracts—met with Trump the day the paper killed the editorial. But that was just an unfortunate coincidence. “When it comes to the appearance of conflict, I am not an ideal owner of the Post,” Bezos conceded. “Every day, somewhere, some Amazon executive or Blue Origin executive, or someone from the other philanthropies and companies I own or invest in is meeting with government officials.” Indeed. In December, after Melania Trump had pitched a documentary about herself to Amazon’s film studio, Bezos had dinner with the incoming first couple, according to the Wall Street Journal. Amazon later agreed to pay a whopping $40 million to license the film; 70 percent of that goes directly to Melania. An Amazon spokesperson said the dinner “was separate from any discussions” about the film. “We licensed the upcoming Melania Trump documentary film and series for one reason and one reason only—because we think customers are going to love it,” the spokesperson said. Trump’s son-in-law Jared Kusher advised his father on Middle Eastern matters during the president’s first term, with questionable results. But that work likely helped Kushner win hundreds of millions of dollars from Saudi Arabia, Qatar, and a United Arab Emirates fund for a private equity firm he launched after leaving the White House. Kushner’s company, Affinity Partners, also landed lucrative development deals with the governments of Serbia and Albania. He’s building a luxury Trump- branded property in a former Yugoslav Ministry of Defense building, empty since NATO bombed it in 1999. And in Albania, Kushner plans to construct a luxury resort on an island once used as a military outpost. Those deals were reportedly brokered by Richard Grenell, who forged ties with both governments while serving as Trump’s special envoy in the Balkans. Both countries want US help joining the European Union despite long-standing corruption challenges, among other issues. Grenell is now Trump’s envoy for special missions, a global portfolio that could make him a valuable friend for any enterprising foreign government. When he took office in 2017, Trump refused to relinquish ownership of his business empire, which offered obvious opportunities for those with means to enrich him. That hasn’t changed. He still owns nine hotels, 18 golf courses, four commercial real estate properties, six estates, and a real estate brokerage. In his first term, these properties took in tens of millions in revenue—including a $15.8 million condo sale to a woman linked to Chinese military intelligence and a $13.5 million mansion sold to an Indonesian politician and businessman (who is developing two Trump golf resorts). Trump no longer owns the Washington, DC, hotel that served as a clubhouse for Republican lobbyists and foreign leaders in his first term. But Trump’s other hotels—like one in New York that saw a huge revenue boost in 2018 hosting the entourage of Saudi Crown Prince Mohammed bin Salman—are still going strong. The Trump Org says it plans to open 10 new properties, including a hotel in the UAE’s Dubai and a golf resort in Oman. The White House claims the Trump family’s businesses don’t create conflicts of interest because “the president’s assets are in a trust managed by his children.” But that trust isn’t blind, so the president can still know who’s helping him—and his kids—get richer. Sure, you could buy a club membership and some Trump-branded golf accessories. But if you really want to help out the president, bring him a whole tournament. That’s what Turkish Airlines—in which the Turkish government controls a 49 percent stake—did when it hosted a tournament at Trump’s course outside DC during his first year in office. After the January 6 coup attempt, the PGA distanced itself from Trump. But the Saudi sovereign wealth fund–owned league LIV Golf rushed to embrace Trump, hosting numerous tournaments at his courses around the United States. The latest took place in April at Trump’s National Doral Golf Club in Miami—just as Trump’s tariffs were crashing the stock market. Trump claims the tournaments are “peanuts” to him financially, but industry experts say each one is a multimillion-dollar deal for his businesses. In a deposition in 2023, Trump bragged that he could sell his Turnberry course in Scotland to LIV Golf for “a fortune,” though he declined to say whether such an offer had actually been made. Trump isn’t the first president to accept corporate contributions for his inauguration, but the scale is unprecedented. Meta and Amazon weren’t shy about their huge donations of $1 million each—the public obsequiousness was the point. Pilgrim’s Pride, owned by a subsidiary of Brazilian meat company JBS, gave $5 million. In April, JBS—which has a history of scandals and legal problems—won SEC approval to be listed on the New York Stock Exchange, a longstanding company goal that the agency had previously blocked. A spokesperson for JBS told Forbes that the inaugural donation was not related approval of the stock listing. Syngenta—an agribusiness giant that is owned by a Chinese government–controlled firm and is currently fighting a Federal Trade Commission lawsuit—kicked in $250,000. In February, the SEC dropped a lawsuit against crypto platform Coinbase, which gave $1 million to Trump’s inauguration. Coinbase, Pilgrim’s Pride, and the SEC didn’t respond to questions from Mother Jones; Amazon noted it also gave to Joe Biden’s inauguration, and Syngenta said it works “with both sides of the aisle.” The money helped Trump—who reportedly kept close tabs on who paid up—raise more than $200 million for lavish inaugural events. Any surplus can be transferred to a political committee or his eventual presidential library. Inaugural committees also offer opportunities for presidential enrichment. In 2022, for example, Trump’s previous inaugural committee paid $750,000 to settle a lawsuit accusing it of overpaying Trump properties that hosted events. While serving as a Trump Org executive, the president’s eldest son has also launched a series of anti-woke ventures. He’s a partner at 1789 Capital, which invests in conservative companies, and he’s slated to join the board of online firearms retailer GrabAGun. Don Jr. doesn’t have an official White House job, but he’s touted his influence over administration personnel decisions and MAGA politics. He recently traveled, twice, to Serbia—where Kushner and the Trump Org are developing a hotel—and lent his support to the country’s embattled president. For moguls who aren’t ready to do a deal with Don Jr., there’s now a more direct way to send him and his business partners money—an exclusive, invite-only DC club called “Executive Branch” that reportedly costs more than a half-million dollars to join. The plan, according to Politico, is “to ensure the C-suite crowd can mingle with Trump advisers and cabinet members without the prying eyes of the press and wanna-be insiders.” Arthur Schwartz, an adviser to Don Jr., downplayed his influence in the White House but declined to respond in greater detail when asked whether his activities create ethical problems. “Write your ridiculous story. Literally no one cares,” Schwartz said via text. “We don’t actually give a fuck.” Subscribe to the Mother Jones Daily to have our top stories delivered directly to your inbox. By signing up, you agree to our privacy policy and terms of use, and to receive messages from Mother Jones and our partners. “Lying.” “Disgusting.” “Scum.” “Slime.” “Corrupt.” “Enemy of the people.” Donald Trump has always made clear what he thinks of journalists. And it’s plain now that his administration intends to do everything it can to stop journalists from reporting things it doesn’t like—which is most things that are true. We’ll say it loud and clear: At Mother Jones, no one gets to tell us what to publish or not publish, because no one owns our fiercely independent newsroom. But that also means we need to directly raise the resources it takes to keep our journalism alive. There’s only one way for that to happen, and it’s readers like you stepping up. The deadline’s almost here. Please help us reach our $150k membership goal by May 31. “Lying.” “Disgusting.” “Scum.” “Slime.” “Corrupt.” “Enemy of the people.” Donald Trump has always made clear what he thinks of journalists. And it’s plain now that his administration intends to do everything it can to stop journalists from reporting things it doesn’t like—which is most things that are true. We’ll say it loud and clear: At Mother Jones, no one gets to tell us what to publish or not publish, because no one owns our fiercely independent newsroom. But that also means we need to directly raise the resources it takes to keep our journalism alive. There’s only one way for that to happen, and it’s readers like you stepping up. The deadline’s almost here. Please help us reach our $150k membership goal by May 31. Jasper Craven Julianne McShane Julianne McShane Molly Taft Caleb Kaufman Reveal Madison Pauly Abby Vesoulis Madison Pauly Damian Carrington Dan Friedman Pema Levy Subscribe to the Mother Jones Daily to have our top stories delivered directly to your inbox. By signing up, you agree to our privacy policy and terms of use, and to receive messages from Mother Jones and our partners. Save big on a full year of investigations, ideas, and insights. Help Mother Jones' reporters dig deep with a tax-deductible donation. Inexpensive, too! Subscribe today and get a full year of Mother Jones for just $19.95. Award-winning photojournalism. Stunning video. Fearless conversations. Subscribe to the Mother Jones Daily to have our top stories delivered directly to your inbox. By signing up, you agree to our privacy policy and terms of use, and to receive messages from Mother Jones and our partners. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. Privacy Manager Copyright © 2025 Mother Jones and the Center for Investigative Reporting. All Rights Reserved.Terms of Service Privacy Policy Can you pitch in a few bucks to help fund Mother Jones' investigative journalism? We're a nonprofit (so it's tax-deductible), and reader support makes up about two-thirds of our budget. We noticed you have an ad blocker on. Can you pitch in a few bucks to help fund Mother Jones' investigative journalism? Sign up for the free Mother Jones Daily newsletter and follow the news that matters.
--------------------------------------------------

Title: Cost-effective solutions for high-throughput enzymatic DNA methylation sequencing
URL: https://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.1011667
Time Published: 2025-05-22T14:00:00Z
Full Content:
Characterizing DNA methylation patterns is important for addressing key questions in evolutionary biology, development, geroscience, and medical genomics. While costs are decreasing, whole-genome DNA methylation profiling remains prohibitively expensive for most population-scale studies, creating a need for cost-effective, reduced representation approaches (i.e., assays that rely on microarrays, enzyme digests, or sequence capture to target a subset of the genome). Most common whole genome and reduced representation techniques rely on bisulfite conversion, which can damage DNA resulting in DNA loss and sequencing biases. Enzymatic methyl sequencing (EM-seq) was recently proposed to overcome these issues, but thorough benchmarking of EM-seq combined with cost-effective, reduced representation strategies is currently lacking. To address this gap, we optimized the Targeted Methylation Sequencing protocol (TMS)—which profiles ~4 million CpG sites—for miniaturization, flexibility, and multispecies use at a cost of ~USD 80. First, we tested modifications to increase throughput and reduce cost, including increasing multiplexing, decreasing DNA input, and using enzymatic rather than mechanical fragmentation to prepare DNA. Second, we compared our optimized TMS protocol to commonly used techniques, specifically the Infinium MethylationEPIC BeadChip (n = 55 paired samples) and whole genome bisulfite sequencing (n = 6 paired samples). In both cases, we found strong agreement between technologies (R2 = 0.97 and 0.99, respectively). Third, we tested the optimized TMS protocol in three non-human primate species (rhesus macaques, geladas, and capuchins). We captured a high percentage (mean = 77.1%) of targeted CpG sites and produced methylation level estimates that agreed with those generated from reduced representation bisulfite sequencing (R2 = 0.98). Finally, we confirmed that estimates of 1) epigenetic age and 2) tissue-specific DNA methylation patterns are strongly recapitulated using data generated from TMS versus other technologies. Altogether, our optimized TMS protocol will enable cost-effective, population-scale studies of genome-wide DNA methylation levels across human and non-human primate species. DNA methylation profiling is important for understanding key questions in biology, but current techniques can be expensive and have technical limitations. Enzymatic methyl sequencing (EM-seq) was proposed as a potential solution, but thorough testing is still needed. In this study, we optimized a new method (Targeted Methylation Sequencing or TMS) to make it more cost-effective, flexible, and applicable to multiple species. We tested modifications to increase sample multiplexing, reduce DNA input, and use enzymatic fragmentation. We compared our optimized TMS protocol to common methylation profiling techniques and found strong agreement in DNA methylation levels. We also successfully applied the optimized TMS protocol to three non-human primate species. Finally, we show that common analyses of DNA methylation data produce similar results using TMS data versus data from other technologies. Together, we hope this work will enable cost-effective, population-scale DNA methylation profiling across human and non-human species. Citation: Longtin A, Watowich MM, Sadoughi B, Petersen RM, Brosnan SF, Buetow K, et al. (2025) Cost-effective solutions for high-throughput enzymatic DNA methylation sequencing. PLoS Genet 21(5): e1011667. https://doi.org/10.1371/journal.pgen.1011667 Editor: Duncan Sproul, The University of Edinburgh MRC Human Genetics Unit, UNITED KINGDOM OF GREAT BRITAIN AND NORTHERN IRELAND Received: September 13, 2024; Accepted: March 27, 2025; Published: May 22, 2025 Copyright: © 2025 Longtin et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited. Data Availability: All NHP data generated as part of this study has been deposited in NCBI’s Sequence Read Archive under accession number PRJNA1156067. The human genomic data generated as part of this study comes from the Tsimane Health and Life History Project (THLHP) and the Orang Asli Health and Lifeways Project (OA HeLP). Both THLHP and OA HeLP’s highest priority is the minimization of risk to study participants. Both projects adhere to the “CARE Principles for Indigenous Data Governance” (Collective Benefit, Authority to Control, Responsibility, and Ethics) and are also committed to the “FAIR Guiding Principles for scientific data management and stewardship” (Findable, Accessible, Interoperable, Reusable). To adhere to these principles while minimizing risks, genomic data from both projects are available via restricted access. These requests can be made via email following the instructions provided here: https://tsimane.anth.ucsb.edu/data.html and here: https://www.orangaslihealth.org/data.html. In both cases, requests for de-identified genomic data will take the form of an application that details the exact uses of the data and the research questions to be addressed, procedures that will be employed for data security and privacy, potential benefits to the study communities, and procedures for assessing and minimizing stigmatizing interpretations of the research results. Both projects are committed to open science and the leadership is available to assist interested investigators in preparing data access requests. All scripts used to perform the analyses described here are available at “https://github.com/alongtin15/TMS-Cost-effective-solutions-for-high-throughput-enzymatic-DNA-methylation-sequencing.” Funding: This work was supported by the National Institute of General Medical Sciences (R35-GM147267 to AJL), National Institute on Aging (R01AG054442 to HK, MDG, and BCT, R61AG078529 to ADM, R01AG060931 and R00AG051764 to NSM, R56AG071023 and R01AG084706 to JPH, DP5OD029586, R01AG088657 and R01AG083736 to AGB), National Institute of Mental Health (R01MH118203 and R01MH096875 to MLP), the National Cancer Institute (P30CA068485 to AGB), the National Science Foundation (BCS-2142090 to AJL, BCS-2010309, BCS-1848900, BCS-2013888 and BCS-1723237 to NSM), the Canadian Institute for Advanced Research (Azrieli Global Scholars Program to AJL), the French National Research Agency under the Investments for the Future (Investissements d’Avenir) programme (ANR-17-EURE-0010 to JS), the Kinship Foundation, (Searle Scholars Program to AJL), the Pew Charitable Trusts (Pew Biomedical Scholars Program to AJL and Pew-Stewart Scholar for Cancer Research Program to AGB), the Burroughs Wellcome Fund (Career Award for Medical Scientists to AGB), the Hevolution Foundation (Hevolution/AFAR New Investigator Award in Aging Biology and Geroscience Research to AGB), the Natural Science and Engineering Research council of Canada (RGPIN-2017-03782 to ADM), and the Canada Research Chairs program (950-231257 to ADM). The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. The views expressed are those of the authors and do not necessarily reflect the views of the funders. Competing interests: The authors have declared that no competing interests exist. Understanding variation in DNA methylation levels across tissues, the lifespan, disease states, and populations is important for addressing key questions in biology. DNA methylation—the covalent addition of methyl groups to cytosines—is a semi-malleable and environmentally-responsive epigenetic modification involved in gene regulation in many species, including our own [1]. Because DNA methylation moderates gene expression throughout the life course, it is critical for processes such as development [2–4], cell programming [5], tissue specificity [6], aging [7–11], and disease progression [12–14]. For example, changes in DNA methylation are considered a “hallmark” of the aging process, with most studies reporting age-associated gains in methylation in hypomethylated regions (e.g., promoters and transcribed regions) and age-associated losses in methylation in hypermethylated regions (e.g., heterochromatic regions, Polycomb-repressed regions) [15–17]. These age-related patterns are so consistent that DNA methylation variation have been used to construct molecular clocks that reliably predict chronological age [18,19]. Further, because DNA methylation is known to respond to environmental inputs, it has been implicated as a mechanism through which diverse environmental exposures can impact long-term physiology and health (e.g., famine [20–24], psychosocial stress [25–29], or infection [30–33]). To profile genome-wide DNA methylation at scale, most studies rely on reduced representation methods: human studies have largely favored microarrays, while non-human studies have favored reduced representation bisulfite sequencing (RRBS) due to the lack of species-specific microarrays (though recent work has led to the development of the Infinium Mouse DNA Methylation BeadChip as well as the Mammalian Methylation Array) [34–36]. Both RRBS and microarrays quantify DNA methylation at a subset (1–5%) of CpGs in the genome, and thus provide a cost-effective strategy relative to genome-wide assays (e.g., whole genome bisulfite sequencing (WGBS)). For example, the Infinium MethylationEPIC v2.0 BeadChip, or EPIC array, covers ~ 930K CpG sites including functional elements identified by the ENCODE project [37], DNase hypersensitive sites, and putatively important sites for human disease and development [38,39]. In contrast, RRBS fragments DNA using the Msp1 enzyme that cuts DNA at CCGG motifs, which following size selection, enriches for 1–5% of the genome with high CpG content such as CpG islands and gene bodies [34,40]. Importantly, both microarrays and RRBS rely on sodium bisulfite, which converts unmethylated cytosines to thymine while leaving methylated cytosines protected from conversion. This chemical reaction requires high pHs and temperatures, which can cause unwanted DNA fragmentation and damage, especially to unmethylated cytosines [41]. Ultimately, such damage can create difficulties during library preparation as well as biases in the downstream data [41–43]. Enzymatic methyl sequencing (EM-seq) offers a useful alternative to bisulfite sequencing with several key benefits: EM-seq relies on enzymatic rather than chemical conversion of unmethylated cytosines to thymine, resulting in substantially less DNA damage [42]. As a result, whole genome EM-seq has been shown to recover more CpGs sites, have lower duplication rates, have better between-replicate correlations, and require less DNA input than WGBS [42]. However, existing EM-seq benchmarked protocols rely on whole genome rather than reduced representation strategies, hindering their adoption especially for population-scale studies. To address this gap, Twist Biosciences recently created a hybrid capture panel that targets ~4 million CpG sites in the human genome and is compatible with EM-seq. The Twist methylation capture reaction uses ~ 550k probes to target functionally relevant CpG sites (e.g., those in enhancers, gene bodies, and near transcription start sites) and to cover ~95% of CpG sites included on the widely used EPIC array [44–47]. Off the shelf, this protocol is similar or lower in cost to existing reduced representation approaches. However, we note that total cost for any sequencing-based approach will depend on the desired coverage (i.e., the average number of reads that cover each CpG site); best practices for average per CpG coverage are still debated, but most studies recommend at least 20x [48–51]. Increased coverage will increase the precision of DNAm estimates, and thus to some degree the desired coverage depends on the anticipated effect size. Here, we aimed to develop and benchmark an optimized and further cost-reduced version of the targeted methylation sequencing (TMS) approach suitable for population-scale studies, including both human and non-human primate (NHP) studies (Fig 1A). To do so, we built upon the off the shelf TMS protocol (Fig 1B), which recommends 8 plexing of samples per capture reaction and 200 ng of DNA input, and tested four multiplexing strategies (12, 24, 48, and 96 plex, using 200 ng of sample input; Fig 1C). We also tested five DNA input amounts (25, 50, 100, 200, and 400ng, using the 12-plex strategy) and other minor protocol modifications such as varying the annealing temperature during hybrid capture and varying the method used for DNA fragmentation (Fig 1C). Following optimization, we assessed: 1) the robustness of our protocol through a direct comparison with the EPIC array and WGBS; 2) the extension of optimized TMS for use in NHP species; and 3) the ability of our protocol to recapitulate biological results (epigenetic age estimates and identification of tissue-specific patterns) obtained from data generated using other technologies (see Table 1 for sample sizes and sample information; Fig 1C). Overall, we found that we were able to miniaturize and optimize the TMS protocol to ~USD 80 per sample, while maintaining data quality and comparability to existing methods. In total, our protocol provides coverage of approximately four times as many CpG sites relative to the EPIC array at one fourth the cost—a ~ 16-fold gain in the data-to-price ratio (S1 Table). [A] To optimize the TMS protocol, we used samples from three human and three NHP populations: the Tsimane of Bolivia, a Vanderbilt University Medical Center cohort, the Orang Asli of Malaysia, rhesus macaques from Cayo Santiago in Puerto Rico, tufted capuchins from captive sites throughout the United States, and gelada monkeys from Ethiopia. Created using BioRender. [B] The TMS protocol begins with DNA fragmentation and adapter ligation. Next, two enzymes, TET2 and APOBEC, are used to oxidize and deaminate the DNA. TET2 recognizes methyl groups attached to cytosines and converts them to Ca/g. APOBEC follows TET2 and converts the unmethylated cytosines to uracils. Following PCR amplification (which converts uracils to thymines), hybrid capture is used to enrich for targeted regions of the genome. Samples are then assayed via high throughput sequencing. Created using Microsoft Powerpoint. [C] Overview of experiments and analyses. The samples used for each set of experiments are noted by a population-specific icon. Icons from Biorender, OpenClipArt, and Microsoft Powerpoint. [A] To optimize the TMS protocol, we used samples from three human and three NHP populations: the Tsimane of Bolivia, a Vanderbilt University Medical Center cohort, the Orang Asli of Malaysia, rhesus macaques from Cayo Santiago in Puerto Rico, tufted capuchins from captive sites throughout the United States, and gelada monkeys from Ethiopia. Created using BioRender. [B] The TMS protocol begins with DNA fragmentation and adapter ligation. Next, two enzymes, TET2 and APOBEC, are used to oxidize and deaminate the DNA. TET2 recognizes methyl groups attached to cytosines and converts them to Ca/g. APOBEC follows TET2 and converts the unmethylated cytosines to uracils. Following PCR amplification (which converts uracils to thymines), hybrid capture is used to enrich for targeted regions of the genome. Samples are then assayed via high throughput sequencing. Created using Microsoft Powerpoint. [C] Overview of experiments and analyses. The samples used for each set of experiments are noted by a population-specific icon. Icons from Biorender, OpenClipArt, and Microsoft Powerpoint. https://doi.org/10.1371/journal.pgen.1011667.g001 https://doi.org/10.1371/journal.pgen.1011667.t001 Using DNA from a human population in Bolivia (Tsimane, see [52]), we tested four multiplexing strategies (12, 24, 48, and 96 plex, using 200ng of DNA sample input) and five DNA input amounts (25, 50, 100, 200, and 400ng, using the 12-plex strategy). Raw quality control metrics such as percent CHH methylation (a proxy for the rate at which unmethylated cytosines are converted to thymine) and mapping efficiency (percent of reads uniquely mapped to the genome) were high for all samples. Mapping efficiency was consistent across plexing strategies (average mapping efficiency: 12-plex = 71.9%, 24-plex = 72.9%, 48-plex = 72.5%, and 96-plex = 73.5%; ANOVA: F-value = 0.843, p-value = 0.472; Fig 2A) but affected by input amount, with higher DNA input having greater mapping efficiency (ANOVA: F-value = 13.57, p-value < 0.001, Fig 2B and S3 Table). CHH methylation was consistently well below 1%, indicative of high conversion rate across all plexing and input strategies (range = 0.1-0.27%; S1 Fig, and S4 and S5 Tables) [53]. [A] High (>70%) mean mapping efficiency across plexing strategies. Each point represents a sample within a plexing strategy and the y-axis represents the percent of reads uniquely mapped per sample. [B] Mapping efficiency increases as input amount increases. Each point represents a 12-plex pool made with varying DNA input amounts per sample, the y-axis represents the percent of reads uniquely mapped per sample. [C] Distribution of median DNA methylation levels for CpG sites located within different chromHMM genomic annotations; annotations from NIH Roadmap Epigenomics and data from the 96-plex, 200 ng input from experiment 1. [D] The total number of CpG sites falling within different chromHMM genomic annotations (using data from the 96-plex, 200 ng input from experiment 1). [E] Percent of reads that are not within the Twist probe set (i.e., off-target reads) following protocol modifications to annealing temperature and methylation enhancer (ME) volume. For each set of protocol conditions, the x-axis represents the percent of mapped reads that do not overlap with the Twist probe set. [F] Percent of Twist probes that are represented within each dataset following protocol modifications to adjust the annealing temperature and ME volume. For each set of protocol conditions, the x-axis represents the percentage of Twist probes that were represented by at least 1 read. [A] High (>70%) mean mapping efficiency across plexing strategies. Each point represents a sample within a plexing strategy and the y-axis represents the percent of reads uniquely mapped per sample. [B] Mapping efficiency increases as input amount increases. Each point represents a 12-plex pool made with varying DNA input amounts per sample, the y-axis represents the percent of reads uniquely mapped per sample. [C] Distribution of median DNA methylation levels for CpG sites located within different chromHMM genomic annotations; annotations from NIH Roadmap Epigenomics and data from the 96-plex, 200 ng input from experiment 1. [D] The total number of CpG sites falling within different chromHMM genomic annotations (using data from the 96-plex, 200 ng input from experiment 1). [E] Percent of reads that are not within the Twist probe set (i.e., off-target reads) following protocol modifications to annealing temperature and methylation enhancer (ME) volume. For each set of protocol conditions, the x-axis represents the percent of mapped reads that do not overlap with the Twist probe set. [F] Percent of Twist probes that are represented within each dataset following protocol modifications to adjust the annealing temperature and ME volume. For each set of protocol conditions, the x-axis represents the percentage of Twist probes that were represented by at least 1 read. https://doi.org/10.1371/journal.pgen.1011667.g002 After filtering for CpG sites with>5x coverage that were within the Twist probe set (+/- 200 bp) and that were covered in the majority of samples in a given experiment, we retained an average of 4,197,008 CpG sites (s.d. = 546,767) across plexing experiments and 4,051,941 CpG sites (s.d. = 93,106) across input experiments (S6 and S7 Tables). On average, this represented 96.42% and 92.19% coverage of the TMS probe set across the plexing and input experiments, respectively (S8 and S9 Tables). Across experiments, we found average coverage of targeted CpG sites to be far greater than our minimum coverage of 5x, ranging from 21-89x across datasets (S2 Fig and S2 Table). In addition to consistently recovering the expected set of CpGs, we also observed extremely repeatable methylation levels across the plexing and input experiments (all R2 > 0.99; S10 and S11 Tables). The CpGs covered by our experiments were distributed across diverse genomic annotations, and the median DNA methylation levels within a given annotation displayed expected patterns (Fig 2C and 2D) [54]. For example, we observed high levels of methylation in quiescent and heterochromatin regions and low levels of methylation in promoters and transcribed regions. In experiments 1 and 2, we used the recommended 65°C annealing temperature during the hybrid capture step—where prepared DNA is bound to the capture probe set to select CpG sites of interest—and the recommended 2uL of methylation enhancer, which increases the efficiency of this reaction. Here, we found that ~3/4 of all of our mapped reads were “on-target”, meaning that they overlapped with the designed probe set and represented successful hybrid capture (S8 and S9 Tables). This suggests that ~ ¼ of reads are “off target” and randomly distributed across the genome rather than within our regions of interest. We therefore performed a third experiment using Tsimane DNA to test two protocol modifications that might decrease the off-target proportion: we increased the annealing temperature (testing 65°C or 68°C) and we varied the amount of methylation enhancer (testing 0uL, 2uL, or 4uL). We note that similar previous work has reported on-target read percentages of 75–85% [55–57], suggesting the capture reaction will likely never be completely efficient. In experiment 3, we found that increasing the annealing temperature from 65°C to 68°C resulted in a lower proportion of off-target reads (ANOVA: F-value = 84.2, p-value < 0.0001; Figs 2E and S4, and S12 Table). Across samples annealed at 65°C, an average of 78.5% of reads were on-target, while this number rose to 84.2% at 68°C. However, this increase in capture efficiency came at a cost to the breadth of CpG sites covered: across samples annealed at 65°C, we observed coverage of on average 92.0% of the probe set, while this number fell to 72.2% for samples annealed at 68°C (Fig 2F, and S13 and S14 Tables). This suggests that higher annealing temperatures lead to greater but more specific binding during the hybrid capture step, and thus the increased capture efficiency comes at the expense of recovering all the expected CpG sites. In general, we did not find meaningful differences across methylation enhancer amounts and we therefore excluded this reagent from downstream experiments (Fig 2E and 2F). Given the loss of certain genomic regions at 68°C, downstream experiments focused on a 65°C annealing temperature. We next performed a fourth experiment focused on protocol optimization, in which we varied the strategies used to fragment genomic DNA prior to EM-seq library preparation: specifically, we tested mechanical fragmentation via Covaris sonication against enzymatic fragmentation with the NEBNext UltraShear reagent. Mechanical fragmentation is the current standard approach but is expensive, requires special equipment, and is labor intensive. Conversely, enzymatic fragmentation is cheaper, does not require special equipment, and is more compatible with automation. For experiments 3 and 4, we used the 96-plex strategy and 200 ng of sample input, since experiments 1 and 2 suggested that data quality does not suffer from higher plexing strategies. These experiments used DNA from a human population in Malaysia, the Orang Asli [58]. Enzymatic fragmentation resulted in a similar number of covered sites as was previously observed with mechanical fragmentation (n = 4,591,123 and 4,523,981 filtered CpG sites for the 10 and 20 minute protocols, respectively). Average site-specific methylation levels were also highly concordant between approaches (mechanical versus 10 min enzymatic: R2 = 0.9875; mechanical versus 20 min enzymatic: R2 = 0.9876; 10 min versus 20 min enzymatic: R2 = 0.9944; S5 Fig). This was also true when we focused on a subset of DNA samples processed using both methods (n = 3; mechanical versus 10 min enzymatic: average R2 = 0.971; mechanical versus 20 min enzymatic: average R2 = 0.971; 10 min versus 20 min enzymatic: average R2 = 0.987, S15 Table). From these experiments, we concluded that enzymatic fragmentation can be substituted into the protocol with no loss to data quality. We also used these data, which represent our “best” protocol (96-plex, 200ng input, 65°C annealing, no methylation enhancer, enzymatic fragmentation), to understand two critical aspects of experimental design—how many reads one would need to generate to achieve a given mean (or median) coverage per CpG site (S6 Fig) and how this mean coverage impacts power to detect differential methylation (S3 Fig). In general, we observe a ~ 1:1 relationship between the number of mapped, paired end reads (in millions) and mean coverage, such that 20 million mapped paired end (40 million total reads) translates to ~20x mean coverage (or ~14x median coverage) per CpG site. Using simulations [49,59] of datasets of different sizes (n = 100, 200, 400) and mean coverages (20x, 40x, and 80x), we found that increasing coverage can provide power gains for smaller sample sizes, but in larger datasets increasing coverage will matter less as power is derived from the overall sample size rather than gains in precision (S3 Fig). To ensure that TMS could perform comparably to the most popular current reduced-representation technology (the EPIC array), we generated paired data for 55 samples using both platforms (and following the 96-plexing, 200 ng input TMS protocol from experiment 1). After filtering, we analyzed 682,295 CpG sites common to both technologies, and found high concordance between per-site DNA methylation levels averaged across all individuals in the dataset (R2 = 0.97; Fig 3A). We also examined correlations between the two technologies when we subsetted to 1) variably methylated CpG sites (i.e., sites with methylation levels >10% and <90%; mean R2 = 0.83; Fig 3B); 2) CpG islands, shores, and shelves (R2 = 0.97, 0.97, 0.94, respectively); 3) hypomethylated (<50% average methylation; R2 = 0.89) regions; 4) hypermethylated (>50% average methylation; R2 = 0.70) regions; and 5) particular EPIC v2 array probe types (S7 and S8 Figs). Because methylation patterns are relatively consistent across individuals for many regions of the human genome, we also confirmed that these correlations were higher for EPIC-TMS data generated from the same sample compared to EPIC-TMS comparisons made between random pairs of samples (mean R2 for all sites: 0.95 versus 0.93 for random sample pairs, mean R2 for variable sites only: 0.83 versus 0.75 for random sample pairs; S9 Fig). [A] Correlation in DNA methylation levels for EPIC array versus TMS (R2 = 0.97). Each point represents the DNA methylation level of a given CpG averaged across 6 samples measured using the EPIC array (x-axis) and 96-plex, 200 ng input TMS (y-axis). The R2 value was generated using linear modeling. Sites were filtered to>5X coverage in >75% of samples within each technology. [B] Histogram of R2 values calculated for each individual sample (i.e., comparing per CpG DNA methylation levels measured on both technologies for a given sample). R2 values are provided when all CpG sites common to both technologies are included, as well as when only variably methylated CpG sites are included. [C] Correlation in DNA methylation levels for WGBS versus TMS (R2 = 0.9871). Each point represents the DNA methylation level of a given CpG averaged across 6 samples measured using WGBS (x-axis) and 96-plex, 200 ng input TMS (y-axis). The R2 value was generated using linear modeling. Sites were filtered to>5X coverage in >75% of samples within each technology. [D] Density plot of the average DNA methylation levels detected for common sites between the three technologies (713,282 sites). Notably, the EPIC array is biased against DNA methylation levels of 100%, as previously observed [51] and explained by the equation used to calculate beta values. [A] Correlation in DNA methylation levels for EPIC array versus TMS (R2 = 0.97). Each point represents the DNA methylation level of a given CpG averaged across 6 samples measured using the EPIC array (x-axis) and 96-plex, 200 ng input TMS (y-axis). The R2 value was generated using linear modeling. Sites were filtered to>5X coverage in >75% of samples within each technology. [B] Histogram of R2 values calculated for each individual sample (i.e., comparing per CpG DNA methylation levels measured on both technologies for a given sample). R2 values are provided when all CpG sites common to both technologies are included, as well as when only variably methylated CpG sites are included. [C] Correlation in DNA methylation levels for WGBS versus TMS (R2 = 0.9871). Each point represents the DNA methylation level of a given CpG averaged across 6 samples measured using WGBS (x-axis) and 96-plex, 200 ng input TMS (y-axis). The R2 value was generated using linear modeling. Sites were filtered to>5X coverage in >75% of samples within each technology. [D] Density plot of the average DNA methylation levels detected for common sites between the three technologies (713,282 sites). Notably, the EPIC array is biased against DNA methylation levels of 100%, as previously observed [51] and explained by the equation used to calculate beta values. https://doi.org/10.1371/journal.pgen.1011667.g003 Of note, the analyses described above reconfirmed a known bias in the EPIC array data [51,60], which does not allow for methylation levels of 100%. This is because EPIC-derived DNA methylation levels are represented as beta values, calculated as the ratio of the intensity of the methylated bead type to the total locus intensity plus an offset value. Due to the addition of the offset value, beta values of 1 are mathematically impossible. As a result, the correlation between average TMS- and EPIC-measured DNA methylation levels is slightly off the x = y line (Fig 3A) and correlations are much lower than the genome-wide average for hypo- as well as hyper-methylated regions. For further validation, we also generated WGBS data for 6 Tsimane samples included in experiment 3 (96-plexing, 200 ng input, 65°C annealing temperature, no ME, mechanical fragmentation). After filtering and merging with the TMS data, we retained 3,078,771 CpG sites covered by both the TMS and WGBS approaches. For these sites, the average methylation levels observed across technologies was highly correlated (R2: 0.9871; Fig 3C). We also found that the genome-wide distribution of DNA methylation levels derived from WGBS was more similar to TMS than to the EPIC array, specifically in that it included many sites with average methylation levels of 100% or close to 100%, as expected (Figs 3D and S10). To enable epigenomic analyses in our close primate relatives, we also tested whether TMS (96-plex, 200ng input protocol from experiment 1) could be effectively applied to three NHP species: tufted capuchins (Sapajus apella; n = 28 samples from blood), rhesus macaques (Macaca mulatta; n = 96 samples from 6 tissues (see S11 Fig and S16 Table)), and geladas (Theropithecus gelada; n = 68 samples from blood). While the probe set is designed from the human genome, NHP species share high levels of sequence homology with humans, especially in coding regions and regions near genes [61], leading us to hypothesize that a majority of CpG sites would be recovered. We mapped the Twist probe sequences to each of the NHP genomes to confirm this intuition, and from this analysis expected to capture 3.0-4.8 million CpG sites across the three species (Fig 4B). Importantly, for the rhesus macaque samples, we also generated paired RRBS data and compared our TMS results to a technology that does not rely on hybrid capture. [A] Optimized TMS in NHPs results in high mapping efficiencies despite the use of human-specific probes. Here, each of the species are mapped to their respective reference genome. We hypothesize that low mapping efficiency in certain rhesus macaque samples is due to variation in sample quality. [B] Number of expected and observed CpG sites covered in each NHP genome. Expected sites were derived from mapping the Twist probes to each NHP genome, while observed sites represent those detected with a coverage > 5X in >75% of samples. [C] Principal components analysis of TMS-derived DNA methylation levels for rhesus macaque samples spanning six distinct tissues. [D] Similar per CpG DNA methylation level estimates using RRBS (x-axis) and optimized TMS (y-axis) (R2 = 0.97). [E] Density plot of linear model R2 values obtained from comparing data generated via optimized TMS and RRBS for the same rhesus macaque samples. R2 values are provided when all CpG sites common to both technologies are included, as well as when only variably methylated (methylation > 10% and methylation < 90%) CpG sites are included. [F] Density curves of the average genome-wide DNA methylation level estimates for each NHP species. Curves show the expected bimodal distribution in which many of the CpG sites in the genome are either hypomethylated or hypermethylated. [A] Optimized TMS in NHPs results in high mapping efficiencies despite the use of human-specific probes. Here, each of the species are mapped to their respective reference genome. We hypothesize that low mapping efficiency in certain rhesus macaque samples is due to variation in sample quality. [B] Number of expected and observed CpG sites covered in each NHP genome. Expected sites were derived from mapping the Twist probes to each NHP genome, while observed sites represent those detected with a coverage > 5X in >75% of samples. [C] Principal components analysis of TMS-derived DNA methylation levels for rhesus macaque samples spanning six distinct tissues. [D] Similar per CpG DNA methylation level estimates using RRBS (x-axis) and optimized TMS (y-axis) (R2 = 0.97). [E] Density plot of linear model R2 values obtained from comparing data generated via optimized TMS and RRBS for the same rhesus macaque samples. R2 values are provided when all CpG sites common to both technologies are included, as well as when only variably methylated (methylation > 10% and methylation < 90%) CpG sites are included. [F] Density curves of the average genome-wide DNA methylation level estimates for each NHP species. Curves show the expected bimodal distribution in which many of the CpG sites in the genome are either hypomethylated or hypermethylated. https://doi.org/10.1371/journal.pgen.1011667.g004 When examining initial quality control metrics, we found that all three NHP species had high mapping efficiencies (average = 81.96% for capuchins, 82.62% for geladas, and 81.35% for macaques; Fig 4A). Further, the average CHH methylation levels were all extremely low (<1%), again suggesting high conversion rates (S12 Fig). Following filtering, we recovered ~ ½ to ¾ of expected CpG sites in the NHP datasets (3,343,133 in capuchin, 5,387,280 in gelada, and 5,486,073 in macaque). The number of sites recovered scales generally with divergence time (capuchins share a common ancestor with humans 35–45 million years ago, geladas and rhesus macaques share a common ancestor with humans 23–28 million years ago) [62]. In all species, we were able to reliably measure more sites than would be typical of RRBS (see below), and we note that some of the between-species variation in performance could be explained by heterogenous read depth (S2 Table) as well as reference assembly quality. In particular, the quality of the rhesus macaque genome is much higher than the gelada or capuchin (using CNEr in R and the N50() and N90() commands [63]): mmul_10 N50 = 153,388,924, N90 = 79,627,064; tgel1 N50 = 147,341,205, N90 = 77,542,005; cimit N50 = 5,274,112, N90 = 1,283,179. When examining average DNA methylation levels across species, we found that, as expected, all exhibited bimodal genome-wide profiles similar to humans (Fig 4F). Further, because the rhesus macaque samples were derived from 6 different tissue types (S11 Fig and S16 Table), we also confirmed that samples displayed expected tissue-specific epigenetic patterns. Specifically, we demonstrated that a Principal Components Analysis (PCA) was able to reliably separate samples by tissue type (Fig 4C), as has been observed in previous studies using both bisulfite sequencing and the EPIC array [64–66]. Studies of NHP species have historically relied on RRBS because of the species-specificity of microarray technologies and the cost barrier of WGBS [49,67,68]. To test how our optimized TMS protocol compares to RRBS, we generated paired data for all 96 rhesus macaque samples. After filtering both datasets to 721,766 common CpG sites, we found a high concordance of the average DNA methylation levels estimated by both technologies (R2 = 0.97; Figs 4D and S13). This remained true when we subsetted specifically to 92,692 variably methylated CpG sites (i.e., sites with average DNA methylation levels >0.1 and <0.9; R2 = 0.5945; Fig 4E). Thus far we have compared DNA methylation levels measured with TMS versus other technologies; if these measurements are robust across platforms, then power to detect biological patterns should also be similar. We thus asked whether data generated from paired samples, but using different technologies, could recapitulate 1) epigenetic age predictions using DNA methylation-based clock algorithms [7,69–73] and 2) tissue-dependent methylation signatures when comparing diverse organ systems. For analysis #1, we used the 55 VUMC cohort samples with paired EPIC and TMS data (focusing on 682,295 CpG sites passing filters and common to both technologies). For analysis #2, we used the 96 rhesus macaque samples with paired RRBS and TMS data (focusing on 391,758 CpG sites passing filters and common to both technologies). For analysis #1, we observed a high correlation between epigenetic age estimates derived from TMS and EPIC data (Fig 5A; average R2 = 0.91). This agreement was high across a variety of existing epigenetic clock algorithms. For analysis #2, we found that effect size estimates of tissue dependency (for example, from linear models comparing DNA methylation levels in liver to all other tissues) were very similar genome-wide when applied to TMS versus RRBS data (Fig 5B and 5C). Additionally, we confirmed that sites identified as significantly associated (FDR < 5%) with a given tissue in RRBS versus TMS data overlapped more than expected by chance (Fig 5D). Together, these results support the ability of TMS data to uncover biological patterns in similar ways as revealed by other technologies. [A] Pearson’s correlation coefficient comparing epigenetic age predictions for five PC-based epigenetic clocks run on TMS versus EPIC v2 array data from the VUMC cohort (n paired samples = 55). All correlations were significant following multiple hypothesis testing (FDR < 5%). [B] Correlation between standardized effect sizes, estimating liver-specific effects, using RRBS versus TMS data (n paired rhesus macaque samples = 96). To derive effect size estimates, models were run comparing the liver to all other tissues. Each point represents the effect size for a given CpG site common to both datasets. [C] Pearson’s correlation coefficient comparing effect sizes for estimates of tissue-specific effects using TMS versus RRBS data (n paired rhesus macaque samples per tissue = 96). Separate models were run for each tissue, comparing the focal tissue on the x-axis to all other other tissues to identify tissue-specific effects. All correlations were significant following multiple hypothesis testing (FDR < 5%). [D] Degree of enrichment (represented as an log2 odds ratio from a Fisher’s Exact test), between CpG sites identified as tissue-specific in TMS versus RRBS data using matched samples. Dashed line represents no enrichment and error bars represent confidence intervals. [A] Pearson’s correlation coefficient comparing epigenetic age predictions for five PC-based epigenetic clocks run on TMS versus EPIC v2 array data from the VUMC cohort (n paired samples = 55). All correlations were significant following multiple hypothesis testing (FDR < 5%). [B] Correlation between standardized effect sizes, estimating liver-specific effects, using RRBS versus TMS data (n paired rhesus macaque samples = 96). To derive effect size estimates, models were run comparing the liver to all other tissues. Each point represents the effect size for a given CpG site common to both datasets. [C] Pearson’s correlation coefficient comparing effect sizes for estimates of tissue-specific effects using TMS versus RRBS data (n paired rhesus macaque samples per tissue = 96). Separate models were run for each tissue, comparing the focal tissue on the x-axis to all other other tissues to identify tissue-specific effects. All correlations were significant following multiple hypothesis testing (FDR < 5%). [D] Degree of enrichment (represented as an log2 odds ratio from a Fisher’s Exact test), between CpG sites identified as tissue-specific in TMS versus RRBS data using matched samples. Dashed line represents no enrichment and error bars represent confidence intervals. https://doi.org/10.1371/journal.pgen.1011667.g005 We developed and benchmarked a multiplexed, cost-effective version of the TMS protocol and applied it to human populations from the US, Bolivia, and Malaysia as well as multiple NHP species. We recommend an optimal protocol for future work (96-plex, 200ng input, 65°C annealing, no methylation enhancer, enzymatic fragmentation), but found that data quality remained high across plexing strategies, input amounts, and protocol modifications. Importantly, the 96-plex version of the TMS protocol—including sequencing to achieve ~ 25x coverage per CpG site on the Illumina NovaSeq X—can currently be performed for ~USD 80 per sample (with roughly half being reagents and labor, and the other half being sequencing on the NovaSeq X platform; S1 Table). Relative to the commonly used EPIC array for human studies, this represents massive savings enabling larger-scale, population-based studies. We recognize that the total cost of TMS will vary by the amount of sequencing performed, and we provide simulations (and modifiable code) based on real TMS coverage distributions for users to explore the impact of coverage on power for a given study design. For example, we find that with a sample size of n = 100, moderate differences in methylation (e.g., 20%) can be identified with high power at relatively low read depths (e.g., 20x), while detecting small differences would require higher read depths. However, the relative impact of coverage on power diminishes at higher sample sizes. Researchers will thus need to tailor their sequencing plan based on both their expected effect size and the number of samples in hand (S3 Fig). We found high concordance between TMS-derived DNA methylation levels and those derived from other commonly used methods—namely the EPIC array, WGBS, and RRBS. WGBS is the gold standard for comprehensive DNA methylation measurement, but is prohibitively expensive for most studies given the breadth of sequencing (to cover the whole genome) and the necessity for deep sequencing (to achieve high levels of precision) [50]. RRBS has filled in as a more cost-effective alternative, but due to the stochastic nature of the Msp1 digestion followed by size selection, not all CpG sites are reliably covered across individuals and missing data can impede downstream analyses (S14 Fig). We note that variation in coverage (and thus precision) across CpG sites will be an issue, to some degree, for any sequencing-based technology. As a result of these challenges, microarray-based methods (such as the EPIC array) have become the most commonly used approach in human genomics. Consequently, many popular bioinformatics pipelines and specialized algorithms for DNA methylation data (e.g., epigenetic clocks or cell type deconvolution [74,75]) are currently keyed to microarrays. While DNA methylation levels derived from TMS are strongly correlated with the EPIC array, it is important to keep in mind that: 1) a small subset of sites are not covered by both technologies, and 2) because microarrays output beta values (equivalent to methylated signal/(total signal + an offset)), the relationship between TMS- and EPIC-derived values cannot be exactly 1:1. We caution that care will thus be needed when applying existing microarray-based algorithms to TMS data, though our initial attempts at doing so with epigenetic clock algorithms do seem to perform well. The study of DNA methylation in NHP species is deeply important to our understanding of gene regulatory evolution [76–78], comparative aging [67,68,79,80], and environmental impacts on phenotype [68,81]. For example, both captive and field-based NHP studies have strongly contributed to our understanding of how social and ecological inputs influence fitness-related traits through changes in DNA methylation [82,83]. These studies have sometimes relied on microarrays (e.g., the Mammalian Methylation Array [36,84–86] or the application of human arrays to NHP species [87–89]). However, given the high costs of arrays, a large proportion of previous work has relied on RRBS [68,80,81,83]. Although RRBS is easily adapted for non-human species, TMS can work with smaller amounts of input DNA than bisulfite-based protocols [42], which can be critical for studies of wild or endangered species. While TMS uses capture probes designed from the human genome, NHPs share high levels of sequence similarity, which we show is sufficient to reliably capture 2–3 million CpG. Though not all ~ 4 million CpG sites are captured, TMS still represents a more consistent and cost-effective approach relative to the alternatives. Notably, we find that TMS is effective in both catarrhine (monkeys of Africa and Asia) and platyrrhine (monkeys of Central and South America) species, suggesting it may be effective in other members of these clades for capturing conserved regions. One potential issue that requires further study is that the probes (which are designed from human genetic variation) do not specifically avoid or take into account within-species polymorphisms. To show that TMS data could detect expected biological patterns, in ways that are comparable to existing technologies, we performed the same analyses of epigenetic age estimation and tissue-specificity in matched TMS, EPIC, and RRBS data, respectively. From these analyses we found that the epigenetic ages estimated with TMS versus EPIC data were highly correlated as were genome-wide estimates of tissue specific patterns from TMS versus RRBS data. The potential portability of epigenetic clock algorithms is particularly encouraging, as this approach is becoming increasingly popular for measuring biological age [90–92], and will be exciting to pair with cost-effective methods going forward. Together, our optimized TMS protocol has the potential to add value and enable larger-scale studies in the many fields that query DNA methylation patterns, such as genetic medicine, developmental biology, evolutionary biology, anthropology, public health, geroscience, and more. For the Tsimane participants, informed consent was collected at three levels: by the individual (formal written consent), by the community, and by the Tsimane Gran Consejo (Tsimane governing body). All study protocols, including the generation of DNA methylation data, were approved by the Institutional Review Boards of the University of New Mexico (#07–157), the University of California Santa Barbara (#3-21-0652), and Universidad Mayor San Simon, Cochabomba. For the Orang Asli participants, informed consent was also collected at multiple levels: first by first describing the project to the community as a whole and seeking the permission of community leaders, and subsequently through individual-specific review of the protocol and formal written consent. The study protocol, including the generation of DNA methylation data, was approved by Vanderbilt University (IRB #212175) as well as the Malaysian Medical Research Ethics Council. For the rhesus macaque samples, the study protocol was approved by the Institutional Animal Care and Use Committee through the University of Puerto Rico’s Caribbean Primate Research Center (IACUC #A400117). For the gelada samples, the study protocol was approved by the Institutional Animal Care and Use Committees at the University of Washington (protocol 4416-01) and Arizona State University (20–1754 R) along with approval from the Ethiopian Wildlife and Conservation Agency. For the tufted capuchin samples, the study protocol was approved by the Institutional Animal Care and Use Committee at the Georgia State University (protocol A20018). Data generation drew on previously collected samples from multiple human and non human primate populations. A brief description of each population is provided below. The Tsimane are an Indigenous horticulturalists population spread across >90 villages in the Bolivian lowlands and totaling approximately 17,000 people [52]. We extracted DNA from 192 venous whole blood (WB) samples collected between the years of 2010–2021 as part of the Tsimane Health and Life History Project (THLHP). The THLHP has continuously collected demographic, behavioral, environmental, and health data along with the provision of medical services for over two decades [93]. Samples were frozen in liquid nitrogen, transferred on dry ice to Arizona State University, and stored at -80°C prior to analysis. The sample set for this project included 103 females and 89 males, with a mean age of 54.3 years old (range 18.0–83.6 years old) (see Table 1). Genomic DNA was extracted using the Zymo Quick-DNA 96 kit (Zymo Research #D3012) following the manufacturer’s instructions. The Orang Asli consist of ~19 ethnolinguistic groups and a total population of ~210,000 people [58]. They traditionally subsist on a mixture of hunting, gathering, fishing, small-scale farming, and trade of forest products [94,95]. We extracted DNA from 88 white blood cell (WBC) samples that were collected in 2023 as part of the Orang Asli Health and Lifeways Project (OA HeLP) [58]. Samples included in data generation were derived from venous blood draws followed by washing with QIAGEN PureGene red blood cell lysis. Samples were stored in liquid nitrogen upon collection, and at -80C for longer term storage. The Orang Asli sample included 46 females and 42 males, with a mean age of 35.3 years old (range 18–78 years old; Table 1). Genomic DNA was extracted using the Zymo Quick-DNA/RNA MagBead kit (Zymo Research #R2131) following the manufacturer’s instructions. We were granted access to de-identified EPIC array data (Infinium MethylationEPIC v2.0 Kit) and TMS data from 55 paired human whole blood samples. These samples were sourced from a healthy cohort recruited through the Vanderbilt University Medical Center (VUMC) in Nashville, TN USA. Due to IRB restrictions, demographic data or other metadata were not available for these samples. We obtained extracted DNA from rhesus macaque tissue samples (n = 96) collected by the Cayo Biobank Research Unit in partnership with the University of Puerto Rico’s Caribbean Primate Research Center (CPRC) [96–100]. Beginning in 2016, samples were collected from individuals living on the island of Cayo Santiago, an NIH-managed free-range colony of provisioned rhesus macaques. Specifically, as part of an ongoing population management plan designed by CPRC, select individuals were culled and tissues from all major organ systems were systematically harvested, stored in a fixative buffer, and frozen at -80C. This data set consists of samples from six different tissue types: adrenal, heart, kidney, lung, liver, and spleen, with 16 samples from each type and samples coming from 23 unique individuals (S3 Table). This dataset includes samples from 11 females and 12 males, ages 3.2 to 19.9 years old (mean 10.6 years old), collected from 2016–2019 (Tables 1 and S3). Genomic DNA was extracted using the Zymo Quick-DNA/RNA MagBead kit (Zymo Research #R2131) following the provided manufacturer’s protocols. We extracted DNA from whole blood from 68 geladas; 21 were female and 47 were male and all were considered adult (i.e., over 4 years old, the minimum average age of reproductive maturation in this species [101]) (Table 1). Gelada samples were collected as part of the Simien Mountains Gelada Research Project (SMGRP) which, since 2017, has carried out annual capture-and-release campaigns to collect morphometric data and whole blood samples from wild Ethiopian geladas [102]. Samples were stored in liquid nitrogen upon collection, and at -80C for longer term storage. Genomic DNA was extracted using the Qiagen DNeasy Blood & Tissue kits (Qiagen #69581) following the provided protocols. Blood was collected from individuals in the captive tufted capuchin monkey colony at Georgia State University in January 2023. Of the 28 capuchins, 19 were female and 9 were male with an average age of 19.4 years old (range 9–41 years old; Table 1). A trained veterinarian anesthetized the monkeys using 13 mg/kg Ketamine, delivered intramuscularly. Whole blood samples were collected during the monkeys’ annual physicals, stored at 4°C upon collection, and shipped to Arizona State University where they were flash frozen into 0.5mL aliquots and stored at -80°C until used for analysis. DNA was extracted using the Qiagen DNeasy Blood & Tissue kits (Qiagen #69581) following the manufacturer’s protocols. We used the Qubit dsDNA assay to determine the quantity of all extracted DNA. DNA libraries were normalized and prepared using the NEBNext Enzymatic Methyl-seq kit (P/N: E7120L) following a modified version of the manufacturer’s protocol that included 9 cycles of PCR for the final library amplification followed by a 0.65X bead cleanup. To prepare for the hybrid capture reaction, the total DNA input requirement (2000ng in this case) was divided by the total number of samples being pooled (12, 24, 48, or 96 as will be discussed below). In the 96-plex experiment, for example, 84ng of DNA from each sample was pooled totaling 8ug, and ¼ of the volume was used for the hybrid reaction and captured using the Human Methylome panel from Twist Biosciences following the manufacturer’s instructions (P/N: 105521). The final post-capture PCR reaction was split into 2 reactions per pool and cleaned with a 1X bead cleanup and then combined. Pool quality was assessed post-hybridization using the Agilent Bioanalyzer and quantified using a qPCR-based method with the KAPA Library Quantification Kit (P/N: KK4873) and the QuantStudio 12K instrument. Prepared library pools were sequenced on the NovaSeq 6000 at the Vanderbilt Technologies for Advanced Genomics (VANTAGE) Core. We used 150 bp paired-end sequencing and generally targeted 30-50M paired-end reads per sample. Real Time Analysis Software (RTA) and NovaSeq Control Software (NCS) (1.8.0; Illumina) were used for base calling. MultiQC (v1.7; Illumina) was used for data quality assessments. For each sample, we applied the Illumina DRAGEN Methylation Pipeline v4.1.23 using the custom bed file from Twist Biosciences. The deliverables from DRAGEN consist of FASTQs, bams, cytosine reports (which include counts of methylated and unmethylated reads per CpG site), and methyl and mapping metric reports. To determine whether TMS can be effectively multiplexed beyond the recommended 8-plex, we used 96 Tsimane samples to test four different multiplexing strategies during capture: 12-, 24-, 48-, and 96-plex. To test whether TMS is robust to DNA input amounts, we tested five input amounts: specifically, 25, 50, 100, 200, and 400 ng of sample were used as input into the EM-seq library prep. Here, we kept the plexing strategy constant (12-plex) and used three Tsimane samples, each represented three times within each pool and included three replicates of a control sample (HG01879 from the 1000 Genomes Project) [103]. To optimize the capture efficiency of Twist target sites, we tested the use of two different annealing temperatures–65° and 68° C–along with the use of a methylation enhancer (ME)– produced by Twist Biosciences (Catalog #103557) consisting of Tris EDTA buffer to block the binding of off-target probes thereby improving on-target capture efficiency. The specific combinations we explored were: testing a 65°C annealing temperature with 0uL (n = 192), 2uL (n = 96), and 4uL (n = 96) of ME and testing a 68°C annealing temperature with 0uL (n = 96) and 2uL (n = 192) of ME. These experiments were conducted with 96-plexed Tsimane samples (n = 192), and using 200 ng of sample input. Next, we tested the use of an enzymatic fragmentation method to replace the Covaris (LE220) mechanical fragmentation approach. Mechanical fragmentation is known to decrease library quality through damage to DNA; however, enzymatic fragmentation is not currently recommended by the TMS protocol. To compare these approaches, we performed the optimized TMS with enzymatic fragmentation using 4uL of NEBNext UltraShear (NEB #M7634S/L) for 10 or 20 minutes. This experiment was conducted using 96-plexed samples from the Orang Asli (n = 88) and using 200 ng of sample input. To evaluate the efficacy of optimized TMS on three NHP species—macaques, geladas, and capuchins—we applied the 96-plex protocol design from experiment 1 with 200 ng input. To compare rhesus macaque TMS to RRBS, we generated RRBS libraries using 150 ng of DNA input in combination with 1ng of lambda phage DNA and 1uL of Msp1—a digestive enzyme which cuts CCGG nucleotide motifs. Next, using NEBNext Ultra II reagents, we performed end repair and adapter ligation to the DNA fragments produced by Msp1 digestion. We then performed bisulfite conversion on the fragments using the EZ-96 DNA Methylation Lightning MagPrep kit (Zymo Research #D5046) following the manufacturer directions. The samples were then PCR amplified for 16 cycles with unique dual indexed sequencing primers. We selected for 180–2000 bp fragments and sequenced on an Illumina NovaSeq S2 flow cell with 2x51bp sequencing [80,104]. For experiments 1, 2, 7, and 8, we used a custom bioinformatics pipeline to process all FASTQ files into counts of methylated versus unmethylated cytosines at each CpG site. For experiments 3, 4, 5, and 6, we used Illumina’s Dynamic Read Analysis for GENomics (DRAGEN) pipeline [105] to process all FASTQ files into counts of methylated versus unmethylated cytosines at each CpG site. Importantly, both our custom pipeline and DRAGEN follow the same general steps and rely on the Bismark suite [106], making them highly comparable. We also processed 7 samples from experiment 4 using both methods to empirically confirm that our custom pipeline and the Illumina DRAGEN pipeline produced near identical results (S15 Fig). For our custom pipeline, we first trimmed adapters using Trimmomatic (version 0.39) [107] and TrimGalore (version 0.6.6) [108] for human and NHP samples, respectively. Following trimming, we used Bismark (version 0.24.0) [106] to map reads to each species’ respective genomes (hg38 for human, mmul10 for rhesus macaque, cimit for capuchin, and tgel1 for gelada). We retained only uniquely mapped reads and used the methylation extractor function within Bismark to extract counts of methylated versus unmethylated cytosines at each cytosine. These files were further filtered for CpG contexts only. For all samples, run through either the custom or DRAGEN pipeline, we extracted two measures of data quality that are automatically calculated by Bismark: the percent of reads that mapped uniquely to the reference genome and the average methylation percentage for cytosines in a CHH context. The latter value serves as a commonly used estimate of the efficiency with which a given protocol converts unmethylated cytosines to thymine, because cytosines located outside of CpG contexts are extremely unlikely to be methylated in the mammalian genome [109,110]. Estimates of CHH methylation were extracted from an automatically generated report file when using Bismark to align the trimmed FASTQ files to the reference genome. For experiments 1 and 2, we tested whether multiplexing strategy and input amount impacted mapping efficiency and percent CHH methylation using a one-way ANOVA test, followed by a pairwise t-test in the case of significance, with the ‘aov’ and ‘pairwise.t.test’ functions in the ‘stats’ R package [111]. For each study, we used the BSseq R package [112] to compile count matrices (derived from our custom pipeline or DRAGEN) across samples and to perform region, coverage, and missingness filtering. For experiments 1, 3, 4, 5, and 6 we used built-in functions in BSseq to filter for sites within the probes regions (+/- 200 bp) and for sites covered at>5X in >75% of samples. We made slight modifications to this filtering pipeline for other experiments. For experiment 2, where n = 3 for each input amount, we relaxed our missingness filter to sites with at least one read observed in at least ⅔ samples. For experiment 7, which focused on NHP genomes for which the probe set coordinates (which are provided in hg38) are irrelevant, we did not perform region filtering. The number of sites analyzed for each experiment (reported in the main text and in S4 Fig) therefore varies slightly depending on sample size, sequencing coverage, and other factors that impact which CpG sites passed our filters. To confirm the fidelity of optimized TMS, we also checked whether CpGs captured by the protocol were distributed as expected throughout different genomic regions (e.g., promoters, enhancers) and that the average methylation levels in different genomic regions were as expected. To do so, we annotated each CpG site by whether it fell into a gene body, promoter, or non-genic region, and by chromatin state. We used hg38 gene body coordinates from Ensembl’s ‘biomaRt’ package in R, and we defined promoter regions as the 2000 bp region upstream of TSSs. We annotated CpGs as falling in chromatin states as defined by hg38 ChromHMM annotations from NIH’s Roadmap Epigenomics Project [54]. We then counted the number of CpG sites that fell in each region (Fig 2C) and evaluated the median methylation across samples (Fig 2D). A subset of our experiments sought to understand and optimize two measures of efficiency of the hybrid capture step: 1) how many of the expected CpG sites from the probe set passed our filtering parameters and were thus analyzable and 2) how many of the reads we generated for a given sample were on-target and putatively captured by the probe set, rather than representing off-target randomly sequenced DNA fragments that do not contribute to analyzable data as they are often sparsely shared between samples. For #1, we used the bedtools (version 2.28.0) [113] intersect command to determine the proportion of CpG sites that are within +/- 200 bp with at least 1 probe [using a bed file available on the Twist Biosciences website (https://www.twistbioscience.com/resources/data-files/twist-human-methylome-panel-target-bed-file)]. For #2, we used the bedtools function bamtobed to convert the mapped reads for each sample into a bed file; because we used a paired end sequencing strategy, each bed coordinate included a fragment start position from R1 and a fragment end position from R2. We then used the bedtools intersect command to determine the proportion of mapped read pairs that are within 200 bp of at least 1 Twist probe. To understand what level of coverage is necessary to detect particular effect sizes in different sample sizes, we conducted a power analysis using simulated data based on the true coverage distribution of 1,000 sites in our TMS dataset, drawing with replacement to simulate sample sizes of n = 100, 200 and 400 (following the methods in [49,59]). We then assigned each sample a binary predictor variable (0 or 1), estimated methylation level differences between groups for a given effect size, and simulated the number of methylated counts per sample we would observe under this scenario by sampling from a binomial distribution (given the number of total counts and the probability of a count being methylated). We simulated data for effect sizes ranging from a 0–20% difference in methylation levels between groups and calculated power as the proportion of sites in which the predictor variable had a significant effect on methylation at a nominal p-value threshold of 0.001. We ran this analysis 3 times, first using the true mean coverage of our dataset (~20x), then again simulating coverage of 40x and 80x by multiplying the total counts of each site by 2 and 4, respectively. We used our filtered BSSeq object from experiment 5 to compare to data from the EPIC array generated for 55 paired human samples (average number of CpG sites measured with EPIC = 936,280; average call rate = 0.999). We downloaded the EPIC CpG coordinates from the Illumina website and merged with the TMS CpG locations, resulting in a shared dataset of 682,295 CpG sites passing filters and common to both technologies. We then performed two analyses to understand consistency. First, we calculated the average per-site methylation level across all samples included in the TMS or EPIC array datasets, respectively. We then ran a linear model testing the relationship between the two sets of average methylation levels using the ‘lm’ function in the ‘stats’ package in R. Second, we used the ‘lm’ function to estimate the R2 value comparing per-site methylation levels for estimates derived from each technology for a given individual (i.e., not averaged across the dataset). This resulted in a distribution of 55 R2 values. Because all humans share canonical methylation patterns, we also compared this distribution to a distribution of 55 R2 values derived from the same analysis after sample identity was permuted. We used the ‘t.test’ function in the ‘stats’ package in R to ask whether these distributions were significantly different. We used a very similar strategy to compare ~ 30x WGBS data generated for six paired Tsimane samples with TMS data generated from experiment 1 (96-plex, 200 ng input). First, we performed low level processing of the WGBS data using Illumina’s DRAGEN pipeline and merged this with our filtered TMS data, resulting in 3,078,771 CpG sites common to both datasets. We calculated the average methylation level across samples reported for each site and technology and ran a linear model using the ‘lm’ function in the ‘stats’ package in R to calculate the R2 value. We did not compare individual-based R2 values to permuted values for this experiment, given the small number of individuals. To estimate the number of CpG sites that we expected to recover when applying the human probe set to each NHP species, we converted the probe bed file to a FASTA file using the bedtools command ‘getfasta’ [113] and the hg38 reference genome. We then used Bismark to map the FASTA file to each non-human primate’s respective genome. From the mapped bam file, we used the ‘bamToBed’ function in bedtools to extract coordinates for the mapped probes and to add a + /- 200 bp offset. Finally, we applied the ‘getfasta’ function in bedtools to extract the sequence for the mapped regions (plus the 200 bp buffer) from the non-human primate genome and to count the number of CpG sites in this region set. Similar to the comparisons between TMS and the EPIC array, we used paired RRBS data for the 96 rhesus macaque samples to directly compare methylation data generated using TMS versus RRBS. To do so, we processed the RRBS data using the same custom pipeline and filtering parameters described for TMS data, with the only modification being that we used the ‘—rrbs’ parameter in TrimGalore to remove unmethylated cytosines artificially introduced during library preparation from the 3’ end of fragments. We merged the filtered TMS and RRBS datasets, resulting in 721,766 CpG sites common to both technologies. As described for the TMS-EPIC array comparison, we then 1) calculated the average per-site methylation level across all samples included in each dataset and compared these vectors using linear models and 2) estimated the R2 value for methylation level estimates derived from each technology for a given individual, and used a t-test to compare this distribution to a distribution for the same analysis where sample identity was permuted (S16 Fig). First, we compared epigenetic age predictions from paired samples that were sequenced on different platforms. We estimated epigenetic age from the PC-based versions of five well-established epigenetic clocks, including the Horvath multi-tissue clock, Hannum blood clock, PhenoAge clock, and telomere length clock. The PC-based versions of these clocks have much higher reliability and less susceptibility to technical noise than the original CpG-site level clocks [73]. We estimated epigenetic age from these clocks using the PC-Clocks R package [114] and calculated the Pearson’s correlation coefficient for estimates from samples generated with TMS versus the EPIC array. Second, we compared tissue-specific effect size estimates between samples generated with RRBS and TMS. Specifically, we asked whether tissue type significantly (FDR < 5%) predicted DNA methylation among the multi-tissue macaque data for each technology, using beta binomial models implemented in the R package ‘aod’. We performed these analyses iteratively to compare a given tissue to all other tissues (for example, comparing liver versus all other tissues to estimate liver-specific effects). We limited this analysis to variably methylated CpG sites (median methylation <90% or >10%). https://doi.org/10.1371/journal.pgen.1011667.s001 (XLSX) https://doi.org/10.1371/journal.pgen.1011667.s002 (XLSX) P-values generated from pairwise t-tests comparing the percentage of reads that were uniquely mapped to the human genome from sequencing data generated using the TMS protocol with varying amounts of input DNA. * represents a significant (p < 0.05) difference in mapping efficiency between conditions. https://doi.org/10.1371/journal.pgen.1011667.s003 (XLSX) P-values generated from pairwise t-tests comparing the percentage of cytosines in a CHH context marked as methylated (an estimate of conversion efficiency). * represents a significant (p < 0.05) difference in percent CHH methylation between conditions. https://doi.org/10.1371/journal.pgen.1011667.s004 (XLSX) P-values generated from pairwise t-tests comparing the percentage of cytosines in a CHH context marked as methylated (an estimate of conversion efficiency). * represents a significant (p < 0.05) difference in percent CHH methylation between conditions. https://doi.org/10.1371/journal.pgen.1011667.s005 (XLSX) Sites are filtered for those within the Twist probe set and covered at>5x coverage in >75% of samples. The average number of reads is provided as the total, such that the number of paired end reads would be ½ the reported value. 200ng of DNA was used for each sample for all plexing experiments. https://doi.org/10.1371/journal.pgen.1011667.s006 (XLSX) Sites are filtered for those within the Twist probe set and covered at>5x coverage in >75% of samples. The average number of reads is provided as the total, such that the number of paired end reads would be ½ the reported value. All input experiments were pooled using the 12-plex strategy. https://doi.org/10.1371/journal.pgen.1011667.s007 (XLSX) The percentage of Twist target probes (n = 551,803) covered by at least one read for each plexing strategy. https://doi.org/10.1371/journal.pgen.1011667.s008 (XLSX) The percentage of Twist target probes (n = 551,803) covered by at least one read for each input amount. https://doi.org/10.1371/journal.pgen.1011667.s009 (XLSX) R2 values generated using linear modeling to compare average site-level methylation between plexing experiments. Average site-level methylation was calculated by averaging the percent methylation for each site across all samples within a given plexing strategy and comparing these with average site-level methylation within shared sites in an alternate plexing strategy. All sites were filtered for>5X coverage in >75% of samples. https://doi.org/10.1371/journal.pgen.1011667.s010 (XLSX) R2 values generated using linear modeling to compare average site-level methylation between input amount experiments. Average site-level methylation was calculated by averaging the percent methylation for each site across all samples within a given input amount experiment and comparing with average site-level methylation within shared sites in an alternate input amount experiment. All sites were filtered for>5X coverage in >75% of samples. https://doi.org/10.1371/journal.pgen.1011667.s011 (XLSX) P-values generated from pairwise t-tests comparing the percentage of the total reads that were not associated with a Twist target probe (within +/- 200 bp) for each capture efficiency experiment. 65C/68C refers to annealing temperature and 0uL/2uL/4uL ME refers to volume of methylation enhancer. * represents a significant (p < 0.05) difference in the percent of probes captured between conditions. https://doi.org/10.1371/journal.pgen.1011667.s012 (XLSX) P-values generated from pairwise t-tests comparing the percentage of Twist target probes covered by at least one read for each capture efficiency experiment. 65C/68C refers to annealing temperature and 0uL/2uL/4uL ME refers to volume of methylation enhancer. * represents a significant (p < 0.05) difference in the percent of probes captured between conditions. https://doi.org/10.1371/journal.pgen.1011667.s013 (XLSX) Sites are filtered for those which are covered at>5x coverage in >75% of samples. https://doi.org/10.1371/journal.pgen.1011667.s014 (XLSX) R2 values generated using linear modeling to compare site-specific methylation for 3 samples, each processed with 3 different fragmentation methods- mechanical, enzymatic for 10 minutes, and enzymatic for 20 minutes. All sites were filtered for>5X coverage in >75% of samples. https://doi.org/10.1371/journal.pgen.1011667.s015 (XLSX) Age, sex, and tissue types for each individual in the rhesus macaque multi-tissue dataset, used to assess the function of TMS in a NHP with a direct comparison to RRBS data generated from these same samples (see also S8 Fig). https://doi.org/10.1371/journal.pgen.1011667.s016 (XLSX) https://doi.org/10.1371/journal.pgen.1011667.s017 (DOCX) Percentage of cytosines in a CHH context marked as methylated (an estimate of conversion efficiency) for varying (A) plexing strategies, and (B) input amounts. The dashed line refers to 1% CHH methylation and the solid line refers to 5% CHH methylation, a common cut off indicative of high levels of unmethylated cytosine conversion. https://doi.org/10.1371/journal.pgen.1011667.s018 (TIFF) (A) Average coverage per CpG site passing filters in a given experiment. Prior to calculations, CpG sites were filtered to include only sites within 200 bp of target probes and those with>5X coverage in more than 75% of samples. (B) Average read depth, in terms of paired-end reads, generated per sample in each experiment. https://doi.org/10.1371/journal.pgen.1011667.s019 (TIFF) Power analyses conducted on data for 1,000 simulated CpG sites (per sample size, effect size, and coverage combination) using the coverage distributions of observed, 96-plex TMS data. Lines represent the power to detect a 0–20% difference in methylation between two groups at a nominal p-value threshold < 0.001. Colors represent different levels of mean coverage per site (20x, 40x, and 80x) and facets represent sample sizes of n = 100, n = 200, and n = 400. https://doi.org/10.1371/journal.pgen.1011667.s020 (TIFF) Number of CpG sites within 200 bp of target probes after filtering for>5X coverage in more than 75% of samples by experiment. Colors are representative of each experiment which are defined in Fig 1C. https://doi.org/10.1371/journal.pgen.1011667.s021 (TIFF) Site-level methylation averaged across 3 samples processed using mechanical fragmentation, enzymatic fragmentation for 10 minutes, and enzymatic fragmentation of 20 minutes. Each point represents a site measured across both fragmentation methods and R2 values were generated using linear modeling. https://doi.org/10.1371/journal.pgen.1011667.s022 (TIFF) We subset the mapped read files for each sample (n = 88) included in our enzymatic fragmentation experiment (experiment 4) to include a random subset of 25, 50, or 75% of the total reads. We calculated the average (A) and median (B) coverage for on-target sites (y-axis) and observed a linear relationship between coverage and the number of subset reads (x -axis), which is useful for estimating what sequencing depth per sample will be needed to obtain various degrees of coverage. https://doi.org/10.1371/journal.pgen.1011667.s023 (TIFF) DNA methylation levels (A: n = 115,982 matched CpG sites; B: n = 575,401 matched CpG sites) averaged across 55 VUMC samples processed using TMS and the EPIC v2 array. Each point represents a site measured across both processing methods and R2 values were generated using linear modeling. https://doi.org/10.1371/journal.pgen.1011667.s024 (TIFF) For samples processed using both TMS and the EPIC array (n = 55 VUMC samples), we assessed the correlation in site-level average methylation levels for (A) hypomethylated regions (<50% average methylation); (B) intermediately methylated regions (average methylation above 10% and below 90%); (C) hypermethylated regions (>50% methylation); (D) UCSC-annotated CpG islands; (E) UCSC-annotated CpG shores (i.e., regions within 2kb of the boundaries of a CpG island); and (F) UCSC-annotated CpG shelves (regions within 2kb and 4kb of the boundaries of a CpG island R2 values were calculated using linear modeling; sample sizes represent the number of CpG sites included in each panel. https://doi.org/10.1371/journal.pgen.1011667.s025 (TIFF) For samples processed using both TMS and the EPIC array, we assessed the correlation in site-level methylation for variable sites (methylation >0.1 and <0.9) and all sites after permuting sample ID randomly. R2 values were generated using linear modeling. https://doi.org/10.1371/journal.pgen.1011667.s026 (TIFF) (A) Density plot showing the average methylation of a site (i.e., across samples) for filtered (>5X coverage in >75% of sites) sites captured between the three technologies (726,597 EPIC Array sites; 4,990,351 TMS sites; and 5,000,659 WGBS sites). Sites were not matched between the three technologies. (B) Average coverage per site of sites captured by WGBS after filtering for>5X coverage in >75% of samples. Median average coverage is 24.0X. https://doi.org/10.1371/journal.pgen.1011667.s027 (TIFF) The majority of individuals had 4 + tissues represented in the dataset. https://doi.org/10.1371/journal.pgen.1011667.s028 (TIFF) Percentage of cytosines in a CHH context marked as methylated (an estimate of conversion efficiency) following optimized TMS using genomic DNA from capuchins, geladas, and macaques. The dashed line refers to 1% CHH methylation and the solid line refers to 5% CHH methylation, a common cut off indicative of high levels of cytosine conversion. https://doi.org/10.1371/journal.pgen.1011667.s029 (TIFF) Site-level DNA methylation estimates averaged across 96 rhesus macaque samples processed using TMS and RRBS. Each point represents a site measured across both fragmentation methods and R2 values were generated using linear modeling. RRBS enriches for CpG dense regions of the genome, which tend to be hypomethylated. https://doi.org/10.1371/journal.pgen.1011667.s030 (TIF) Sites filtered for>5X coverage in >75% of samples processed using a given technology. A greater number of sites are covered consistently across all 96 samples using TMS compared to RRBS. https://doi.org/10.1371/journal.pgen.1011667.s031 (TIFF) Each point represents the average methylation at a given site for 88 samples that were processed using both pipelines (R2 = 0.9972). https://doi.org/10.1371/journal.pgen.1011667.s032 (TIFF) For samples processed using both TMS and RRBS, we assessed the correlation in site-level methylation for all sites after permuting sample ID randomly and compared them to non-permuted, or matched, sample IDs. R2 values were generated using linear modeling. Using a t.test, we found a significant difference between the means of the two samples (t = 7.6796, p-value = 8.224 x 10–13, mean of matched samples: 0.8345, mean of permuted samples: 0.7508). https://doi.org/10.1371/journal.pgen.1011667.s033 (TIFF) First, we thank the Tsimane and Orang Asli study participants and communities for their involvement and support. We also thank all members of the research teams associated with the Tsimane Health and Life Histories Project, the Orang Asli Health and Lifeways Project, the Simien Mountains Gelada Research Project, and the Caribbean Primate Research Center Cayo Santiago Field Station. Second, we thank the members of the Lea Lab at Vanderbilt University, the SMack Lab at Arizona State University, and the Vanderbilt Technologies for Advanced Genomics (VANTAGE) team for their feedback, expertise, and support in completing this work. Third, we are grateful to the research infrastructure provided by Vanderbilt University’s Advanced Computing Center for Research and Education and Arizona State University’s Sol Computing clusters. Fourth, we thank Dr. Rex Howard and Dr. Michael Hart, the GSU veterinarians, for their assistance in collecting the capuchin blood samples. The Cayo Biobank Research Unit Scientific Stakeholders are: Susan Antón, Lauren Brent, James Higham, Melween Martínez, Amanda Melin, Michael Montague, Michael Platt, Jerome Sallet, and Noah Snyder-Mackler.
--------------------------------------------------

Title: Sam Altman's $6.4 billion bet on Jony Ive has Zuckerberg-like qualities
URL: https://www.cnbc.com/2025/05/22/sam-altmans-6point4-billion-bet-on-jony-ive-zuckerberg-like-qualities.html
Time Published: 2025-05-22T12:00:01Z
Description: OpenAI is making a big bet in purchasing Jony Ive's nascent startup, but it's a deal reminiscent of some made by Mark Zuckerberg.
--------------------------------------------------

Title: D&AD Awards 2025: who won what?
URL: https://www.creativeboom.com/news/dad-awards-2025-who-won-what/
Time Published: 2025-05-22T11:52:00Z
Full Content:
With the judging process changing dramatically this year, which agencies won big in the D&AD Awards 2025? With the judging process changing dramatically this year, which agencies won big in the D&AD Awards 2025? For many, it's the biggest and most nerve-wracking night in the creative calendar. D&AD held its 63rd awards ceremony last night (Thursday 22 May) at London's Southbank Centre and handed out a record-breaking 668 Pencils across all categories. Most significantly, three coveted Black Pencils—the highest D&AD accolade—were awarded to groundbreaking work that exemplified the year's themes of commercial impact and narrative excellence. With no quota system for Pencil awards, D&AD maintains its reputation for uncompromising standards. In some years, no Black Pencils are given out at all, making the awarding of three a significant achievement for these winners: Designing Paris 2024 by W Conran Design won in Graphic Design for its innovative approach to sports marketing. The judges highlighted how the design created a unifying yet distinctive feel that successfully merged heritage elements with modern sporting aesthetics. A$AP Rocky - Tailor Swif by Iconoclast LA won in Music Videos, exemplifying the year's emphasis on narrative storytelling. The winning work featured long-form storytelling where music was integral to the narrative structure rather than an afterthought, demonstrating how narrative arcs can transcend different mediums and artistic disciplines. Spreadbeats by FCB New York won in Digital Marketing for what the agency described as "Innovation through nostalgic technologies." This work aligned with a trend of brands cleverly inserting themselves into events, sports, and games through inventive approaches. This year's judging process marked a major evolution in D&AD's approach. Jurors emphasised the critical importance of commercial viability, seeking work that demonstrates tangible business impact and drives meaningful behavioural change rather than celebrating creativity for its own sake. In other words, if there was ever a danger that agencies would make work primarily to win awards rather than serve the client, that's now history. In 2025, you have to do both. "This year's awards celebrated the power of design not just as a form of art but as a catalyst for commercial success and behavioural change," noted Dara Lynch, CEO of D&AD. "The idea is that innovative ideas must possess both aesthetic appeal and tangible impact." The Black Pencil winners perfectly embodied this philosophy. For instance, W Conran Design's 'Designing Paris 2024' was recognised for illustrating how design thinking can transform an entire city's attitude and behaviour. Judges from the Graphic Design category praised it as "a breakthrough for sports marketing and traditional sports marketing aesthetics," noting its playful yet scalable approach that successfully blended heritage with contemporary sport. Alongside commercial effectiveness, jurors were looking for exceptional craft. From Radio and Audio to Film, their message was clear: in an increasingly automated creative landscape, looking good simply isn't enough. "The resurgence of craftsmanship stands as a reminder that in an era of automation, true excellence lies in the thoughtful execution of ideas," Dara explained. "It's not enough just to look good; true creative excellence must also leave a lasting impression." This was particularly evident in D&AD's new Creator Content category, where judges called for clearer definitions of craft excellence and higher standards in content creation. The winning entries all demonstrated work that creates meaningful impact and genuinely engages audiences. The 2025 D&AD Awards achieved unprecedented global reach, with entries submitted from 86 countries worldwide—the highest number in the awards' history. This international participation resulted in over 30,000 pieces of work from 11,689 total entries. "It's exciting to see so many brands and companies refreshing their identities," said D&AD trustee Lisa Smith, global executive creative director at Jones Knowles Ritchie. "Judging has been challenging. Too many entries follow the same established design codes and trends, making everything start to look and feel alike, regardless of category. The work that stood out was the kind that breaks away from the expected: inspiring, well-crafted, and truly fit for purpose." The awards ceremony also recognised outstanding agency performance across various specialisations. FCB New York claimed the prestigious title of Advertising Agency of the Year, while Serviceplan Design was named Design Agency of the Year for the first time. DIVISION continued their remarkable winning streak, earning Production Company of the Year for the fifth consecutive year. In the network categories, Serviceplan took home Independent Network of the Year, while FCB was also named Network of the Year. Apple received recognition as Client of the Year, acknowledging its commitment to creative excellence across its marketing initiatives. The 2025 D&AD President's Award went to Koichiro Tanaka, founder of interdisciplinary creative boutique Projector. Chosen by D&AD President Kwame Taylor-Hayford, Tanaka was recognised for his pioneering work at the intersection of storytelling, interactivity, and craft, which helped define a formative era in digital creativity. "I'm very honoured," Tanaka said. "To receive this award from D&AD, which has lived longer than I have, and to feel part of its long story is now something I live with. Being recognised with care and an open heart is a special thing to me." Kwame, who is co-founder of Kin, explained his choice: "Koichiro Tanaka's pioneering work at the intersection of storytelling, interactivity, and craft helped define a formative era in digital creativity. His career journey, bold ideas, and meticulous attention to detail have been a constant source of personal inspiration." Beyond the three Black Pencils, the ceremony awarded three White Pencils, 48 Yellow Pencils, 176 Graphite Pencils and 434 Wood Pencils. Additionally, four Future Impact Pencils were awarded, recognising work that demonstrates potential for significant positive change. The diverse range of winning work spanned 44 categories, from traditional advertising and design to emerging areas like Gaming & Virtual Worlds and the newly introduced Creator Content category. Craft categories collectively garnered 198 Pencils, underscoring the year's emphasis on exceptional execution. The complete list of Pencil-winning work and shortlisted entries are showcased on the D&AD website, while the official D&AD Rankings will be released alongside the digital D&AD Annual. These rankings provide definitive tables of the most successful companies, networks, countries and clients based on the awards results. Get the best of Creative Boom delivered to your inbox weekly Creative Boom empowers and uplifts the creative community. Since 2009, we've been delivering the best in creativity through news, inspiration, insights, and advice to help you stand out and succeed in a competitive industry. Creative Boom™ © 2025 Creative Boom Ltd. Registered in England and Wales #07437294.
--------------------------------------------------

Title: Ryan Reynolds-Linked MNTN Goes Public at $16 Per Share to Grow Connected TV Ads
URL: https://www.adweek.com/convergent-tv/ctv-ryan-reynolds-mntn-ipo-public-16-per-share/
Time Published: 2025-05-22T11:21:40Z
Full Content:
We deliver! Get curated industry news straight to your inbox. Subscribe to Adweek newsletters. Connected TV advertising firm MNTN is making its market debut on the New York Stock Exchange. The company announced a $16-per-share IPO on May 21, the high end of its expected range. The share price puts MNTN’s valuation at around $1.2 billion. Actor Ryan Reynolds has served as MNTN’s chief creative officer since the company bought his creative shop, Maximum Effort, in 2021. MNTN divested from Maximum Effort in March, but Reynolds stayed on as CCO. However, Reynolds didn’t travel to New York for the IPO this week, MNTN founder and chief executive officer Mark Douglas told ADWEEK. MNTN aims to capitalize on the demand the company has seen for performance-focused CTV ad placements by going public, Douglas said. MNTN’s advertisers are primarily small and medium-sized businesses, with 96% using the ad platform to experiment with TV advertising for the first time. According to bankers’ estimates, there was demand for 14 times the shares that MNTN had available to sell, Douglas said, which helped bolster the case for an IPO. “The market responded,” he said. “A lot of our customers depend on Google, Meta, and now MNTN to help drive the growth in their business. So we want to be transparent in terms of the strength of our business, [in terms of] scale and so forth, to those companies.” MNTN’s offering consists of 11.7 million shares in total, with 8.4 million from the company itself and another 3.3 million from existing stockholders. Stockholders also gave underwriters—which include Morgan Stanley, Citigroup, and Evercore—the option to buy an additional 1.8 million shares at the initial price within 30 days of the IPO. While MNTN didn’t make an overall profit in 2024, adjusted earnings before interest, taxes, depreciation, and amortization (also known ads EBITDA) were $39 million, Douglas noted. He expects that to increase in 2025. MNTN Is Selling Ryan Reynolds’ Agency Maximum Effort, SEC Filing Shows As it eyes the next phase of growth, MNTN plans to continue focusing on small and medium-sized businesses, Douglas said. The company will also invest in product innovation using artificial intelligence. “We like large brands also—we have some of those,” Douglas said about MNTN’s mix of clients. “But in the time it takes to get one of those, you can add 6,000 smaller brands on the platform.” Douglas sees MNTN’s role as a matchmaker rather than as an advertising company, connecting consumers with brands and products they might love through 30-second ads on popular streaming platforms like HBO Max, Bravo, and ESPN. “It’s a really impactful medium,” Douglas said. Douglas founded the company in 2009 as SteelHouse, rebranding to MNTN in 2021. Kathryn Lundstrom is Adweek's sustainability editor.
--------------------------------------------------

Title: 8 Best AI SEO Tools for 2025 (Tested Firsthand)
URL: https://www.semrush.com/blog/best-ai-seo-tools/
Time Published: 2025-05-22T10:43:00Z
Full Content:
As a content marketer, I regularly use AI SEO software to analyze data faster, optimize content, and achieve higher rankings. But with so many tools making bold claims, it's hard to separate the genuinely useful from the overhyped. That's why I’ve tested dozens of AI SEO tools myself and narrowed them down to the 8 that actually deliver results. Here's my detailed review of each. Spoiler alert: here are my favorite AI tools for SEO: 1. ContentShake AI for generating SEO-friendly content 2. Semrush Copilot for personalized SEO recommendations 3. Clearscope for SEO content optimization 4. SurferSEO for advanced SEO content creation 5. ChatGPT for brainstorming and data analysis 6. Copy.ai for automating your SEO workflows 7. SERP Gap Analyzer for finding keyword opportunities 8. Perplexity for conducting online content research Let’s look at each of them in detail. Semrush Content Toolkit is the smart writing tool we've developed at Semrush. It combines AI with our proprietary SEO data to help you create entire articles, find content ideas, and write in your unique brand voice. I recommend using this tool for creating SEO-friendly drafts, especially if you’re not an SEO expert. I love that it finds your target keywords and analyzes SERPs so that it can optimize your content for search intent—something SEO beginners usually struggle with. Let’s look at a sample workflow. First, enter a high-level content idea and get dozens of data-driven keyword and topic suggestions. Next, click ‘Start writing’ and choose whether you want to create an SEO content brief or generate a full blog post from scratch. This will open the setup window, where you can adjust your target keywords, content length, and brand voice. Check the ‘Add an extra SEO boost’ box for advanced SERP analysis and click ‘Create article’. In just a few minutes, your draft will be ready. You can then use the blog editor to add more original ideas and further enhance your content. Content Toolkit also includes an integrated AI chat, an AI image generator, and a content optimization tool to improve your copy. Pros Cons Very easy to use—ideal for beginners with little SEO experience Less suited to large teams and enterprise users Uses Semrush’s data to optimize AI-generated content Some stock images suggested by the tool may not always be relevant Covers the entire content creation cycle, from ideation to publishing Semrush Copilot is an AI-powered assistant that offers personalized recommendations based on your SEO performance. Copilot analyzes all of your Semrush data from tools like Site Audit, Backlink Gap, and Keyword Gap.. It then consolidates the issues and action items into tailored recommendation cards, related to your keyword rankings, domain authority, organic competitors, and more. I use it to regularly check SEO recommendations and spot potential issues, such as with technical SEO or backlinks. When I see a recommendation or alert, I head to each specific tool to investigate it further: Pros Cons Makes it easy to prioritize action items for SEO performance Exclusive to Semrush users: not compatible with other tools Helps navigate the many tools in Semrush’s suite While the reports provide a lot of useful data, some users may want more in-depth summaries Offers daily alerts to keep you posted about real-time concerns Converts data into actionable insights in order of priority Clearscope is an AI content platform that helps with keyword research, content optimization, and analytics. The content optimization feature offers somewhat similar benefits to other AI content tools. But the workflow is different. When you add an existing URL and your target keyword, the tool will analyze the published page for you. It will then provide suggestions for improvement, such as: The tool also helps with topic research and generates basic SEO reports. For example, it tracks your ranking positions and provides high-level content analytics metrics. Pros Cons Automates the process of building content briefs and optimizing content More expensive than most SEO content creation tools and has no free trial Keep tabs on published content to find opportunities to refresh content Doesn’t offer many content creation possibilities Provides content analytics, which simplifies performance analysis Content analytics and keyword research tools are somewhat basic, and the content grade simply checks if you’ve covered all related keywords SurferSEO offers a suite of SEO tools to discover keywords, find content ideas, create outlines, optimize content, and more. Many content teams use it for content optimization. Similar to Clearscope, it analyzes the organic results for your target keywords and makes content structure recommendations. Surfer also offers some useful features for search engine optimizers, including a topical map, an SERP analyzer, and a content audit tool. A word of caution: I’d be careful with metrics like optimal content length and the recommended number of headings. While they might be useful suggestions, you’ll know best if they make sense for your particular content piece. Finally, while you can generate full articles, they each cost an additional $19, with a maximum of 30 articles on the cheapest plan. Pros Cons Easy to use when working with content writers, as it helps to track keywords in your drafts Might not be the best choice for beginners and folks looking for more automation as it involves more steps and requires more SEO experience Sleek and modern UI Limited credits make it difficult to use freely The Google Docs extension makes it easy to work anywhere The Google Docs extension can get a bit buggy Lots of data-driven tools useful for SEOs and content marketers The Content Editor doesn’t let you adjust your targets 💡 Find out who comes out on top in our ContentShake AI vs SurferSEO comparison. In the screenshot above, ChatGPT claims it can do a variety of SEO tasks. But I would take this with a pinch of salt. ChatGPT, along with other AI chatbots like Gemini and Claude, is amazing for brainstorming and analysis. But it doesn’t have any real-time SEO data and is not adapted for long-form content creation. So, I’d only use it together with an actual SEO tool that has access to SERP data. That said, ChatGPT is useful for other SEO tasks such as: Let’s look at an example. I downloaded a list of keywords from Semrush, uploaded the CSV file to ChatGPT, and asked it to group my keywords by category: Here’s the result: This is just one of many examples of how ChatGPT can save you time with SEO tasks—as long as you already have the data you need. Pros Cons Powerful AI with integrated data analysis features Doesn't offer real-time SEO data and specialized SEO output ChatGPT's best model (ChatGPT 4-o) is free for all users The interface is not optimized for long-form content creation Beginner-friendly interface and affordable price point Prone to factual errors and inaccurate responses 💡 Check out how ChatGPT compares to ContentShake AI. Copy.ai is an AI-powered automation platform with content creation tools for sales and marketing needs. Like other AI writing tools, it was originally used primarily for content creation. But it has recently shifted its focus towards automation. Copy.ai's new standout feature is its ability to create multi-step workflows for SEO processes. You can use the library of workflow templates for different use cases, such as writing a blog post, an SEO brief, and so on. The tool also lets you design custom workflows to automate different parts of your SEO setup. Here’s what it looks like: Pros Cons The unique workflow creation feature works great for enterprise SEO teams and large agencies with lots of processes It’s somewhat hard to use and might take a while to figure out Offers a sleek and modern UI Doesn’t offer any SEO data Integrates with 2,000+ tools SERP Gap Analyzer helps you discover low-difficulty keywords for your content plan. You can use it to find relevant keywords with higher search volume and less competition. This is especially helpful if you’re in a saturated niche and have to compete with lots of high-authority websites. Just enter your site and a seed topic to run an analysis. The tool then analyzes the search results for multiple relevant keywords and displays the ranking difficulty for each keyword. You can access other insights such as global search volume, SERP weaknesses, and more. Pros Cons Solves one of the key SEO pains—finding high-potential content topics Doesn’t integrate with most other SEO tools, except Semrush Generates briefs, outlines, and other content Can be a bit confusing to figure out Documents SERP analysis in detail to guide SEO efforts Perplexity is a conversational AI tool that’s connected to the Internet. It lets you parse the web, find information quickly, and conduct deep web research. Wondering why I included this tool in a list of SEO software? Because effective SEO is closely connected to quality content. And quality content needs fresh information and useful examples. I often use Perplexity to find out in minutes what could otherwise take hours of scouring web sources. For example: imagine I need to find an example of a content marketing campaign for small businesses for an article I’m writing. Perplexity will use various online sources and instantly suggest interesting ideas: Pros Cons Conducts image and video search There’s a limited number of questions that you can ask in a day on the free plan Organizes research with collections of multiple threads Limited to research and doesn’t serve very well for content creation Performs focused research on specific sites, such as YouTube, Reddit, etc. Sometimes hallucinates AI SEO tools use artificial intelligence to automate and fast-track SEO processes, such as keyword research, SERP analysis, and content creation. They can help you make better data-based decisions, increase productivity, and save time on routine tasks. Earlier this year, I surveyed over 2,500 small businesses and asked them about their experience with AI. And guess what? 68% of companies report higher SEO and content marketing ROI when using AI. There has been a lot of talk lately about Google “penalizing” AI-generated content. Let’s get this sorted once and for all. Google doesn’t care if it’s AI copy or not. It cares only about your content being genuine, useful, and relevant. Here’s what else they say: “Poor quality content isn't a new challenge for Google Search to deal with. We've been tackling poor quality content created both by humans and automation for years.” We’ve further proven that AI-generated content ranks nearly as well as human-written copy—as long as it offers value and original insights. I wouldn’t recommend relying solely on AI to create content for SEO and content creation. Why? Because AI doesn’t have the unique topical expertise that you need to create compelling content. But using AI as a tool to create content under your supervision and with your input? Or leveraging it to generate content ideas, cluster keywords, and analyze SEO data? Why not! That’s a wrap on my weeks of testing and shortlisting my favorite AI SEO tools. The key takeaway? Choose a tool that suits your specific needs and context. For example: Remember: there’s a limit to what AI tools can do. Use them to get rid of routine tasks, and to support your work, not replace it, and you won’t be disappointed. Margarita Loktionova Keyword search volume is the average number of monthly searches for a search term in a particular location. Google Keyword Planner is a free tool that lets you research the queries people type into Google. Learn how to get backlinks by responding to media requests, creating link bait, finding broken links, & more.
--------------------------------------------------