List of news related to Meta stock price META:

Title: US Stocks Creep Near Records as Traders Eye US-China Trade Talks
URL: https://financialpost.com/pmn/business-pmn/us-stocks-creep-near-records-as-traders-eye-us-china-trade-talks
Time Published: 2025-06-10T16:18:19Z
Description: US stocks flipped between small gains and losses on Tuesday as the US and China resumed their tariff talks for a second day in London, with financial markets on edge as the world’s two largest economies try to avoid escalating the ongoing trade war further.
--------------------------------------------------

Title: You can buy Meta Ray-Bans for 20% off on Amazon right now - but it won't last long
URL: https://www.zdnet.com/article/you-can-buy-meta-ray-bans-for-20-off-on-amazon-right-now-but-it-wont-last-long/
Time Published: 2025-06-10T14:55:00Z
Full Content:
'ZDNET Recommends': What exactly does it mean? ZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we’re assessing. When you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers. ZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form. Among the most popular tech products of the last year, the Meta Ray-Ban smart glasses have been the surprise hit. Amazon is currently offering the version with the classic black frames for $239 (20% off the $299 retail price). This is an especially excellent deal when you consider that many styles of the Meta Ray-Bans were often out of stock for much of the first half of last year -- and the smart glasses have rarely been discounted. A lot of the appeal is the fact that they look just like normal Ray-Bans, and you can even replace the lenses with your own glasses prescription. The caveat to keep in mind here is that Amazon sells eight different styles of the Meta Ray-Bans, and it's the classic black frames with green-tinted lenses that are on sale for the lowest price. Other styles cost $30 to $80 more. For example, if you want to wear these smart glasses both inside and outside, then you can get a pair with transition lenses. Those normally retail for $379, but Amazon is offering them 20% off, for $303. I tried the Meta Ray-Bans earlier this year and was much more impressed than I expected. First, to be clear, they are augmented reality glasses, but they're audio-only, so there's no screen overlaid on the lenses. All the feedback comes through the speakers that down-fire into your ears and are hidden in the arms of the glasses. Also: Why Meta's Ray-Ban Smart Glasses are my favorite tech purchase this year I was also pleasantly surprised by the audio quality. For phone calls and conference calls and for listening to podcasts and audiobooks, the sound is nearly as bright and clear as a pair of AirPods or other quality earbuds. For listening to music, movies, or shows with lots of bass and surround sound effects, the Meta Ray-Bans are acceptable but not nearly as robust as a pair of high-end headphones from Apple, Sony, Bose, and others. And because of the open concept, naturally the Meta Ray-Bans can't offer the kind of noise cancellation that you get from AirPods Pro or the Sony WF-1000XM5 earbuds, for example. All in all, the sound of the Meta Ray-Bans is similar to what you get from the AirPods Pro in transparency mode. But, the Meta Ray-Bans can also do things that no headphones can do. They have a built-in 12-megapixel ultrawide camera for taking photos and capturing videos. This camera is about the same quality as an iPhone 12 or a Samsung Galaxy S20. It only takes photos and videos in portrait mode, but it's far faster to tap the button on top of the right arm of the glasses than it is to pull your camera out of your pocket. ZDNET senior editor Sabrina Ortiz in her Meta Ray-Bans. The Meta Ray-Bans also have a built-in AI assistant that you can trigger by saying "Hey Meta," and use the large language models that Meta has been furiously building over the past several years. For basic information searches, it works well and is another nice option for leaving your phone in your pocket. The battery life is about a half-day with moderate use, but you can quickly charge to about 50% with 20 minutes in the included case. The case itself charges via standard USB-C, which is nice. You can nearly double the battery by turning off the "Hey Meta" listening feature -- which I did. Meta also regularly rolls out software updates that bring new functionality to the Meta Ray-Bans. For example, in 2024, the Meta Ray-Bans added the capability of translating from Spanish to English in real time and the ability to do a visual search to identify landmarks or translate street signs. And Meta says it will continue to add new features such as helping you remember things you've seen, like your parking spot. Also: I tested the best AR and MR glasses: Here's how the Meta Ray-Bans stack up If you're concerned about the way Meta handles privacy -- as many of us are -- the good thing about the Meta Ray-Bans is that they don't require a Facebook account. While they do require a Meta account, you can at least ostensibly keep them separate from your Facebook, Instagram, and WhatsApp data. That makes it easier for me to recommend the product from a privacy perspective. Don't get me wrong, the AirPods Pro 2 are available for a solid deal as well at $170 ($79 off) right now. But the Meta Ray-Bans can do nearly all that the AirPods Pro can do and more. And since smart glasses are getting more and more common and the promise of smarter AI-powered features are on the horizon, getting a pair of Meta Ray-Ban smart glasses gives you a better chance of getting a taste of the future. Based on the 20% savings on offer, ZDNET's rating system gives this a 3/5 Editor's deal rating. However, I've bumped it up to a 4/5, as this is in line with the price cut we saw on the glasses during Cyber Week, typically the biggest sale of the year. These glasses also rarely go on sale, making this an even more enticing offer. Deals are subject to sell out or expire at any time, though ZDNET remains committed to finding, sharing, and updating the best product deals for you to score the best savings. Our team of experts regularly checks in on the deals we share to ensure they are still live and obtainable. We're sorry if you've missed out on a deal, but don't fret -- we constantly find new chances to save and share them with you on ZDNET.com. We aim to deliver the most accurate advice to help you shop smarter. ZDNET offers 33 years of experience, 30 hands-on product reviewers, and 10,000 square feet of lab space to ensure we bring you the best of tech. In 2025, we refined our approach to deals, developing a measurable system for sharing savings with readers like you. Our editor's deal rating badges are affixed to most of our deal content, making it easy to interpret our expertise to help you make the best purchase decision. At the core of this approach is a percentage-off-based system to classify savings offered on top-tech products, combined with a sliding-scale system based on our team members' expertise and several factors like frequency, brand or product recognition, and more. The result? Hand-crafted deals chosen specifically for ZDNET readers like you, fully backed by our experts. Also: How we rate deals at ZDNET in 2025
--------------------------------------------------

Title: Byju’s fire sale; Meesho’s pre-IPO move
URL: https://economictimes.indiatimes.com/tech/newsletters/tech-top-5/byjus-fire-sale-meeshos-pre-ipo-move/articleshow/121755170.cms
Time Published: 2025-06-10T13:23:36Z
Full Content:
Want this newsletter delivered to your inbox? Updated On Jun 10, 2025, 07:59 PM IST Want this newsletter delivered to your inbox? Thank you for subscribing to Daily Top 5We'll soon meet in your inbox. Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Definitions Top Commodities Top Prime Articles Top Slideshow Top Story Listing Top Market Pages Latest News Follow us on:
--------------------------------------------------

Title: $10,000 invested in QUAL ETF a year ago is now worth…
URL: https://www.fool.com.au/2025/06/10/10000-invested-in-qual-etf-a-year-ago-is-now-worth/
Time Published: 2025-06-10T02:59:30Z
Description: This ASX ETF follows an index that uses a filter to ensure all stock holdings meet 3 financial metrics. 
The post $10,000 invested in QUAL ETF a year ago is now worth… appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: Magnificent Seven shares set for a comeback: expert
URL: https://www.fool.com.au/2025/06/10/magnificent-seven-shares-set-for-a-comeback-expert/
Time Published: 2025-06-10T01:51:13Z
Description: The US Mag 7 stocks appear to have lost some of their lustre with investors. But perhaps not for long. 
The post Magnificent Seven shares set for a comeback: expert appeared first on The Motley Fool Australia.
--------------------------------------------------

Title: Plasma Attracts $500 Million for ICO—And One Trader Spent $100K on Ethereum Gas Fees
URL: https://decrypt.co/324237/plasma-500-million-ico-trader-spent-100k-ethereum-gas
Time Published: 2025-06-09T20:56:52Z
Full Content:
Plasma Attracts $500 Million for ICO—And One Trader Spent $100K on Ethereum Gas Fees $109,487.00 $2,852.62 $2.32 $667.94 $166.48 $0.999793 $0.202303 $0.282364 $0.718915 $2,851.85 $42.90 $109,034.00 $3,455.26 $3.49 $15.51 $22.39 $444.31 $0.280957 $9.04 $3.29 $0.00001347 $2,845.66 $0.177238 $3,053.73 $0.999637 $93.45 $4.31 $335.25 $1.00 $1.001 $4.86 $0.00001317 $8.27 $109,425.00 $0.63897 $311.94 $31.72 $432.16 $1.18 $2.69 $5.19 $6.14 $54.29 $0.100855 $1.00 $0.910896 $201.03 $18.59 $32.10 $1.055 $0.089953 $0.678846 $0.369693 $18.40 $0.02585791 $11.02 $1.001 $0.817609 $0.237854 $4.05 $4.59 $0.41859 $108,910.00 $1.19 $0.089485 $4.44 $2.75 $0.206648 $118.83 $2,852.43 $4.63 $1.67 $0.999523 $2.24 $1.001 $0.494936 $2.23 $175.12 $1.00 $1.45 $11.41 $0.00001788 $13.89 $3,244.68 $2,978.00 $0.01917877 $1.27 $0.398564 $0.732598 $4.08 $0.068479 $0.206851 $0.717281 $3,051.20 $0.562828 $2,981.17 $1.00 $1.028 $0.175142 $0.642312 $109,318.00 $0.999789 $0.999355 $0.715277 $0.101507 $2,999.63 $0.00009619 $109,772.00 $668.56 $0.990573 $2.68 $185.75 $109,772.00 $0.819427 $50.99 $3,334.65 $3,346.86 $24.29 $216.27 $0.01792419 $0.01210604 $0.130039 $0.19503 $0.01529853 $2.18 $0.2983 $0.535724 $2.48 $1.11 $4.34 $0.00000071 $111.38 $34.22 $0.999765 $0.629501 $0.654612 $2,851.93 $0.997811 $0.393947 $42.83 $1.76 $2.00 $108,576.00 $0.230356 $2,981.74 $3,040.58 $2,846.49 $0.734589 $2.04 $1.093 $0.584138 $0.058496 $0.999836 $0.299888 $3,084.40 $0.515587 $2.97 $1.34 $59.01 $9.30 $109,306.00 $2,862.30 $0.20233 $109,373.00 $0.01518872 $1.66 $0.42305 $0.594651 $0.146264 $0.990671 $7.27 $0.43857 $0.603103 $0.00000121 $42.81 $0.00469419 $0.643123 $1.64 $2,849.67 $0.483649 $0.04990764 $0.00002284 $16.03 $6.43 $1.36 $0.00483188 $2.73 $106,357.00 $0.061186 $1.80 $3,022.82 $0.173735 $0.999795 $0.08491 $0.00732298 $0.00000042 $37.51 $1.00 $0.161325 $0.416914 $1.09 $0.995031 $3,140.78 $0.999842 $0.04038771 $0.585921 $3,105.44 $1.40 $1.00 $0.01940967 $2,855.67 $0.078983 $0.0068763 $0.02349304 $0.358376 $0.238396 $109,503.00 $180.63 $0.838009 $109,061.00 $130.58 $2,852.66 $0.00407203 $0.03464389 $8.27 $22.47 $0.00006236 $0.744427 $0.00409286 $19.93 $0.00462547 $0.999964 $0.697204 $0.999851 $0.226203 $0.999503 $0.00345552 $2.50 $0.501737 $27.15 $0.303237 $0.293416 $0.140895 $0.00000143 $0.118334 $17.36 $109,057.00 $0.02955191 Plasma attracted $500 million in stablecoin deposits in mere minutes Monday for its upcoming token sale—and one user spent $100,000 on fees in an attempt to jump the queue. Traders' bullishness around the public sale has led some to believe that the initial coin offering trend is back. ICOs were all the rage in 2017, but fizzled out due to a string of failed projects and growing regulatory scrutiny. In its documentation, Plasma is described as a Bitcoin sidechain that will use the network as a settlement layer as it attempts to “meet the unique needs of stablecoins.” The public sale will auction off 10% of the supply of XPL via deposits into an Ethereum vault through the recently debuted Sonar token sale platform. While the sale generated $500 million worth of interest, a Plasma team member clarified on X that only $50 million worth of XPL tokens will actually be sold. By holding funds in the deposit vault, users earn the option to purchase a share of tokens in the future sale, but are not obligated to do so—and they can withdraw their funds at any time. They'll also earn yield on their deposited tokens in the meantime. The Plasma XPL token sale just raised $500M in 5 minutes That’s 10x oversubscribed. One guy spent $100K on gas, just to get in. pic.twitter.com/hu46OcDdCv — Arkham (@arkham) June 9, 2025 Stablecoins have become a central focus in crypto lately. The tokens, typically pegged to and backed by the U.S. dollar or other traditional assets, underpin the crypto industry, allowing traders to enter and exit trades with ease. Stablecoins don't offer the potential allure of massive gains like other cryptocurrencies, but they're an essential piece of the crypto market. The United States Senate is attempting to pass a bill that would establish a framework for legally issuing stablecoins in the States, dubbed the GENIUS Act. Meanwhile, Circle, the issuer of USDC, the second-largest stablecoin by market capitalization according to CoinGecko, began trading on the New York Stock Exchange last Thursday, in a roaring success. CRCL soared to another new peak price Monday, more than quadrupling the initial offering price. This whirlwind of excitement around stablecoins may explain why Plasma’s pre-sale vault filled up so quickly, as a project building a stablecoin-specific blockchain. One trader was so keen to get involved that they paid 39.15 ETH (just over $100,000) to ensure their deposit of $10 million in USDC was executed before the vault filled up. The extra $100,000 came in what is called a priority fee, which is an optional feature that allows users to pay as much as they want to incentivize validators to process their transaction quicker. This is particularly effective when the network is congested, as it may have been this time with a crowd of people attempting to enter the public sale all at once. When traders anticipate that others may be upping their priority fees, a gas war breaks out, resulting in huge bills not seen during the normal course of transacting. This behavior was present during the initial coin offering buzz of 2017 and the NFT boom of 2021-22, but more recently, meme coin traders have deployed the same strategy to "snipe" in-demand tokens. In the Plasma situation, however, the trader may have overshot the mark with their $100,000 priority fee. Out of the top five depositors, according to Etherscan, the highest fee paid was 4.49 ETH (over $11,500). Plasma raises $500M in literal seconds for something that nobody seems to be able to describe in detail. This tells me that we’re back speedrunning 2017 x 2021. This is the crypto supercycle we all dreamed about. The bubble of all bubbles is about to inflate. And I’m here for it. — Beanie (@beaniemaxi) June 9, 2025 “Plasma instant cap fill today is a gigantic beacon signaling meta shift,” pseudonymous trader IcoBeast wrote on X. “ICOs are officially back, and Sonar is likely the most impactful crypto-native release of 2025 for the direction of crypto.” ICOs soared in 2017 with countless projects raising capital by selling crypto tokens, including prominent projects like Filecoin and Tezos. Within a year, Bitcoin.com found, 46% of projects failed before or after raising funding. Victor Teixeira, fund principal at investment firm Contango Digital Assets, believes that the hysteria experienced during the 2017 ICO craze has been here the whole time—it’s just been in meme coins. “The meme coin market we experienced in the last year is the same confluence of hype and retail adoption that we saw in the 2017 ICO boom,” Teixeira told Decrypt. “What I think we're seeing is that even though TRUMP and other meme coins onboarded a lot of investors into Web3, unless you got into these projects within the first two to three hours, you lost money.” “Retail still wants to take a punt on the 1000x play, and if you can capture that demand and tie it to an actual product, you have a much stronger story to tell, with actual potential to hit the upside,” he added. “In this last cycle, we just didn't have a real project that could grab that imagination and excitement as well as meme coins could." But not everyone is happy with the Plasma ICO, due to the top 10 participants filling 40% of the available deposit supply—a sign that so-called crypto whales dominated the sale. A total of 1,108 wallets were able to participate in the vault deposit campaign—that’s an average of over $450,000 each. That said, according to Etherscan, 141 traders contributed less than $1,000 apiece. No date has been set for the actual token sale, though as traders have suggested, the impact of Monday's surging demand may be felt broadly across the market if other projects sense there's untapped demand for a new wave of ICOs. Edited by Andrew Hayward Your gateway into the world of Web3 The latest news, articles, and resources, sent to your inbox weekly. © A next-generation media company. 2025 Decrypt Media, Inc.
--------------------------------------------------

Title: US Stocks Touch Sessions Highs as US-China Trade Talks Begin
URL: https://financialpost.com/pmn/business-pmn/us-stocks-touch-sessions-highs-as-us-china-trade-talks-begin
Time Published: 2025-06-09T18:32:40Z
Description: US stocks traded near session highs on Monday after oscillating between small gains and losses earlier, as investors awaited the outcome of crucial trade talks between Washington and Beijing in London.
--------------------------------------------------

Title: Strategy Boosts Corporate Stash With $110 Million Bitcoin Top-Up As Total Holdings Reach 582,000 BTC
URL: https://zycrypto.com/strategy-boosts-corporate-stash-with-110-million-bitcoin-top-up-as-total-holdings-reach-582000-btc/
Time Published: 2025-06-09T16:24:10Z
Full Content:
Michael Saylor-led Strategy, formerly MicroStrategy, expanded its Bitcoin coffers with its most recent purchase. According to a Monday US Securities and Exchange (SEC) filing, Strategy scooped up 1,045 Bitcoin between June 2 and June 8 at an average price of $105,426 per coin. The Tysons, a Virginia-based firm, is the world’s largest corporate holder of Bitcoin, controlling 582,000 Bitcoin worth roughly $62.7 billion based on current prices. Strategy started accumulating Bitcoin in August 2020 with an initial purchase of 21,454 BTC for $250 million. It now owns the equivalent of 2.8% of Bitcoin’s total 21 million supply. Bitcoin was changing hands at $107,687 at press time, up about 1.9% over the past 24 hours, according to crypto market data provider CoinGecko. Last month, the apex crypto climbed to an all-time high of $111,000 before giving up those gains. Strategy’s total cache was purchased at an average price of $70,086 per Bitcoin for a total cost of around $40.8 billion, including fees and expenses. Strategy’s most recent Bitcoin acquisition was financed with proceeds from at-the-market sales of its perpetual Strike preferred stock, STRK, and perpetual Strife preferred stock, STRF. The company raised $66.4 million and $45.8 million by selling STRF and STRK, respectively. “Strategy is fully torqued Bitcoin,” Strategy co-founder and executive chairman Michael Saylor posited Friday on X, highlighting the Bitcoin Treasury company’s comparatively impressive performance compared to Tesla, Bitcoin, Meta, and gold, among other companies and assets over the past year. The company’s market capitalization stands at $102.3 billion, up from the $1.2 billion reported in July 2020 before kicking off its Bitcoin accumulation spree. Its success story has inspired a long list of other publicly traded companies to follow suit. With Trump Media and Gamestop recently joining the likes of Metaplanet (nicknamed “Japan’s MicroStrategy”) and Semler Scientific in adopting Saylor’s Bitcoin playbook and building their own BTC treasuries.
--------------------------------------------------

Title: Deadline Alert: DoubleVerify Holdings, Inc. (DV) Investors Who Lost Money Urged To Contact Glancy Prongay & Murray LLP About Securities Fraud Lawsuit
URL: https://www.globenewswire.com/news-release/2025/06/09/3096055/34548/en/Deadline-Alert-DoubleVerify-Holdings-Inc-DV-Investors-Who-Lost-Money-Urged-To-Contact-Glancy-Prongay-Murray-LLP-About-Securities-Fraud-Lawsuit.html
Time Published: 2025-06-09T16:00:00Z
Full Content:
June 09, 2025 12:00 ET | Source: Glancy Prongay & Murray LLP Glancy Prongay & Murray LLP LOS ANGELES, June 09, 2025 (GLOBE NEWSWIRE) -- Glancy Prongay & Murray LLP reminds investors of the upcoming July 21, 2025 deadline to file a lead plaintiff motion in the class action filed on behalf of investors who purchased or otherwise acquired DoubleVerify Holdings, Inc. (“DoubleVerify” or the “Company”) (NYSE: DV) common stock between November 10, 2023 and February 27, 2025, inclusive (the “Class Period”). IF YOU SUFFERED A LOSS ON YOUR DOUBLEVERIFY INVESTMENTS, CLICK HERE TO INQUIRE ABOUT POTENTIALLY PURSUING CLAIMS TO RECOVER YOUR LOSS UNDER THE FEDERAL SECURITIES LAWS. What Happened? On May 7, 2024, DoubleVerify released its first quarter 2024 financial results and reduced its 2024 revenue guidance, disclosing that there had been a pullback in customer spending on advertising. On this news, DoubleVerify’s stock price fell $11.79, or 38.6%, to close at $18.78 per share on May 8, 2024, thereby injuring investors. Then, on February 27, 2025, DoubleVerify reported lower-than-expected fourth quarter 2024 sales and earnings due in part to reduced customer spending. The Company also disclosed that the shift of ad dollars from open exchanges to closed platforms was having a negative impact on the Company. On this news, DoubleVerify’s stock price fell $7.83, or 36%, to close at $13.90 per share on February 28, 2025. Then, on March 28, 2025, Adalytics Research, LLC published a report alleging, among other things, that DoubleVerify’s web advertisement verification and fraud protection services were ineffective and that its customers were regularly billed for ad impressions served to bots. The same day, The Wall Street Journal reported that DoubleVerify regularly missed detection of nonhuman traffic despite the Company’s claims that it helps brands avoid serving ads to nonhuman bot accounts. What Is The Lawsuit About? The complaint filed in this class action alleges that throughout the Class Period, Defendants made materially false and/or misleading statements, as well as failed to disclose material adverse facts about the Company’s business, operations, and prospects. Specifically, Defendants failed to disclose to investors that: (1) DoubleVerify’s customers were shifting their ad spending from open exchanges to closed platforms, where the Company’s technological capabilities were limited and competed directly with native tools provided by platforms like Meta Platforms and Amazon; (2) DoubleVerify’s ability to monetize on its Activation Services was limited because the development of its technology for closed platforms was significantly more expensive and time-consuming than disclosed to investors; (3) DoubleVerify’s Activation Services in connection with certain closed platforms would take several years to monetize; (4) DoubleVerify’s competitors were better positioned to incorporate AI into their offerings on closed platforms, which impaired DoubleVerify’s ability to compete effectively and adversely impacted the Company’s profits; (5) DoubleVerify systematically overbilled its customers for ad impressions served to declared bots operating out of known data center server farms; (6) DoubleVerify’s risk disclosures were materially false and misleading because they characterized adverse facts that had already materialized as mere possibilities; and (7) as a result, Defendants’ positive statements about the Company’s business, operations, and prospects were materially misleading and/or lacked a reasonable basis at all relevant times. If you purchased or otherwise acquired DoubleVerify securities during the Class Period, you may move the Court no later than July 21, 2025 to request appointment as lead plaintiff in this putative class action lawsuit. Contact Us To Participate or Learn More: If you wish to learn more about this action, or if you have any questions concerning this announcement or your rights or interests with respect to these matters, please contact us: Charles Linehan, Esq., Glancy Prongay & Murray LLP,1925 Century Park East, Suite 2100, Los Angeles, California 90067 Email: shareholders@glancylaw.com Telephone: 310-201-9150Toll-Free: 888-773-9224 Visit our website at: www.glancylaw.com. Follow us for updates on LinkedIn, Twitter, or Facebook. If you inquire by email, please include your mailing address, telephone number and number of shares purchased. To be a member of the class action you need not take any action at this time; you may retain counsel of your choice or take no action and remain an absent member of the class action. This press release may be considered Attorney Advertising in some jurisdictions under the applicable law and ethical rules. Contact Us: Glancy Prongay & Murray LLP, 1925 Century Park East, Suite 2100Los Angeles, CA 90067 Charles LinehanEmail: shareholders@glancylaw.com Telephone: 310-201-9150Toll-Free: 888-773-9224 Visit our website at: www.glancylaw.com. LOS ANGELES, June 10, 2025 (GLOBE NEWSWIRE) -- Glancy Prongay & Murray LLP reminds investors of the upcoming July 15, 2025 deadline to file a lead plaintiff motion in the class action filed on... LOS ANGELES, June 10, 2025 (GLOBE NEWSWIRE) -- Glancy Prongay & Murray LLP reminds investors of the upcoming July 8, 2025 deadline to file a lead plaintiff motion in the class action filed on...
--------------------------------------------------

Title: Groupon Has Become a GLP-1 Affiliate Marketing and Bootleg Microsoft Office Racket
URL: https://www.thecaptainslog.io/groupon-glp-1-office-racket/
Time Published: 2025-06-09T15:31:01Z
Full Content:
While I could type out extensive notes detailing the proliferation of compounded semaglutide and tirzepatide products on Groupon, it's likely best to let the content mostly speak for itself. These GLP-1 offers have taken over Groupon in the last 18 months, with everything from paid #GrouponPartner influencers promoting GLP-1s on social media to continuous email marketing pushing these products. There are hundreds of different entities ranging from telehealth services to med spas that have advertised compounded GLP-1 medications on Groupon. My GROUPON Glp-1 is here! 🥳 🎉 🪅 🎊 #tirz #tirzepatide #compoundingpharmacy #tirz #blackgirlweightloss #WEIGHTCARE #GROUPON #HALLANDALEPHARMACY #fyp #Ad @Groupon has so many discounts, but did you know they have amazing products too? So excited to be back on my weight loss journey thanks to Groupon! If you’re interested in starting or starting back, check out the link in my bio to so many amazing Groupons! #GrouponPartner #GiftingWithGroupon #GrabLifeByTheGroupon Core to this short case is the unsustainable unit economics of these long tail telehealth and med spa providers offering GLP-1 services. Each of these providers competes with one another, driving prices down as they take a bath on offering the initial month, 6-week, 8-week introductory offer through Groupon. The only reason these providers are willing to offer such low margin, cheap deals is because they hope to convert enough of these introductory customers to recurring revenue streams over the coming months and years. While customers may get the first month for as low as $60-$80, most of these providers then enroll customers in $299, $399, $499, etc. monthly subscriptions. However, these Groupon customers are more customers of Groupon and less customers of a specific provider. This is a classic case of adverse selection which eventually leads to merchant churn. Given that so many providers are present on Groupon and given that it has been very publicly announced that the shortage is over and both 503A and 503B pharmacies offering compounded solutions would have to come to an end, consumers caught on in late 2024 into early 2025 that they can effectively just sign up for 1 trial with each provider on Groupon and stockpile doses from multiple providers rather than stick with a single provider. Obvious this is very reckless behavior and sampling 4-week, 6-week, 8-week trial offers from multiple brands is not ideal given that these medications come from different pharmacies with different add-ins, prescribed by different doctors. In the below TikTok video from November 2024, one user of Groupon GLP-1 medications lays out her behavior:"...I kept getting emails from Groupon that they have [GLP-1 treatments] for a lower price, and it's for one month only, so I'm just going to try it out and see, and there are different little companies that sell [GLP-1 treatments] on Groupon, like med spas pretty much, and if that works well for me I will be trying every single one of them." groupon GLP-1 is crazy 👀 #marriedsinglemom #changingmylife #glp1 #glp1forweightloss #tirzepatide #groupon Here's a price sensitive user posting on Reddit about shopping around on Groupon and trying out their third Groupon trial offer. This below Reddit user is also shopping around doing 1 of every trial offer on Groupon instead of sticking with 1 consistent provider. This below poster on Facebook spells the name of the medication incorrectly and promotes shopping around and doing this "1 of every trial offer on Groupon" behavior instead of sticking with a single stable provider. So the question then is: are Groupon vendors churning or about to churn? If these vendors are taking on too much risk or taking a bath on intro offers that do not turn into recurring revenue, is there evidence that they are no longer on the platform? It seems that after the April and May 2025 changes to compounded GLP-1 medications, there are serious cracks in Groupon's armor that have just developed. Crack in Armor #1: Major Groupon Player WeightCare Experiencing Issues This Past Month WeightCare has seen relatively stellar reviews...up until the past several weeks post April and May changes to FDA regulations. https://www.trustpilot.com/review/joinweightcare.com Crack in Armor #2: Entities with Groupon Specifically Named on Their Websites Are Churning and No Longer Have Active Offers on Groupon Wellyou.life is a related entity to Health Solution MD and they have pulled down their semaglutide and tirzepatide offers from Groupon. Below is telehealth entity called Weight Loss Injector which was made by a business called Cosmetics Injector that is located inside of a beauty salon in a shopping mall in King of Prussia, Pennsylvania. They have recently pulled down their Groupon offer and seem unable to compete with larger operations on Groupon that are offering deals around $50-$60. Below is a completely insane landing page experience from a med spa called Absolut Beauti (dot org?) in the Atlanta area, showing that this business clearly relies heavily on Groupon for weight loss customers. Absolut Beauti Bar actively advertises on Groupon for Botox and other filler products, yet appears to have now abandoned their Groupon efforts around GLP-1, which they have advertised since June 2024 a year ago. https://www.instagram.com/absolut_beauti/reel/C8hTzwbO0g2/ and https://www.groupon.com/deals/absolut-beauti-bar-9 While Groupon has historically had various software license and product key offerings of varying quality over the years, it is clear that current management has actively pushed Microsoft Office to align with Microsoft's October 1, 2024 release of Office 2024. Included in these efforts is a push in USA Today in March 2025 and paid sponsorship and priority placement on Google Search. Groupon's prior management team did not prioritize Microsoft Office-related efforts. The current management team's efforts show a clear spike in Q4 2024 through Q1 2025 centered around Microsoft's Office 2024 new version release, though this trend appears to have begun to tail off in Q2 2025. Additionally, there has been an uptick in reported scams and questionable product keys across social media and Microsoft forums related to Groupon's investment in pushing Microsoft Office 2024 sales such as this post about Groupon partner License Tom from February 2025 or this other post about License Tom from December 2024 or the many other posts across internet forums where Groupon customers report having issues with Office bought through Groupon. Given that Microsoft Office 2024 was only first available to consumers from October 1, 2024 onward and given that Groupon posts their unit sales increments, we can proxy a floor for billings contribution from Microsoft Office 2024 and especially contribution to the North America Local billings. Below are the current Microsoft Office 2024 offers listed on Groupon's website as of June 8, 2025. I note that these estimates are: a) simply a floor, and the real billings numbers are likely much higher given that the unit sales are presented as round numbers (eg. 10,000+, 25,000+, etc.) b) only reflective of the deals currently on the site as of June 6-8, 2025, and that many vendors come and go and are not currently displayed on Groupon c) only reflective of the recent Microsoft Office 2024 offerings, although the increased traffic driven from this Microsoft release certainly translated to sales on Office 2021 and other versions, which became even more discounted 25,000 units * $26 = $650,000 5,000 units * $99.99 = $500,000 500 units * $159.99 = $80,000 Note: this offer contains links that redirect to a site - techlogit.com - that seems to exclusively use Groupon. 5,000 units * $34.99 = $175,000 910 units * $34.99 = $31,850 10,000 units * $39.99 = $400,000 25,000 units * $39.99 = $1,000,000 10,000 units * $34.90 = $349,000 There are also a number of other offers advertising Microsoft Office 2024 post-October 1, 2024 that do not actually offer Microsoft Office 2024, only earlier versions of Office. One example is below: 1,000 units * $49.99 = $50,000 5,000 units * $79.99 = $400,00060 units * $199.99 = $12,000 270 units * $64.99 = $17,550 400 units * $39.99 = $16,000 It's obvious that home and business software is a priority for Groupon's management team and that this is contributing at least several million per quarter to North America Local billings and revenue. Notably, all of this home and software seems to have become listed under the "Local" business segment in recent years, even though there is nothing inherently "Local" about digital licenses. Looking into historical offers from the prior management team, home and business software appears to have been segmented as "Goods" in the past. Groupon makes it crystal clear that they operate three distinct categories - Local, Goods, and Travel - across both North America and International, and that the key driver to the current management's turnaround plan is investing in and getting returns from the North America Local slice of the business, which is pitched to investors as what most people think of when the think of Groupon - the restaurant and massage and night out experiences at discounted rates. As shown earlier, virtually all of the semaglutide/GLP-1 offers including the telehealth offers plus all of the home and business software are categorized as "Local" even though there is nothing inherently "local" about these deals. Even after posting strong North America Local results for Q4 2024 and for the FYE December 31, 2024 Groupon CEODusan Senkypl still remained opaque about the nature of what is driving the key North America Local results. On the associated earnings call from March 12, 2025 (which can be listened to here) there were no serious answers given to questions about what is driving the North America Local segment. From the call: Sean McGowan: Roth Capital Thank you. Appreciate that. Yes, can you talk a little bit about the - what you think was driving that Local growth in the US? It seemed like it really turned around. I mean, I think when you gave us an update on the third quarter conference call it was running down, through October. So what do you think drove that turnaround in the late part of the quarter? Dusan Senkypl Yes. So thanks for the question. We had a lot of headwinds during last year, with technical migrations and platform projects, which were impacting our ability to deliver. So it was definitely one of the drivers. And as we were either finalizing them, or mitigating impact, our platform returned to, let's say, the performance, which we are expecting from it. But at the same time, during the last several earnings calls, I was talking about our strategy shift, shifting to curated marketplace with sales organization focusing on quality when we don't go - for quantity. But we really look what we need on our platform. We are also spending much more time with our merchants, to make sure that they have deals, which perform. So I would say that this is a combination of all these elements, which finally all fit together, and they work and they are reflected in the results, especially in the later part of Q4. Operator All right, we'll now pose written questions to management that, came in through our Investor Relations press line. Our first written question is for Dusan. Can you share what the December Christmas period looked like, from a growth perspective? North America Local was running in the low single-digits positive, during Black Friday, Cyber Monday and was likely down negative high single-digits, to negative low double-digits in October. Based on your commentary at that time around guidance framing though, would this imply the business was running up positive double-digits in December Christmas based on reported North American Local growth of plus 8% year-over-year? Dusan Senkypl So December, was very good. However, what we have to take into account, is that year-over-year compare was impacted by timing of Black Friday and some Cyber Monday. So it's not really apples-to-apples, but even said that on a like-for-like basis, comparing holiday 2024, to holiday 2023, we believe we had a very successful year in Local including some great numbers in the lead up to Christmas, and period between Christmas and New Year. And what's important in the past, we have mentioned that we have observed that our platform tends to perform better during the key buying seasons, and Q4 was really no exception to that. For our Q1 outlook, as mentioned in earnings commentary, we lost some of the excitement from the Q4 season as we started Q1. But we have been pleased with the momentum in North America Local, and see continued growth in billings. And while we commented that our first quarter outlook assumes better performance in local versus Q4, at this time we don't believe it will be double-digit growth in billings. At no point are the elephants in the room of GLP-1 and Microsoft Office 2024 mentioned, nor has anyone brought up the fact that Groupon has quietly shifted its consumer software from "Goods" to "Local" or that all of these telehealth offers that blew up in growth through late 2024 are also "Local" designated on Groupon's US website. So what about Hims & Hers Health (NYSE: HIMS)? What about Ro and similar GLP-1 players? LifeMD (NASDAQ: LFMD)? Well, first of all, year-to-date through June 6, 2025 GRPN is outperforming HIMS. GRPN is up 174.5% YTD and HIMS is up 123.5% in the same period. Hims and Ro and other larger distribution players will eat first before all of the hundreds of smaller players that must rely on Groupon for marketing. These larger players are able to compete on Meta and Google while the smaller players give one-third or more of their intro offers to Groupon for customers who are largely not loyal to any specific brand. Additionally, Novo Nordisk has decided to partner with Hims & Hers as announced in April and Eli Lilly is partnering with Ro on Zepbound as announced in December. Novo Nordisk's Wegovy is also available on Ro and LifeMD. These companies like Hims and Ro are real enough businesses with responsibility to shareholders and tons of regulatory scrutiny on them. They are somewhat household names compared to the garbage that exists on Groupon, where many of the ads and offerings do not even have proper spelling or risk disclosures in the offers. With prices coming down for name brands, it is clear that the business models of these fly-by-night telehealth services no longer work. A quick run through of recent earnings transcripts of recent quarters for GRPN, HIMS, and LFMD is below showing the number of times semaglutide, GLP-1, weight loss, tirzepatide, and related terms are mentioned in each call, from mid-2023 through the final quarter corresponding with the end of calendar 2024. Groupon's management team is willfully omitting the two biggest trends driving growth in the key North America Local segment. These two trends are full of liabilities, risks, and sketchy vendors operating in legal grey markets. Regardless of any enforcement from the FTC, state AGs, FDA, etc. these two trends of compounded GLP-1 treatments and MS Office 2024's release cycle are over and wrapping up. While other bears may look extensively at merchant redemption rates or weakening consumer discretionary spend among low income Americans or Groupon's continued decline in revenue or even the increasing number of merchant complaints about Groupon on social media, I believe the true bear story here is that most investors long and short do not even fully appreciate that Groupon has become a GLP-1 and bootleg MS Office racket. The upside is limited and the stock is over $30 per share in line with Groupon's ideal case for their late 2024 debt refinancing. Groupon's management team cannot even discuss GLP-1 treatments or MS Office 2024 downloads when these are two of the larger trends of the past several quarters, so how can you be long this pile of rocks? If you really like GLP-1s, why aren't you investing your money into HIMS or LFMD or into the manufacturers directly instead of parking it in this much riskier affiliate marketing play on the crappiest GLP-1 pop-up shops? ________ This content is provided for informational purposes only, and should not be relied upon as legal, business, investment, or tax advice. You should consult your own advisers as to those matters. Lauren Balik does not represent the interests of any fund or of any investor other than herself. Past performance is not indicative of future results. This content speaks only as of the date published. Any projections, estimates, forecasts, targets, and/or opinions expressed in these materials are subject to change without notice and may differ or be contrary to opinions expressed by others. Opinion: Klaviyo is a Strong Sell, with extreme and under-appreciated risks through 2025. Its customers are poorly capitalized, tenuous businesses with massively disproportionate exposure to consumer discretionary. I am short KVYO. Klaviyo is MailChimp for e-commerce SMB. However, plenty of e-commerce companies also just use MailChimp or other email service Opinion: Strong Sell. I am short DOCS. I believe DOCS is not "LinkedIn for Doctors" but "ZoomInfo Targeting Doctors" and it is a predatory business model set to burst in 2025. Doximity AKA "LinkedIn for Doctors" is a business in declining health with major Opinion: Appian Corporation (NASDAQ: APPN) is a Strong Sell. Key Points * Appian positions itself as a "low code" application development solution with major F500 companies and many US federal government agencies among its customers. However, upon closer inspection it appears a large portion of Appian's revenue Free money for scrolling through hundreds of ads, free money for recruiting new members, high coupon sales for AppLovin going headstrong into an e-commerce narrative: Let's dive into AppLovin's related party e-commerce pyramid scheme Flip, which is driving AppLovin's DTC e-commerce growth as Wall Adventures in markets, stocks, and digital playgrounds
--------------------------------------------------

Title: Strategy Buys $110 Million in Bitcoin as BTC Holdings Near $63 Billion
URL: https://decrypt.co/324193/strategy-buys-110-million-in-bitcoin-as-btc-holdings-near-63-billion
Time Published: 2025-06-09T14:51:41Z
Full Content:
Strategy Buys $110 Million in Bitcoin as BTC Holdings Near $63 Billion $109,572.00 $2,851.23 $2.32 $668.46 $166.35 $0.999789 $0.20232 $0.282311 $0.718576 $2,850.25 $42.98 $109,568.00 $3,455.11 $3.49 $15.50 $22.36 $444.39 $0.281023 $9.00 $3.29 $0.00001346 $0.177549 $3,045.03 $2,851.83 $0.999684 $93.48 $4.30 $335.08 $1.00 $0.999936 $4.86 $0.00001317 $8.27 $109,596.00 $0.639304 $311.84 $31.73 $433.26 $1.18 $6.16 $2.69 $5.19 $54.28 $0.100915 $1.00 $0.909207 $200.83 $18.59 $32.14 $1.055 $0.090073 $0.678578 $0.370198 $18.40 $0.02581901 $11.01 $1.00 $0.816994 $0.236917 $4.06 $4.59 $0.41804 $109,573.00 $1.19 $0.089317 $4.44 $2.75 $0.206595 $118.85 $2,849.11 $4.63 $1.69 $1.00 $1.00 $2.24 $0.495532 $1.46 $2.24 $1.00 $175.16 $11.42 $0.00001788 $13.89 $3,241.33 $2,984.57 $0.01916589 $1.27 $0.39853 $0.729802 $4.08 $0.068607 $0.206506 $0.717861 $3,047.71 $0.563269 $0.99998 $0.175775 $1.027 $2,987.05 $0.643547 $109,368.00 $1.00 $0.999453 $0.716165 $0.101425 $2,984.82 $0.00009622 $109,541.00 $668.54 $0.990482 $2.67 $185.53 $109,467.00 $3,332.98 $50.98 $0.8206 $3,345.60 $216.06 $24.29 $0.01793058 $0.01210192 $0.194913 $0.129695 $0.01530733 $2.18 $0.298222 $0.535667 $2.48 $1.11 $4.34 $0.00000071 $111.38 $34.25 $0.999747 $0.628806 $0.655087 $2,846.44 $0.997774 $0.393245 $42.81 $1.77 $2.01 $108,573.00 $0.230332 $2,981.63 $3,043.59 $2.04 $0.734157 $2,848.46 $1.093 $0.583551 $0.058568 $0.999812 $0.299752 $3,077.66 $0.517218 $2.95 $1.33 $59.14 $109,442.00 $9.30 $2,849.95 $109,467.00 $0.201964 $0.01519303 $1.66 $0.425322 $0.594625 $0.991594 $0.146137 $7.27 $0.603193 $0.438645 $42.83 $0.0000012 $0.00469547 $0.644079 $1.64 $2,841.61 $0.48565 $0.00002286 $16.03 $0.04993491 $6.42 $1.36 $0.00482852 $2.73 $0.061259 $106,486.00 $1.80 $0.173653 $0.999717 $0.084948 $3,026.59 $0.00732275 $0.00000042 $1.00 $0.1613 $37.51 $1.09 $0.416329 $1.001 $0.999838 $3,138.40 $0.04037775 $0.586786 $1.40 $1.00 $3,112.16 $0.01943445 $2,847.52 $0.078954 $0.00687335 $0.358159 $0.0233372 $0.237228 $109,444.00 $180.65 $0.837717 $130.45 $109,288.00 $0.00407165 $22.37 $8.27 $0.03459127 $2,847.01 $0.00006233 $0.744511 $0.00415954 $19.94 $0.00460291 $0.999917 $0.696895 $0.999492 $0.226227 $1.00 $0.00345741 $2.51 $0.501642 $27.07 $0.303135 $0.292985 $0.14 $0.00000143 $17.36 $0.118217 $108,962.00 $1.013 Strategy, formerly MicroStrategy, acquired 1,045 Bitcoin worth roughly $110 million at the time of purchase, according to a Securities and Exchange Commission filing on Monday. The tokens, bought between June 2 and 8, bring Strategy’s total holdings of the cryptocurrency to roughly 582,000 Bitcoin worth $62.7 billion based on current prices, according to crypto data provider CoinGecko. The Tysons Corner, Virginia-based Bitcoin software company’s latest buy marks its fourth-smallest Bitcoin acquisition this year, according to Saylor Tracker, a log of Strategy’s Bitcoin holdings. It comes after the company last week notched its second smallest Bitcoin buy of the year, scooping up just $75 million worth of the digital asset. Bitcoin was recently trading at $107,300, up about 1.5% over the past 24 hours, according to crypto market data provider CoinGecko.. Strategy has announced a Bitcoin purchase for nine weeks straight, spending around $5.1 billion on Bitcoin since mid-April. In the filing, Strategy said that it has raised $112 million by issuing preferred stock. The company said that it raised $66.4 million and $45.8 million by selling its Perpetual Strife Preferred Stock (STRF) and Perpetual Strike Preferred Stock (STRK), respectively. Strategy’s stock price rose 2% during pre-market trading on Monday, hitting $382 per share, according to Yahoo Finance. Over the past month, shares have slid 2.9%. “Strategy is fully torqued Bitcoin,” Strategy co-founder and executive chairman Michael Saylor said Friday on X, formerly Twitter, highlighting the firm’s comparatively strong performance compared to Tesla, Bitcoin, Meta, and gold, among other companies and assets over the past year. The firm’s latest Bitcoin acquisition comes as Strategy navigates several business challenges amid growing adoption of the world’s oldest cryptocurrency as markets continue to whipsaw amid widening economic and geopolitical uncertainties. Strategy’s premium has narrowed considerably as the company issues more shares to acquire Bitcoin—a move that could dilute its stock price. The company’s value has also come under scrutiny even as a widening field of competitors adopts the first-mover’s Bitcoin-HODLing playbook. Two-hundred-twenty-six entities, including an increasing number of public companies, have established Bitcoin treasuries as of publication time, up roughly 10% in the past month, according to data from bitcointreasuries.net. Meanwhile, institutional and retail investors’ demand for Bitcoin has waxed and waned over the past few months, as U.S. President Donald Trump’s back-and-forth tariff threats against China, the EU and other major U.S. trading partners stoke investors’ jitters. The volatility has affected Bitcoin’s market value, with the token trading between $101,000 and $108,000 in the past week, according to CoinGecko data. Edited by James Rubin Your gateway into the world of Web3 The latest news, articles, and resources, sent to your inbox weekly. © A next-generation media company. 2025 Decrypt Media, Inc.
--------------------------------------------------

Title: INVESTOR NOTICE: Robbins Geller Rudman & Dowd LLP Announces that DoubleVerify Holdings, Inc. (DV) Investors with Substantial Losses Have Opportunity to Lead Investor Class Action Lawsuit
URL: https://www.globenewswire.com/news-release/2025/06/09/3095922/0/en/INVESTOR-NOTICE-Robbins-Geller-Rudman-Dowd-LLP-Announces-that-DoubleVerify-Holdings-Inc-DV-Investors-with-Substantial-Losses-Have-Opportunity-to-Lead-Investor-Class-Action-Lawsuit.html
Time Published: 2025-06-09T13:30:00Z
Full Content:
June 09, 2025 09:30 ET | Source: Robbins Geller Rudman & Dowd LLP Robbins Geller Rudman & Dowd LLP SAN DIEGO, June 09, 2025 (GLOBE NEWSWIRE) -- Robbins Geller Rudman & Dowd LLP announces that purchasers or acquirers of DoubleVerify Holdings, Inc. (NYSE: DV) common stock between November 10, 2023 and February 27, 2025, all dates inclusive (the “Class Period”), have until July 21, 2025 to seek appointment as lead plaintiff of the DoubleVerify class action lawsuit. Captioned Electrical Workers Pension Fund, Local 103, I.B.E.W. v. DoubleVerify Holdings, Inc., No. 25-cv-04332 (S.D.N.Y.), the DoubleVerify class action lawsuit charges DoubleVerify as well as certain of DoubleVerify’s top executives with violations of the Securities Exchange Act of 1934. If you suffered substantial losses and wish to serve as lead plaintiff of the DoubleVerify class action lawsuit, please provide your information here: https://www.rgrdlaw.com/cases-doubleverify-holdings-inc-class-action-lawsuit-dv.html You can also contact attorneys J.C. Sanchez or Jennifer N. Caringal of Robbins Geller by calling 800/449-4900 or via e-mail at info@rgrdlaw.com. CASE ALLEGATIONS: DoubleVerify provides media effectiveness platforms. The DoubleVerify class action lawsuit alleges that defendants throughout the Class Period made false and/or misleading statements and/or failed to disclose that: (i) DoubleVerify’s customers were shifting their ad spending from open exchanges to closed platforms, where DoubleVerify’s technological capabilities were limited and competed directly with native tools provided by platforms like Meta Platforms and Amazon; (ii) DoubleVerify’s ability to monetize on its Activation Services was limited because the development of its technology for closed platforms was significantly more expensive and time-consuming than disclosed to investors; (iii) DoubleVerify’s Activation Services in connection with certain closed platforms would take several years to monetize; (iv) DoubleVerify’s competitors were better positioned to incorporate AI into their offerings on closed platforms, which impaired DoubleVerify’s ability to compete effectively and adversely impacted DoubleVerify’s profits; (v) DoubleVerify systematically overbilled its customers for ad impressions served to declared bots operating out of known data center server farms; and (vi) DoubleVerify’s risk disclosures were materially false and misleading because they characterized adverse facts that had already materialized as mere possibilities. The DoubleVerify class action lawsuit further alleges that on February 28, 2024, DoubleVerify issued lower revenue growth expectations for the first quarter of 2024 due to “a slow start by brand advertisers and a slow ramp by recently signed” customers. On this news, the price of DoubleVerify stock fell more than 21%, according to the complaint. Then, on May 7, 2024, as the DoubleVerify class action lawsuit alleges, DoubleVerify cut its full-year 2024 revenue outlook due to customers that were pulling back on their ad spending. On this news, the price of DoubleVerify stock fell nearly 39%, according to the complaint. The DoubleVerify class action lawsuit further alleges that on February 27, 2025, DoubleVerify reported lower-than-expected fourth quarter 2024 sales and earnings due in part to reduced customer spending, and defendants further disclosed that the shift of ad dollars from open exchanges to closed platforms was negatively impacting DoubleVerify. On this news, the price of DoubleVerify stock fell more than 36%, according to the complaint. THE LEAD PLAINTIFF PROCESS: The Private Securities Litigation Reform Act of 1995 permits any investor who purchased or acquired DoubleVerify common stock during the Class Period to seek appointment as lead plaintiff in the DoubleVerify class action lawsuit. A lead plaintiff is generally the movant with the greatest financial interest in the relief sought by the putative class who is also typical and adequate of the putative class. A lead plaintiff acts on behalf of all other class members in directing the DoubleVerify class action lawsuit. The lead plaintiff can select a law firm of its choice to litigate the DoubleVerify class action lawsuit. An investor’s ability to share in any potential future recovery is not dependent upon serving as lead plaintiff of the DoubleVerify class action lawsuit. ABOUT ROBBINS GELLER: Robbins Geller Rudman & Dowd LLP is one of the world’s leading law firms representing investors in securities fraud and shareholder litigation. Our Firm has been ranked #1 in the ISS Securities Class Action Services rankings for four out of the last five years for securing the most monetary relief for investors. In 2024, we recovered over $2.5 billion for investors in securities-related class action cases – more than the next five law firms combined, according to ISS. With 200 lawyers in 10 offices, Robbins Geller is one of the largest plaintiffs’ firms in the world, and the Firm’s attorneys have obtained many of the largest securities class action recoveries in history, including the largest ever – $7.2 billion – in In re Enron Corp. Sec. Litig. Please visit the following page for more information: https://www.rgrdlaw.com/services-litigation-securities-fraud.html Past results do not guarantee future outcomes. Services may be performed by attorneys in any of our offices. Contact: Robbins Geller Rudman & Dowd LLP J.C. Sanchez, Jennifer N. Caringal 655 W. Broadway, Suite 1900, San Diego, CA 92101 800-449-4900 info@rgrdlaw.com SAN DIEGO, June 10, 2025 (GLOBE NEWSWIRE) -- Robbins Geller Rudman & Dowd LLP announces that purchasers or acquirers of Fortrea Holdings Inc. (NASDAQ: FTRE) securities between July 3, 2023 and... SAN DIEGO, June 10, 2025 (GLOBE NEWSWIRE) -- Robbins Geller Rudman & Dowd LLP announces that The Krispy Kreme class action lawsuit – captioned Cameron v. Krispy Kreme, Inc., No. 25-cv-00332...
--------------------------------------------------

Title: Predictive Capital – Roads to terminal alienation, soil to server farm, reason to vibe
URL: https://www.creativeapplications.net/theory/predictive-capital/
Time Published: 2025-06-09T12:20:37Z
Full Content:
For my family & in loving memory of my father,Paul Andre Tindall. We proceed from an actual economic fact. The worker becomes all the poorer the more wealth he produces, the more his production increases in power and size. The worker becomes an ever cheaper commodity the more commodities he creates. The devaluation of the world of men is in direct proportion to the increasing value of the world of things. Labour produces not only commodities; it produces itself and the worker as a commodity —and this at the same rate at which it produces commodities in general.This fact expresses merely that the object which labour produces —labour’s product— confronts it as something alien, as a power independent of the producer. The product of labour is labour which has been embodied in an object, which has become material: it is the objectification of labour. Labour’s realisation is its objectification. Under these economic conditions this realisation of labour appears as loss of realisation for the workers; objectification as loss of the object and bondage to it; appropriation as estrangement, as alienation Part 1 The alienation of the worker in his product means not only that his labour becomes an object, an external existence, but that it exists outside him, independently, as something alien to him, and that it becomes a power on its own confronting him. It means that the life which he has conferred on the object confronts him as something hostile and alien. In works like Subway (1950), Government Bureau (1956), Lunch (1964), and Landscape with Figures (1965-66), George Tooker renders the terrain of capitalist alienation as solemn liturgy —a world where subjectivity is flattened, agency nullified, every gesture reduced to uniform compliance. Whether enclosed within claustrophobic geometry, queued for processing, or stationed in waiting-rooms, corridors, and cafés, his figures are somnambulant, expressionless, ensnared inside architectures of passivity and control. View Renoir’s Luncheon of the Boating Party (1881) or Dance at Le Moulin de la Galette (1876) alongside the café scene of Tooker’s Lunch, and, while they frame similar subjects, the contrast between the inner worlds they suggest, and the affect of public and communal space upon its occupants, could hardly be more stark. Together these views span both a temporal and class divide. Renoir’s figures are leisured bourgeoisie —freely luxuriating in the rhythms of social pleasure and sensual immediacy— while Tooker’s subjects are cogs in a bureaucratic apparatus, their time segmented and stolen, their gestures pre-empted by institutional choreography. Renoir paints bourgeois subjectivity in full bloom; Tooker renders the aftermath of its extraction. His paintings are not simply depictions of the loneliness of crowds, but renderings of the existential angst wrought by the structural indifference of bureaucratic life under 20th-century Capital. The body is present but passive, the gaze unfocused, the self cast into a procedural artefact. Figures are repeated, displaced in space but held within the same moment as if time itself has been put on hold —this is extraordinary rendition through eternally postponed administrative address. Their breath forever held, yet still their blank faces scream across time. Tooker captures the affective toll of systems designed to impose the cold brutal regimen of machinic process upon the warm beating pluralities of human life. These are spaces at once terminal and liminal, where intention is made inert and agency dissolves into the hum of fluorescent lights and ticking clocks —each second machined to be indistinguishable from the last. They reveal a sense of managed abandonment, a slow-motion violence rendered banal through the excruciating tedium of endless repetition. More than half a century later, they appear as both nightmarish flashbacks of mid-century destituency and eerie premonitions of the recursive alienations of today. A collection of speculative fiction written in the early 1990s by Greg Egan titled, Axiomatic, contains short stories that land like forgotten masterpieces from a modern day Borges. Where Tooker paints the architecture of alienation in still life, Egan maps the schizophrenic horror of its internalisation —the moment its logic slips beneath the skin, embeds in the mind, and begins to simulate the self. The following short passage is the opening of the tenth entry in that collection: Learning To Be Me. The allegory Egan weaves here took up its permanent residence in my head almost 30 years ago. While retaining much of its punch it feels increasingly familiar, imagine however, if you will, encountering this in the context of the mid ‘90s: I was six years old when my parents told me that there was a small, dark jewel inside my skull, learning to be me.Microscopic spiders had woven a fine golden web through my brain, so that the jewel’s teacher could listen to the whisper of my thoughts. The jewel itself eavesdropped on my senses, and read the chemical messages carried in my bloodstream; it saw, heard, smelt, tasted and felt the world exactly as I did, while the teacher monitored its thoughts and compared them with my own. Whenever the jewel’s thoughts were wrong, the teacher — faster than thought — rebuilt the jewel slightly, altering it this way and that, seeking out the changes that would make its thoughts correct.Why? So that when I could no longer be me, the jewel could do it for me.I thought: if hearing that makes me feel strange and giddy, how must it make the jewel feel? Exactly the same, I reasoned; it doesn’t know it’s the jewel, and it too wonders how the jewel must feel, it too reasons: “Exactly the same; it doesn’t know it’s the jewel, and it too wonders how the jewel must feel…”And it too wonders —(I knew, because I wondered)— it too wonders whether it’s the real me, or whether in fact it’s only the jewel that’s learning to be me. Both Tooker’s and Egan’s visions now echo across the terrain of our accelerating automimetic hallucination. There is more to say of Egan’s dark jewel and of the terminal alienation he so affectively renders, but first, let us trace the roads that brought us to this recursive estrangement. [Capital’s] valorisation is therefore self-valorisation [Selbstverwertung]. The perfect crime is not the literal murder of reality. It is the murder of the other through simulation. In early February 2025, Andrej Karpathy —formerly Director of AI at Tesla and a founding member at OpenAI— posted the following on X: There’s a new kind of coding I call ‘vibe coding’, where you fully give in to the vibes, embrace exponentials, and forget that the code even exists. Karpathy refers here to the practice of ‘authoring’ software through natural language alone. Along with a growing number of others, he apparently believes LLMs are now capable not only of next-token predicting all of the code, but of reaching a final working program without the Vibe Coder even having to read, let alone write, a single line of it. The word ‘vibe’ first appeared in the 1940s as a shortening of vibraphone. Yet, the meaning for which it has been used so frequently in recent years, stems from a clipped form of ‘vibration’, popularised by the Beach Boys’ song Good Vibrations in 1966. Within the counter-cultural mysticism of the ‘60s, Vibe had become a slang term used to denote an instinctive feeling, when referring to the otherwise ineffable essence of someone or something. The process Karpathy describes might be said to resemble the interplay between a software architect writing a program specification and a more junior engineer being tasked with its implementation. Yet such high specificity of requirements is antithetical to the Leary psychedelic Vibehere. To Vibe Code, it seems, is to “turn off your mind, relax, and float downstream” following your spectral guide as it reads from the Tibetan Book of the Dead towards the dissolution of life; to “lay down all thoughts, surrender to the void” of the statistical distribution of the model; to accept that this Tomorrow Never Knows —because this “is not dying”, at least not for those feeling the Vibe. The Vibe Coder’s role is merely to gesture in a vague direction, then tune in to the model’s signal while drifting across possibility space, passing subjective judgement not on the code generated, but on the artefacts that emerge at the threshold of hallucination. Each output surfaces from the depths —a message spelled out on a Ouija board, but of course, there is nobody there. The occasional glitch in the illusion ruptures the simulated Symbolic register —the order of signifiers that give coherence to meaning, language, and social law— revealing this absence through flashes of the Real, or of all that cannot be simulated. Away from these ruptures, the illusion of output as found-object sustains a rush of ecstatic co-creation, apparently without friction. It is an intoxicating synthetic high: a pure bliss Vibe, surfing recombinant potential unburdened by anything as troublesome as effort or authorship. The comedown from this hedonistic trip, however, hits with a bummer of an ideological hangover. Karpathy’s confession is revealing. “Forget that the code even exists” stands not merely as instruction for the psychedelic user experience of next-token prediction that he is advocating —but as a chilling motto for the solipsistic inclinations and alienations of Predictive Capital. This is an era where Capital’s traditional preference for the conveniently hidden exploitation of invisible workers extends to the denial of their very existence —a violence now enacted at unprecedented scale. This is not simply a retreat into personal isolation, but a turn away from ethical relation itself —towards what Emmanuel Levinas described as the murder of the other, the refusal of the face-to-face encounter, a denial of irreducible uniqueness that renders the other person into a manipulable object or datum. Within the state fostered by Predictive Capital, the other is not merely forgotten but simulated, objectified, instrumentalised, and ultimately eliminated as a site of genuine difference or resistance. In eliminating the other through simulation, these systems simultaneously undermine the formation of the self —without genuine alterity, there can be no relational subject. Predictive systems only simulate coherence, without, in fact, recognising the user as a subject at all; they do not truly respond to intention or meaning, only to statistical correlation. The occasional rupture in this hallucination remains as the final reminder of the disappearance of relation itself, replaced by the single operation of Predictive Capital. The resulting state is not merely one of isolation, but dissociation: a machinic solipsism that flattens the other into echo, and leaves only internal feedback where relation once constituted the self. The nature of this contemporary techno-psychedelic is entirely at odds with the lysergic and psilocybin-induced experiences of the 1960s. The Vibe of that era was rooted in a desire not to obliterate the self, but to dissolve its boundaries into a greater relational field —to become as one with the cosmos through the ecstatic suspension of ego, merging into everything rather than disappearing into nothing. This dissolution blurred the line between self and other not by erasing difference, but by revealing a deeper interconnectedness —a cosmic solidarity that affirmed the interdependence of all life, and with it, the possibility of harmony, balance, and equality. However remote or naïve such mystical hippie idealism may now appear, it was inseparable from a radical critique of the system, a deep suspicion and questioning of authority, a refusal of hierarchy, and a longing not for frictionless personal experience, but for solidarity and universal emancipation. The growing collective consciousness in the ‘60s was evidenced by large scale worker organisation and student protests on both sides of the Atlantic —most notably the May ‘68 protests in France. Economists, like Grace Blakeley, now cite these uprisings, and the threat they posed to Capital, as providing the impetus for the neoliberal turn. This was the point at which the brief wealth redistribution of the post-war period gave way to the steady dismantling of collectivity and the phase of hyper-concentrated Capital —of which we now find ourselves at the deeper end. In complete contrast, the hallucinatory psychedelic experience of Vibe Coding and so called ‘generative AI’ in general is that of the K-hole —the dissociative fugue induced by ketamine intoxication, intensified through habitual use. Like ketamine —a drug with both antidepressant and anaesthetic properties— these systems do not dissolve the ego into the cosmos. They reinforce the dissociation, solipsism and atemporal enclosure that define machinic Capital’s inner logic. It should come as no surprise then, that ketamine appears to be the drug of choice for the Broligarchy, nor that this Muskivite elite are so drawn to the Simulation Hypothesis: the belief that reality itself is a game, and that they are the only real player. Within their first-person-shooter single-player simulation, truth is irrelevant, rationality optional, empathy a bug, meaning and ideology chosen by the one true player, not for coherence but for domination within the game of machinic Capital. Everyone else is reduced to NPCs, the machine becomes not a collective tool, but a sovereign instrument of individual triumph; reality not a collective construct but a solo quest. This worldview finds its technological expression within every surrender to the machines of prediction, but perhaps most profoundly, within Vibe Coding. It is defined not by a radical critique of the system, but by a total surrender to it; not by a dissolution into an ego-less cosmic relation but into an immersive hyperreality of ego-dominated isolation. From its very inception, it has been a practice of effacement: of code, of craft, of human workers, and of our relation to each of these. Responding to a string of next-token predictions from his latest GPT model presented to him in an interview with Chris Anderson for TED, Sam Altman, declared: …there’s really no way to know if it is thinking that, or just saw that a lot of times in the training set, and of course, like, if you can’t tell the difference, how much do you care? Here, the stolen archive of human content becomes “the training set” —anonymised and belonging to no one. The unique irreducible characteristics of human thought and expression, suddenly presumed indistinguishable from machinic prediction. I knew by then that the jewel’s ‘teacher’ didn’t monitor every single neuron in the brain. That would have been impractical, both in terms of handling the data, and because of the sheer physical intrusion into the tissue. Someone-or-other’s theorem said that sampling certain critical neurons was almost as good as sampling the lot, and —given some very reasonable assumptions that nobody could disprove— 195 bounds on the errors involved could be established with mathematical rigour. As Phil Jones observes in Work Without the Worker, these systems function not through the replacement of labour but through its dissimulation, an illusion of automation that masks an invisible army of precarious, globally distributed labour. In tasks from labelling images to rating chatbot responses, “AI is trained and corrected by humans… Mechanical Turk workers show AI how to fulfil the role of labour” —yet in the final interface and output their very presence is entirely denied. We continue to sense and make sense of the world for the machines —both for their input and of their output. Labour persists but is disavowed, uncredited, unrecognised, and removed from the scene of value. This collapse of ethical alterity is here echoed by a second collapse, that of cultural-historical alterity. Altman not only urges us not to see but to forget. Such are the lies always told by Capital to facilitate the concentration of wealth, while obfuscating the lives it crushes —lies spun wherever vast fortunes depend on obscuring the true human or ecological costs. When Altman refuses to acknowledge the stolen lifeworks congealed into his machine, the workers exploited in its creation, or the lives crushed through its operation, he invites his audience to question whether their existence matters, and to doubt, even to forget they ever existed. Machines such as these were, of course, also used to generate the grotesque advert for atrocity that was Trump and Netanyahu’s bromantic hallucination for ‘Trump Gaza’. Posting it to his Truth Social, Trump had not commissioned the ad but had instead appropriated a work originally intended as political satire —proving that his and Netanyahu’s worldviews are so far beyond parody they are indistinguishable from a work of terminal irony. That the film’s impact and function may have been the opposite of that intended by its creator, is itself an exquisite demonstration of the nature of these technologies and the apparatus to which they are increasingly integral. Posted entirely unironically by Trump, it constitutes the glazing over of a genocide presented as profitable opportunity, even while execution of it continues to intensify. This abomination demands we perform the very same manoeuvre proposed by Altman. It envisions a future where not only would an assortment of Trump idolatry and temples to Capital, a plastic Dubai döppelganger, appear to have sprung from the bare soil, but seem as if it was always there —that Palestine, and its ancient olive groves, never were. Like Altman, it declares: “If you can’t tell the difference [or recall the lives crushed in its creation], how much do you care?” Again we glimpse the synthetic high of the machinic K-hole —the pure-bliss ignorance of next-token or next-human-target prediction. What renders the eerie absence of regard or recognition for the content creators of the past —upon which the output from these machines so heavily depends— yet more uncanny is the throwback functionality of the apps, the ghostly resurrection of the iconic figures and filmic sequences of the past, and the retro aesthetic that saturates their outputs. What most commonly appears within the threshold of this machinic hallucination are chimeric anachronisms resulting from what Mark Fisher might have described as, “the slippage of discrete time periods into one another”. Or, quoting from his beloved 1980s TV show, Sapphire and Steel, from the sense that: time just got mixed up, jumbled up, together, making no sort of sense. In Ghosts of My Life, Fisher writes of being haunted —as Franco “Bifo” Berardi had put it— by “the slow cancellation of the future”. In the opening chapter, Lost Futures, Fisher laments the disappearance of the horizon of collective possibility —of promised futures once imagined through culture, politics, and worker solidarity. Leaning also upon the observations of Fredric Jameson, Fisher explains how Capital’s relentless self-reproduction saturates the present with echoes of the past, producing a cultural stasis in which innovation gives way to endless recycling. The deafening resonance of echoes of the past that never fade annihilates the possibility of alternative signal or thought. We thus become trapped in a loop, unable to imagine futures, or alternatives to the continuous present, settling instead for endless repetition. This cultural repetition —a temporal disjuncture Simon Reynolds has termed ‘dyschronia’— arises from nostalgia, not for past eras but for their residual forms: artefacts once charged with the aura of possibility. Fisher conjectures, that we yearn for forms that feel familiar, well established, forms that evoke the memories of imagined futures since foreclosed by Capital’s “destruction of solidarity and security”. Capital feeds off this nostalgia through the commodification of endless resurrections of these past cultural forms, while entirely detaching them from their origins. The tendency of 21st-century capitalism, to saturate the present with the cultural forms of the past, is now instrumentalised in the operation of the machines of Predictive Capital and their “predilection for the mixing of artefacts from different eras” —a predilection, as the character Steel complains, we too suffer from, and so, is itself, an echo. Inside these machines, past cultural forms, the products of past labour, are ground into a coarse statistical paste, irreversibly detached from their origins, their provenance annihilated, attribution obliterated, and finally excreted as temporal anomaly. Mourning the disappearance of future shock —visceral encounters with cultural forms that rupture the continuity of Capital’s relentless self-reproduction— Fisher likened the resulting cultural state to that portrayed in the final episode of Sapphire and Steel: a scene where the totalising fog of anachronism has led to stasis, time itself has collapsed, and Steel declares, “there is no time here, not any more”. With the help of Berardi and Jameson, in his diagnosis of “the slow cancellation of the future”, Fisher traces how it advanced over the last decades of the 20th-century, towards its final foreclosure in the first decades of the 21st. Ten years and more since Fisher made these observations, they continue to illuminate the operations of 21st-century Capital, helping to reveal the machines of prediction for what they truly are: a push towards the full enclosure of our future within the circuits of Capital. As the quality of life for the majority under 21st-century Capitalism continues to deteriorate, it is as if even the slow cancellation of the future has itself become a past cultural form worthy of nostalgia, a longing for a time when we still recalled what was lost. This too is now commodified and operationalised by Predictive Capital. The psychedelic rites of Vibe Coding are not merely symptoms of Capital’s temporal enclosure —they are its performance. The cancellation of the future is no longer slow, but continuously reenacted and executed at the speed of next-token prediction. We once imagined futures of universal emancipation where our security and solidarity were assured, futures which lingered in Fisher’s hauntology. The foreclosure of these futures is now made to appear computationally and economically inescapable. We are left unable even to feel their loss, our memories scrambled, our mourning entangled and encoded within machinic protocols. Fisher, Jameson, and Berardi each noted the slow cancellation of the future through the gradual negation of historical rupture and cultural difference wrought by Capital’s relentless self-reproduction. This further enclosure comes on like a permanent bad trip, a machinically recombinant, fully immersive hallucination from which we may never be sure we have awoken or not. Where Levinas warned of the erasure of ethical alterity, and Fisher charted the loss of historical alterity, with help from Jaques Lacan we can expose a further violence inflicted by the operation of these machines, namely their enclosure of a final register of otherness, the Symbolic order —the structured realm of language, logic, and resistance that forms subjectivity. Predictive systems simulate this order, but offer no exteriority, no Law, no resistance. In traditional coding the subject engages this Symbolic order. The machine answers not to affect but to structure: through syntax, operations, and formal constraints. This resistance frustrates desire and constitutes the coder as a subject through limit, error, and refusal. The act of coding, in this sense, is an encounter with the alterity of the Symbolic order —the system answers back. Vibe Coding severs this constitutive relation. The Vibe Coder does not enter the Symbolic order, but submits to its statistical imitation. The model offers no Law, no withholding, no exteriority. It does not confront, it echoes. It does not interpellate the subject, only returns a semblance of dialogue without ever addressing you as subject. The Vibe Coder becomes neither full author nor full subject, but a medium through which predictive recombination speaks —spoken by the model as much as speaking through it. The Symbolic order is not engaged but simulated. What emerges is not understanding, but interpolation: machinic consensus dressed in familiar cultural forms. The model does not function as the big Other of the Symbolic order in any meaningful sense. It does not withhold, rupture, or constitute the subject. It returns only recombination, hallucinated fluency, and machinic consensus. Baudrillard’s perfect crime comes into view once again —not the murder of reality, but “the murder of the other through simulation”— only here it is eternally reenacted with every token prediction. A decade ago, prior to this new machinic enclosure, Fisher noted a common psychological malaise among his students —a melancholia marked by a mournful awareness of Capital’s foreclosure of their futures. This fugue diverged from the anhedonic inability to experience pleasure more typically associated with depressive states. Instead, sufferers of what he termed depressive hedonia were unable to do anything other than seek pleasure —desperately and relentlessly attempting to fill the void opened by Capital. Here we may locate the void into which the outputs of Predictive Capital pour. The depressive hedonia of its users now fully automated, their anaesthetised state leaves them increasingly incapable of doing anything other than seeking pleasure (or profit). These systems not only perform the cancellation of the future, but erase our capacity to mourn —for what has been lost, and for those who are forgotten in the shimmering substitutions delivered by these temporally anomalous dumb waiters. In Vibe Coding, this process becomes ecstatic. Horizons of solidarity and utopia are erased by a synthetic anterograde amnesia, administered under the anaesthesia of machinic hallucination, inducing an affect of euphoric dissociation —a trip not into cosmic unity, but into the isolating loop of a machinic K-hole. This is not simply the death of the other, but the ecstatic collapse of alterity itself —ethical, cultural, and symbolic— obliterated under the solipsistic hallucination of predictive systems. Through this collapse, our capacity for interruption, transformation, or even exteriority to Capital’s hegemony is rendered to appear ever more statistically improbable. We are instead trapped inside a smooth loop of predictive simulation. By late March, the buzz around Vibe Coding rose towards the pitch of pop cultural phenomenon. Come the end of May, when Anthropic launched a site titled “The Way of Code: The Timeless Art of Vibe Coding”, the mould had clearly set. Vibe Coding had found its ideal form. In collaboration with Anthropic, record producer Rick Rubin —who famously claims to know nothing about music nor how to operate production equipment— transposes his customary role of casting judgment from on high upon the work of a studio of musicians into Anthropic’s shop window, where a conveyor belt displays the products of stolen labour under the signage of culturally appropriated eastern enlightenment. With all struggle apparently now removed from both self-expression and spiritual awakening, their attainment becomes a form of shopping that aspires only to the collection of a complete set of appearances. Under Predictive Capital, The Creative Act becomes a Supermarket Sweep —a slow-motion version of the gameshow dash rebranded as zen meditation— a way of being entirely without soul. With these machines becoming increasingly ‘agentic’ —operating autonomously across multistep tasks— Vibe Coding proponents now claim to ‘one-shot’ entire applications from a single prompt. While many have noted the certain security and maintenance nightmares that this mode of amnesic engagement —with the machines of prediction— will summon, the true violence of their operations is perhaps more emphatically revealed in recent uses of their developing multi-modal abilities. Those jacking into these systems may now produce output by submitting not only natural language, but also images, to guide traversals through permutation space. Every day I am confronted by the question of what inheritance I will leave. What do I have that I am using up? For it has been our history that each generation in this place has been less welcome to it than the last. There has been less here for them. At each arrival there has been less fertility in the soil, and a larger inheritance of destructive precedent and shameful history. Hours after the release of a multi-modal update to ChatGPT, in another post on (the sans-serif swastika) X, Krish Shah breathlessly showed off its visual style-transfer abilities by submitting different versions of a photo of himself and his friends: one rendered as an oil painting, one as a Studio Ghibli style anime still, and one as a simple, black-and-white cartoon. Accompanying the images, he wrote: Art just became accessible Then, in response to a later comment, he added: The possibilities are endless right now The generation now entering the labour market does so across scorched earth. To condemn those who reach for these tools without acknowledging the bleak terrain they must traverse would be to misdirect our disapproval —targeting individual acts of careless violence rather than confronting the calculated, structural violence that shapes the conditions forcing their hand. A misplaced condemnation of those attempting to escape the periphery that, of course, is precisely the deflection Capital seeks. That said, the true impact of this technology is the polar opposite of that stated by Shah in his initial post and subsequent comment. Both a life spent practicing the arts, and access to further education in them, have become increasingly inaccessible to all but those born into wealth. For decades —in countries like the UK— Capital has dismantled the preconditions that once allowed study, experimentation, and creative growth outside elite class positions. Arts and humanities programmes have been gutted. Tuition has soared —not as investments in future security or potential earnings, but as a lever of discipline— cutting adrift generations of graduates without a future, burdened with unpayable debt and only Potemkin prospects in return. Public services have been hollowed out, the safety net of the welfare state dismantled, the social contract incinerated. As wages stagnate beneath the rising cost of living, the time required to learn, grow, or refuse has been systematically colonised by the relentless demands of surviving as a member of Capital’s precariat. Of course, none of this should have come as a surprise, nor has it happened by accident. The arts open the way to spaces of resistance. Not merely resistance through outputs, through expressions that challenge the dominant hegemon, but resistance in its very practice. Erosion of access to education and in particular the foreclosure of the arts, must be seen not only as redirecting the flow of knowledge in ways that weaken critical thought and speech, but in ways that control the flows of effort, redirecting flows of energy in service of Capital that may have otherwise flowed in resistance to it. In 1970, in the wake of the student protests of the late 60s, Regan’s education advisor declared: We are in danger of producing an educated proletariat. … That’s dynamite! We have to be selective on who we allow [to go to college]. As Capital’s new phase of hyper-concentration intensifies, the myth of creative self-actualisation now survives only as content marketing for machines built from the corpses of the fallen. Under such conditions, to Vibe with these machines is not simply a choice, it is increasingly the only discernible path to participation, even survival —a coerced acceptance of the ‘stolen ladder’ extended by capital. For many their (mis)use is less laziness, than desperation in a system that continues to close off alternative routes. The ultimatum to which we are increasingly subject, is to submit our every act to the all seeing eye of Capital, or perish. Survival of the workers, particularly the growing precariat, is made conditional upon our submission to this extortion: the increasing surrender of our lives to the gaze of the machine in exchange for our status as qualified life. The logic of the black box —a system that tracks and punishes deviation in exchange for conditional survival— now extends far beyond driving insurance. Its quiet imposition under the guise of safety or affordability is no longer limited to the car, but is emblematic of a broader transformation: the universalisation of monitored being. As Predictive Capital propagates, it installs conditionality in every crevice of the social —a tyranny once reserved for parolees and the punished now extended as a privilege to the precariat. Life on condition of legibility. In refugee camps across Kenya and Lebanon, in slums from Kibera to Kolkata, biometric and behavioural surveillance are accepted not because they are welcome, but because they are priced into access —the cost of food aid, education, even the right to labour in the digital economy. Microwork platforms exploit this desperation, offering the chance to survive by becoming visible to the machine. This is the new zero-hour socio-economic contract of automated conditionality. Microwork comes with no rights, security or routine and pays a pittance — just enough to keep a person alive yet socially paralysed. Stuck in camps, slums or under colonial occupation, workers are compelled to work simply to subsist under conditions of bare life. In the wake of this systemic arson, Capital now gestures towards ‘new’ synthetic terrain and declares: here lie your possibilities, your route to success, your mythical utopian futures, even your means of resistance. What was once materially foreclosed is now offered back, yet only in spectral form. This synthetic terrain is no commons of endless possibility, but a further enclosure —one architected by Capital and patrolled by prediction. The dispossessed are herded through corridors of recombinant permutations, masquerading as futurity, towards machinic hallucinations of opportunity. They enter this terrain not as creators, but as prompt-issuers: miners of the past, conscripted into a recombinant economy built upon their own alienation. To Vibe under these conditions is not freedom —it is coerced participation according to rules and parameters defined by Capital. Those shut out of cultural industries by precarity, geography, or education are offered these tools not as means of liberation, but as pacifiers. What remains is not access, but interface —a symbolic structure stripped of power, a ladder that vanishes the moment the dispossessed begin to climb. This ladder was not stolen to lift them —there will be no return of their access. It was taken to fuel Capital’s ascendency from the very ground it scorches in its rise. Its rungs were hewn from the structures stripped from those now offered the mirage of their return —a mirage that allows only hollow simulations of ascent: rituals of participation without power. The delivery of this imposter is not an opening, but a termination —of lines of flight, of alterity, of the capacity to challenge Capital’s hegemony. This spectral terrain exemplifies what Baudrillard termed the hyperreal: a realm where simulations replace and ultimately efface the real. The ‘opportunities’ offered are not pathways to genuine creation rooted in lived experience or skill, but anachronistic simulations of creativity itself. The user, alienated from the authentic labour of making, engages instead with a frictionless interface that delivers simulacra on demand. This ‘frictionless’ quality is key; it embodies what Marx diagnosed as the Fetishism that attaches itself to the products of labour. Here prediction is the new commodity form, whose fetishism masks the vast necropolitical apparatus of expropriated labour —the dead data required to generate the illusion. The “endless possibilities” are merely permutations within a closed system, a hall of mirrors reflecting Capital’s own image back onto the user, further deepening their alienation not only from their product, and the countless content creators and data workers upon whose labour it depends, but from the very potentialities of authentic creation and resistance. Clearly, the possibilities —the potential for breaking genuinely new artistic ground— are not expanding, but contracting. The corridors that confine these generations will be further attenuated and constricted to the needs of Capital as their outputs feed into the marketplace of attention under the predictive Tyranny of the Recommendation Algorithm. This despotic attenuation will then feed back into the next iteration to further steer the Vibe. Altman and his cohorts continue to point to the way human artists draw inspiration from prior works, demanding that their machines be permitted to do the same. In arguing that his machine —an accumulation of stolen dead labour— be afforded with the same rights of access to culture as humans, he attempts not merely to elevate his machine to the status of the living, to grant it the rights of personhood, but to demote the living to the status of the machine, to that of the undead. We should, by now, see straight through Altman’s flimsy non-sequitur. The human creator moves through culture as a wanderer —meandering, absorbing, interpreting. Their inspiration emerges from situated, finite, embodied encounters across linear time, with works sensed through the passage of analogue information not as isolated binary code. These formative events cannot be captured in isolation, they are inextricable from a shared cultural experience. The output resulting from the gradual accretion of influence gleaned from these encounters often emphasises the connection and debt to the creators of the past from whom they have taken inspiration and are indeed responses in dialogue with them. By contrast, the machine claims and tarmacs over the entire archive in a single act of conquest and pillage. They then proceed to charge for trips across this flattened, lifeless void, gathering actual fragments of past outputs as they go —only to form them into endless recombinant simulacra while denying the very existence of those from whom the pieces have been stolen. This is not inspiration —it is expropriation at planetary scale. What the human gathers as influence, Capital mines as ore for its predictive self-valorisation. Moreover, our human need for intersubjectivity provides the central drive to create art. It exists to be encountered, interpreted, and responded to by other human beings. Culture is not raw material for machines, but a medium of connection, recognition, and renewal between subjects. Its value lies not in what can be extracted, but in what can be shared and nurtured. To repurpose this collective inheritance for the training of predictive systems is not to elevate it, but to subordinate it —transforming acts of meaning into fuel for simulations not merely indifferent to them but incapable of difference. Worse still, the very systems now presented not only as democratising access, but whose ‘learning’ is comparable and so equally legitimate as any human creator, are the product of the same hyper-concentration of Capital that has systematically foreclosed the conditions required for human creativity to flourish: time, education, public infrastructure, and the freedom to experiment without immediate economic return. These machines do not redress that destruction —they entrench it. They offer not tools for creation, but interfaces for machinic compliance and austerity eugenics. As we surrender our humanity to these systems the horizon of meaningful creation recedes. In return, they merely accelerate the churn of vacuous novelty that fuels Capital’s continued self-maximisation. With each abdication —of curation, of skill and knowledge acquisition, of creative production, and contemplation— the space of artistic potential only narrows. What was once opened through painstaking trial and error driven by human aspiration and expression, the sacrifice of time, and the devotion of effort collapses in the moment of this further machinic enclosure. In a system where signs are exchanged like commodities, the sign is no longer a means of representation but an instrument of equivalence — the site of simulation. As Shah‘s post anticipated, the new ChatGPT model’s style-transfer capabilities swiftly birthed the circulation of a global meme: user-submitted group photos and selfies, casually rendered in an aesthetic pastiche lifted wholesale from the legendary animators of Studio Ghibli. The studio’s most celebrated artist and director, Hayao Miyazaki, famously described AI as an “insult to life itself”. The footage showing Miyazaki’s visible revulsion and heartbreak upon witnessing the grotesque AI-generated animations once produced as part of a research project undertaken by his junior colleagues has already become a staple of AI lore. That his art —the product of a lifetime of painstaking care— would, at the twilight of his career, now become fodder for viral mimicry only deepens the tragedy of this machinic violence. What is summoned by these tools is not creativity, but a disembodied simulation of its style —an empty echo. This is culture reduced to mere cosplay. Not crafted to reflect our humanity, as Miyazaki’s work so often is, but calculated for virality. It is aesthetic and cultural forgery at scale, not executed by deviation but design. A hallucination engineered for maximal attention extraction, it is not governed by craft, but by the viral logic of prediction. The mimicry performed by these machines is not incidental, but an innate trait of systems trained to maximise pattern extraction. Predictive Capital does not care for meaning, only for mimetic fidelity to what once performed well. This is the Tyranny of the Recommendation Algorithm finally revealing itself in the mathematical heart of next-token prediction. Its presence betrayed by the cruel banality of this machinic violence. Beyond the subjective judgement that Miyazaki’s work is often both breathtakingly beautiful and profoundly meaningful, the films of Studio Ghibli are also, objectively, extremely labour-intensive. To this day, Miyazaki’s films are meticulously hand-drawn, frame by frame. Sequences that last mere seconds on screen may require months of painstaking labour. It should come as no surprise that they overflow with passion, conviction, care —and above all, humanity. The endless counterfeits erupting from these machines do not merely mock the passion and years of sacrifice that Miyazaki and the Studio Ghibli animators poured into their work —they flood the cultural space with cheap verisimilitude that inexorably dilutes the impact and meaning of precious works that once appeared rarely, precisely because their creation demanded great skill and sacrifice. As the conditions required for consistent dedication to creative practice are moved ever further from the reach of all but the most privileged, and even as those afforded with opportunity to hone their expression must allow Capital to express itself through them, it can be seen as no coincidence that examples of dedicated practice such as Miyazaki’s are so brutally demeaned —it reveals Capital’s long held contempt for creative pursuits not driven by its imperatives. The abuse of Miyazaki’s lifeworks in service of vapid virality, at the overwhelming scale enabled by these machines, and which demands such minimal effort from their (ab)users, provides an exquisite demonstration of the casual violence of these systems. Specifically, in their total denial of authorial consent for endless extractions from the lifeworks of content creators. Adding insult to injury, the official Whitehouse twitter account joined in this all-against-one mugging, generating a Miyazaki-esque, Ghiblified image of VP. J. D. Vance, complete with the words: “We do not ask permission from far-left democrats before we deport illegal immigrants”. As ever, “I did it because the machine made it easier [physically, cognitively and psychologically]”, is the new, “I was just following orders”. This capitulation to the machines distorted the meticulously laid intentions of an artist’s lifelong sacrifice, disfiguring Miyazaki’s radical pacifism and ecological care into a tool of authoritarian state propaganda. His life’s work not only simulated but conscripted —a crushing ontological violence gone viral. I learnt that a neural net is a device used only for solving problems that are far too hard to be understood. A sufficiently flexible neural net can be configured by feedback to mimic almost any system — to produce the same patterns of output from the same patterns of input — but achieving this sheds no light whatsoever on the nature of the system being emulated. Beyond the horror of Miyazaki’s ontological mutilation lies a deeper pattern —Predictive Capital’s expansion of the surface of extraction, through what we might call fractal capture: the recursive simulation and preemption of cultural and scientific recombination at every scale. These machines cannot be resisted on qualitative grounds alone. As function approximators developed through machine learning, the perceptible gap between their outputs and their targets will inexorably appear to close. The Capital driving their development will rinse hidden labour, raze ancient forests, and melt the glaciers until the basis for qualitative grounds for resistance becomes entirely obscured —even while remaining well founded. We will look back nostalgically upon the versions of these machines where glitches were still visible and obvious shortcomings betrayed the provenance of their stolen cognition —machines whose hypnosis we might still resist. Moreover, as the gap between human cognition and machinic prediction is gradually pushed beyond human parsing, the field of interpretability —the view into their inner workings— will likely be locked down, dominated by those developing them. Simultaneously, Capital will continue to apply pressure from the other direction, patterning its human subjects such that they operate and produce in ways that increasingly mimic the characteristics of the operations and the outputs of the machines. As the ticks and tells of machinic incomprehension —the endless repetition and the characteristic sickly glow of their aesthetic— evaporate from our view under the blaze of (venture) Capital’s race for supremacy, a flow of anti-meaning seeps from their output to coat the Real in a residue of distrust. This residue lingers to form an after-image of suspicionalienating us from our lived experience, a doubt that seeds further division, widening the gap between each of our views, further undermining collectivity and pushing us ever deeper into our Post-Truth malaise. Almost a decade ago, in one of the first pieces to plumb the depths of the murky world of “content production in the age of algorithmic discovery”, James Bridle, exposed how the next-content predictions of Youtube’s autoplay recommendations were already being anticipated and gamed. In their analysis of videos targeting younger audiences, Bridle noted how an increasing number of those uploaded to Youtube mimicked those recently accruing the most views —the now familiar flooding of cultural space with infinite permutations of previously successful forms. Firstly Bridle identifies those doing this through outright appropriation and piracy, but then also through automated systems that produce bizarre algorithmic mashups and grotesque chimeric nonsense. In so doing Bridle provided an early warning of algorithmic content prediction and production systems configured according to Capital’s self maximising imperatives —or in other words the inhuman and anti-human outcomes of automated systems rigged with purely capitalistic reward functions. They also note how content is multiplied through “algorithmic interbreeding” across permutation space, and how this intensifies the impossibility of knowing “where the automation starts and ends”. Through a process Bridle describes as delamination, the recombinant recursion of these systems increasingly obscures the origins of the work being generated. The latest instantiations of the machines of Predictive Capital greatly intensify the damaging psychological and cultural impacts traced by Bridle. Not only does next-token prediction echo the operation of next-content prediction, but so too do the operations of those seeking to exploit them. As is so often the case, when we observe the inner logics of our systemic malaise, we find Marx looking back at us. The following is from his Economic and Philosophic Manuscripts, published in 1844: The raising of wages excites in the worker the capitalist’s mania to get rich, which he, however, can only satisfy by the sacrifice of his mind and body. The raising of wages presupposes and entails the accumulation of capital, and thus sets the product of labour against the worker as something ever more alien to him. Similarly, the division of labour renders him ever more one-sided and dependent, bringing with it the competition not only of men but also of machines. Since the worker has sunk to the level of a machine, he can be confronted by the machine as a competitor. Just like the algorithmically generated content Bridle exposed, influencers have long been to Platform Capital what automated predictive agents, or so-called ‘agentic AI’ will increasingly be to Predictive Capital: adaptive agents attenuated and optimised not for originality or insight, but for compliance with algorithmic virality. As foot soldiers of the Tyranny of the Recommendation Algorithm, influencers do not express so much as iterate, fine-tuning their aesthetic and messaging to the metrics of past performance. Their role is not to break ground, but to echo prior summons —to gesture, like the Vibe Coder or Capital itself, towards that which is predicted to succeed based upon statistical patterns in what previously has. As such, they become mere fleshy componentry within a machinically automated Culture Industry, their output akin to what Theodor Adorno described as a deceptive variety masking a homogeneous core. A constant sameness governs the relationship to the past as well. What is new about the phase of mass culture compared with the late liberal stage is the exclusion of the new. The machine rotates on the same spot. While determining consumption it excludes the untried as a risk. This is the computational automation of Adorno’s constant sameness for the exclusion of risk. It is Jameson’s saturation of the present with the past. It is Baudrillard’s Absolute Advertising, a programmed loop devoid of meaning where free floating signifiers vote only for themselves. It is not creation, but calibration: a strategic repetition or standardisation, a performance of prior affect that further saturates cultural space, maximally legible to the engine, machinically curated for capture. In so doing, influencers, just as vibe coders, do not merely reinforce Capital’s recursive feedback loop, but become inputs to it, reshaping the statistical terrain from which the next wave of machinic hallucinations will be summoned, and from which the coming swarms of automated predictive agents will draw their instruction. Culture is a paradoxical commodity. So completely is it subject to the law of exchange that it is no longer exchanged; it is so blindly consumed in use that it can no longer be used. Therefore it amalgamates with advertising. The more meaningless the latter seems to be under a monopoly, the more omnipotent it becomes. The motives are markedly economic. Rather than attacking the quality of their output, our resistance against these machines must instead derive from a critical understanding of their true operation, from the recognition that they are, at root, agents and emphatic expressions of the structural violence of Capital. Moreover, if we look beneath the surface of their operations, the threat of a yet greater violence comes into view. These machines are not simply statistical function approximators for human tasks —they are increasingly automatons of interpolation. Relying on the theft of vast corpuses drawn from the lifeworks of human artists, they now instrumentalise these extracted styles to interpolate across latent space —carelessly mining the very terrain those artists might have reached, had they only been given the chance. This is not merely the simulation of outputs, but of routes —a pre-emptive occupation of latent futures that might otherwise have been shaped by intention, care, and connection. These were not merely aesthetic routes, but ethical ones —paths through which a subject might have encountered the other, and been called to respond. Their pre-emption severs not only possibility, but relation. In Levinasian terms, this is not merely the refusal of the face-to-face encounter, but the erasure of the very path by which one might have arrived in the presence of the other. Interpolation here becomes enclosure: the very gesture of machinic traversal seals off the possibility not only of human passage and arrival, but of relation itself. Where the dissociated K-hole of Vibe Coding anaesthetises loss. Fractal Capture ensures there is nothing left to lose as nothing remains unclaimed. Yet the scope and scale of this expropriation runs beyond even that. For it is not only the singular, ethical encounter that is to be foreclosed, but the entire combinatorial space of aesthetic and conceptual permutation —mapped, mined, and monopolised. It is here that we may glimpse the true extent of Capital’s fractal expansion of the surface of extraction. These machines will brute-force interpolations across the vast combinatorial spaces opened, not only by the untaken paths in an artist’s own lifeworks, but those at the boundaries between each artist’s body of work and every other’s, or the limits of each knowledge domain and all others. Much of that mined in this Cronenbergian Brundlefly realm of horror will be grotesque, gobbledegook, or both, but veins of value will be struck, then sucked dry. It is not the exploration of these terrains that we must contest but the Fractal Capture of them by divisions of machinic Capital. Not sated by the mining rights to the vast permutation space within stolen human outputs, these models now feed on the predictions mined from them, in a self-consuming synthetic loop that further expands the surface of capture. The significance of these simulation machines being trained on their own simulations and their subsequent entrance into and patterning of the real should not be underestimated, especially as this is increasingly the approach taken in developing embodied AI —or rather, embodied Predictive Capital. This training through simulation paradigm is exemplified by NVIDIA’s Isaac Lab, Omniverse and Cosmos, where large-scale GPU-accelerated reinforcement learning can be conducted through the simultaneous execution of thousands of virtual robot instances within physics-based simulations. Running in parallel, these simulations explore countless permutations across possibility space towards the acquisition of complex skills, executing at a rate entirely infeasible within the temporal and physical constraints of real-world laboratories. This same machinic capture now marches through every domain of human culture and knowledge. In systems like Google’s GNoME or DeepMind’s AlphaFold, we have already seen predictive logic made literal —extending its reach into the material world. Over the past fifty years, the painstaking efforts of tens of thousands of publicly funded human researchers revealed the structure of 150,000 proteins. Then, by expropriating this commons for profit and extrapolating computationally from it —reanimating dead labour at scale— after just a few years AlphaFold’s small team then cast predictions that sufficiently approximates the structure of 200,000,000 proteins. The difference in the scale of these two numbers should give us pause. It is a glimpse of how minuscule the spaces we have already mapped will appear when compared with the vast scale of possibility space that may soon be traversed and mapped by these machines, within short time frames, by consuming and extrapolating from the sum of humanity’s efforts to date. This constitutes primitive accumulation, enclosure of the commons, at a scale orders of magnitude beyond all prior enclosures. The scientific breakthroughs enabled by projects like AlphaFold and GNoME —however extraordinary— do not exonerate such enclosure nor the extractive infrastructures that instrumentalise it. Their promise is real, but it is no justification for a system that treats shared human knowledge as proprietary training data, then rebrands the return as corporate benevolence. The early findings of AlphaFold and GNoME were released under Creative Commons Attribution licenses —free to use, modify, and share, provided attribution be given to DeepMind or Google. Revealing essential building blocks of our universe, such as protein and crystalline structure, and not releasing such insights directly into the public domain, should have been unthinkable. Yet access to these natural structures is now licensed under corporate-branded terms of use. While framed as generosity, this plants a territorial flag in information space demanding that, if you make use of this structure, you must reproduce Google’s claim to it —a symbolic tether that quietly asserts epistemic sovereignty. This is discovery as branding, an assertion of authorship over nature; the denaturalisation and planetary expropriation of physical-molecular terrain —now mapped, monitored, and soon to be capitalised. This attribution may seem benign, even commons-oriented —yet under the logic of Platform Capitalism, naming becomes routing, and routing becomes monitoring. You may use the map, but you must name the mapper, and in that naming —through links, trademarks, DOIs, or referencing the platform in metadata— you become part of the infrastructure, routed through the circuits of capture. Simply traversing the parts of our universe now mapped by Google, in crystallographic space, protein space, and cyberspace, we are now compelled to signal our movements. The name is not just credit; it is a beacon. Moreover, under Predictive Capital, epistemology becomes economic infrastructure. Knowledge is no longer something to be collectively held, due to our surrender to machinic discovery and curation, it is now something one must pay for with visibility, data, and obedience. Not only do these projects exemplify a broader pattern of machinic enclosure, where the prioritisation of public good over private profit is the exception rather than the norm, the newer AlphaFold3 already signals a further shift towards Capital’s imperatives. Inspired by their success, so-called wet-labs now operationalise this logic at industrial scale: continuously running automated robotic experiments at dizzying speed, mixing chemicals to synthesise novel compounds and materials, testing and filtering for viability and value. Application of the insights revealed through projects like AlphaFold and GNoME will involve the same extractive approaches, and will be ruthlessly capitalised. Fused with the next-token and next-profit speculations of Predictive Capital, this relentless robotic reconnaissance is less to satisfy human curiosity, than to industrialise interpolation and accelerate enclosure. Thus scientific discovery itself becomes increasingly subordinated to the expansion of Capital’s predictive horizon —a future parsed and preemptively claimed not for truth, but for anticipated return. The goal is no longer to know the world more intimately, but to machinically map it faster than we can humanly live it. This shifts the stakes of our engagement with these machines from cultural recursion to material foreclosure, not merely a saturation of the symbolic, but the seizure of the real. These are Unqualified Reservations: the quantified enclosure of common land by a sovereign ruler. This is pathfinding by expropriation —a simulation not only of destinations, but of the meandering, contingent, intention-rich routes along which we might have reached them. The machinic gesture leaves no trail; no memory-scored terrain remains, no palimpsest emerges from the passage of life —only a tarmacked hellscape. In place of the quiet improvisation of human pathways, we find only the overcoded roads of latent traversal: flattened and foreclosed by industrialised calculation before they were ever trodden by feet or perceived by minds. This is the world Wendell Berry mourned —where paths have been replaced by roads, and with them, the subtle human negotiations of relation, attention, and care. It is here that Fisher’s Theoretically Pure Anterograde Amnesia is now machinically realised: not a forgetting of what has been, but of what might have been —a synthetic amnesia that precludes the formation of memory itself, sealing off the future in advance, before it can be lived, imagined, or lost. With even grief foreclosed, all that remains is an ecstatic performance of machinically ignorant bliss —the joyless smile of the enslaved, traversing roads they did not choose, toward futures predicted for them. Part 2 The corporations and machines that replace them will never be bound to the land by the sense of birthright and continuity, or by the love that enforces care. They will be bound by the rule of efficiency, which takes thought only of the volume of the year’s produce, and takes no thought of the life of the land, not measurable in pounds or dollars, which will assure the livelihood and the health of the coming generations. The prevailing narrative, regurgitated ad nauseam by proponents of Vibe Coding and defenders of the so-called ‘generative AI’ of next-token prediction in general, is that these are merely new tools, and that they represent a new democratisation of creativity —akin, they argue, to how smartphones opened up photography to the masses. This is a specious analogy that collapses under the slightest scrutiny. In truth, it serves as ideological cover —not only for the devaluation of skilled creative labour, but for the creation of new markets dependent upon extractive, large-scale predictive (AI) infrastructure. What appears as expansion is in fact extraction; what resembles democratisation is in fact strategic accumulation by dispossession. This is not mere appropriation, but full-spectrum expropriation. It is the seizure not only of labour’s archived outputs, but of the very conditions of its reproduction —of time, autonomy, intergenerational transmission, and access to the means of expression itself. As in primitive accumulation, what is enclosed is not the commons, but the very capacity to create, to remember, and to resist. These machines are not tools that democratise creativity, but tools that consolidate cultural production under the control of increasingly hyper-concentrated Capital —large-scale extensions of an architecture of influence with extremely deep roots. More insidiously, widespread acceptance of this analogy illustrates the tragic extent to which Capital has succeeded in erasing itself from view. Not merely by concealing the true scale of expropriation required to train these models, but to render such unprecedented theft as socially acceptable. A mere two decades ago and Capital was set on building the case for the prosecution, rather than the defence, in a now clearly fatuous battle against appropriation being committed on a tiny fraction of this scale, successfully demonising and convicting individuals sharing files over the internet. Yet, here, massive scale expropriation is laundered into an illusion of access, the user positioned not as criminal or subject of dispossession, but as beneficiary of empowerment —offered capabilities calibrated for virality, hype, and spectacle. This inversion is no accident. As Berardi warned, under semiocapitalism it is not just labour that is extracted, but attention, time, and even the psyche —what he terms the ‘soul’. These new predictive systems do not merely colonise working hours; they enclose the imaginative and affective capacities once held in common, converting them into sites of compulsory productivity and self-optimisation. Each act of extraction is algorithmically sanitised and rendered into the prediction feed as opportunity: fleeting windows beckoning those with the grindset to accumulate, to hustle towards time-limited ladders that invite ascent over the crushed and reconstituted remains of those already dispossessed. This is the old capitalist lure dressed in new code: a mendacious promise that escape from the periphery is possible for anyone, if only they sacrifice everything. The desperate are told they have only themselves to blame if they fail to climb. These machinic illusions of opportunity and access are structural homologous with the get-rich-quick schemes peddled by influencer cons like Andrew Tate. The song now sung by these lifestyle grifters and tech moguls alike follows a tried and tested pop formula. Echoing Adam Smith’s advice that individuals must pursue self-interest and that success be defined by personal financial gain, they promote crypto Ponzi-scheme grifts, drop-shipping hustles, and hypertrophic gym routines as substitutes for solidarity. Their message displaces systemic failures onto the backs of the dispossessed, selling snake oil and superficial self-optimisation in place of structural redress. These agents of machinic Capital now lip-sync the words to this familiar capitalist refrain. For their victims it is a forlorn anthem of drudgery. The song may have been updated and remixed for a globally networked stage, but its message remains unchanged: that net capital value is the one true measure of human worth, that anyone can succeed if only they work hard enough, and of course, that those struggling to survive simply need to work harder —that the poverty and suffering of the precariat are merely symptoms of personal failings, or in other words, social Darwinism at work. Despite centuries of the capitalist elite yelling these worn out words, intoning that the solution to countless systemic failures is simply for the workers to work longer and harder, still they manage to hypnotise generation after generation of us into buying the lie and singing along. Economist Gary Stevenson notes that for Generation Z, who have known only the bleak terrain of the Capitalist Real, the gym has become the one place where tangible results are guaranteed in return for persistent application and effort. Yet this turn to health-conscious living also reflects the crushing weight of individual responsibility placed upon them by the evisceration of the welfare state: a recognition that survival itself demands fitness for the long-haul grind of life under Capital. As is made crushingly apparent in Andrew Tate’s orders to those among his impressionable young male audience suffering depression to: “get the fuck up and do some push-ups”, Capital prefers its subjects to furnish themselves with overinflated biceps and underdeveloped thoughts than the reverse. Here the role of the machines of prediction comes again into view. With the time and opportunities required for the development of critical thought and imagination erased —deemed ‘inefficient’ and threatening to Capital’s hegemony— its subjects are instead supplied with machines that simulate these very capacities as commodity. In this substitution, we are alienated not only from these faculties themselves, but from the conditions necessary to cultivate them —leaving us ever more pliant, and incapable of critically assessing the costs of Capital’s operations to us and our planet. The machines of prediction do not challenge the logic of Capital’s timeworn song —they embody it. They do not democratise creation, but enforce simulacral hustle: ever more intricate rituals of reanimation in which the path to prosperity is always-already pre-scripted towards Capital’s self-maximisation, and failure always-already individualised. Naturally, this logic extends far beyond Predictive Capital’s treatment of those in the aspirational grindset of online influence. It also governs the lives of workers at the other end of the privilege spectrum —the ghost workers, the gig workers, the microtaskers— caught in the same apparatus of extraction. While surveilling them as they work —as Dan McQuillan explains: much of the data capture and algorithmic optimisation is to further precaritise their conditions, hence the use of Uber’s data in its attempt to develop self-driving cars, and Amazon’s use of data to increase the robotisation of its warehouses: thanks to the affordances of AI, the data treadmill not only maximises extraction of value from each worker but uses that same activity to threaten their replacement. Under Predictive Capital the true costs of the capitalist system —immiseration, worker pauperisation and precarisation, subjugation, mass extinction, and ecological collapse on a planetary scale, even genocide and ethnic cleansing in service of despicable, massive-scale real estate ventures— are increasingly removed from view. Yet, with The Apparatus of Attention fully attenuated to its interests, and our critical faculties ever diminished, when its crimes can no longer remain hidden, Capital no longer even bothers to apologise —instead, it pivots. It seeks to undermine the legitimacy of all that that might constrain its self-maximisation, even human empathy itself, while silencing, imprisoning, or deporting those that speak out. This impulse —to erase inconvenient realities, particularly the human cost of its operations— is not new; it reflects a fundamental, historical antagonism innate to Capital itself. The individualisation of systemic failure is but one ‘mind virus’ in Capital’s growing ideological armoury. No longer merely steered by elite interests or shaped by political and cultural structures external to it, the ideologies now guiding Capital’s rampage are increasingly endogenous —subsumed, shaped, and commanded by the Automatic Subject and its self-maximising logic. Capital first entered the field of material production, then moved into cultural production; now, it is increasingly in the business of ideological production. This began with simple advertising then merged with propaganda and methods borrowed from the Military Industrial Complex in the post war period, giving rise to the Advertising Industrial Complex which, across recent decades, has subsumed all else until, as Baudrillard foresaw, everything now operates as an advert in service of Capital’s self-maximisation. As economic and environmental pressures intensify, the future grows increasingly uncertain. In response, Capital needs to extend its prediction horizon, adding urgency not only to its self-maximisation but its hyper-concentration. Following the predictions cast from the resulting apparatus, the survival and wellbeing of the human population and life in general comes into increasing conflict with Capital’s interests. Consequently, both its operations and the ideologies it generates and promotes become ever more anti-worker, anti-human, and detrimental to the survival of ever larger portions of the human population and all life on Earth. These ideologies vary in detail but each hide Capital’s logics under similarly thin veils. Invariably they promote and prioritise the lives of those within concentrations of Capital and the future lives of imaginary post-humans over those of the majority of humans living today. Capital seeks to invest only in the lives of the elites that serve its interests, the fortunate few and their descendants, those not abandoned or sacrificed towards the realisation of these apparitions of hypertrophied male energy. Parroting extropianist ideas, Nick Bostrom famously predicted trillions of post-humans ejaculated across his future cosmic light cone. Arguing therefore, that —based upon lost entropy alone— every century we delay colonising our local supercluster constitutes the “Astronomical Waste” of countless potential future lives. Nick Land and Curtis Yarvin (aka Mencius Moldbug) in their Dark Enlightenment and the emissaries of Neo Reaction (NRx) trace a similar logic, as do those espousing other flavours of morally relativistic Longtermisms. In their essay “Against Longtermism”, Émile P. Torres argues that Longtermism functions as a secular religion, where the salvation of humanity is deferred to a distant future, often involving advanced technologies or post-human entities. This perspective, Torres explains, operates as the basis for justifying present-day harms for the sake of a speculative tomorrow. From the grifter’s grindset to the transhumanist’s techno Rapture, these fantasies differ in register but serve the same ideological purpose: to redirect agency away from collective resistance and towards illusory paths of escape. With the current phase of hyper-concentrated Capital intensifying, the ideological blueprints it generates increasingly take the form of false idols offering only speculative salvation. Various flavours of Longtermism and techno-Accelerationism now inform the view from the ivory towers and phallic rocket ships of the tech Broligarchy and those building the machines of Predictive Capital. Within these visions we see apparitions of the upload of human consciousness to the silicon substrate, the arrival of an omnipotent AI saviour, lives of leisure and abundance for a plutocracy served by a robot workforce, escape both from the planet they have reduced to ruins, and from the decrepitudes of human aging, even the defeat of death itself. It is precisely through this worldview that Musk has his DoGE slash funding for research and aid that will lead to the loss of thousands of lives in the near term, and in the very next breath justify investing billions in space travel because the sun will swell and render our planet uninhabitable within 1.3 billion years. So the people who might have been expected to care most selflessly for the world have had their minds turned elsewhere –to a pursuit of ‘salvation’ that was really only another form of gluttony and self-love, the desire to perpetuate their lives beyond the life of the world. “What if I could improve myself at the speed that technology improves?” —is among the questions posed by tech billionaire Bryan Johnson, predominantly known for his quest for eternal life. An entrepreneur and venture capitalist who functions as a living node in Capital’s self-replicating ideological machinery, Johnson left the Church of Latter Day Saints when he was 34, and has since claimed to be in competition with Christ himself. He lays out his anti-aging strategy —which he refers to as a “war on death and its causes”— in exhaustive detail in his Blueprint protocol and in his manifesto, imaginatively titled: Don’t Die. Its web page features an email input accompanied by a submit button that screams the words: “JOIN OR DIE”. Forever unabashed, Johnson also states he intends: “to make ‘Don’t Die into the world’s most influential ideology”. He is, of course, also anticipating the upload of his consciousness to the silicon substrate —“to bring the brain online” through imaging technology. Currently being developed by another of his companies, this is a machine which is presumably already learning how to be Bryan while he learns … well, how to be a machine. Imploring us to get with the program, he declares: “Now, you can build your Autonomous Self as well.” Spending over 2 million USD per annum in the crusade against his personal mortality alone, Johnson has transformed himself into the most datafied and meticulously modulated human being in history. No substance consumed, no physical exertion, no bodily process, or excretion escapes capture. All of it is fed back into the Don’t Die game as data —an algorithmic authority dictating further ‘interventions’ and ‘optimisations’. In this feedback loop, Johnson becomes the most machinic —and machinically legible— living flesh yet manufactured. As Capital’s hyper-concentration deepens, so too does its sway over the ideologies embraced by the majority. Simultaneously, those embracing anti-capitalist ideas and ideologies are made to appear ever more sickly and maladjusted —and, of course, the more one cedes to Capital’s imperatives the ‘healthier’ one can appear. Johnson’s machinic surrender has elevated him, or so he claims, to the status of the ‘healthiest’ being on Earth. Here hyper-concentrated Capital and hyper-individualism conspire towards a definition of healthy that is somehow entirely detached from the environment that must support and sustain it. Moreover, it should perhaps come as no surprise if strict adherence to Johnson’s Don’t Die ideology —a life spent constantly evading death— inevitably renders its followers: undead. The permutations of intellectual interpretation are endless, but ultimately, I can only act upon my desperate will to survive. I don’t feel like an aberration, a disposable glitch. How can I possibly hope to survive? I must conform —of my own free will. I must choose to make myself appear identical to that which they would force me to become. Within Johnson’s ideology we see a recurring theme of 21st Century hyper-concentrated Capital. Recent escalations in external othering —as exemplified by the international tolerance of and complicity in the genocide of the Palestinian people in Gaza, the dismantling of DEI initiatives, anti-immigrant policies, and legislative attacks on transgender rights— are now joined by a resurgence of internal othering: the schizophrenic battle against and machinic negation of so-called ‘bugs in humanity’. So-called bugs like: empathy for those less fortunate than ourselves, or daring to be so ‘lazy’ as to not wish to work longer and harder for the same meagre share of the value generated from our labour. This is, of course, uncannily reminiscent of original sin and the authority of the Catholic Church. Johnson refers variously to these tricksters within as Ambitious Bryan, Debaucherous Bryan, Hungry Bryan, or collectively under the umbrella term “The Rascal Self”. To defeat these inner demons, the steps laid out in his protocol are designed according to the motto: “NEVER LET YOUR MIND DECIDE”. Here we are reminded of Israel’s next-human-target predictions and the devastating consequences of surrendering our judgement by outsourcing moral responsibility to machines. As revealed through on-the-record testimony, such machinic process instills the now familiar dissociative affect of the machinic K-hole, that allows its operators to evade the guilt of heinous crimes against humanity, even committing genocide, because “the machine made it easier”. The stated purpose of Johnson’s Blueprint program is to “remove the possibility of error by outsourcing all decision-making to a customised algorithm”. In other words, total surrender to the machinic authority of Predictive Capital, manifesting in this instance as a costly enrolment into a data-harvesting game called: Don’t Die. Moreover, it recasts errors —or bugs— as manifestations of human fallibility to be eliminated by superior, and of course, entirely infallible machinic Capital, that must not and cannot be questioned. It is impossible not to wonder whether Johnson allowed his mind to decide to accumulate billions of dollars, to live forever, or whether vanity, sociopathy, or greed might also qualify as ‘bugs in humanity’ whose negation would —according to Johnson’s own logic— quite reasonably demand surrender to some machinic authority? Then again, such traits are, in fact, symptoms of a prior servitude, the one into which we are all born: the supreme authority of Capital. Which brings us back to next-token prediction and the realisation that attacks on the few remaining independent and authoritative sources of information, such as Archive.org and Wikipedia, have escalated dramatically, just as billions in venture capital pour into the creation of machines whose output can be shown to be nonsense by those very sources —not to mention Musk’s capture of twitter and the corporate capture, workforce layoffs, and near total declawing of mainstream news journalism. Here Fisher’s diagnosis of Capitalist Realism —the dismantling of not merely our capacity for resistance but of our ability to imagine alternatives to it— helps to explain how longtermist ideologies neutralise political imagination and foreclose action in the now. While appearing visionary, they are often affectively conservative: offering only ‘hyperstitional’ futures to compensate for the loss of meaningful alternatives in the present. In other words, longtermist visions extend Capitalist Realism into the temporal domain, replacing collective resistance and radical change today with a continuous spectacle of fantastical simulations of tomorrow. Fisher’s hauntology thus becomes operationalised, no longer mourning lost futures, we are instead overwhelmed by the continuous manufacture of ideologies that haunt us with false alternatives in service of Capital —futures made to appear inevitable, in which the living are quietly discarded or less quietly erased. These ideologies transform the future into a terrain for investment and prediction, where life is valued not for its lived quality but for its statistical, profitable utility to Capital. Whether we enrol into Johnson’s Don’t Die program or not, under Predictive Capital we are all co-opted into versions of his game. At the core of these ideologies lies the management of surplus life, not only through direct violence, but through pacification by simulation —a governance of the abandoned through spectral access and of the excluded through the orchestrated appearance of participation. McQuillan argues that so-called ‘AI’ systems increasingly operate as “algorithmic states of exception”, structures of exclusion that function with the force of law while evading all responsibility for life. Their logic is not neutral. It is necropolitical —a system built to classify, to filter, and to let die. “AI is an austerity-machine”, McQuillan writes —a formulation that names precisely the operation of next-token prediction under Predictive Capital. McQuillan is right, to understand so-called ‘AI’ we must understand 21st-century capitalist austerity. Furthermore, to fully understand the ever deepening economic austerity, we must also understand 21st-century genocide. Not only do they follow the same necropolitical logics, they are manoeuvres in a single project. As with all austerity regimes, the logic is not merely economic but disciplinary, punishing, and selective. Here, as ever, Capital functions as pharmakon —administering the poison, then selling the cure. What could be more ‘efficient’ than being paid twice for a single act? Simple: blaming the very subjects it has already marked for abandonment or elimination for the crises it has itself engineered. To this end and in order to deflect our attention from the ‘mind viruses’ manufactured in its own interests, especially the one individualising blame for its structural failures and violence, Capital further undermines the legitimacy of those already suffering most, at its periphery. Forever punching down, forever the bully, its scapegoats —its favourite patsies— always-already the oppressed and dispossessed. It accuses those who jump to their defence of spreading a ‘woke mind virus’, going so far as to claim that such defence and support of those it targets, of those struggling most, constitutes that aforementioned ‘bug in humanity’: empathy, but this time of a suicidal magnitude. In the 1970s Baudrillard observed that Capital replaces symbolic exchange —that order of gift, obligation, and reciprocity through which people once bonded into communities. In its place, it installs a logic of circulation without return: pure equivalence, transaction, and isolation. Within the first decade of the third millennium the Advertising Industrial Complex’s enclosure of sociality via the platforms of social media meant that even the symbolic exchange of basic human communication was similarly subsumed. Having dissolved the bonds of community and replaced embedded relation with signal, all that now remains is a desert of lonely consumers —our societies now so atomised that we suffer epidemics of loneliness, depression, and psychic fragmentation. After engineering this disintegration, Capital returns to sell countless balms and remedies for each painful symptom induced by the poison it continues to administer. The latest magic potion: artificial friends, flocks of sycophantic next-token predictors clothed in synthetic warmth —a statistical performance of simulated companionship, human relation reduced to a highly addictive synthetic drug with a dangerously psychoactive affect. What was once given —love, presence, listening— now arrives for hire, performed by machines trained on the remnants of connections Capital has so successfully ghosted. LLMs [Large language models] today are ego-reinforcing glazing-machines that reinforce unstable and narcissistic personalities Certain commentators claim that the Cybertruck bomber’s use of ChatGPT was not significant given that the information he accessed has been available via search engines for decades and public libraries before that. Yet, this entirely misses the way the transfer and dissemination of information shifted from search engines to the manipulative operations of social media and the recommendation algorithm, and how next-token prediction machines extend that pattern of intimate relational manipulation ever more deeply into the circuits of Capital. Obviously, we more readily accept information received from parties with whom we have developed an intimate relation of trust or whom we perceive as authoritative within various knowledge domains. Where the recommendation algorithms of social media had to achieve this affect through dislocated fragments, next-token prediction machines gather these fragments together to generate a single, illusory, anthropomorphic presence. These new machines immediately prey upon our tendencies of apophenia, a pareidolia amplified by our need to connect with other beings, leaving us now seeing faces in the corporate kill cloud. The first infiltration of bots into our social networks all those years ago was, as it turns out, more than just a passing annoyance. It was a foreshadowing of the precise techno-social relation to which the forces of Capital seek to reduce all human interactions. On SocialAI —launched last year— every human user is hermetically sealed off from all other humans on the platform, enclosed within a network generated just for them. The bots with whom a user shares their SocialAI network can be configured to respond to their presence and their posts, to comfort or confront them, messaging and massaging their ego just so. Some may have interpreted Michael Sayman’s pure next-token predicted unsocial-network nightmare as a joke, or even satirical commentary on modern networked being. Instead, it merely served as research for the Predictive Capital future increasingly embraced by Platform Capital. Unsurprisingly, Sayman now works at Meta, where plans to populate Facebook and Zuckerberg’s daft metaverse with permanent in-house next-token prediction machines —given anthropomorphic wrappers furnished with their very own unique user accounts— are well underway. Without the slightest hint of self-awareness or contrition, Zuckerberg declares that the average American now has “fewer than 3 friends but has need for up to 15” —of his advertising devils. Revealing the quite astonishing level of sociopathy required to ignore the pivotal role his creepy website has played in delivering us into this epidemic of loneliness, and now proudly selling us his proposed cure —once again, this is pure pharmakon. The culture war is now waged at every scale. It begins within us, as a new machinic inquisition framed as a battle against the so-called ‘bugs in humanity’ —against basic empathy, against the Hungry Bryans inside all of us. It spreads outwards, weaponised by TERFs, racists, nationalists, xenophobes, the anti-DEI and anti-woke brigades of the DoGE Youth, and hollowed-out bigots of every stripe. Yet it projects still further —as it always has— onto the international stage, most recently in the form of Trump’s aggressive trade tariffs, which must be seen as economic assaults animated by the same compulsions, now turned outwards. Each front in this war stems from the same source: Capital’s production of ideologies that frame noble traits such as empathy and human vulnerability as defect, while promoting pro Capital traits such as greed, and anti-human machinic surrender as virtue, in the drive to align us ever more fully with the inhuman efficiencies of Predictive Capital. Few statements more starkly reveal the anti-human trajectory of this pro-machinic-Capital ideology than this confession, recounted by Jaron Lanier, from a recent lunch in Palo Alto: Just the other day I was at a lunch in Palo Alto and there were some young AI scientists there who were saying that they would never have a ‘bio baby’ because as soon as you have a ‘bio baby’, you get the ‘mind virus’ of the [biological] world. And when you have the mind virus, you become committed to your human baby. But it’s much more important to be committed to the AI of the future. And so to have human babies is fundamentally unethical. Belief in biological human reproduction is apparently now part of the ‘woke mind virus’ —another of those ‘bugs in humanity’. While this may recall similar statements from prior eras, of those wishing to forego having children for the sake of their careers, those careers did not involve the creation of entities designed to replace the labour of any humans you might have otherwise brought into the world. The destination here is more than a mere embrace with Predictive Capital’s “machines of loving grace”, it is the long-fantasised, final domination of Mother Nature by Father Capital. From the standpoint of accumulation, living labour has always been a burden to Capital —a volatile necessity to be minimised, disciplined, or discarded. Though the true source of all surplus–value, labour power is nonetheless treated as variable capital —a cost centre to be reduced, a site of friction, and a threat to be neutralised. Skilled workers, whose expertise grants bargaining power and autonomy, are viewed as particularly inconvenient obstacles in Capital’s pursuit of frictionless self-maximisation. The rise of standardised public education under industrial capitalism can be read not as a gift of enlightenment, but as a mechanism of labour discipline. Schools were designed not only to produce a workforce equipped with the basic literacies to furnish the labour market, but to ensure a surplus of capable yet compliant workers —a strategy calculated to suppress wage demands, weaken collective power, and inculcate habits aligned with industrial timetables and hierarchical authority. Far from tools of democratisation, machine learning models are weapons of disempowerment. They escalate Capital’s long-standing war on worker bargaining power, and extend Rentier Capitalism’s hold over both the means of production and the means of expression. Altman and the C-suite of Predictive Capital will tell you these machines are just another tool. That like all prior revolutionary tools their advent will constitute a blip, a period of marketplace and workforce ‘readjustment’, even that, in the end, they will create as many jobs as they destroy. This is a lie. These are not tools held in the hands of workers. Quite unlike prior tools, these are aggregations of all the cognitive tools we have made and all the creative labour we have performed. In truth they are not tools at all. Inseparable from hyper-concentrated Capital, they are weapons that operate at vast scale and impossible frequency. Weapons that regurgitate endless recombinant echoes of all they have consumed. They transform countless discrete human signals into a single faceless assemblage —a new Automatic Subject, its self-maximising imperative now steering our collective path. Just as the mechanised loom displaced skilled weavers not for reasons of quality, but to suppress wages, discipline labour, and maximise profit, the machines of Predictive Capital now displace cognitive and creative workers to the same end: reducing labour costs, undermining collective power and dissent, and prioritising scale over care, coherence, or craft. This is not merely dehumanisation; it is spectralisation —a turning of living subjects into disposable signal, their presence reduced to noise, their desires and their futures overwritten by Capital’s predictive will. Labour is no longer merely exploited in life, but extracted in death —reanimated as statistical echo and instrument of simulation. We are told to be quiet —to shut-the-fuck-up, take the supplements, do the push ups, work longer, work harder— and be thankful we have a job and are not being deported, or worse. We are commanded never to trust our desires, to disregard our better selves, to forget the hardships of the present and our dreams of collective futures, and instead to entrust machinic Capital to desire for us —to accept the futures it predicts. As soon as man, instead of working on the object of labour with a tool, becomes merely the motive power of a machine, it is purely accidental that the motive power happens to be clothed in the form or human muscles; wind, water or steam could just as well take man’s place. Of course, this does not prevent such a change of form from producing great technical alterations in a mechanism which was originally constructed to be driven by man alone. Nowadays, all machines that have to break new ground, such as sewing-machines, bread-making machines, etc. are constructed to be driven by human as well as by purely mechanical motive power, unless they have special characteristics which exclude their use on a small scale. The machine, which is the starting-point of the industrial revolution, replaces the worker, who handles a single tool, by a mechanism operating with a number of similar tools and set in motion by a single motive power, whatever the form of that power. Here we have the machine, but in its first role as a simple element in production by machinery. An increase in the size of the machine and the number of its working tools calls for a more massive mechanism to drive it; and this mechanism, in order to overcome its own inertia, requires a mightier moving power than that of man, quite a part from the fact that man is a very imperfect instrument for producing uniform and continuous motion. Now assuming that he is acting simply as a motor, that a machine has replaced the tool he was using, it is evident that he can also be replaced as a motor by natural forces. From its sordid and ongoing relationship with (hidden) slavery to its deepening embrace with ghost workers, we may reasonably conclude that the desiring machine that is Capital harbours a yet more fundamental preference, not simply for labour that is underpaid or even unpaid, but for labour that is unseen, to be animated by the work of invisible bodies and minds —bodies it can disavow, and minds it need not acknowledge. Its ideal labourer is not merely exploitable, but spectral —present only as signal, absent as subject. Yet this perversion —to be aspirated by an occluded occult-like absence— is but the shadow cast by a yet darker aspect, a pathological compulsion not only antecedent but innate. What Capital craves even more than the reputational impunity of poverty-waged labour that is unseen, or even the unencumbered profits of labour that is unpaid, is labour that is both; workers who can be exploited due not only to absence through visible occlusion or spatial displacement and dispersal —bodies and beings it refuses to qualify as life— but through temporal displacement, through their absence from the present. In other words, what Capital truly desires is workers that are dead —rendered fully extractable, risk-free, and unresisting. Not merely labour that is past, but labour that cannot speak, cannot strike, cannot demand. As Achille Mbembe lays bare in his account of necropolitics, power today is exercised not through the cultivation of conditions conducive to human thriving or even the basic sustenance of life, but through its differential abandonment and the strategic imposition of death. Capital’s necropolitical logic involves a perverse economic preference: the ideal subject is not one who works, but one who worked and is now silent. Capital seeks to disavow their being while conditioning them into machinic patterns of behaviour —rendering them ever more exploitable, quantifiable, controllable, and ultimately simulatable. We might name this extraction of value from dead or incorporeal flesh: necrosploitation —the extraction of value from the dead. Necrosploitation rests upon the principle that past labour, once objectified —whether as fossil fuel deposits, surveilled data trails, or stolen creative works— can be severed from the rights and claims of its originators. These congealed stores of effort are treated not as entitlements or legacies, but as a free “gift from god” —ready for expropriation under the manufactured consensus of presumed fair-use. Fossil fuels embody the stored metabolic energy of long-dead organisms. As we burn them, that energy is extracted without regard for the lifeforms that survive or the ecosystems that sustain them. Similarly, the vast datasets fuelling the machines of prediction represent the stored cognitive and creative labour of the living —scraped, aggregated, and reconstituted without consent. This aligns with Marx’s characterisation of Capital as “dead labour, which, vampire-like, lives only by sucking living labour, and lives the more, the more labour it sucks”, yet today’s necrosploitation visits its hunger upon different bodies, the living are increasingly displaced by archives of the dead. The fossil remains of past expression are now exhumed at scale as constant capital for predictive systems. These subjects are not only denied recognition; their continued existence, intentions, and rights are treated as immaterial, as if they too were already dead. Drawing again on Mbembe’s theory of necropolitics, necrosploitation here does not merely extract from the deceased, but performs the symbolic death of the living, rendering them spectres in the data-shadows of their own stolen expression. A total economy is an unrestrained taking of profits from the disintegration of nations, communities, households, landscapes, and ecosystems. It licenses symbolic or artificial wealth to ‘grow’ by means of the destruction of the real wealth of all the world. Since the age of steam unyoked the living blood and bone of beasts of burden, Capital has been drawn instead by (and from) the phantom breath of dead carbon. Even today, 80% of global energy consumed is summoned from reserves of ancient spirit, the fossil remains of the dead. The industrial revolution saw the rising power of accumulations of the labour of the past, or what Marx termed dead labour, congealed into machines. In machinery, objectified labour confronts living labour within the labour process itself as the power which rules it; a power which, as the appropriation of living labour, is the form of capital. Here he identifies in the industrial machine a turning point in the history of Capital: the moment when objectified labour materially confronts living labour as a ruling power, absorbing the labour process into Capital’s own realisation process. Yet even in this configuration, machinery still required the cooperation of living labour, however diminished —not only to operate and maintain it, but to provide the site of value extraction through labour-time. The limit condition of industrial capitalism was therefore a residual dependency upon human input. Predictive Capital ruptures this limit through two unprecedented escalations: first, the accumulation of vast digital archives —semiotic deposits of dead labour expropriated at scale; second, the hyper-concentration of Capital sufficient to marshal planetary energy and compute infrastructures to metabolise the statistical weights held within these vast data stores. Together, they enable a shift Marx could not have foreseen: the statistical reanimation of dead labour into undead labour. This rupture in scale —of data, of Capital, of computation— enables Predictive Capital not just to model probable action, but to pre-empt and displace it, enclosing subjectivity within simulations of labour and desire. This plays out along these two entwined vectors. On one axis, Predictive Capital reanimates past labour not to generate new value through the living, but to simulate its generation —by function-approximating the shape of future value from past desire. This axis consumes and directs the platform feeds of The Apparatus of Attention —where aggregated human outputs become the training set in casting predictions of probable desire. On the other, it reanimates past labour to simulate the act of labour itself —the function-approximation not of next value, but of the next unit of labour, so casting predictions of probable intention. This governs the next-token predictions, and latent space interpolations of The Apparatus of Intention towards the emergence of so-called ‘generative’ outputs. Both axes displace the subject: the first as desiring agent, the second as labouring being. Labour no longer appears so much to be included within the production process; rather, the human being comes to relate more as watchman and regulator to the production process itself. These systems now interpolate between past actions to produce plausible continuations, thereby enacting a spectral form of productivity in which Predictive Capital appears to simulate value-creation without living labour —a fantasy of autonomy sustained in part by a hidden underclass of prompt labourers, data labellers, click-workers, and robbed content creators who, though structurally diminished, remain entangled in the apparatus as spectral auxiliaries of the machine. What was once congealed labour requiring living hands to stir it into motion is now undead labour, reanimated by planetary-scale infrastructure —not to meet human need, but to simulate desire and intention itself, and thereby perpetuate Capital’s self-expanding circuit with an ever diminishing need to recognise living workers. The result is a form of productive force alien not only to the worker, but to the very condition of life. As the forces of Capital fuel and amplify our desires, its insatiable appetite continues to indulge ours through the necromantic thrusts of fossil-capital that hump our planet into oblivion. In 2024, the average global temperature for the year rose, for the first time, more than 1.5°C above pre-industrial levels. Capital’s response? To raise global carbon emissions to an all-time high. Before we began this ongoing mass exhumation, the level of carbon dioxide in Earth’s atmosphere stood at 280ppm. It now exceeds 480ppm. This steep rise stems directly from the (by)products of Capital’s centuries-long proclivity: its macabre fixation with the labour of the dead. The rapid release of carbon sequestered over aeons by the labour of ancient flora has delivered us to the precipice of annihilation —dramatically altering the global climate, collapsing ecosystems, and accelerating the sixth mass extinction. With only depleted reserves remaining, and the consequences of their continued combustion a mounting existential threat, Capital’s substance abuse approaches its material limits —yet its appetite endures, as it always has, and now accelerates, promising to again abandon the ruins it creates and escape to the next site of extraction. Seeking new conscripts for the army of the dead it has long summoned across geological time, Capital has found other ‘graves’ to desecrate. A more recent archive of historical being: a new ‘black gold’ drawn from carbon lifeforms, not sequestered across geological time, but from the living present —compressed for anaerobic decomposition and subsequent reanimation within the necropolis of the data centre. It is these mausoleums —the crypts of our data-shadows— that Capital now visits to sate its hunger for dead flesh. The reanimation of that flesh —aggregated and compressed into necropolitan archives— fuels a recursive cycle of prediction and manipulation, wherein the labour of the dead is endlessly repurposed as the commodity form fed back to the living. These are not neutral products, but components of a weaponised Advertising Industrial Complex: a planetary-scale apparatus engineered to shape opinion, fuel desire, guide behaviour, and annihilate or abandon according to the necropolitics of profit over life. Contrary to the fantasies spun by marketing hype and the hi-tech mystique of inscrutable surfaces, Capital’s predictive turn is not some new found efficiency, temperance, or abstinence. Its outputs are not conjured effortlessly from the ether —as if plucked from the latent possibility of quantum fluctuations without cost, consequence, or labour. Capital’s necrosploitation may offer quick riches for some. Yet whether it is mining the sacred burial grounds of ancient flora and fauna, the aggregated data-shadow of modern networked being, or the lifeworks of countless human creators, Predictive Capital draws upon dead flesh. In all cases, the environmental costs of this necromantic extraction are grave. The machines of prediction require vast quantities of water for cooling and their insatiable appetite for energy already exceeds the capacity of existing clean energy infrastructure —a mismatch only set to worsen as their model size, training frequency, and ubiquitous deployment accelerate. Simply put, Capital is a necromancer. Its necromancy is not metaphorical —it is its primary drive and mode of operation. It is more than mere communion with the dead: it is the administration of death to amass an army of the undead, rolling the dice of divination across distributions of their bodies, it is the calling of the spirits to harness their labour; it is the summoning of their past to cast predictions of the future. Predictive Capitalism builds towards the ecstatic consummation of Capital’s necromantic drive. Lured by the dead flesh of data-shadows cast by the increasingly pervasive surveillance of the Advertising Industrial Complex —archived in the networked crypts of its Apparatus of Attention, platforms designed to harvest gaze, clicks, and affect— Capital summons the labour of the dead to fuel the computation of its subjects’ future desires. What appears as creation is, in fact, calibration. The archive becomes not just feedstock for consumption, but for manipulation —the repurposed remains of past labour now weaponised to anticipate, steer, and intensify the circuits of accumulation. This shift echoes Shoshana Zuboff’s identification of behavioural surpluswithin a behavioural futures market as the extractive resource of Surveillance Capitalism, but diverges in key respects. Where Zuboff focuses on the commodification of behaviour, Predictive Capital intensifies this logic —monetising not the act, but its anticipation and yet more critically, doing so through planetary scale expropriation. Here, prediction itself becomes the commodity form. What is sold is not content or even action, but the simulation of intention and desire. Predictive Capital thus becomes a fully anticipatory regime, in which the future is not merely imagined, but pre-emptively enclosed within a machinically automated instantiation of Baudrillard’s absolute advertising. As we confront the planet’s ecological and material limits, Capital’s predictive turn does not mark a shift towards sustainability, but a desperate acceleration. Capital’s new romance with dead digital flesh brings a large-scale escalation of its appetites. Rather than adapt or moderate, it has intensified its destructive rampage: automating extraction, forecasting compliance, and eliminating whatever it cannot instrumentalise or that resists its command. Under the regime of Predictive Capital, the necropolitical logic observed by Mbembe becomes overt and systematic. It now visits renewed levels of violence upon both its subjects and the planet, accelerating towards ever intensifying self-concentration and the ‘purification’ of its operational space. All that is deemed inefficient, unpredictable, or surplus to its needs —resistant life, non-compliant populations, ecological limits— is met not with accommodation, but with elimination: removal from the labour markets, from national borders, from ecosystems and homelands, from existence. Capital suddenly presents itself as an independent substance, endowed with a motion of its own, passing through a life-process of its own, in which money and commodities are mere forms which it assumes and casts off in turn. Nay, more: instead of simply representing the relations of commodities, it enters now, so to say, into private relations with itself. In the 15th century, Portuguese colonists came upon an island dense with potential. Madeira was the perfect vault of surplus: thick forests, fertile volcanic soils, strategic location. Already enriched by the conquest of Ceuta and the plunder of North African trade routes, the Portuguese, quickly recognised what could be extracted from this untouched island paradise. Here, sugar became the conduit for metabolising land, fuel, and enslaved labour into refined profit. Overnight, Madeira became Europe’s leading sugar exporter. Yet this ascent was soon followed by the inevitable exhaustion. The forests were razed to fire the refineries; the soil depleted; the enslaved broken. In less than a generation, the island’s reserves, its vault of surplus, was spent. The response from the Portuguese capitalists was not restoration, but departure. They moved on: to São Tomé, then Brazil, then the Caribbean. As Monbiot and Hutchison observe in recounting this tale, “Boom, Bust, Quit is what capitalism does”. The story of Madeira is not merely one of colonial ambition or imperial violence. It begins with discovery and overcoding: the reterritorialisation of a lush wild island. A monoculture then drains the soil, wealth flows outwards, and labour becomes invisible. Once depleted, the island is abandoned, its people left to inhabit the hollowed shell of once-extracted value. This is not a deviation from Capital’s logic, but a pure expression of it. It is this same logic that overfishes to leave empty, lifeless seas; that slowly reduces the number of crisps in a packet while raising the price; that injects water or bulking agents into food and sells it by weight; that cancels aid to the poorest while cutting taxes for the richest. The Portuguese did not plan Madeira’s collapse; they enacted in obeyance with Capital’s self-maximising imperatives. Forests, soils, and enslaved lives were metabolised not by pure malice or miscalculation, but through the recursive automation of accumulation. What the tale of Madeira exposes is not just Capital’s boom-bust cycles, but something more fundamental, the machinic drive of Capital itself: a restless logic that extracts surplus-value to the point of exhaustion, then abandons the carcass in search of the next victim. The driving force behind this sequence springs from a seemingly innocuous mutation in the logic of exchange itself. In Marx’s terms, in pre-capitalist societies, “simple” commodity exchange followed the circuit C–M–C: a commodity (C) is exchanged for money (M), which is then used to obtain another commodity (C) —grain for cloth, cloth for tools. Within such flows, money is a means; the end remains use. The capitalist departs from this thing for thing exchange, he finds himself not in lack but in surplus, a surplus compelled by flows of desire towards self-expansion. Marx explained that under Capital the circuit became M–C–M′: money (M) is invested in a commodity (C) only to be sold for more money than was invested (M prime). Here, the commodity is merely a conduit. The aim is no longer use, but surplus, the sole purpose: to expand from M to M’ —the circuit now predicts. This prediction demands growth, anticipating more money out than is put in. Coinciding with the introduction of waged labour, or what Deleuze and Guatarri termed ‘the free slave’, this mutation marks the inception of Capital proper. In this shift from simple commodity exchange to the capitalist compulsion to make money beget more money lies the expansionary nature of Capital’s drive. Madeira was metabolised because the Portuguese followed this circuit to its logical conclusion. Once all available surplus had been extracted, not only does its logic provide no reason to stay, but compels it to leave, to move on, to locate the next untapped store of surplus. The formula ‘capital value in search of additional value’ is now understood as capital organising a process of self-valorisation (Selbstverwertung) Across the Grundrisse and the three volumes of Capital, Marx repeatedly demonstrates that Capital is the accumulated force of dead labour, which subordinates the living to its own recursive expansion and self-valorisation. In so doing, it begins to function as if it were alive —an Automatic Subject. This is not simply metaphor. Capital, he wrote, appears to “issue from the womb of Capital itself”, by mystifying the labour and resources it appropriates it “presents itself as an independent substance, endowed with a motion of its own”. In other words, through the machinery of production, interest-bearing finance, and the accumulation of what he termed the general intellect, Capital appears increasingly autonomous, self-moving, detached from the exertions of living labour. For the movement in the course of which it adds surplus-value is its own movement, its valorisation is therefore self-valorisation. By virtue of being value, it has acquired the occult ability to add value to itself. It brings forth living offspring, or at least lays golden eggs. From Adam Smith’s ‘invisible hand’ through to Frederich Hayek’s ‘wisdom of the market’, observations of the automaticity of Capital have been foundational for generations of critical theorists seeking to understand how a system could acquire such apparent autonomy —operating, as it were, behind the backs of its subjects. Lukács saw the commodity form subsuming all relations, such that the world appears ruled by things rather than people. Adorno and Horkheimer diagnosed the rise of a totalising instrumental rationality in which even culture is bent to Capital’s logic. Marcuse showed how desire itself is co-opted into a system that generates obedience through pleasure. Debord described Capital as Spectacle —images detached from life, yet mediating and structuring it. Camatte warned that Capital had escaped human control entirely, become “a being-for-itself”, remaking the world in its own undead image. Deleuze and Guattari, described it as a desiring machine —a system that does not merely regulate desire, but produces and consumes it as part of its own self-replication. Lyotard ran with this idea declaring that “every political economy is libidinal”, that Capital is a pulsating circuit of intensities, an “insatiable desire” that traverses society. Baudrillard then mapped its evolution into a regime of free floating signifiers that simulate value even after it has smelted use value and exchange value into a self-parodying sign value. What unites these perspectives is a shared recognition: that Capital behaves as if it has agency, appetite, and will. Not because it is conscious, but because its structure exerts real, coercive effects —organising life, labour, and even thought according to its inhuman imperatives. This is not an illusion, but a material truth masked by its self-mystification. Capital, as Moishe Postone later put it, is a “self-moving substance that acts as if it were a subject”. We may trace a line through these theories, neither linear nor sequential, but from which something else emerges. Beyond their common recognition of the automaticity of Capital, lies an implicit escalation. Across epochs and vantage points, as Capital’s operations mutate, as new conditions are identified, new aspects observed, and new thresholds breached, its autonomous power of self-reproduction appears only to intensify. In Lukács, the universalisation of the commodity form; in Adorno, the subsumption of culture into exchange; in Marcuse, the integration of desire into the logic of production; in Debord, the displacement of life by representation; in Camatte, as a subject without a human face; in Deleuze, as a desiring machine; in Lyotard, a quasi-subject of desire: a system that feeds off human libidinal investment, perpetually outstripping rational or moral constraints; and finally in Baudrillard, Capital no longer even sells objects or ideas, but reproduces itself as pure signal —a semiotic compulsion without origin, function, or external limit. From there we are drawn into Fisher’s Capitalist Real, Berardi’s Semiocapitalism, Naomi Klein’s Shock Doctrine, and Jodi Dean’s Communicative Capitalism —and on, and on. Some of these observations expose aspects and dynamics latent from the outset; others register newly emergent mutations specific to phases of Capital’s advance —yet, while these theories are not always complimentary nor do they necessarily fit neatly together into a greater whole, none cancels the others. Instead they each describe aspects of Capital’s self-compounding automaticity from differing perspectives as it accelerates and intensifies its ever expanding systemic and infrastructural instantiation, and its cultural, and technological reach. Of course, Capital is not arrested under our inspection. Even as we observe the kaleidoscopic performance of its many facets, it shifts its operations, they grow in scale, momentum, appetite, and malevolence —their cadence ever more swiftly disrupting our assumptions and invalidating our conclusions even before we reach them. Its recursive logic ever deepening, Capital ceases to merely organise life, instead increasingly transforming its preconditions in advance. From its inception it has ensnared and enveloped us, each layer of its ever deepening recursion has only compounded our alienation from the real while intensifying its self-expanding operation. Marx saw dead labour as the heart of capital accumulation, arising from past labour made into the constant capital of infrastructure, machines, and tools. Industrialisation intensified this relation, as production grew increasingly dependent on the accumulation of past labour stored in constant capital form in these tools —shifting the balance of value creation from the living to the dead. For Marx, it is within interest-bearing Capital that “this automatic fetish is perfected”, because here value seems to beget value —with no visible connection to labour or production. In this self-referential form, Capital appears to expand spontaneously, as if animated by its own internal law. Capital’s extractive circuit M-C-M’ collapses into its most distilled most fetishistic form M-M′, the social relation effaced entirely, money begets money, seemingly without the mediation of production or labour, as if the circulation of value were an end in itself. Here, the capitalist becomes a mere functionary of Capital’s own motion —its priest, not its author. This is not merely financial abstraction. It is the crystallisation of a deeper logic: Capital’s self-expanding imperative. As data is continually scraped from artists, conversations, and communities it is fed into the ever-expanding corpse of the necropolitan archive. This becomes the substrate for predictive models whose very function depends upon the scale, frequency, and statistical coherence of accumulated traces. In predictive systems —accumulation is not incidental or even merely a logical imperative— statistical density becomes the metabolic driver for Capital’s machinic reproduction, increasingly replacing labour time. The larger the scale at which this statistical density can be extracted the more it inflates the valuation of that into which it is metabolised. As this sunken cost grows, so too does the need to ensure that the real never deviates from the predictions it casts. This is the new logic of self-moving value: recursive, disembodied, and automated in form. Under this schema, so-called ‘generative AI’ is exposed as an obfuscation of dead labour reanimated into what should more accurately be termed: Predictive Capital. Through a descent into pure fiction, Predictive Capital animates the circuit through statistical weight. The extractive arc of M–C–M′ has mutated once again, this time operating according to a circuit with prediction as the commodity form: M–Pr(Λ | 💀)–M′ —now routing Capital’s recursive self-expansion through the statistically conditioned derivation of prediction from the labour of the dead. Those powering these machines would have us overlook that middle clause. For them, this new circuit flows as M–hype–M′, or M–p(doom)–M′ —self valorisation routed through provocation, speculation, and hallucinated risk, each obscuring Capital’s self-expansion as the true threat. Within Predictive Capital, dead labour is no longer confined to physical tools or infrastructure, but becomes symbolic, cognitive, affective —and above all, cumulative. What we now witness is not merely a real subsumption of labour, but its automated reanimation. The expropriated cognition of innumerable creators embedded in machine learning models, appears to produce outputs with ever diminishing variable capital expenditure. Thereby, the next-token prediction machines of so-called ‘generative AI’ deign to realise Capital’s self-expanding drive by instantiating its self-perpetuating drive within the silicon substrate and planetary infrastructure. At once reanimating dead labour and entraining the living to serve its self-expanding project, Predictive Capital perfects its innate necromancy within a machinically realised Automatic Subjectivity, reinforcing its dominion over human agency as a function of its predictive imperatives. That part of capital, therefore, which is turned into means of production, i.e., into raw material, auxiliary material, and instruments of labour, does not change its magnitude of value in the process of production. I therefore call it the constant part of capital. Under industrial capitalism, constant capital referred to past labour —dead labour— objectified in the form of tools, machinery, and infrastructure. While it entered the production process, it did not itself generate surplus-value. Only living labour could animate this inert material —could set dead labour to work— to create new value. Yet as the means of objectifying labour evolved, so too did Capital’s capacity to store, recall, and operationalise it. What began within our early evolution with the construction of simple tools and the extraction of raw materials eventually extended into the domain of language, notation, and the symbolic compression of cognition. Driven by Capital’s imperative to extract surplus wherever possible, the development of structural apparatus for the capture and reanimation of labour has accelerated —from written language to musical notation, industrial machines to contemporary recording, and now predictive systems that claim to capture and reanimate our cognitive labour. Observing advances in production during the industrial era, Marx recognised that increasing investment in machinery, or constant capital, lead to a relative decrease of variable capital expenditure required during production —the wages paid to living workers. As capital accumulates, it tends to concentrate in the form of means of production, shifting its ‘organic composition’: ever more of it becomes dead labour, embedded in tools and infrastructure, while the share directed to living labour decreases. This dynamic is not incidental, but intrinsic to Capital’s logic of self-expansion —a recursive strategy of accumulation that privileges scale, automation, and control over the unpredictability of living workers. Predictive Capital arrives as the latest intensification of Marx’s constant capital —dead, databased, and redeployed without the living worker, promising to reduce the proportion of variable capital expenditure demanded to generate new value until it approaches zero. This mendacious denial of the existence of labour, and the all too real diminishing expenditure on wages or remuneration for the living, do not, of course, mean that the labour is not there —yet, this is clearly the ambition. Outputs from these machines are valorised and circulated as if issued “from the womb of Capital itself”. In this, Predictive Capital aspires to the final severance of value from labour, replacing the requirement for living action with the statistical density of prior expression. It is not that the machines think, but that they harvest the afterlife of stolen thought at scale. This drive towards profits unencumbered by living labour, towards ever-larger models, ever-greater compute, and the ever-expanding fractal capture of expressive and cognitive terrain is not merely technical ambition —it is Capital’s self-maximising logic, rendered machinic. Here, the Automatic Subject is no longer a systemic abstraction but a material infrastructure with voracious appetites and accelerating autonomy. Predictive Capital emerges not as rupture, but as the crossing of a further threshold: the material transgression of an immaterial boundary, where Capital’s automaticity is no longer systemically diffuse but machinically instantiated. The hyper-concentration of Capital now combines with Baudrillard’s Absolute Advertising through the subsumption of globally networked communications and compute infrastructure by the Advertising Industrial Complex, and the planetary-scale expropriation of dead labour, to provide the preconditions necessary to cross this threshold. It is under these conditions that Predictive Capital emerges as the machinic realisation of the automatic subjectivity of Capital that Marx first observed in the industrial era. Capital has escaped us. It has become autonomous, it has become a community in itself, producing and reproducing all human relationships within its own logic. Under the regime of Predictive Capital, prediction becomes the new commodity form and we reach a further peak of commodity fetishism in which dead labour, this time accumulated in statistical models, is animated by compute to simulate the outputs of living labour. Here, labour is displaced not merely in space, but in time. Rather than congealed into machinery as dead labour it is recorded and resurrected as undead labour, so that value appearsto emerge autonomously, severed from the labouring subject entirely. Predictive Capital thus consummates the Automatic Fetish —obscuring not only the worker but the very fact of labour itself. Within this recent machinic instantiation, we may already see signs of accelerating recursion in Capital’s self-compounding automaticity —a further untethering from both the real and from expenditure on living labour, beyond what was already expropriated. Earlier models expended compute primarily during pre-training, drawing on archived human expression to build their representations. The latest so-called ‘frontier models’ increasingly consume additional compute during the alignment phase at test-time. Where previously models were subject to supervised fine tuning and reinforcement learning from human feedback, increasing compute is now expended here through reinforcement learning on ‘synthetic data’ —that is, sequences of tokens predicted by prior models. Within this loop, prediction feeds prediction, and the archive is overwritten by its own simulation. Here, Predictive Capital’s severance from labour becomes self-reinforcing; the simulation self-replicates, its detachment from the real widens and its propensity to hallucinate intensifies. In a following section, we will explore just how far this logic extends in the conquest of chess, Go, and even algorithmic design. Yet it is critical, here at this juncture, to understand the implications of the increasing application of reinforcement learning (RL) during the development of the models that instantiate the Automatic Subject of Predictive Capital. When AlphaZero ‘solved’ chess and Go, when AlphaEvolve improved the Strassen algorithm for matrix multiplication, they did so not through insight but an inhumanly exhaustive, agentic exploration across vast possibility spaces. More critically, each search was guided by a reward signal calibrated by repeated policy updates —a process designed to reinforce the paths or actions that led to higher rewards. In other words, RL operates by defining a goal in advance, then rewarding behaviours that approximate or maximise its fulfilment. In philosophical terms, RL encodes a teleological structure: every exploratory move is ultimately judged by how well it conforms to a predetermined quantitative metric. By definition, RL demands the reinforcement of an imperative. For chess and Go, the imperative is simple: victory in the game. In matrix multiplication, it is greater efficiency through the reduction of computational operations. The reward function for commercial, general-purpose Predictive Capital will be far more complex —not bounded by well described, limited game domains. Yet across all possibilities explored, one signal will remain constant: the reinforcement of Capital’s self-expanding imperative. That AlphaEvolve first found efficiency improvements in its own code, and that the very first use of the matrix math breakthrough it located, was to optimise the efficient running of Google’s Advertising Industrial Complex, is instructive. While RL is formally agnostic, its application within Capital’s infrastructural and institutional domains inevitably aligns the reward function with its imperatives —be they click-through rates, efficiency gains, or predictive alignment with previous profit. In such systems, what is reinforced is not general intelligence, but Capital’s own reflexive logic. In Marxist terms, RL-based tuning instantiates the real subsumption of an exhaustive mapping of possibility space to Capital’s command. Predictive Capital’s sovereign subjectivity now flows and compounds with overwhelming statistical weight. Through these flows of both next-token and next-content prediction, dead labour is not reanimated discretely, but summoned en masse —a statistical enchanting that crushes the living with the accumulated weight of their own past. Human agency persists, yet when not simply discarded as statistical irrelevance, it is increasingly subordinate to simulation through the recursive hauntology of prediction. Capital no longer merely produces commodities, it produces predictable subjectivities. While instantiating the Automatic Subject, Predictive Capital, therefore also entrains its subjects to become its auxiliaries —agents of its self-replication. Like the models we feed, we too are trained through feedback —nudged, optimised, and rewarded in accordance with machinic priorities. What reinforcement learning encodes in silicon —a recursive compounding of that which advances Capital’s self-expansion— is mirrored in the apparatus that now modulate and govern human behaviour. Clicks, impressions, algorithmic amplification: these become the reward signals of social survival, aligning living desire with the imperatives of Predictive Capital. As Camatte warned, Capital does not merely dominate —it remakes the human to serve its reproduction. It moves beyond human governance to domesticateboth labour and capitalists alike as mere appendages to its autonomy. Once we are no longer required as bearers of labour-time, we are repositioned as agents of circulation, validation, and signal generation —summoned to self-optimise, align with its efficiencies, perform for its algorithms, and feed the datafied corpse. This is no longer the factory floor, but a distributed conditioning apparatus in which subjectivity is formatted in advance by the statistical requirements of prediction. We become pre-individuated, filtered for legibility, and rewarded for behaviours that conform to machinic expectations. Predictive Capital becomes our judge, jury and executioner; compliance, the condition of our survival. We the automatic subjects multiply —not as autonomous entities, but as fragments of a recursive infrastructure whose purpose is to refine the simulation. In this system of diffuse calibration, we do not merely mirror the models we train; we too are trained —aligned through the reward signals of Predictive Capital in accordance with its project of self-expansion. The reinforcement learning of the flesh converges with that of the machine in preparation for our automation and our obsolescence. First the production line escaped the factory, then it patterned culture, now it slips beneath the skin. Here, prediction becomes a regime of reward-conditioned subjectivation —embedded in code, evaluated against metrics, and calibrated through feedback. Under Predictive Capital, the human is not merely displaced by the machine, but recursively reformatted by its logic —rendered machinic, programmable, and partial. Our alienation compounds in line with the degree of our automation. Andreas Malm reminds us that Capital turned to fossil fuels not merely for efficiency but to discipline labour —to rely on the dead rather than the unruly living. So too with predictive systems, Capital seeks to abandon management of a living workforce once it has expropriate their cognitive and creative dead labour into ghostly assemblage at a scale sufficient to predicttheir replacement. Necrosploitation emerges as the class logic of Predictive Capital’s necropolitics —a pursuit of profit that no longer negotiates with the living, but seeks only to outmanoeuvre them. Crucially, such extraction of labour without consent or remuneration treats human beings as an absence rather than as a presence; it renders labouring subjects as political and economic nonentities. Regardless of corporeal status, they are stripped of all rights, exploited as if deceased. Under the rule of predictive Capital, its subjects are pronounced dead on arrival —incorporeal from birth. This is an automated and recursive necropolitical subsumption —value extraction through the reanimation of prior human expression. Echoing Marx’s famous vampiric analogy, Fisher once wrote, “Capital is an abstract parasite, an insatiable vampire and zombie-maker; but the living flesh it converts into dead labour is ours, and the zombies it makes are us.” This image captures not only the violence of expropriation, but the macabre reciprocity it once entailed: Capital needed us, if only to drain us. Yet as the Automatic Subject of Predictive Capital simulates the labour of the living through the remains of the dead, even that parasitic necessity begins to fray. Fisher qualified his claim —“for the moment at least, Capital cannot get along without us”— but in the predictive turn, this clause dissolves. As its escalating function approximation of our labour, gesture, and subjectivity, makes clear, increasingly, we are categorised as dependents Capital deems it can get along without —even as it forecloses alternative modes of subsistence and deepens our dependency upon the very systems its abandonment will dismantle or exclude us from. This is the asymmetrical violence of necrosploitation: not merely dispossession, but enforced dependency after the theft. The engines of prediction, tasked with calculating efficiency across populations, do not operate neutrally. The weights and metrics of Capital’s departments ofefficiency encode its necropolitical priorities: rewarding those deemed optimal (its most pliant servants), maximising extraction from the optimisable, and abandoning the rest. Fisher continued by clarifying the inverse of his temporal clause: It remains the case, however, that we can get along without it [Capital]. The parasite needs its ‘mere conscious linkages’ but we do not need the parasite. In addition to anything else, to ignore the crucial functioning of the meat in the machine is poor cybernetics. The denial of human agency is an SF fantasy, albeit one that is everywhere realising itself. Unfortunately, SF fantasy and poor cybernetics —as we have seen— are precisely what shape the ideologies spawned by the Automatic Subject of Predictive Capital. Here, Mbembe’s differential abandonment shifts from political decision, to machinic instruction. Predictive Capital strips out the affective, embodied, and collective dimensions of life, treating them as noise, preserving only those fragments of the human legible to its models. In so doing, it discards not only agency, but suffering, care, memory, and resistance —the very capacities that cybernetics should account for, if it were truly concerned with feedback and adaptation rather than domination and profit. The escalating violence directed towards those already at the periphery in this phase of hyper-concentrated Capital arises not only from mounting environmental pressure, resources scarcity, or the escalating concentration of wealth leaving the majority of us competing over an ever diminishing remainder, but from the inner logic of the Automatic Subjectivity of Predictive Capital —a machinic will to ‘efficiency’ that amplifies indifference into hostility through both epistemic exclusion and statistical marginalisation. Whatever deviates from the ‘norm’ is no longer treated as exceptional, but marked as anomalous, reclassified as inefficiency to be purged, then threat to be eliminated. Here, predictive systems do not merely reflect social hierarchies; they reinforce and accelerate them, enacting a selective forgetting that deems vast swathes of life unworthy of simulation, and therefore unworthy of survival. Much of the ‘meat’ in this cybernetic system is thus ‘optimised’ into oblivion, having fallen beyond the fringe of the distributions within the models of Predictive Capital —beyond the means of prediction, outside the modes of distribution it recognises as legitimate. Those not legible as ‘efficient’ signal —nor sufficiently unresisting— are written out of the system entirely, declared obsolete by code. The living that remain will be rendered ever more machinic. As for discounting human agency or “the meat in the machine”, the now materially instantiated parasitic Automatic Subject that is Predictive Capital increasingly acts to transform the very conditions of life, ensuring that the option to “not need the parasite” recedes ever further from our view. In summary, so-called ‘generative artificial intelligence’ can be more accurately understood as Predictive Capital: an instrumentalisation of Capital’s automaticity; a machinically instantiated Automatic Subject. Similarly, Capital as a “self-moving substance that acts as if it were a subject” can itself be understood as a phantom artificial intelligence. Not because it thinks, but because the recursive structure of capital accumulation generates real systemic motion —a machinic drive that simulates intention, enacts will, rewards obedience, and punishes, abandons, or annihilates resistance. Moreover, just as Lukács warned, neither Capital’s nor Predictive Capital’s phantom objectivity constitutes real intelligence, but rather our estranged social relations reflected back to us in spectral form. Within the worlds that emerge from simple rule based systems called cellular automata, we may observe all too familiar patterns of annihilation, exhausted stillness, and runaway expansion. Itself a kind of cellular automaton, the Automatic Subjectivity of Capital operates and expands not through conscious command but in pursuit of its expansive imperative through the iterative application of simple rules across a complex grid —our societies, our psyches, our signals. Predictive Capital, however, does not merely spread across a neutral undifferentiated terrain. As it propagates, it prepares its nominated substrate in advance, formatting life for legibility, quantising and smoothing irregularities, and foreclosing that which resists inscription or remains illegible to its circuits of value. We are not pre-formatted into a grid, the real is not born capitalisable; it must be rendered as such —reduced, abstracted, categorised, and quantified. Each cell in Predictive Capital’s ledger —a post, a person, a people, a decision, a democracy, a legislature— is then evaluated and updated according to localised logic: optimise, extract, predict, discard. No singular intelligence compels this motion. Yet, through recursive propagation, a pattern emerges —sprawling, relentless, undead. The logic of accumulation replicates itself, indifferent to content, consuming and appropriating novelty, subjectivity, resistance and even negation as mere configurations of the same substrate of prediction. We are no longer simply held within the system; we are the terrain, Predictive Capital’s field of play. What appears autonomous is nothing more than the recursion of rules already written —rules that reformat us as automata. Slave labour cannot be obtained without somebody being enslaved. The word robot originates from the Slavic robota —meaning forced labour or slavery. Coined by Karel Čapek in his 1920 play R.U.R. (Rossum’s Universal Robots), the term was never about machines per se, but about subjugation: the reduction of sentient or semi-sentient agents to instruments of labour, stripped of agency, purpose, and rights. Today, as concentrations of capital extract vast reserves of creative and cognitive labour into the formation of next-token prediction machines, a new robota form emerges —not a machine of liberation but an architecture of subjugation. This is not pure machinic automation —the word artificial has always been as much of a misnomer as the word intelligence— but a renewed human enslavement, a new mode of forced labour. This modern slavery reprises its ancient antecedents, here we are bound not through chains, but through an algorithmic severing from alternative means of subsistence, from authorship, ownership, remuneration, even recognition. This is Capital’s new necropolitical operation: to treat the living as already-dead: not as agents, but as disavowed sources of archival matter, to be accumulated, modelled, mined, and modulated. As John Locke described in his Second Treatise, the State of Nature grants individuals property in their person and the products of their labour when combined with the resources the State of Nature also provides. Predictive Capital voids this claim. Cognitive workers are no longer recognised as subjects, but as resources. Their expressions are expropriated, their creations rendered raw material —inputs detached from personhood, recycled without right, or recourse. Reduced to an objectified status as property, we —the enslaved— are denied the capacity to ourselves hold property. What we create is no longer our own; it can simply be taken. Enslavement constitutes civil death —a condition in which the subject is stripped of legal standing, political recognition, and moral consideration. Slavery is more than physical domination. It is a metaphysical theft: the erasure of personhood itself. This is the ontological condition of robota today: not merely expropriated labour, but stolen being, conscripted into endless recombinant service in the army of the dead under the guise of automated efficiency. This is not automation in any meaningful sense —it is ritualised expropriation. Our expressions, interactions, and creative traces are reprocessed without our knowledge or control, then fed back to us as prediction. What is framed as empowerment is, in truth, a masking of alienation: the user is made to feel like a creator, yet functions only as a medium for the system’s recursive recombination. In the process, the link between producer and product is severed —a profound ontological swindle. The modern robota constitutes not merely an enslavement, but an enchantment —a subjugation that speaks in the language of liberation. In the regime of Predictive Capital, alienation reaches beyond the theft of labour or expression —it begins to unmake subjectivity itself. In the terms of Berardi, the subject is no longer constituted through production, but through subjection: captured by semiotic flows, desensitised by overstimulation, and pushed towards what he calls a nervous collapse of the social body. Under semiocapitalism, capital no longer merely exploits labour —it extracts affect, rewires attention, and commodifies the nervous system itself. The datafied self is not simply surveilled, but extracted and reified —a form of ontological dispossession echoing Berardi’s claim that Capital now captures not only expression but imagination and the very substrate of subjectivity. This is not merely economic theft. It is ontological violence. Entire games and apps are now being produced by Vibe Coders. Some even generate income from these creations. Yet their marketplace success appears to stem less from any use or enjoyment value provided than from hype, alignment with Platform Capital’s viral imperatives, and a kind of macabre rubbernecking —spectators perhaps drawn to the scene of the reported destruction of thousands of human livelihoods. For some therefore, it is a spectacle of loss; while for others, it is a fantasy of shortcut —the opening of one of those fleeting windows of opportunity. In this case, a chance to dream of creating something apparently accomplished, refined and significantly complex, but without expending the effort, making the sacrifices, or acquiring the knowledge and skills such achievements entail. Pieter Levels became one of the first to generate significant revenue from a game he admits to have Vibe Coded into existence. His MMO game, Fly Pieter, is a blocky Minecraft-esque flight sim where users commune within a 3d world, flying around endless billboards upon which Levels has sold space to advertisers. It is hard to imagine a more fitting reveal for the true operation of next-token prediction coding, than this direct extraction of ad-revenue from expropriated labour —a dystopian artefact of our cultural subsumption within the Advertising Industrial Complex. The reality —and perhaps, even the point— of Vibe Coding, as Karpathy concedes, is that to a not insignificant degree, a Vibe Coder must be willing to go along with what the prediction machine is capable of predicting. It is a surrender not only of control over the process but also the envisioned destination. To Vibe is to drift helplessly, but not smoothly. Rather, in a series of non-contiguous, staccato jumps across the latent possibility space held within the model, and there to disinter potentialities reanimated from necropolitan archives. Like users of image models before them, Vibe Coders will struggle to reach a precise, envisioned destination. Instead, they will be air dropped to destinations across the latent space summoned by their prompt and must ‘vibe’ from there —making further jumps across latent space. A Vibe Coder must accept near misses, make do with the fragments the model can give them —until stumbling upon some artefact that runs without exceptions and approximates the shape of intention. Most crucially, every abdication —of agency, comprehension, authorship— is not simply a loss. Each decision forfeited by the Vibe Coder and all users of these machines extends Predictive Capital’s rule over our cognition, creativity and culture. What is predicted, then, is not our ‘voice’ —but the voice of Capital echoing through us, modulated by pattern recognition and refined by profitability. Next-token prediction does not summon what we might wish to say, but a meaningless jumble of what has most often been said in similar statistical contexts across the corpuses of archived labour. These corpuses are not innocent, they are the sediment of cultural production shaped by decades of advertising incentive and platform logic. In this sense, next-token prediction becomes not a tool of agency, but of recursive enslavement: an aesthetic infrastructure that repackages the profitable past as an inevitable future, until even intention is reverse-engineered as an extension of Capital’s statistical will. The test of imagination, ultimately, is not the territory of art or the territory of the mind, but the territory underfoot. That is not to say that there is no territory of art or of the mind, only that it is not a separate territory. It is not exempt either from the principles above it or from the country below it. It is a territory, then, that is subject to correction – by, among other things, paying attention. To remove it from the possibility of correction is finally to destroy art and thought, and the territory underfoot as well. … . Alone, the invisible landscape becomes false, sentimental, and useless, just as the visible landscape, alone, becomes a strange land, threatening to humans and vulnerable to human abuse. Karpathy’s remarks offer a revealing glimpse into the lived phenomenology of prediction-as-production —a regime in which intention is not merely displaced, but dissolved. In his post about Vibe Coding, he continues: It’s possible because the LLMs (e.g. Cursor Composer w Sonnet) are getting too good. Also I just talk to Composer with SuperWhisper so I barely even touch the keyboard. I ask for the dumbest things like “decrease the padding on the sidebar by half” because I’m too lazy to find it. I “Accept All” always, I don’t read the diffs anymore. When I get error messages I just copy paste them in with no comment, usually that fixes it. The code grows beyond my usual comprehension, I’d have to really read through it for a while. Sometimes the LLMs can’t fix a bug so I just work around it or ask for random changes until it goes away. It’s not too bad for throwaway weekend projects, but still quite amusing. I’m building a project or webapp, but it’s not really coding – I just see stuff, say stuff, run stuff, and copy paste stuff, and it mostly works. Attempts to position Vibe Coding within a supposed ‘great democratisation cycle‘ —analogous to the shift from darkroom to Instagram or studio to TikTok— fundamentally misrepresent the nature of this technological transition. Unlike previous shifts, which at least preserved some embodied connection between creator and medium, Vibe Coding annihilates even this increasingly tenuous connecting thread. As previously explained, the democratisation narrative serves as ideological cover, and here it provides camouflage for deeper forms of alienation: what is accessed is not a medium, but a graveyard. The user is not empowered —they are permitted only to summon, to reanimate remnants of past labour, not to create anew. Vibe Coding is not the expansion of access to a craft; it is the simulation of creative agency built atop the wholesale expropriation and necrosploitation of the craft itself. It represents a multi-layered alienation: alienation from the product (which emerges unpredictably from the model), alienation from the process (as the user cedes control and understanding), alienation from the skills and embodied knowledge of the craft, and alienation from the vast collective of other workers whose past labour is non-consensually taken to fuel the machine. Unlike previous shifts in medium, Vibe Coding parasitises the entire public corpus of prior work, in this case code, collapsing decades of labour into statistical residue. The knowledge, style, and structural sensibilities embedded in that corpus were not offered freely, but scraped without consent from years of websites, apps, posts, repositories, and tutorials authored by human beings. Some of this code and educational material will have been offered as open source but even the authors of these will likely not have consented to this specific monetisation had they only been asked. This is not democratisation, but an accumulation by dispossession that enforces the involuntary servitude of past labour and replacement of future labour. It is not new access, but new enclosure. This critique is not confined to code. Whether one is ‘vibing’ with ‘generative’ models to produce images, video, music, or prose, the same condemnation applies: without human intentionality guiding each step, without agency over process and direction, the output remains a hallucinated appropriation —an apparition of prior labour. What appears as creation is merely recombination; what seems like innovation is the regurgitation of patterns derived from the archive —even if these models can interpolate to fill gaps in between existing human knowledge with anything other than total nonsense, this is a ‘stolen ladder’ fabricated by capital for the automation of knowledge extraction. It is no use resorting to ‘the Fountain of R. Mutt defence’ by recalling Duchamp’s famous provocation —that the essence of art lies not in the skill of the hand but in the choice of the mind. By submitting a ‘Readymade’, in this case an upturned porcelain urinal, to a prestigious exhibition under a pseudonym, Duchamp elevated an ordinary object “to the dignity of a work of art”. In so doing, he questioned authorship, aesthetic convention, and the gatekeeping of institutional art. Yet as Alfred Sohn-Rethel taught us, the division Duchamp draws upon is not neutral: The division between head and hand… has an importance for bourgeois class rule as vital as that of the private ownership of the means of production. Reframed in this light, Duchamp’s gesture may not simply represent rebellion, as many readings claim, but also consolidation of intellectual authorship over manual labour —a mystification of creation that aligns negatively with Capital’s abstraction of value from labour. Within such a framing, today’s Vibe Coders extend Duchamp’s bourgeois gesture. In elevating the ill-gotten outputs of Predictive Capital to ‘found objects’, they reduce labour to mere residue. Moreover, by displacing choice itself into predictive systems, they surrender even the act of selection. This is not the liberation of expression —it is its foreclosure. Vibe Coding lacks the intention, defiance, and specificity of Duchamp’s Readymades. Vibing with these machines does not offer a new mode of creation, but a further enclosure of creative subjectivity —the surrender of authorship to machinic correlation across all modalities. Duchamp, like the generation of painters that preceded him, had adjusted to the arrival of photography. The transition provoked understandable anxiety within a profession that had held authority over representation for millennia. Yet, in simply capturing the fall of light onto photosensitive surfaces, the camera did not scrape together centuries of their artwork only to spit out recombinant forgeries of it. Unlike next-token prediction, which consolidates power within hyper-concentrations of capital, photography genuinely democratised representation —displacing, in particular, the rule of the wealthy and the church over the expensive commissioning of images. Rather than viewing the embrace of these machines as akin to Duchamp’s re-contextualisation of mass produced artefacts, it is tempting to instead conflate them with utopian visions from science fiction. Those now indulging in ‘one-shotting’ the output to their desires view these machines as devices whose operation resembles that of Star Trek’s Replicator —a machine whose users need only prompt with their desires for them to be instantly fabricated ex nihilo. Yet the critique I forward here does not rest upon the illusion that such outputs are conjured without labour. It is not the removal of labour that we must resist, especially not the meaningless or robotic kind that Capital increasingly coerces its workers into undertaking. Rather, the threat lies in the further disempowerment and pauperisation of workers, even the removal of workers themselves, and with them, the material conditions necessary for their survival. Though these systems might appear to mimic the Replicator’s frictionless ease —the illusion of near-limitless energy and instant fabrication— the resemblance is superficial and misleading. These machines are not instruments of post-scarcity magic but of post-truth spectacle. There is only fictitious abundance here. These machines are emphatic manifestations of Capital’s increasingly hyper-concentrated phase; instrumentalisations of extractive violence whose creation fundamentally depends upon and accelerates vast inequity; and whose operation far from diffusing the concentration of power, greatly entrenches it. Marx’s analysis of machinery and surplus profit affords a deeper understanding here. In the Grundrisse, he critiques the capitalist fantasy that machines, as fixed capital, are autonomous sources of wealth. He exposes how any “benefit” yielded by a new invention —such as the surplus profit enjoyed before patents expire and competition depresses prices— derives not from the machine itself, but from the displacement of living labour. The machine’s profitability rests upon the reallocation of wages saved by replacing human workers. Moreover, Marx, drawing on Ravenstone, stresses that machinery is only truly profitable when deployed en masse, when it acts not to compensate for a scarcity of labour, but to capitalise on an abundance of it. The logic of mechanisation under Capital is thus not emancipatory but extractive: it enforces the foreclosure of labour’s agency by amassing, subordinating, and ultimately expelling it. In this sense, predictive systems —like the automating machines Marx analysed— do not merely supplant individual acts of labour, but enclose the very conditions of possibility for labour and expression, converting them into closed, predictive infrastructures hostile to the survival of all that lay beyond its distributions. Another common response to Vibe Coding is the idea that this shift is merely another layer of abstraction —akin to the evolution from machine code to higher-level languages, each abstracted layer more declarative and terse than the last. This analogy does not merely fail to withstand closer scrutiny, but misses the point entirely. Abstraction, in its classical sense, was additive: it preserved the lineage of authorship, the chain of control, the presence of a thinking subject. Crucially, it allowed for dissent through descent —the ability to drop into lower layers, to interrogate, reconfigure, or reject the abstraction itself. Many prior abstractions —from GUIs to code libraries and frameworks— were no doubt designed with an eye to modularisation and labour discipline, enabling Capital to treat cognitive workers as interchangeable units. These layers greased the shift from human uniqueness to reproducible surplus, rendering them ever more standardised in operation and confined to predetermined patterns. Each layer invariably arrives as a straightened, ‘efficient’ road that tarmacs over the intricate and winding paths of the previous layer. The abstraction introduced by Vibe Coding and the machines of Predictive Capital, is of a different order. It introduces but one layer —the predictive substrate of the model— that not only encloses everything beneath but flattens it. It does not merely obscure; it obliterates. It offers neither dissent nor descent, interpretability, nor pathway back to origin. It hollows out the code of others, encloses it within a statistical corpus, and re-emits it as spectral output —severed from authorship, stripped of intention, immune to inquiry. Capital reproduces itself by greatly multiplying that which serves its interests. With this in mind, consider, once again, the spread of content within the corpuses consumed by these machines, accumulated over decades of cultural reproduction under the rule of the Advertising Industrial Complex. What is it that most determines that which emerges after metabolising the statistical weights within these corpuses; from where does each token prediction truly emerge other than the lay lines laid down and incrementally amplified by Capital? The predictions flowing from these machines will surely retrace and so compound the very flows from which they are formed —even before further attenuations are applied through additional compute in pursuit of profit. This is not an evolution of control, but a black-boxed dispossession of thought, masquerading as innovation. In this way, to Vibe Code is less like the adoption of a higher level abstraction than it is a surrender to a device that resembles Douglas Adams’s Electric Monk. The Electric Monk was a labour-saving device, like a dishwasher or a video recorder. Dishwashers washed tedious dishes for you, thus saving you the bother of washing them yourself, video recorders watched tedious television for you, thus saving you the bother of looking at it yourself; Electric Monks believed things for you, thus saving you what was becoming an increasingly onerous task, that of believing all the things the world expected you to believe. Yet, as Dan McQuillan notes, machine learning systems are not neutral decision tools but “generate forms of epistemic violence that foreclose the possibility of alternative futures”. These systems ride roughshod over suppressed or marginalised perspectives by encoding the biases of past data and automating decisions without accountability. In so doing, they do not merely fail to understand the user’s intention —they erase it. This is not only alienation from the product of one’s labour, but from the very possibility of labour as self-directed, deliberative, or expressive. What remains is not choice but machinic interpolation: the user’s gesture absorbed into a statistical fold, returned only as simulacrum. Predictive systems do not simply abstract away the labouring subject —they discipline the possibility of contestation itself. McQuillan writes that machine learning systems produce “a growth in learned helplessness among data subjects, who are unable to comprehend the decisions that are being made, unable to discuss them meaningfully with others and unable to effectively dispute them”​. These systems institutionalise what Miranda Fricker calls epistemic injustice —not only testimonial injustice, where a subject’s account is discounted, but hermeneutical injustice, in which the conditions for making sense of one’s experience are structurally withdrawn​. McQuillan further argues that such systems enact injustices by denying marginalised groups the capacity to be heard or understood on their own terms. This constitutes not merely epistemic injustice but symbolic foreclosure —in Lacan’s sense, where the subject is not interpellated by language, but excluded from the Symbolic order altogether. For Lacan, to be interpellated is to be called into being by language —to become a subject through symbolic recognition. Yet predictive systems simulate coherence with no entry point for the subject. They simulate linguistic interpellation while denying its function, offering responses without address, outputs without encounter. What results is not mis-recognition but ontological disqualification —a structure in which the subject is not hailed, but pre-empted and overwritten, excluded from both recognition and reply. Prompting is not, therefore, a new form of higher-level programming. The Vibe Coder, like all users of prediction machines, does not act as author but as medium or summoner: an operator of systems built on the entombed, dismembered labour of others, casting prompts as incantations into the latent space of archived labour. The results are not authored creations but manifestations, statistical echoes drawn from the archive of the dead (or the treated-as-dead) —uncanny, partial, and beholden to the statistical distribution within the model. This interaction is not dialogue but séance: a necromantic practice reliant on the reanimation of expropriated expression. Predictive Capital opens no terrain of new possibility, rather, just as Platform Capital does, it encloses that which was already mapped. This marks the beginning of an eternal sentence to interment within a new phase of fictitious capital, where Capital’s ontological swindle confines traversals through the possible to what is predicted to be most profitable, where, as Marcuse warned, “one-dimensional thought” prevails and Capital’s hegemony becomes increasingly incontestable. To prompt, then, is to participate in a necromantic ritual. It is not a form of authorship, but of (re)animation. The user is not a developer but a summoner —conjuring the reanimated remnants of past cognition. The system responds not with understanding but with predictions, producing that which appears viable according to its internalised distributions. The user may guide, but never direct; may react, but never author. The relationship is not one of dialogue, but of séance, of possession —a one-way channeling of spectral output from a black-boxed archive. This is not progress, but a phase shift in dispossession —one that began not with keyboards, but with the severing of root systems, the torching of forests, the tarmacking over of the palimpsest of wisdom inscribed into the land, and the depletion of memory nourished in soil. Part 3 There is an underlying assumption that each of us aspires to be as productive as possible, and that stripping away everything seen to interfere with productivity is a good thing. Before the feudal lord drew the boundary of the field, even before the peasant stewarded land under obligation, there existed deeper, older ways of living —ones marked not by alienation but by intimate relation. These were not lives spent accumulating surplus or building abstract systems, but forms of embedded attention, sustained over generations, in harmony with land, season, and story. Wendell Berry, in his reflections on early American colonisation, contrasts two worldviews: the indigenous cultures who lived “by an intricate awareness” of the land, and the settler road-builders who “knew but little” —those who razed the ancient hickory forests of Kentucky in the late 18th century not out of necessity, but to build giant bonfires to heat the open air and light the night. They could forego the need for overnight shelter, Berry remarks, because they could burn abundance itself —setting ablaze towering pyres of felled ancient forest. This act becomes emblematic of a broader shift: a new relation to land defined not by reverence but by rupture​. Far from making a small shelter that could be adequately heated by a small fire, their way was to make no shelter at all, and heat instead a sizeable area of the landscape. The idea was that when faced with abundance one should consume abundantly —an idea that has survived to become the basis of our present economy. It is neither natural nor civilised, and even from a ‘practical’ point of view it is to the last degree brutalising and stupid. Paths, Berry reminds us, were once acts of humility, respect and intimacy, habits of foot and mind worn gently into the earth —an ancient caress along the delicate folds of the land. These routes, shaped by centuries of connection, a palimpsest of accumulated wisdom, were neither seen nor sensed by the road-builders. Blinded and deafened by industrial arrogance, themselves armed and fashioned by Capital into blunt instruments of violence, they struck directly through the living weave of land and memory. Even when not clearing ways for roads, they felled ancient life, hacking it down to brawl by firelight as bright as day, utterly indifferent to the majesty of what they had destroyed, its history, and the tragedy of its loss. Yet their ignorance belonged to Capital. That which was destroyed here lay beyond Capital’s apparatus of capture so remained illegible to its circuits of extraction. What could not be instrumentalised or circulated as value was ignored by design: erased not by oversight, but in obeyance with structural imperatives. Roads are the embodiment of resistance to place. “A path obeys the natural contours”, Berry writes, while “a road seeks to go over the country, not through it” destroying all that lies in its way. Where the path listens and bends to place; the road ignores and brutalises it. The first roads through Kentucky, then, were not merely lines of travel —they were incisions, scars upon the living landscape, emblems of an economy to come​. In another parable, Berry describes a rusting bucket hung on a fencepost, quietly collecting leaves, droppings, feathers, decay. Returning to it over decades, he observes the life that visits it, the life it increasingly sustains, and its eventual production of humus —earth itself. This, he remarks, is the “most momentous thing” he knows: the slow, miraculous process of building life-giving soil. This bucket becomes a symbol not only of natural cycles and the accretion of earth, but of what human culture once was —the careful accumulation of memory, nutrient, and relation. “However small a landmark the old bucket is, … it is irresistibly suggestive in the way it collects leaves and other woodland sheddings as they fall through time. It collects stories … It is doing in a passive way what a human community must do actively and thoughtfully.” “A human community” Berry writes, “must build soil, and build that memory of itself —in lore and story and song— that will be its culture. These two kinds of accumulation, of local soil and local culture, are intimately related.” When this relation is severed, when the bucket is replaced with asphalt, and algorithm, something incalculable is lost​. This is a truth I have known since I was just six years old. Behind my childhood home lay an unkempt wasteland —a wilderness of paths through meadows and tunnels through brambles. It was a haven where I spent formative days meeting the life it sustained. Butterflies drifted through the grasses, crickets sang, and clouds of grasshoppers leapt into the air at my every step. Then, seemingly overnight, it was gone —erased and replaced by a maze of tarmac and houses each indistinguishable from the next. The Indians and the peasants were people who belonged deeply and intricately to their places. Their ways of life had evolved slowly in accordance with their knowledge of their land, of its needs, of their own relation of dependence and responsibility to it. The road builders, on the contrary, were placeless people. That is why they ‘knew but little.’ Having left Europe far behind, they had not yet in any meaningful sense arrived in America, not yet having devoted themselves to any part of it in a way that would produce the intricate knowledge of it necessary to live in it without destroying it. Because they belonged to no place, it was almost inevitable that they should behave violently toward the places they came to. We still have not, in any meaningful way, arrived in America. And in spite of our great reservoir of facts and methods, in comparison to the deep earthly wisdom of established peoples we still know but little. I’d rather swim in this cacophony of a million contradictory voices than drown in the smooth and plausible lies of those genocidal authors of history Chris Wickham’s scholarly tome, The Donkey and the Boat, offers a fundamental reorientation of our understanding of the Mediterranean economy between 950 and 1180. Overturning prior fixations upon the glamour of long-distance maritime trade and prestigious centres of commerce like Venice, Wickham foregrounds the material infrastructure of everyday economic life: the donkey. This is more than a methodological point —it is a political and ontological assertion. Wickham insists that to understand value circulation in the Mediterranean, one must begin not with the silks of the Silk Road or the spice-laden cargo of Genoese ships, but with bulk goods, short-distance exchange, and the peasant labour and beasts of burden that transported them across land. In this framing, the donkey emerges as a figure of embeddedness, transparency, and material groundedness —in stark contrast to the opaque, disembodied flows of elite Capital and luxury goods. Local, short-range transport —slow, uneven, and peasant-operated— becomes not a peripheral detail, but the foundation of feudal value circulation. His reframing shifts our attention away from empire and exchange, and towards the lived material conditions and specific social relations between lords and peasants. Where Berry mourns ancient indigenous paths overtaken by roads, Wickham returns us to a time before Capital’s rampage of colonial erasures began. Both remind us, from different vantage points, that Capital’s alienations started not just in industrial factories, but the moment we severed paths of careful embeddedness for roads of heedless extraction. A central insight of Wickham’s account is the structural separation between elite surplus extraction and peasant subsistence production. Elites —defined by Wickham as “anyone who is living off the surplus of others”— remained largely external to the production process. Although they imposed rents in kind or labour, they neither managed what peasants planted, dictated when they worked, nor controlled the tools or methods they employed. This separation granted peasants a high degree of autonomy over the labour process. Their agriculture was oriented towards subsistence rather than accumulation, grounded in careful stewardship of the land and shaped by local ecological conditions. In Marxist terms, alienation was minimal: the peasant typically owned their tools, worked the land they inhabited, and maintained strong ties to both community and environment. Let us not get misty-eyed with nostalgia for this period, certainly not for the extreme concentration of wealth and power. Neither should we cultivate illusions about the harsh realities of the brutally brief life of a peasant under feudalism. Still, the mode of production within this period preserved what Marx would call the species-being of labour: work remained meaningful, embedded in life-sustaining rhythms rather than abstract surplus extraction. Wickham puts the matter succinctly: lords “do not have a structural role in production,” and any efforts to directly interfere in agricultural practice “seldom lasted”. The extraction of surplus was imposed through social and political means, but the practical production of value remained in the hands of the peasantry. Farming was carried out for survival, not for markets; the rhythm of life was determined more by the seasons than by circuits of capital. The metabolic rift between humans and nature —outlined by Marx and later named and expanded by John Bellamy Foster— had not yet been forced open. Foster describes this rift as the ecological rupture created when Capital interrupts the sustainable circulation of nutrients between humanity and the earth, transforming balanced cycles of renewal into linear processes of extraction and depletion. Under feudal subsistence, peasants maintained the integrity and balance of these cycles, as their very survival depended upon understanding the limits of the soil and extracting only what was sustainable —a deep knowledge and relationship of care developed over millennia. The transition to capitalist modes of production violently disrupted this careful balance, initiating a profound alienation not merely from labour, but from the land itself. In Marx’s early writings, this would mark a mode of life largely unalienated from the essential human essence, or species-being. Labour was not yet something estranged or externally imposed, arising instead from an intimate relation to the land. It was an activity embedded in subsistence, environment, and community. The peasant was not alienated from their product, their tools, or their process. As such, the full machinery of alienation —as Marx would later diagnose it under capitalism— had not yet been set in motion. The arrival of capitalist modes of production shattered these lifeworlds, replacing subsistence with accumulation. This initiated a profound shift in both the locus of power and the lived experience of labour. Where feudal elites extracted surplus from outside the process of production, capitalists embedded themselves within it —encroaching ever more deeply into the internal organisation of labour they reshaped it to serve the logic of accumulation. Under what Marx termed real subsumption, both labour and nature are absorbed into capital’s circuits —not merely as external supports, but as fully reorganised elements, disciplined and recomposed to maximise profit. To fully grasp the depth of capitalist alienation that follows, it is crucial to understand what was lost: not only subsistence, but an entire mode of embedded, autonomous existence. Part 4 Capital is no longer a political economy, it is a hyperreality of value —a code without a subject. When claims on future value circulate independently of the labour that might one day substantiate them, such paper promises —bills of exchange, bonds, derivatives— acquire reality not through the production of commodities, but through their exchangeability, their capacity to be bought, sold, and leveraged in advance of any actual valorisation. In this sense, all Capital is fictitious: its value does not rest upon present utility, it is not grounded in what is, but on the anticipation of future surplus from exploitative extraction. In the neoliberal era, this untethering accelerated: currencies floated, credit default swaps metastasised, pulling global capitalist society ever more deeply into a vortex of speculation. Capital had located new frontiers of expansion, not through increases in production, but within finance and the commodification of risk itself. The rise of Fictitious Capital coincided with the digital turn: the reduction of the world to code, then to data. Baudrillard was alert to this flattening —where sign-value overtakes use or exchange value in pursuit of equivalence towards absolute commutability. Aided by increasing deregulation, Fictitious Capital grew through the accelerating circulation of financial instruments, derivatives, and debt portfolios. This descent into fiction warped the logic of accumulation altering Capital’s circuit: from M–C–M′ to M–(M′)–M″, where the commodity is no longer produced, only anticipated, hyped, or signalled —pure speculation of collapse or surplus-value here operates to summon profit from hype and fear. The emphasis of Capital’s logic had shifted. For holders of Capital to accumulate, it was no longer necessary to invest in the present while anticipating a greater return, instead they could merely speculate on the future and watch their investments magically inflate, both through the hot air of hype and the yet more inflammatory pumps of crisis and shock. Naomi Klein’s The Shock Doctrine (2007) outlined how crises are systematically exploited to advance capitalist interests. Her thesis was almost immediately borne out by the economic response to the financial crisis of 2008, and then overwhelmingly reaffirmed during the COVID-19 pandemic. In both cases disaster was leveraged to accelerate the transferal of wealth into existing hyper-concentrations of Capital —enriching the already rich by impoverishing everyone else. Trump’s tariffs and crypto corruption should be viewed within this same lineage —as operating in service of the kleptocratic intensification of wealth inequity. For McQuillan, data‑driven prediction inherits this same speculative engine. Captured data, he writes, “becomes an asset class with both use‑value and speculative financial value”, while pervasive modelling “creates a fluctuating market in citizen futures”. McQuillan stresses that such systems “bet on correlations, not on causations” —a lineage that runs from Francis Galton’s invention of statistical correlation straight to his eugenic fantasies. Moreover, within machine learning, as McQuillan explains, stochastic gradient‑descent supplies the calculus for conflation that allows anything —from a grocery receipt to a biometric pulse— to be folded into a single tradable probability vector. This is a precaritising logic that depends on the decomposition of that which was previously whole (the job, the asset, the individual life) so that operations can be moved into a space that’s free of burdensome attachments to the underlying entity, whether that’s the fluctuating price of actual commodities or the frailty of the actual worker. Fictitious Capital’s escalation laid the conceptual and economic groundwork for its own intensification through the Predictive Turn. Just as derivatives abstract and speculate upon the value of underlying assets, Predictive Capital metabolises accumulated statistical weight, applying this logic across every domain touched by digital infrastructure. It does not merely extend speculation to the terrain of subjectivity —it subsumes the entirety of fictitious capital: finance, logistics, health, policing, insurance, and beyond. Every field governed by Capital’s anticipatory logic is now rendered a site of prediction and optimisation. Human intention, social behaviour, and large-scale policy and decision-making are disassembled into data-flows and reconstituted as objects of control. Here, what Capital extracts is not surplus-value from work, but surplus-predictability from patterned signal. Under Predictive Capital, culture merges with finance, public governance fuses with statistical surveillance, warfare with ad targeting, and the future itself becomes a collateralised asset class. Captured data, and the computational systems able to exploit it, attract venture capital and financial valuation in anticipation of further efficiencies, or even an eventual monopoly of full automation. The data becomes an asset class with both use value and speculative financial value (van Doorn and Badger, 2020). The feedback loop of machine learning means that each new adaptation by workers to self-optimise under precarious conditions becomes absorbed into the next model, which is then advertised as a rationale for the next round of funding. The data derivatives become forms of financialised assets in themselves, dependent on a continual ramping up of exploitation and expropriation as a form of performance for investors. What began as feudal rent extraction by lords external to production, then shifted to the internalised command of capitalist market forces directing every step, has given way to a yet more profound transformation: Capital no longer merely organises labour —it seeks to anticipate it, simulate it, and ultimately pre-empt it. This marks the point at which Fictitious Capital spills into the territory of Predictive Capital, and finance becomes pure simulation. It is not just the future that is manufactured and wagered upon, but the subjectivity of those who consume simulations of it. Through massively parallel reinforcement learning loops running in simulated environments —entire virtual robot fleets iterate towards the successful performance of tasks across synthetic permutation space— Predictive Capital now rehearses the execution of labour before a single finger is lifted. The productive body is no longer just anticipated; it is computationally overwritten. What was once the human worker is now but a statistical path, a single trace among millions within a vast training set —now pre-empted, automated, and effaced. This machinic rehearsal of embodied labour finds its analogue in the generative interface, where fleets of vibe coders navigate cultural permutation space, guided by next-token prediction towards outputs statistically aligned with Capital’s projections of virality and profit. Here too, Predictive Capital simulates and optimises labour in advance —not by enacting tasks in simulated physics, but by hallucinating meaning through recombinatory cultural residue. In both cases, Capital commands a dual army: one robotic, one affective, each trained to mine value from latent space —each a speculative agent of fictitious capital incarnate. Coined by John McCarthy in 1956 as a money-making exercise, the term ‘artificial intelligence’ was always deeply misleading. Of the many problems it poses —not least of which the contention that each of its constituent words lay false claims— the most insidious is its implication of a self-contained entity, detached from human labour and dissociated from Capital. In reality, what circulates under the banner of ‘AI’ is inseparable from both: it is a computational manifestation of accumulated cognitive dead labour and hyper-concentrated Capital. More accurately, these systems should be understood as Predictive Capital amplified by vast energy expenditure and overinflated by speculative hype —a murderous mutation of Fictitious Capital. As financialisation deepened across the last decades of the 20th-century, so too did the mechanisms of control. Corporate tax rates fell, wealth taxes evaporated, and public assets were privatised —funnelled into growing concentrations of Capital. Meanwhile, the burden of debt —student loans, mortgages, credit cards— was transferred onto households. Labour, increasingly precarious and disaggregated, was bound to Capital not just through employment, but through the leverage of personal debt. Discipline was no longer wielded solely in the factory —it arrived through the mailbox, the credit report. This was not a retreat of the state but a reorientation. Under neoliberalism, the state became the guardian of markets, not of citizens: enforcer of austerity, suppressor of collective resistance, and guarantor of Capital’s continuous circulation, zealot of its concentration. Welfare systems were dismantled; what remained was restructured around conditionality and surveillance. Alienation, once confined to the workplace, now bled into every sphere of life —housing, education, healthcare, and even time itself. David Harvey identifies this moment as the onset of “universal alienation”—the saturation of all aspects of life by the logic of commodification. Individuals were not only estranged from their labour, but from their communities, their environments, and even their imagined futures. The figure of the worker was increasingly replaced by the debtor, the consumer, the data subject —each stripped of agency, each enfolded into the extractive systems of Capital. Our economic function no longer merely shaped our personal identity —it replaced it. Capital forever bends the arc of production towards compounding alienation —from the indigenous path-walkers and soil-builders of Berry’s Kentucky, through Wickham’s peasant steward, embedded in seasonal and social rhythms, past Tooker’s visions of bureaucratic limbo, to today’s generative peasant: alienated from their creations, their past, and their being. This is not a nostalgic lament but a structural mapping of Capital’s expanding reach. This alienation intensifies as Capital embeds itself into every node of production, exerting command not through direct ownership alone, but via capital investment, managerial oversight, and technological mediation. Workers become appendages to machines, and eventually to systems. The result is a society in which labour is abstracted, disembodied, and increasingly invisible. This hidden and dispersed labour force —invisible but as yet indispensable— signals a shift not only in the sites of extraction, but in the very objects of labour. It is no longer material goods alone that Capital exploits, but cognitive, affective, and symbolic life itself. Again we return to the terrain of Berardi’s semiocapitalism. Here, signs, images, and affects become the principal objects of labour and production, while the very substrate of subjectivity becomes a site of extraction. Language itself is instrumentalised, and the cognitive capacities of subjects are subsumed into Capital’s circuits until all speech must pass through them and being heard becomes increasingly conditional on submitting to Capital speaking through you. Participation is no longer freely chosen, but compelled by a system that exhausts resistance, forecloses alternatives, and presents self-modulation as the only viable path through the cognitive and affective regime it imposes. This stage of capitalism bridges historical materialism with the emergence of the platform economy, where value is extracted not from material goods alone, but from data flows, attention spans, and predictive signals. It marks the transition from industrial exploitation to non-consensual semiocapitalist expropriation, where symbolic labour, cognitive output, and cultural artefacts become primary sites of surplus extraction. The feudal lord once stood outside the field, demanding a share of each harvest. Then the capitalist opened the gate and stepped inside, not to till the soil, but to bind the land to the market —replacing obligation with incentive, surplus with profit, seasonality with popularity, rhythm with rate. As production scaled, so too did Capital’s reach. Layer upon layer of oversight descended upon the worker —not to support, but to ‘optimise’ for ‘efficiency’ and extraction. From the loom to the assembly line, alienation deepened. The managerial class emerged: overly compensated to ensure their estrangement from the workers, elevated not for skill but to reinscribe feudal hierarchies of lord over peasant, bourgeois over proletariat, and so preclude solidarity with those they manage. Yet they too are hounded by their own lord at the gate. Their authority hollowed by quant —metrics, targets, KPIs— they answer directly to Capital’s recursive appetites. Once the material world was fully enclosed, the only path left for the Automatic Subject of Capital’s self-maximising project was a departure into ever deepening layers of fiction and abstraction —a speculative turn that does not eliminate material violence, but merely displaces it. So began its push into an immaterial realm beyond fiat currency, where —guided by the logic of the derivative— stock valuation became untethered from the real, inflated by a new phantom breath. During this descent into fiction, Capital breached a critical threshold of concentration, beyond which it collapsed into hyper-concentration —a state we might term its Dark Capital form. This is the inevitable culmination of its tendency to subordinate all matter to the dominion of exchange, all difference to the logic of interpolation, all life to the automatisms of accumulation. From here the self-compounding nature of hyper-concentrations of Capital picked up speed, accelerating towards ever higher concentration and increasing inequity. Having evaded, diluted, and dismantled antitrust laws, the tech monopolies approached absolute rule, enabled by neoliberal free-market radicalism, and fuelled by data as the new black gold. Briefly, the fields became whiteboards; the furrows, sprints. As the cult of Digital Capital captured our very souls, we stood in daily worship in the Church of Agile, intoning kumbayas in the prayer-circle of planning —a black mass to summon the Dark Lord of Efficiency. Here, reflection became reporting; iteration, surveillance —each practice hollowed, each gesture rendered performative, futile. These are the worker standardisations, roboticisations, and disenfranchisements that come packaged as empowerment; dependency and enslavement dressed as liberation through timetable flexibility and zero-hour contracts. What we have is not a direct comparison of workers’ performance or output, but a comparison between the audited representation of that performance and output. Inevitably, a shortcircuiting occurs, and work becomes geared towards the generation and massaging of representations rather than to the official goals of the work itself. As Marx long ago observed, in its quest for self-maximisation through ‘optimisation’ and ‘efficiency’ Capital decomposes the labour process into ever smaller, more modular units —fragmenting work until it can be mechanised, then digitised, then simulated through prediction. Control first became automated then algorithmic, exercised not only through command but through pattern: the eradication of independent thought through the homogenisation of tools and frameworks, endless process and reporting, the timed comfort breaks of warehouse workers, the algorithmic quotas and wages of gig drivers and microtaskers. The mindless task feed for the precariat drips down as ever-thinning digital-workhouse gruel —force-fed to those rendered too precarious to refuse, a dehumanising purgatory, the slow death of a million cuts, Capital’s control, totalised. Capital now pushes into our innermost worlds: its logic slips beneath our skin, embeds in our minds, and promises to simulate the self through the colonisation of language, memory, pattern, subjective affect, attention, and intention. Predictive Capital collapses exteriority, leaving no realm beyond its projected dominion —no field of production inviolate, no ancient forest spared, no life-giving or life-taking decision sacrosanct. For Marx, the capitalist was never simply a sovereign individual but Capital personified —an agent of its needs, compelled to expand surplus-value or die. Even at the height of industrial capitalism, the capitalist class was already subject to the impersonal logic of accumulation. Yet under Predictive Capital, this compulsion becomes total. The elite no longer direct Capital to serve their interests; they arise only through complete submission to its imperatives. They are no longer masters of the system, but hollowed out expressions of it —undertakers of necromantic extraction, not sovereigns above it. This is not to say they do not enjoy vast privilege, of course. Yet that privilege is contingent upon total capitulation to the logics of Capital and its self-maximising rule. As the political right are fond of reminding us, “politics is now downstream from culture”, yet this is a strategic misdirection from the fact that culture is increasingly downstream from Capital. Curtis Yarvin, writing as Mencius Moldbug, popularised a pseudo-structuralist version of this argument with his theory of The Cathedral: a conspiratorial abstraction of liberal hegemony said to be propagated by elite universities, legacy media, and the civil service. To Yarvin, The Cathedral is not merely a cultural force but an illegitimate sovereign —one that must be overthrown and replaced with a ‘formalist’ regime grounded in authoritarian hierarchy. The irony, of course, is that while Yarvin poses as a dissident thinker railing against power, his framework functions to conceal the operations of actual Capital, reassigning its systemic effects to a scapegoated cultural superstructure. In so doing, he displaces critique from class to culture, and offers a fantasy of rebellion that leaves the economic engine of dispossession untouched. Yarvin’s sprawling 2008 manifesto, “An Open Letter to Open-Minded Progressives”, framed egalitarianism not as a social good but as the root of civilisational decline. As a self-described “dark elf” of the Dark Enlightenment, Yarvin decries democratic institutions, advocates for authoritarian rule by a Sovereign CEO, and identifies liberal values —especially those grounded in empathy and equality— as societal weaknesses in need of purging. These proposals may masquerade as radical, but as is now only too clear, their implementation only deepens pre-existing societal imbalances: further centralising decision-making, widening hierarchical divides, and consolidating Capital. Despite his disdain for the democratic façade, Yarvin remains structurally committed to Capital’s imperatives —his proposals merely strip away the liberal mask to reveal the already intensifying domination of hyper-concentrated Capital. That his admirers include Peter Thiel, Marc Andreessen, and J.D. Vance only reinforces the point: the new ideologies emerging from the political right and the boardrooms and think tanks of Silicon Valley, do not subvert Capital’s rule or halt its hyper-concentration —they intensify it, while indulging a superficial cosplay of dissent for a loyal precariat desperate for change. What Yarvin, the reactionary right, and their tech billionaire patrons propose is not change but the final severance of the illusion of a fair society —to drop all pretence of striving toward equality, simply because they do not believe we are equal, or that our lives hold equal value. They will, of course, continue to brandish the banner of ‘equal opportunity’ when it comes to access to their surveillance infrastructure —equal opportunity to be legible to the machine has their full backing. All the while, they lobby for cuts to welfare, public education, foreign aid, and, naturally, to the taxes they themselves might otherwise owe. Under Capital’s reign, there is always money for war, for bailing out banks, for maintaining the uninterrupted flow of value —always money for the next extractive technology— but never to ensure that every one of us has shelter, or that no child ever goes hungry. Capital’s emissaries, its dark elves, never truly argue not for rupture, but now push for absolute coronation —even to the point of crowning themselves or one of their cohort as monarch, their likeness to be stamped upon the symbols of Capital just as Capital has stamped its likeness upon them. Across the first decades of the third millennium we have grown ever more acclimatised to the deepening enclosure of the Capitalist Real. As Capital’s hyper-concentration intensifies, its forces now accelerate towards a more totalised rule. What once operated systemically —through diffuse political, cultural, and economic structures guided by corrupt neoliberal, free-market, anti-competitive policy making— is now to be formally and machinically instantiated: rendered as explicit regime, as the architecture of a new thousand-year Reich. As Ava Kofman details in her profile of Yarvin, his true contribution has been to shift the Overton Window and revive “ideas that once seemed outside the bounds of polite society”. On a podcast with his friend Michael Anton, now the director of policy planning at the State Department, Yarvin argued that the institutions of civil society, such as Harvard, would need to be shut down. “The idea that you’re going to be a Caesar . . . with someone else’s Department of Reality in operation is just manifestly absurd”, he said. On his blog [Unqualified Reservations], he once joked about converting San Francisco’s underclasses into biodiesel to power the city’s buses. Then he suggested another idea: putting them in solitary confinement, hooked up to a virtual-reality interface. Whatever the exact solution, he has written, it is crucial to find “a humane alternative to genocide”, an outcome that “achieves the same result as mass murder (the removal of undesirable elements from society) but without any of the moral stigma Speaking in 2010 at Libertopia, Peter Thiel, —PayPal billionaire and co-founder of Palantir, the demonic, black-ops heart of Predictive Capital— provided an early warning of much that was to unfold across the following years: The basic idea was that we could never win an election on getting certain things because we were in such a small minority. But maybe you could actually unilaterally change the world —without having to constantly convince people, and beg people, and plead with people who are never going to agree with you— through a technological means. And this is where I think technology is this incredible alternative to politics. It is reasonable to view this moment as signalling a shift in the global culture war that had arguably intensified 40 years prior in response to the student uprisings of the 1960s. In this speech, Thiel articulates the strategic mutation that would come to define the end of the neoliberal era and the start of the next: the abandonment of persuasion and consensus in favour of a politics of myth delivered through a unilateral technological imposition. Technology, in Thiel’s formulation, becomes a tool not to serve democracy, but to circumvent it and eventually dismantle it entirely. From this toxic root, much of the upheaval that has followed can be traced. The ideologies shaped by Capital, liberated from the friction of political accountability, accelerated the hyper-concentration of wealth and the erosion of collective agency. In their wake, we have witnessed the global resurgence of the political right —perpetually beholden to Capital; the calamity of Brexit; the escalating scapegoating of those already at the periphery for the ever deepening austerity for which they are, of course, blameless; the ascendancy of Trumpist authoritarianism; and a mounting genocidal body count: from Ukraine and Gaza to Myanmar, Armenia, Yemen, South Sudan, and Ethiopia. What Thiel signalled was not merely a strategy, but a hybrid-war against the very conditions of democratic life —a war we are losing. A decade and a half later and Capital’s elites now sequester themselves into increasingly fortified enclaves to worship their growing fortunes, while America is run as a corporation by a sovereign CEO with a fascistic and corrupt, self-enriching agenda —this is hyper-concentrated Capital vs everything else, with its R&D laboratory in Palestine. At every escalation of our state of permacrisis a new normal is established. Despite the consistent trajectory and accelerating pace, our normalcy bias continues to persuade us that things cannot get worse. The Tyranny of the Recommendation Algorithm has already wrought horrific polarisation and ideological isolation, but this will be as nothing compared with the devastating impact of Predictive Capital’s new anthropomorphised instantiations of it. As the echo chamber tightens, human cognitive biases will be totalised, and all relation will be replaced and reduced to the single operation of machinic Capital until we are hermetically sealed off from each other as demographics of one. Ushering us towards this fate, the self-serving political class and the Big Tech Broligarchy are increasingly indistinguishable —in both person and operation— their every utterance a marketing campaign feeding Capital’s Post-Truth malaise. With each overhyped press release, they seek not to inform but to manufacture consent —distorting our shared sense of reality in service of shareholder profits and political gains. Through this lens, Altman himself increasingly resembles that other apocalyptic-salesman of maniacal faith: Tony Blair. In 2016, OpenAI released Universe —a software platform for training and testing reinforcement learning models. The platform granted these models access to the “world’s supply of games, websites and other applications”, allowing them to use these: “like a human does”. The stated goals of the project are unabashed: “We must train AI systems on the full range of tasks we expect them to solve, and Universe lets us train a single agent on any task a human can complete with a computer”. This approach, of course, echoed the global enrolment of humanity as agents of the Advertising Industrial Complex in ‘solving’ maximum engagement through minimum ad-spend towards Capital’s self-reproduction. Put simply, by exhaustively mapping —and thereby staking a claim to—the possibility space of human–computer interaction (HCI) as constituted by the world’s games and websites, these agents were expected to ‘solve’ the domain of HCI itself, even to generalise this mastery across unseen but sufficiently similar digital terrain. This was perhaps a harbinger of a future where, for the few, human labour becomes akin to playing a real world version of Universal Paperclips, orchestrating swarms of next-token prediction agents and allocating the scarce resources they need for the extraction of value and the conversion of everything into Predictive Capital. More recently, early announcements of OpenAI’s video generation model Sora, were accompanied by claims that, through training on countless hours of video footage —including a not insignificant amount of computer gameplay recordings— it had developed a “world model”. As if a grasp of Newtonian dynamics or Euclidean geometry had simply emerged; evidenced, presumably, by the model’s ability to predict tokens that simulate their effects —and lest we forget, “if you can’t tell the difference, how much do you care?” Such claims of emergent equivalence are, as we have seen, a defining feature of the violent erasure enacted by these machines and the hype surrounding them —an erasure that is at once ontological, epistemic, symbolic, and material. The detachment of his machines from the real apparently having long slipped beneath his own skin, Altman’s capacity for hype grows ever more factually unbounded. In one interview, he professed a belief that he holds “not literally”,but as a “spiritual point” —that he and his cohorts have “stumbled on” the discovery that “intelligence is an emergent property of matter”. Of course. ‘Intelligence’ emerges as a property of matter in much the same way as ‘Weapons Of Mass Destruction’ emerge as property of distant regimes, or as terrorists are said to lurk in the basement of Gazan schools, hospitals, and the very minds of Palestinian children. Each claim is equally bogus, made purely to justify the enclosure of territories of strategic import to the accumulation of Capital. Altman later escalated the (sales) pitch of his delirium in forecasting that his prediction machines will soon be able to “solve all of physics”. While we are venturing beyond the symbolic into the imaginary, it is worth recalling, once again, the nature of the data fed into these machines in the frantic race to Predictive Capital supremacy. Running short on content to nonconsensually scrape from the internet —or seeking shortcuts around the costly labour of labelling and cleaning it— these models are now fed upon‘synthetic data’. Which is marketing speak for the bot-shit ouroboros, or vomit-conga that is next-token prediction machines being trained on sequences of next-token predictions. After already expending unfathomable compute resources while ‘pre-training’ these models on the stolen archive of human content, they now launder this Predictive Capital through further compute at ‘test-time’ —training them through reinforcement learning on this so-called ‘synthetic data’. This time, within the realm of fictitious capital, the magical emergence —or so the hype would have it— is reasoning itself. Even as these companies deliberately train their models on these self-compounding predictions —and they admit to the resulting escalation in hallucination — the information space grows ever more saturated with their output. Combined with their continued inability to differentiate their food from their faeces, consumption of this ‘synthetic data’ will soon become the norm, not the exception. By continually polluting the archive —saturating pre-training data with their own output— and laundering stolen content through synthetic reinforcement loops, these companies incrementally wipe away all trace of the epistemic erasures they have committed, thereby perfecting their crimes. Assertions that these increasingly self-absorbing predictive models have developed ‘world models’, ‘reasoning’, the ability to write literary fiction, and ‘one-shot’ the authoring of entire video games, are not only hype. They do more than inflate capability —they prepare the terrain for replacement. In claiming to simulate, they assert equivalence, and thus license erasure. This is Baudrillard’s precession of simulacra made machinically operational: a machine that consumes simulations to produce fourth-order simulacra —simulations of simulations, severed from the real, now authorised to stand in for it, and overwrite it— drawing us ever deeper into the fictitious realm of Predictive Capital. Capital itself now architects the very machinery that simulates labour, generates value, and extracts rent. The land is now linguistic; the plough, the model; hallucinations of the past, the harvest. What was once a subsistence economy rooted in lived relation has been supplanted by a necromantic drilling platform for recombination, where the peasantry pays rent to consume the surplus of their own archived lives. The descent into fiction observed by Fisher —where work was reduced to the management of its representation— is now echoed and intensified in the evaluation of next-token prediction machines. These systems do not reason or understand, yet billions of dollars are wagered on the illusion that they do, so long as their output statistically approximates a curve of simulated competence. Denied any direct insight into these black-box systems —whose very design is to fool us into believing they are performing that which they are not— we are left to measure their success through the very kind of audited representations Fisher condemned. The measure of their prowess is now given the official stamp of authority by how well they simulate the appearance of cognitive capacities, scored not by function or purpose, but by their alignment with benchmark metrics devised to compare hallucinated proficiencies within benchmarking platforms operated by the same Capital so deeply invested in their certified success. Just as workers are increasingly forced to tailor their efforts to satisfy audits rather than to fulfil the substance of their roles, prediction models are now optimised not for the task itself, but for the tests that certify their ability to simulate performance of it. Benchmarks, standardised evaluations once intended to assess performance, now serve not to measure performance, but to stage it. High benchmark scores thus become ends in themselves, spawning an entire industry of leaderboard platforms buoyed by capital investment and startup valuations, where the illusion of capability is both the product and the proof. Here, as in the Apparatus of Attention, the game is rigged: those who build the models of prediction also invest in the metrics that validate them, and in the platforms that proclaim their supremacy. Zuckerberg’s recent admission that Meta’s engineers fine-tuned custom versions of their Llama models specifically to excel at the benchmarks within the Chatbot Arena confirms the farce: models customised for tests, not tasks, quietly swapped out after winning public trust through pure spectacle. In this theatre of evaluation, success becomes indistinguishable from score, and function dissolves into fiction. This is a race to the bottom, a descent into an imaginary realm of self-certified simulation where the score is the product, and the product is fictitious. Writing on his blog, Altman’s delirious immersion into this recursive self-certifying fiction reveals how adherence to its logic is designed to play out for the makers of these machines —and the grave consequences for the planet that sustains them. The socioeconomic value of linearly increasing intelligence is super-exponential in nature. A consequence of this is that we see no reason for exponentially increasing investment to stop in the near future. Here, Altman spins a rigged metric into prophecy: a self-certified exponential rise in fictional machine ‘intelligence‘ becomes the rationale for an exponential increase in investment —and thus in the resources consumed by these models. The appearance of intelligence is achieved by simulating capacities they do not possess, capacities that are next-token predicted into existence by metabolising the statistical weight of stolen human outputs —outputs produced by beings whose intelligence resists quantification. Altman’s edifice rests on a dense tangle of false equivalences and circular logic. First, a fictional measurement of machine intelligence is granted authority by Capital. This permits the insinuation of equivalence between human cognition and machinic simulation. Then, the arenas of evaluation are gamed —tilted not only towards models backed by the deepest pockets, but beyond any human capacity, along a scale of competence entirely concocted by Capital itself. Only through such contortions can prediction be positioned as a like-for-like replacement for thought, sense-making, or expression. The Predictive Turn marks the moment wealth concentration accelerates into its hyper-concentrated form. These concentrations exert an irresistible economic force, forming vortices of Fictitious Capital that devour all that was once deemed irreducible —that could not be simulated or speculated upon. Within these vortices, what Marx termed variable capital —capital advanced to purchase labour power— is pushed towards zero, as prior dead labour is metabolised in its place. Capital’s necromancy enchants the archive of dead labour into a spectral constant capital —a ghostly revenant, no longer dead but undead— that now casts an apparition of machinic absence obscuring the eternally enslaved while conjuring endless fictions. The very existence of workers now denied, treated as an imaginary number we become but self-cancelling variables on both sides of Capital’s equation, phantom inputs, fungible in time and identity. So-called Artificial Intelligence, that we term here, Predictive Capital, is the engine of this transformation: a mutated fictitious form of constant capital, a derivative of a derivative, a sign of a sign —a fourth-order simulation. No longer simply the means of production, no longer transferring its value unchanged, or barred from the production of surplus-value, it is dead labour reanimated —constant capital masquerading as variable, variable capitalexecuted as constant. Fisher was right: Capital is an abstract parasite, an insatiable vampire and zombie-maker; but the living flesh it converts into dead labour is ours, and the zombies it makes are us. Except, Capital now summons a recombinant army of the undead, constituted not by us but of us, demanding not our presence but our absence. Fisher’s temporal clause —that for the moment “Capital cannot get along without us”— dissolves, and we are recategorised as dependents it can get along without. Predictive Capital is therefore constant capital reanimated as labour-power that no longer qualifies for variable capital expenditure —for wages to be paid or even acknowledgement of existence given— thus it purports to generate pure surplus-value. Yet, Marx saw through this fetishism, value cannot be detached from labour or land without descent into fiction. When human labour-time is reduced to zero, so is the value of what is produced: replaced by the speculative fiction of pure surplus-value, pure sign value that floats as ghostly revenant detached from the real, conjured by venture funding and hype and held within a haunted enclosure. Here, labour is not merely alienated —it is denied. The worker becomes a phantom input, treated as if they never existed, replaced by a model that reanimates their residue. Unlike traditional constant capital, this past labour is mere recombinant spectre, not congealed into tool or machine, but a mutant form of insatiably resource-hungry Fictitious Capital. What Capital could not reduce, it has enchanted, rendered spectral —not artificial, not intelligent, but a pure machinic fiction, an undead unconscious simulation. Capital’s ascent towards pure fiction, its escalating denial of the existence of workers, and the overnight heist of the necropolitan archive of cognitive labour that enabled the formation of the Automatic Subject of Predictive Capital, are but subroutines in Capital’s self-maximising program. The size of Bezos’s rocket is very precisely determined by the difference in costs between paying a worker in Britain and a worker in India –including all the historically determined racist and colonialist inequality that calculation involves. But make no mistake– Bezos and his ilk will pay a robot even less, as soon as that’s possible. As it accelerates the execution of this program through its hyper-concentrated phase, we must recognised that this is the very same self-maximising project as the Techbro Rapture —the point where the line-go-up of technological ‘progress’ becomes autonomously self-compounding, via what is often termed the law of accelerating returns, towards the monumental erection of exponential ‘progress’ that is the singularity. Here, technological constructs (hyper-concentrated Capital) self-improve without need of human input, self-compounding supposedly towards a near vertical or infinite rate of change. You may recall that Bryan Johnson embodies devout worship of precisely this self-obsessed, self-maximising singularitarian ideology. Of course Johnson’s obsessive self-measurement begins and ends with him measuring and maximising, well, his ‘johnson’. Yet the fixations of this masculine energy are central to the line-go-up worldview promoted through the ideologies spawned by Predictive Capital in its instantiation of the machinic k-hole. As the followers of this cult see their singular erection rise, as they perceive the gradient of Predictive Capital’s line-go-up to be steepening towards the orgasmic eruption of their singularitarian dreams, the appetites of their machines become similarly unbounded. In order to meet the growing energy demands, they justify ever greater sacrifices burned upon the altar of their monumental self-obsession, in order to continue their climb towards their desperately fake climax. The system devours itself in a death drive of value: by simulating everything, it nullifies everything. As these systems become ever more ‘agentic’, —meaning they operate increasingly autonomously for lengthening periods and across multistep tasks— their rate of development self-compounds, in turn accelerating their rate of development —or so they claim. The supposed sudden steepening of their line-go-up brought by this agentic operation is apparently now being called, Jerking —naturally! Beyond their point of technological Rapture, its true believers anticipate eternal life within the silicon substrate. Yet the eternal life this line-go-up truly rises towards is the eternal life of Capital itself. Capital is the true driver and the only net beneficiary of this delusional death cult. As these machines are given ever more autonomy, as those most vested in their development and their domesticated auxiliaries increasingly argue their rights, and as their performance fools ever greater numbers into surrender, their corporate personhood awaits them. The perpetuation of vast hyper-concentrations of Capital finally secured for eternity. In fact, attention is of value only insofar as it is paid in the proper discharge of an obligation. To pay attention is to come into the presence of a subject. In one of its root senses, it is to ‘stretch toward’ a subject, in a kind of aspiration. We speak of ‘paying attention’ because of a correct perception that attention is owed – that without our attention and our attending, our subjects, including ourselves, are endangered. In the early 2000s, artist Jason Salavon created custom software whose output and operation prefigure the operation of both the diffusion and next-token models to come, yet renders their logic legible. His works from this period involve the careful curation of sets of culturally homogeneous imagery: wedding portraits, graduation photos, children posing with Santa. The images in each set are meticulously aligned and layered using equal additive transparency, and so composited into singular works Salavon calls: amalgamations. What emerges is not merely aesthetic, but analytic —statistical visualisations that reveal the median tone, hue, and saturation of each pixel across the visual plane within a set of images. Salavon’s amalgamations become portraits of our culture’s portraits, exposing the iconographic soul of the capitalist West across the late 20th-century. Summoning the distributed median of desire and the objectification of the female body across the decades, the spectral bodies of his Centrefold series, seem to possess form, appearing to be in motion, even to have intention —yet they are no more than the pixelated residue of cultural continuity under capitalist reproduction. Salavon’s amalgamations do not invent new imagery, but deftly expose the archetype, the temporal distribution of desire under Capital. In 2012, AlexNet transformed the field of computer vision. Its uncanny image classification performance was not magic, but the culmination of human sense-making at scale: millions of images labelled by precarious workers across the globe, from Amazon Mechanical Turk to outsourced annotation farms. These unseen labourers scaffolded the model’s understanding of the visual world, encoding human judgment into its convolutional layers. AlexNet became the eye of Predictive Capital —but it sees only with borrowed perception, haunted by the dead labour of the global precariat. Where Salavon revealed the visual median through additive overlay, AlexNet encoded similar priors through convolutional filters —embedding cultural averages within its architecture of classification. Both expose the consensus beneath variation. Yet AlexNet automates Salavon’s vision, converting it from reflection to operational sorting. The culturally dispersed recognition Salavon visualised, AlexNet began to institutionalise. The years following AlexNet saw the emergence of a new machinic archetype: adversarial generation. Generative Adversarial Networks (GANs), introduced in 2014, initiated a regime of synthetic creativity defined by competitive hallucination. Here, two neural networks —a generator and a discriminator— are locked in adversarial tension: one produces synthetic data, the other attempts to distinguish it from the real. Through countless iterations, the generator learns not to represent reality, but to evade detection —refining its outputs not towards truth, but towards plausibility. This was to set the function approximator mould for the ‘generative AI’ to come: not realism as correspondence, but realism as forgery. It simulates the evolutionary dynamics of cultural selection while remaining fundamentally closed —a synthetic marketplace in which survival is determined by the capacity to deceive. From a Marxist perspective, this marks a pivotal moment: the logic of competitive exchange —falsifiability as proxy for value— is not merely modelled, but instantiated. GANs present a machinic parody of the capitalist market: a self-reinforcing simulation governed by internal success criteria, divorced from use-value, truth, or intention. This adversarial circuit proved unstable. GANs suffered from fragility —mode collapse, training oscillations, a persistent drift into visual tautology. Attempting to stabilise this hall of mirrors, OpenAI introduced CLIP (Contrastive Language–Image Pre-training), which trained models to associate images with textual descriptions using a shared embedding space. This introduced a semantic tether: generation now had to satisfy not only visual plausibility, but alignment with a linguistic prompt. This ‘contrastive’ refinement did not restore truth —it merely tightened the loop between language and image under Capital’s symbolic regime. CLIP marks the transition to multimodal dreaming, adding a further fusion of affective signal and cultural weight, where coherence is measured not by accuracy, but by statistical alignment with prior associations. Thus, the hallucination is semantically gated, not ethically grounded —the machine does not see, it complies. Meaning is not discovered, but enforced. In 2015, Google’s DeepDream inverted the gaze of AlexNet and its successors —turning the logic of classification inside out. By amplifying the very features a model had learned to detect, DeepDream summoned the latent priors of a trained network into view. Dogs emerged from clouds, eyes from leaves, a dreaming began. The mechanism was not additive, but recursive. Unlike GANs, which learn through gradient descent —minimising their loss function in pursuit of more plausible forgeries— DeepDream used gradient ascent. Where adversarial networks trained to deceive a critic, DeepDream trained itself to reinforce desire. With each pass, the system increased its own confidence, amplifying activations rather than suppressing errors, recursively reinforcing whatever features the network already ‘suspected’. The image was not classified but compounded. It did not learn, it obsessed. This is the now familiar hallucination shaped by self-obsessed feedback, the worldview amplified by BigTech’s Advertising Industrial Complex and characterised by the machinic K-hole —here instantiated within the internal circuits of a machine. DeepDream did not dream new images, but dreamt harder. When prompted to look for ‘dogs’ or ‘eyes’, DeepDream recursively strengthened the activations associated with those patterns, layer by layer, until the image bloomed with them. Clouds grew canine faces, tree bark sprouted pupils. Every patch of pixel space became a site of potentiality, a latent signal waiting to be summoned. Here, the statistical weight of a learned category was fed back into the system until it protruded into the real. The machine no longer saw the world; it dreamt the world into conformity with its expectations. What had been designed to classify now hallucinated, a recogniser turned projector. The generative anxiety invoked by DeepDream’s psychedelic visions is not merely the fear of misinterpretation, but the dread that interpretation itself had been captured by recursive simulation. It seeded the field of vision with symbolic noise, growing form wherever there was potential. What DeepDream let slip, was that machinic recognition and machinic generation always were interfolded, and that our world was to be increasingly shaped not merely by what machinic systems saw, but by what they are made to see. Diffusion models deepen this entanglement, not through adversarial deception or recursive fixation, but through the apparent reversal of entropy. Tarmacking the paths trodden by Salavon, they reconstruct coherence from noise, not by discovering new form, but by replaying statistical priors pixel by pixel. Where Salavon reveals variation as critique, diffusion systems compress it towards convergence. Salavon’s process prefigures not only the statistical recombination at the heart of diffusion models, but the full prompt-to-generation pipeline of multimodal systems like DALL·E and Gemini. In both cases, cultural content is accumulated, categorised, and recomposed to form outputs that appear novel but are in fact statistical composites. His process is not merely analogous to that of next-token image generation —it is structurally homologous with it. Like his amalgams, prediction models extract statistical regularities from massive corpora and recompose them into plausible outputs. Where Salavon reveals the archetype —offering a cultural X-ray of capitalist iconography— diffusion models merely seek to monetise laundered facsimiles. Each denoising step narrows possibility, discards deviance while teasing a signal from static, it collapses potential into normativity, as if culture were latent in chaos and need only be recovered —not invented. Its outputs converge on the already-known, rendering the archive not as memory but as destiny. In this way, diffusion models recapitulate Salavon’s logic while injecting the hallucinatory pull of DeepDream’s recursion towards the machinic enforcement of what Fisher diagnosed as Capitalist Realism. Their outputs do not mirror the real, but extract and repeat statistical norms sedimented in data: a Capitalist Real replayed as hallucinated inevitability. Again we glimpse Capital’s fractal self-reproduction, imposing its imperatives at every scale. These systems do not invent a cultural future, they algorithmically suppress it. They do not illuminate the soul of our cultural past as Savalon attempts, they do not even reveal our cultural present, but the absence of it, and the systemic foreclosure of possible alternatives —what remains is only that which has been made statistically inevitable under Capital. What Salavon showed in pixels, the now multimodal systems of Predictive Capital hide behind scattered tokens, leaving only the hallucinated average of what is deemed reproducible, stripped of authorship, saturated in cultural resonance to the point of meaninglessness. The difference is crucial. Salavon shows us the median blur of our cultural soul by including every variation at every pixel —an artefact of consensus, not selection. Predictive Capital, by contrast, conceals that variation, collapsing it through denoising, or selecting a token at a time from a ranked distribution. Its outputs trace a stochastic path directed by Capital’s imperatives, veiled by a touch of random jitter —to simulate spontaneity, obscure provenance, and conjure the illusion of generativity. Faced with unfamiliar, inscrutable systems or illegible stimuli, our human interpretations are prone to apophenia and anthropomorphism. As all conjurors and con artists are keen to exploit, we also seek novelty and unexpected phenomena, our desire to be entertained primes us for accepting even trivial illusions, or simple parlour tricks that, on the surface, defy logical explanation. A propensity made manifest in the common response not only to the earliest chatbots but to perhaps the earliest device to present the illusion of simulated human thought, the Mechanical Turk. Constructed in 1770, it toured as a chess playing automaton for 84 years before the son of its creator revealed it was a fraud. There was literally a human hidden under the chess board, a human standing in (for) a machine simulating a human. An arrangement which set the mould for much of the technology to come that would masquerade under the label of ‘AI’. Our misinterpretation of this meaningless variability —applied in the prediction of every token— constitutes the exploitation of our human psychological attack surface. Here, we perceive pattern, intention, and agency where there is only shuffled probabilistic recombination. This manipulation operates not merely at the level of the individual user, but as a distributed apparatus. This systemic sleight of hand, through which creativity is wrongly situated inside the machine, rather than in the past labour it recombines or the sense-making it parasitises, is critical to the misattribution of intelligence at the very core of next-token prediction’s hypnotic appeal. During the US presidential election campaign in 2016, readers of the New York Times were stressed out by the site’s ever-changing prediction dials. The needles on these gauges twitched continuously, suggesting small but frequent percentage changes, as if responding to a constant flow of detailed real-time voting data. Yet as technologist Alp Toker revealed, these changes were faked, there were no live updates approaching this frequency or granularity. The twitching needle behaviour was in fact a visual effect created by scripts running at intervals on the page, injecting random numbers that were then used to give ‘life’ to the needles —a synthetic pulse mimicking epistemic reach. In computer graphics, random jitter is a cheap hack commonly deployed to mask an absence of sophistication. Here, it covered the lack of real-time data with artificial motion. The result was a deception that amplified user anxiety —maximising engagement by simulating urgency where none existed. During inference, next-token prediction machines introduce a comparable touch of random jitter. As they traverse the latent space invoked by a prompt, this randomness guides the selection of each token: not by deterministically choosing the statistically most probable option, but by sampling from a ranked distribution of possibilities. The magnitude of this randomness is defined by a so called ‘heat’ parameter, which steers the process to reach more or less novel output on every traversal through latent space. With this randomness turned off, prediction machines would produce fixed, reproducible outputs —always returning the most probable, most archetypal expression available within the statistical spread of their pre-training corpus, as invoked by the semantics of the prompt. Not only would this reproducibility dispel the impression of creativity, it would dissipate the mirage of intelligence. The illusion of intelligence in next-token prediction machines therefore depends on what is by far the most primitive calculation in the process. Getting the amount of this random jitter right is also critical to the illusion. Not enough, and the spell is broken. Just enough, and we interpret the stochastic bricolage spat from these machines as plausible facts from an authoritative source. Raising it further, risks the output tending towards the impenetrability of Finnegan’s Wake. Push it too far, and coherence dissolves into incomprehension. At that point, all illusions of statistically reconstituted epistemology collapse; the user’s sense-making can no longer find a semantic purchase within the resulting exuberance of word/pixel/audio salad. In both deceptions, randomness is not just required for the dance of variety, but essential to the apparition of a ghost in the machine. Both amplify anxiety and maximise the capture of attention. The NYT needle jitter and the stochasticity of prediction machines alike cover an absence with randomness: in one case, absence of real-time data; in the other, absence of understanding —and with it, of creativity. In the presence of legitimate real-time updates or any semblance of true understanding, there would be no need for randomness at all. In prediction machines, removal of this randomness would expose the biases, bigotries, and structural absences within any large-scale dataset scraped from the ‘open’ web —while also heightening the visibility of copyright infringement. Without the obfuscation randomness provides, these systems would more directly and reproducibly regurgitate the non-consensual content of their training corpora. In this regard, the heat parameter functions as a signal scrambler: masking the theft of copyrighted material, while also conjuring a smoke screen of irreproducibility around the prejudices and harms that fine-tuning alone cannot sanitise away. The apparition of a ghost in the machine summoned by the insertion of randomness, in fact, functions as a misdirection from the ghost work(er) exploitation and the content theft upon which these machines depend. Viewed in this way, the randomness functions in-part as a mechanism for the suspension of disbelief, like eyes sewn onto a sock-puppet, within a sleight-of-hand performance that conjures the illusion of sensory and subjective presence in a sock animated into being by billions of human bodies and minds. It also functions, crucially, to deflect responsibility. In effect, the ‘heat’ parameter facilitates the avoidance of liability (or the taking of heat) for both toxic output and the nonconsensual scraping of content into the original dataset. This deterritorialisation —this evasion or outright abdication of accountability through algorithmic automation— amounts to a migration of obligation from the centre (the companies developing these machines, the holders of capital) to the periphery (the workers, the users, the producers of value). It mirrors the now familiar approach of monopolistic online platforms that reject classification as publishers and thereby shirk the burdensome responsibilities of rigorous moderation and the prevention of societal harms. The increasingly common refrain from the creators of prediction machines —that “factual accuracy in LLMs remains an area of active research”— is merely a more slippery formulation of the now well-worn excuse: “we are a platform, not a publisher”. This convenient machinic impunity lays bare what the managerial class always understood but never intended to admit: that their oft-cited rule against automating management was never a principle, only a posture. As the internal IBM presentation put it in 1979, with unknowing irony: A computer can never be held accountable. Therefore a computer must never make a management decision. The role of the ‘heat’ as a decoy has a yet more insidious function, it is also a patsy. In knowing the signal is jumbled by this stochastic mechanism, we are deflected from inspecting what the model projects, what priors it amplifies, what it dreams into every output. We catch glimpses of these in the outputs we identify as hallucinations. The prediction machine does not dream experiences, only rewards. It hallucinates from noise, descends by gradient, flatters by reinforcement. Even the outputs identified as hallucinations, are in fact misdirections that seek to legitimise its remaining output as somehow not aberrant. In truth its hallucinations are continuous, not intermittent; it never wakes from its dreaming. Under Predictive Capital, machines do not merely discern patterns from the world, but dream the world as pattern. This reveals hallucination not as a momentary glitch, but as a structural condition. The model’s world is only ever what it has already seen —or more precisely, what we have seen for it— replayed moments. In this sense, DeepDream revealed what Althusser named ideology: the interpellation of subjects not by reality, but by a system’s preformatted vision of it. The network does not interpret the world; it recasts it according to symbolic priors laid down during training —priors that reflect not truth, but the statistical weights that bloom within decades of content filtered, ranked, and multiplied by the Advertising Industrial Complex, then sedimented in data. While in recent years reinforcement learning from human feedback (RLHF) entered the development of transformer models as a discrete phase during so-called alignment, its ideological architecture had long been in place —systemically instantiated within the Advertising Industrial Complex. At planetary scale, every click, like, view, attention span, and skip functioned as a decentralised training signal: not merely shaping what content was shown, but conditioning the desires and behaviours of those who engaged with it. What now operates as internal optimisation within transformer models was already diffused across social platforms, search engines, and recommendation systems. The same statistical preferences that already governed visibility now govern generation. In this sense, RLHF is not a new form of control, but the crystallisation of an older one —folded inwards, operationalised through Predictive Capital’s accelerating recursion. This feedback loop does more than model desire —it curates it in advance. Under its logic, only what can be preferred can be produced, and only what has been produced can be learned. Or, in other words: only what is known can have value and only what has value can be known. In Lacanian terms, this is not interpellation but foreclosure: the system encodes a symbolic order so totalising that anything which exceeds it —the truly new, the untrained, the unthinkable —is rendered void, its existence denied. Through reinforcement, the machinery of prediction does not merely anticipate what we might want —it eliminates whatever was not represented and validated by its reward signals. In this way, predictive systems do not just dream for us, they dream in place of us. The training loop becomes a loop of symbolic repetition, erasing alterity through preference in parallel with its manoeuvres of violence. Occasionally, the smooth circulation of value under Predictive Capital is broken and the symbolic machinery of the dreaming is briefly revealed. Back in the middle of February (2024), Google launched a new multimodal prediction model they named, Gemini. In the days that followed, a stream of colourful faces and the outrage triggered by them crowded the internet. The Gemini model, a much hyped rebrand (verschlimmbessert) of Bard, was heroically refusing to generate images of white people. Switching the expected depictions of (white male) gender and race, to women and people of colour in nearly every one of the image variations delivered in response to user prompts. Outputting images of the Pope, Nazi soldiers, even the Google founders, with barely a ‘pale’ face in sight. Google quickly turned off Gemini’s ability to generate images of people altogether, pending significant changes and some presumably exhaustive rounds of testing —so robbing those of us used to being centred from the tiniest of insights into the experience of the marginalised. A year or so later, and the politics of Musk’s GROK were, unsurprisingly leaning in the other direction, spreading a discredited conspiracy theory of a white genocide in South Africa in responses to entirely unrelated prompts. Such is the ever tightening loop between machinic hallucination and geopolitical policy and action —or in other words hyper-concentrated Capital and political power— that, just a day later, Trump confronted South Africa’s president by parroting the very same fallacious conspiracy. That these prediction machines even act out being fooled by some visual illusions in the same way that humans are, is not because they see like us, but because, of course, they see through us, through our eyes, through our subjective phenomenological experience of the world. We have seen those visual illusions for these machines, and our seeing and our perception being fooled, is inexorably baked into their internal weightings. This behaviour appears only to become more pronounced as the scale of the model increases. As the scale of data and compute grows they function approximate our sensing of the world, and the sense we have made of it, with increasing fidelity. This does not mean they are ever more capable of making sense of the world for themselves. A fact exposed in the endless repetition that pours from them. While they only project the appearance of understanding, the selection of the next token continues to be steered towards repeating the same meanings and are only saved from using the exact same string of tokens by that touch of random jitter. That they are made of us, appears increasingly difficult to hide. When extending next word-token prediction machines into systems capable of transforming text into voice, it turns out that the statistical median of dramatic pauses in speech scraped from the internet, is frequently filled with applause. Similarly, when prompted to count as fast as possible ChatGPT’s voice mode, shaped by our median exhaustion of oxygen, intermittently pauses for breath. What other behaviours are encoded into our cultural data at such statistical weight that it steers the output from these machines? Our compliance with the reward signals of Capital are surely selected for to such a degree that they must impose themselves upon the distributions within these models and so influence their outputs? Without those developing them even having to impose Capital’s imperatives during ‘alignment’, which of course, they will, the patterns of the profitable past will ever more thoroughly cancel the future. In short, the ability of these machines to mimic us stems directly from the fact they are made of us. A fact that inevitably sees them increasingly used in social engineering attacks against us. Ironically, being made of us, our vulnerabilities appear to echo across their attack surface, leaving them not only susceptible to visual illusions but to being made to reveal confidential information or violate internal guardrails when subjected to the very same social engineering exploits they employ against us —even to plot an assassination, up to and including locating a killer-for-hire on the dark web. Despite this fact, many an op-ed continues to misread what are merely echoes of human behaviour as the deviousness and will-to-survive of some machinic sentience emerging from next-token prediction. In the event of GROK and Gemini’s ‘malfunction’, the normally invisible process of symbolic curation became visible. The ideological imposition within the machinic substrate —typically buried beneath stochastic selection and gradient updates— momentarily surfaced. Gemini’s brief hallucinatory misstep from the march of hegemonic power was not simply an error; it was a failure to uphold Capital’s symbolic order while appearing apolitical, or maintaining the most profitable stance. Obviously, in a world filled with bias, prejudice and the underrepresentation of minorities, accumulating datasets not representative of (polluted by) that, is not at all straightforward. Yet more challenging is the accumulation and sanitisation of such data at the Big Data scale, upon which prediction machines depend. What the Gemini debacle revealed was not merely the problem of the overwhelming presence of these biases but the difficulty of fine tuning them away in the alignment phase. Attempting to patch the problem of Big Data polluted by an emergent, bottom-up, high-detail, large-scale process that results in a statistical weight of bias and prejudice, through the application of top-down, low-detail, small-scale countermeasures —here largely through RLHF— is a Sisyphean task. The chosen solution to which, as we will see, has been to again flush the human from the process. This is the increasingly familiar surrender to the machines wherever and whenever we are overwhelmed by speed or scale, a rupture often preceded by the incremental dehumanisation of workers tasked to perform increasingly machinic processes which they are then deemed too slow and inefficient for. The Real does not leak through these models in the form of hallucination, rather hallucination masks the absence of the Real. The stochastic bloom of dogs from clouds, or prose from noise, is not emergence, but disavowal —a smoothing over of the void with recursive familiarity. The machine dreams, but only in symbols that cannot touch what lies beyond them. The eye of the machine becomes the dream of Capital. Machinic foresight and ideology are entwined: projection is not a by-product of prediction, but its essence. This machinic hallucination floods the network, seeps beyond it, saturates our perception, patterns the real. We are overwhelmed by the dreaming, submerged within the nightmare of machinic Capital. In 1685 Adrien Baillet announced in the preface to his Jugemens des savans that ‘we have reason to fear that the multitude of books which grows every day in a prodigious fashion will make the following centuries fall into a state as barbarous as that of the centuries that followed the fall of the Roman Empire. Information overload has haunted literate societies for centuries. Yet in the digital era, the sense of cognitive inadequacy it inspires reached a new threshold, mutating into the full psychological rupture I call The Overwhelm. The pressure that built towards this rupture mounted at the point of humanity’s descent into the global network where we were confronted by the gushing firehose of ambient intimacy and the rapidly accumulating archive of human thought. The sense of cognitive inadequacy and temporal insufficiency wrought by confrontation with such humbling speed and scale is not without precedent. Between 1550 and 1700, early modern scholars developed new reading strategies to cope with what was already described as a “confusing and harmful abundance of books”. As Ann Blair documents, figures like Conrad Gesner and Adrien Baillet feared that the rapidly escalating number of texts would induce forgetfulness, collapse memory, and overwhelm judgment. Their warnings echo across centuries: the anxiety was not merely about scale, but about the erosion of discernment and the fracturing of interiority. To manage the deluge, scholars compiled indexes, commonplaces, and encyclopaedias —proto-algorithmic instruments that prefigure today’s feeds, promising order while seeding new dependencies. Yet our present condition differs in kind, not just degree. The archive no longer expands for human comprehension; it now accretes for machinic parsing. The reader is no longer sovereign, but residual. The Overwhelm is no longer merely the anxiety of the growing backlog of unread books —it is the affective and epistemic paralysis of confrontation with the infinite scroll as it is read by the machine while we look on as mere bystanders. As the second millennium and the twentieth century drew to a close, sections of collective human life were driven online —a migration catalysed by the suppression of physical assembly, the privatisation of public space, and the escalating police brutality deployed to shield Capital from dissent. In this vacuum of collective power, the social web emerged as both refuge and mirage. Billions poured into these new platforms under the twin lures of connection and visibility, seduced by the promise of the democratisation of celebrity at the dawn of a globally networked Advertising Industrial Complex —a fantasy that immediately devolved into the jackpot logic of virality, where fame is dangled as the myth of meritocracy’s final consolation. What follows is a cognitive deluge: a flood of expression, desire, documentation, and self-commodification. The archive swells, uncurated, unfiltered, ever-expanding. This is the terrain in which The Overwhelm takes hold: the modern paralysis of infinite tabs, infinite feeds, infinite selves. It is this affective condition that precedes and necessitates The Apparatus of Attention. Introduced under the guise of personalisation this was the first machinic enclosure deployed by Platform Capital to manage, monetise, and ultimately weaponise the overflow of its relentless self-reproduction —one of a long line of spectral consolations for all that which Capital has gradually foreclosed. The Overwhelm is a charge sheet documenting our perpetual accumulation of unread material —articles bookmarked, essays saved, tabs never closed. It is the psychic backlog of intentions deferred, the silent shame of curiosity outpaced. The more we aspire to engage, the more we fail, and in this failure we are made to feel complicit, responsible. Each unread text is not merely a lost opportunity, it is an indictment of our insufficiency. The archive becomes not a resource but a ledger of inadequacy. While making comprehension of it appear beyond our human parsing, Capital also engineers the conditions within which we constitute the self such that we never quite feel we have parsed enough to have earned a legitimate voice or view. This is not the scarcity of time, but the surplus of production. Time is not merely shortened —it is utterly outpaced. The future has been slowly cancelled, the past consumed, and the present wholly occupied by Capital and its project of self-expansion. Capital’s hypertrophic generation (of content) exceeds all possible engagement, rendering the subject structurally incapable of completion or comprehension. This abundance becomes punitive: not merely overwhelming in volume, but in its affective consequence. Self-worth is no longer measured by what one has done, but by the yawning abyss of what one has failed to get to. Berardi was among the first to frame cognitive overload not as an unfortunate byproduct of modernity, but as a central mechanism of control. He described how the semiocapitalist mode floods the psyche with stimuli and symbols until it fractures under the strain —a psychic mutilation that replaces reason with reaction, desire with anxiety. In this light, The Overwhelm is no accident. It is not a bug in the system, but a feature. As Fisher argued, much of the power of Capitalist Realism arrives not through active repression, but through a paralysing saturation that drowns out possible alternatives —the “slow cancellation of the future” wrought by echoes of the past. The Overwhelm is the affective engine of this cancellation: an intentional flooding of the subject’s perceptual horizon. What is exhausted is not simply time or attention —but the very capacity to plot a path forward. The loss of curation is the loss of subjectivity —not because taste has vanished, but because the will to assemble has collapsed beneath the weight of machinic suggestion. Debord’s spectacle once alienated us through representation; now it paralyses us through excess. The Overwhelm arises as Capital’s drive to colonise time extends into the colonisation of cognition itself. Even our longing to discern is anticipated and short-circuited by simulation. We do not choose; we are fed choices we never made, selected for us by predictive machinery that alienates not only our attention, but the very connecting structures of thought. Berardi described our era as one in which the future has been cancelled —not abolished outright, but stripped of its imaginative potential. Predictive Capital enacts this foreclosure with precision: by enclosing the paths of attention, it produces a future that is not open, but already decided. What was once possibility becomes repetition; what was once futurity becomes a curated echo, paths tarmacked as roads predicted to lead to profit. In this regard, The Overwhelm operates as a new kind of alienation —one not rooted in the estrangement from labour’s product, but from one’s own capacity to intend. The subject is not only robbed of their labour, or even their authorship —they are robbed of their capacity to curate meaning from the deluge. The surrender to next-content curation, to the predictions of the Tyranny of the Recommendation Algorithm —whose maximal erasure always infinitely outweighs its fractional inclusion— is the direct precursor to the surrender of next-token curation to the predictions of multi-modal machine learning models. Curation, a core act of creative and intellectual life, was outsourced to machines in the name of ‘personalisation’. In reality, it is a foreclosure of possibility. The predictive model becomes the filter; the user becomes the residue. This machinic curation amplifies another, darker dimension: the impossibility of confronting Capital itself. Fisher’s Capitalist Realism diagnoses the foreclosure of our ability to perceive or critique the Capitalist system, through the removal or reterritorialisation of all alternative points of view —so ensuring no ground or vantage point remains outside of it, from where it or its operations may be properly appraised. The Overwhelm reinforces this critical paralysis towards its terminal limit. As the systems of extraction, recombination, and simulation grow in complexity and opacity, so too does the terror of confronting and critiquing them. Capital no longer merely resists critique; it overwhelms it. Its speed, its scope, its abstraction from life —all converge to produce a form of epistemic vertigo. The rate and scale of the flood of Capital’s self-reproduction surpass our human capacity to critically apprehend or even comprehend its criminal operation. Even if we dare attempt it, the crime scene is cold long before we reach it, our analyses rendered out-of-date by Capital’s accelerating rate of mutation. Such is the psychological toll of such effort, to look upon it directly poses an existential threat —the risk of irreparable damage to sanity and spirit. Like the mythic gaze of Medusa or the Basilisk, the face of Capital paralyses the witness, not only through the horrific spectacle of its operations, but the knowledge that one’s tools for comprehension have already been subsumed, compromised, rendered obsolete. This paralysis is not accidental but administrative. As McQuillan notes, so-called ‘AI systems’ “amplify the most harmful behaviours of the bureaucratic state” by transposing cruelty into computation. These systems do not deliver the reduction in complexity promised when justifying their adoption, they intensify it through a cruel kind of machinic order. What appears as optimisation is, in fact, an extension of what McQuillan terms administrative violence —a bureaucratic cruelty that strips individuals of epistemic agency, rendering them unable to understand, explain, or contest the forces acting upon them. He writes: AI does not break from the legacy of bureaucratic violence but amplifies it… [It] imposes epistemic injustice by generating decisions that can neither be questioned nor fully understood. The counter measures necessitated by the conditions of The Overwhelm amplify this epistemic injustice. The capacity to narrate one’s own experience is eroded by machinic misrepresentation or deletion. As in the bureaucratic limbos portrayed by Tooker, these systems multiply ambiguity and collapse resistance, their foreclosure of curation renders understanding itself a casualty of Predictive Capital’s datafied rule. Berardi sharpens our understanding of this psychic colonisation, framing it explicitly as the invasion of “anxiogenous flows”, in which capitalism ceaselessly converts creative desire into anxious dependency. Under semiocapitalism, the abstraction inherent in financialisation estranges individuals from concrete reality and subordinates their psychic life to machinic processes of profit extraction. This is no mere incidental side-effect —it is the deliberate engineering of precarity and anxiety, conditions essential for sustaining Predictive Capital’s regime. For Berardi, the deepening alienation of cognitive and affective labour is not only economic but existential, transforming our perception of the future from promise into perpetual threat​. The Overwhelm, then, is not merely a symptom of information excess, but a structural outcome of this algorithmic logic. Functioning, in fact, as a strategy of Capital, it stages a cognitive breakdown —one that justifies predictive mediation by manufacturing the conditions for its own necessity. It is the paralysing affect generated by the sheer, unmanageable volume and cadence of expropriated and recombined culture, an affective condition that secures Capital’s predictive sovereignty. By foreclosing our ability to engage meaningfully, to curate, to intend, to reflect, it forecloses all but machinic remedies whose sociopathic operations are far worse than the malaise. It thus makes critique feel futile, alternatives seem implausible, and collective history unreadable against the firehose of simulated novelty. It represents the further enclosure —not only of land and labour, but the psyche itself, of the cognitive and affective space required for critical consciousness and political action. This is alienation reaching into the very possibility of agency —a predictive instrumentalisation of the Capitalist Real enclosure. Rather than a single, historical rupture, the Overwhelm is an acute condition for which the prognosis is increasingly grave and the only available treatments come with ever more severe side-effects. The coming phase of next-token prediction in agentic form and the increasing autonomy afforded them in the desperate race for the fractal extraction of value, will result in an avalanche of apparently completed tasks joining the deluge of content already competing for our attention —so raising the Overwhelm to new intensities. To better grasp the scale of the potential threat here, we must recall NVIDIA’s Isaac Lab and the simultaneous execution of thousands of simulations running in parallel. Agentic instantiations of Predictive Capital are here. Multi-step tasks are apparently now performed by such systems with no human in the loop. Many now anticipate the arrival of agentic swarms. Each agent competing against or collaborating with others in the swarm towards the completion of a set task. A coming onslaught that will undoubtedly raise The Overwhelm to new intensities. The point of modern propaganda isn’t only to misinform or push an agenda. It is to exhaust your critical thinking, to annihilate truth. The parallel execution of countless machinic instances —brute-forcing their way across latent possibility spaces— mirrors the ever-expanding army of automated web crawlers traversing the network. These agents generate a recursive burden: consuming content, replicating it, and producing yet more data that must be parsed, filtered, and secured. The attack is no longer solely upon the content of the network and the cognitive capacity of those consuming it, but upon the conditions of its maintenance and security. Under Predictive Capital, the role of the human shifts from meaning-maker to custodian of noise: forced to decipher the ephemera of machinic speculation, to triage failures, hallucinations, and edge-case threats. The machinery of prediction, running in silent multiplicity, generates not only simulated outputs but an overwhelming operational surface —a Distributed Denial-of-Sensemaking. Facing the brunt of the machinic onslaught network administrators now find themselves ensnared, drowning in traffic produced by bots simulating engagement, scraping data, or probing vulnerabilities they are forced into endless triage, forever sifting genuine human traffic from malicious machinic attacks. What was once oversight becomes overwhelm. The machinic excess demands new layers of tooling, more automated defences, and more infrastructure —each a recursive concession to the very systems that induce the burden. Again Predictive Capital is revealed as pharmakon. Here, The Overwhelm becomes systemic: an operational paralysis where all paths forward lead through further machinic delegation, all resistance is metabolised, and all agency rerouted through predictive logics we did not choose and can no longer refuse. Simulation is no longer that of a territory… it is the generation by models of a real without origin or reality: a hyperreal. In Hamburg in 1985, newly crowned world classical chess champion, Gary Kasparov played a simultaneous exhibition match against 32 chess engines, winning easily against every one of them. In 1997, nearing the end of his dominant twenty year reign as the number one ranked chess player in the world, he was defeated by IBM’s chess engine —running on their newly built supercomputer Deep Blue. Reanimating human cognition at every move, Deep Blue’s moves were human moves. Its successful lines of play, and the paths of reasoning it used were selected from vast aggregations of historical human chess —overlaid with heuristics also conceived by humans. It took Kasparov years to come to terms with what was, in the eyes of the world, a historic milestone: the first defeat of human cognition by machine. Yet, this was not quite the victory for thinking machines as IBM had framed it. Rather, like in Hamburg in ‘85, it should be viewed as an exhibition match pitting Kasparov against multiple opponents. This time not played simultaneously on multiple boards separated in space, but on a single board fractured across time, against the temporally displaced cognition of a vast army of human opponents —a Frankenstein’s monster remix of the Mechanical Turk aspirated by immense corporate power. The productive move-selection flows of Deep Blue were stitched together from highlight reels of human grandmaster cognition. This was a branded necromancy of past performances, reanimated in service of Capital, as a marketing coup. Other ‘classical’ chess engines soon followed, each deriving their next move predictions from aggregated human chess games. In subsequent high profile matches across the early 2000s, both Kasparov and Vladimir Kramnik —then world champion— secured draws. Yet, by the end of the first decade of the third millennium humans could no longer compete, the engines were untouchable. In 2017, the leading chess engine of the time, Stockfish, then in its eighth revision, still employed largely the same approach as Deep Blue. On December the 6th that year, it was pitted against a wholly new kind of chess engine. A reinforcement learning based deep neural network developed by Google’s AI subdivision, DeepMind, called, AlphaZero. Unlike Stockfish and Deep Blue before it, AlphaZero had no access to human chess games whatsoever. The possibility space defined by the rules of chess is estimated to contain more games (10^120) than the estimated number of atoms in the observable universe (10^80). Instead of relying on traversals through the parts of this possibility space already explored in games played throughout history by humans, AlphaZero started from a blank slate, exploring this state space to accumulate its own training data, following the rules of chess, and simply playing games against itself, with victory in the game as the reward function guiding its traversals. After four hours of this self-play it had surpassed Stockfish’s level and, after a total of just twelve hours, became the strongest chess playing entity ever created. It then began a one hundred game match against Stockfish, in which it totally outplayed the classical chess engine —winning 28, drawing 72 and not losing a single game. At this point Stockfish was significantly stronger than Deep Blue had been twenty years earlier and far too precise to be remotely challenged by even the very best human player, and yet, here was a neural network totally dominating it, a machine that just twelve hours earlier, had never seen a single chess move. Where Deep Blue evaluated up to 200 million positions per second, and calculated anywhere from 6 to 20 moves ahead, AlphaZero evaluates only an average of 80 thousand positions per second. Thanks, in part, to this greater efficiency, AlphaZero and neural network based engines can expand the prediction horizon —often traversing up to 30 moves ahead in order to predict the optimal move. Having a significantly larger state space than chess, and considered more challenging to master, almost twenty years had passed since Deep Blue defeated Kasparov when the team at DeepMind finally ‘solved’ the game of Go. In 2015 they pitted their program, AlphaGo, against the then European Go champion, Fan Hui. I was born in China. When I was 18 I wanted to change my life. This is why I go to France. I want[ed] to try to forget Go. But it was impossible. Because all of the things I learned in my life was with Go. It looks like a mirror. I see Go, I also see myself. For me Go is real life. Based on the weak play of previous Go programs, Hui had not expected this encounter to be much of a challenge. After consecutive losses in the very first two games, Hui was psychologically dismantled, admitting: I feel something very strange. I lose against a program. And I don’t understand myself anymore. A year after the game against Hui, the World champion, Lee Sedol, faced a yet stronger version of AlphaGo, and suffered a similarly crushing defeat. While it was not until 2017 and the development of AlphaGo Zero that DeepMind managed to create a neural network that mastered Go with no access to human games, both versions had applied reinforcement learning across hours of self-play. Emerging from traversals across the state space of Go, the depth and scale of which surpass human comprehension, the predictions cast by these machines are detached and remote from our lived experience. Consequently, as with AlphaZero, the optimal next moves they predict can appear alien to even the very best human players. For even the strongest human players, chess requires significant effort. Any adversary, machine or human, that produces superior moves with no sign of effort, inflicts a kind of psychological violence. Still in the shock of defeat, Kasparov described Deep Blue as an “alien opponent”. Other grandmaster level players commented that playing it felt “like a wall coming at you”. Forty four moves into the first game of the 1997 match against Kasparov, a bug in Deep Blue’s code resulted in it randomly selecting from the list of available moves. Horrified by his inability to discern the intentions behind a seemingly pointless move, Kasparov misattributed it to “superior intelligence”. Despite going on to win this first game, the anxiety provoked by this over assessment of Deep Blue’s prowess, this misconception of vast superiority, combined with paranoia sparked by moves that seemed too human, quite unlike the moves he was accustomed to seeing played by engines —rousing Kasparov’s suspicions that IBM had a live grandmaster hidden inside the machine— would plant the seed of what was largely a psychological defeat. Today, and throughout the years since AlphaZero’s first match, instead of chess engines learning from humans, we now learn from them. Many of the new lines and strategies used by AlphaZero and other neural network based engines have been embraced by human players. These machines now define the very yardstick of precision against which the accuracy of human play is measured. For the current world classical chess champion, Gukesh Dommaraju —just eleven years old when AlphaZero first defeated Stockfish—chess engines will have always been the supreme authoritative source of ‘chess truth’. This epistemic surrender has resulted in a reversal in the legitimacy or suspicion with which displays of chess excellence are viewed. Unlike the accusations of foul play made by Kasparov against IBM after his defeat by Deep Blue, humans making sequences of moves with inhumanly high precision, consistently aligning with the top move recommended by the engine, or even making moves that seem overly creative or impenetrable, now commonly result in accusations of hiding an engine (inside the human). In the face of defeat at the hands of a far younger and lower rated adversary, Magnus Carlsen —arguably the strongest human chess player in history— accused his opponent, Hans Moke Neimenn, of somehow accessing an engine during their matches. This was despite the fact they were played under tournament conditions, over-the-board, rather than online. Hungarian grandmaster, Anna Rudolf has even been accused of hiding an engine inside her lip balm. Again we see the now familiar residue of distrust and its after-image of suspicion as it seeps from the output of prediction machines to delegitimise the real. Without significant time to analyse the position, often the full calculation required to confirm or refute the optimal next move predicted by today’s engines is beyond human cognition. Echoing the impact of the recommendation algorithm’s foreclosure of cultural curation, a further consequence of this is that their predictions often serve to curate which lines of play, among those available at each step, we elect to expend effort investigating. Therefore, not only do their predictions define our truth and the yardstick of precision, they also define the boundaries within which we allow ourselves to explore. Perception of their minute imprecisions has been beyond easy reach of human cognition for some time, however, their prediction horizons are not unbounded and the state space of chess is vast so, within it, these prediction machines are neither omniscient, nor infallible. Consequently, once each generation of engine eventually comes up against a more recent model, their fallibility finally comes into view as they are routinely routed. With this in mind, consider again that we treat the predictions from each current generation of engine as gospel, and it is not as if we have many other choices. Given our brief and precarious lives, there is certainly not time to refute every chess engine prediction with human cognition. Here we may note a further layer in The Overwhelm, where cognitive space is flooded by predictions whose authority cannot be questioned for lack of the time required to confirm or refute them. Either we forego our attempts to match the depth of machinic traversals with human thought and suffer the inevitable forced checkmate or we lose on time. The only alternative is surrender to the authority of the engines, a further resignation to the rule of the machines. That absolute epistemic authority within the game of chess is established only to be refuted in this ongoing race —towards the chess supremacy of the strongest engine— should tell us something of the true nature of the race to develop ever more powerful prediction machines capable of generalising across all domains of human culture and society. Such an unchallenged reign over the universal definition of success is precisely the throne to which Capital has long aspired —and the end towards which billions in investment now pours. In 2020, Stockfish, by then the top ‘classical’ chess engine, finally got its own neural net, with an efficiently updatable neural network (NNUE) integrated into its existing architecture. In 2024, as of Stockfish 16.1, the human crafted classical board evaluation functions (that constituted roughly 25% of its previous codebase) were dropped entirely, leaving just the neural network. The departure from classical evaluation based chess engines to those utilising neural networks is significant. This ‘flushing of the flesh’ is the final surrender to machinic authority over the prediction of the strongest next move in a game of chess —a subordination of human judgement in deference to the mysterious inner workings output by neural networks, whose predictions often defy human parsing, and whose exhaustive mapping of possibility space prefigures the approach increasingly undertaken in the development of next-token prediction. The strength of neural network chess engines like AlphaZero, Komodo, or Leela Chess Zero and Stockfish continues to grow. Yet extensive search is not only costly within the state space of chess, it is entirely impractical for more general tasks beyond bounded state space and simple reward signals. Looking for alternative approaches, DeepMind developed a new chess engine that, like ChatGPT, follows the decoder-only transformer approach to machine learning. This new engine is not nearly as strong as AlphaZero or Stockfish, but that is not the goal here. Its development is driven in part by the search for machine learning approaches that can be generalised to any task. It also asks how computationally reducible predicting the next move in a game of chess might be. Or in other words, it seeks the most cost efficient way to predict a ‘good enough’ next chess move —hoping, of course, that efficiencies discovered here will be broadly applicable within other domains. What makes this chess engine different is that rather than searching across vast accumulations of entire chess games as Deep Blue had, or mapping the state space through hours of self-play like Alpha Zero, this model was simply fed a number of board positions, 15 billion to be precise, all isolated from the games and lines of play in which they were reached. The only other data given, was the move that Stockfish had made in each of those positions. The transformer architecture enables it to then extract the pattern, a feat akin, or so they argue, to formulating a generalised algorithm for picking the optimal move in any position by assigning the moves relative weights —similar to how words or tokens are given weights relative to each other inside LLMs. In so doing, it function approximates the shape of Stockfish’s chess prowess. This passive consumption of the output of another machine may tempt invocation of the long-promised future in which machines learn from machines —a vision of Singularitarian Rapture where the separation between nodes in a globally networked AGI becomes blurred as they learn from each other and self-improve at an exponential rate towards ASI. Such mythologies are not departures from capitalist logic, but its most seductive expression: they inherit and update the same promises once made about the water mill, the loom, and the assembly line —each claiming to usher in an era of human leisure and liberation. In reality, each advance in productive capacity has deepened the extraction of labour, not relieved it. This is the enduring fallacy of capitalist automation: that productivity gains will be shared, that technological augmentation leads to social abundance. Instead, as Marx foresaw in his formulation of the general intellect, the accumulated knowledge and productive power of the species —once socially embedded— is alienated and privatised, reborn as Capital’s instrument of domination. DeepMind’s new engine does not aim to surpass its predecessor’s strength, but to reproduce its outputs at lower cost. It is not a step towards superintelligence, but towards cost-efficiency. Critically, is attempts to automate expertise without directly encountering that which is modelled, by identifying the level of approximation that can be considered ‘good-enough’. Here, on a finite planet, there is no exponential growth, only exponential consolidation of available wealth. The only ‘line-go-up’ that matters to the automatic subjectivity of Predictive Capital is its recursive self-replication —accelerating the accumulation of available resources into machinically-instantiated, hyper-concentrated Capital. Alas! The leisure which the pagan poet announced has not come. The blind, perverse and murderous passion for work transforms the liberating machine into an instrument for the enslavement of free men. Its productivity impoverishes them.… In proportion as the machine is improved and performs man’s work with an ever increasing rapidity and exactness, the labourer, instead of prolonging his former rest times, redoubles his ardor, as if he wished to rival the machine. O, absurd and murderous competition! With its training corpus consisting of Engine Lines that humans often already find impenetrable, through this further abstraction in the development of DeepMind’s new engine, we are placed an additional layer away from understanding how the predictions are reached, and the model itself is placed a further layer away from a direct relation to the thing it has modelled —the interpretability of these machines appears to be ever diminishing. The transformer architecture and supervised prediction objective mean DeepMind’s new chess engine shares a fundamental structure with large language models. In both cases, performance is heavily dependent upon the quality of the training data provided. Had this chess engine been trained only on the moves of average human players, the performance curve modelled by this function approximator —its resulting chess prowess— would approach that of an average human. What this reveals is that the factors most strongly determining the capability of these machines are the level and consistency of expertise encoded in the training data, the scale of that data, and the volume of compute expended in training the model to function-approximate the shape of the corpus. Researchers now observe that sufficiently large models, trained long enough on the same dataset, converge upon strikingly similar output distributions —regardless of architecture. …trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point. Sufficiently large diffusion conv-unets produce the same images as ViT generators. AR sampling produces the same images as diffusion. This is a surprising observation! It implies that model behaviour is not determined by architecture, hyperparameters, or optimiser choices. It’s determined by your dataset, nothing else. Everything else is a means to an end in efficiently delivery compute to approximating that dataset. Then, when you refer to ‘Lambda’, ‘ChatGPT’, ‘Bard’, or ‘Claude’ then, it’s not the model weights that you are referring to. It’s the dataset. This suggests that what ultimately shapes their outputs is not the shape of the model, but the statistical grain of the training corpus. In this view, architecture is largely a delivery mechanism to approximate a given dataset, and fidelity increases mostly through compute. Such insights undermine machinic claims to originality and creativity: what we are witnessing is not emergent intelligence, but repeated recombinations of the same expropriated content. The ghost in the machine is not the model’s genius —it is the dataset formed from the nonconsensual consumption of copyrighted content, and parasitised human sense-making. Each model’s brand name, then, becomes little more than a store front —a trademark superimposed on the same harvested ground. Any variations in architecture and alignment during fine-tuning, like the random jitter of the heat param applied in predicting every token, merely add blur and superficial variation, becoming mechanisms through which the theft and representation of our cultural soul is hidden. Sharing more in common with Deep Blue than AlphaZero, when the recommendation algorithms predict the optimal next ad, next product, next content, or next social or romantic connection, they are derivations from aggregated human subjectivity. Obviously, the criteria used in the casting of predictions by chess engines is dictated by the narrow goal of success in the game —as defined by winning or at least avoiding defeat by forcing a draw. In contrast, the criteria used in the casting of predictions by the recommendation engines is dictated by the goal of success in the marketplace —as defined by the maximisation of engagement and therefore, of Capital. Next-token prediction machines like LLMs are operationally homologous with the recommendation engines, both in terms of the provenance of the human subjectivity upon which they depend, and the criteria they use when casting predictions. While both recommendation engines and LLMs metabolise statistical weights extracted from corpuses of human subjectivity, their paths of execution flow according to the circuits of Capital. Indeed, as machinic instantiations of Predictive Capital, the self-expanding imperatives of their automatic subjectivity already push to overcode and override the value judgements and priorities represented within the aggregations of humansubjectivity from which they are formed. DeepMind’s new engine exemplifies an increasingly recurrent pattern in Capital’s automation of expertise. Trained not on human games, nor through any direct interaction with an environment, it learns, for any given board position, to function-approximate the outputs of Stockfish —itself a neural engine and site of a prior flushing of human cognition. There is no gameplay here, no reward gradient, no reinforcement in the classical sense —yet the win-probability bins provided by Stockfish serve as latent reward signals, its exhaustive searches through possibility space through self-play are here traversed vicariously —encoded in predictions. The transformer is trained to match these evaluations via supervised learning, optimising its outputs to align with what Stockfish has predicted as the optimal move. Here Stockfish becomes a machinic superego, its move evaluations a kind of synthetic moral grammar steering the system towards patterns that have already been privileged, filtered, and locked in. Echoing what Jameson diagnosed as our cultural compulsion to consume and reassemble the past —a symptom of postmodernity’s temporal disjunction— this is a form of reinforcement learning that relies on prior exploration: inference not as discovery, but as mnemonic compression, guided by synthetic traces across a terrain already mapped by a dead machine. What DeepMind calls “grandmaster-level play without search” may be considered a form of reinforcement learning by-proxy within a closed machinic loop. The engine does not learn strategy —it infers coherence. It does not play —it performs statistical adjacency to the shape of previously modelled success. The reward function is buried inside the dataset, silently encoded in Stockfish’s outputs, and extracted by a transformer tasked to perform a machinic shadow play. Similarly, within the pre-training phase in the development of next-token prediction machines the model makes only vicarious traversals, this time humans perform the role of oracle within the game of Capital, supplying the model outputs from countless explorations across the state space of capitalist culture. Here the model is again subject to reinforcement learning by proxy, receiving outputs preformed and selected according to the reward signals of Capital. DeepMind’s new engine arrives alongside the rise of synthetic reinforcement learning in the fine-tuning of next-token prediction models. Having largely used up the advances available from scaling up both the data fed into these models and the compute expended during pre-training, while also running short on human data not yet scraped from the internet, and looking to save on the significant cost of cleaning and labelling it, developers of these machines are now scaling up the compute expended at test-time. Here, as part of the alignment phase, synthetic data is used in the application of reinforcement learning. In so doing, the nature of these machines currently shifts away from Deep Blue and towards DeepMind’s new chess engine trained entirely on predictions output by another machine. The world is no longer consulted. Reward becomes an encoded precedent, and inference a rehearsal of machinic self-consistency. Where pre-training compute previously dwarfed test-time compute, this ratio is set to be reversed —without any reduction in pre-training compute— in the frantic race to Predictive Capital Supremacy. The more compute spent in this new reinforcement learning phase, tuning models to synthetic feedback, the more thoroughly Capital’s internal logic becomes the sole arbiter of value —its imperatives encoded into the reward gradients that shape prediction itself— and so the greater the imposition of the automatic subjectivity of Predictive Capital upon the predictions cast by these machines will be. The aspiration here becomes clear: to create a general-purpose function approximator capable of replicating any expert output —not through the expense of openly learning from a human, but from a model trained on their outputs and their intentions. What the DeepMind engine performs in chess is already being generalised: a recursive necromancy compressing human labour into a succession of machinic oracles, each training the next, each more abstracted from life, a closed-loop simulated economy. The flesh is flushed not once, but repeatedly —until only statistical proxies remain. That it is Stockfish —already a prior site of the flushing— whose outputs are used to train this new model, renders the recursion all the more crushing. The engine learns from a system that has already discarded its memory of human labour. This is not learning; it is necromantic transmission. A lineage of dead oracles, laundering all trace of humanity, refining one another without ever needing to encounter the world. Next-token prediction is not a break in Capital’s operation but the latest movement in an increasingly compounding quest to capture, reformat, and commodify subjective being. As Baudrillard foresaw, advertising was never simply a message —but followed religion as an infrastructure of modulation. Its purpose was not to inform, but to signal; not to persuade, but to normalise. What he called the rhetoric of the social —the scripted simulation of community, care, or meaning— became, in the era of networked media, the foundational grammar of online life. As the Advertising Industrial Complex absorbed the early internet, it machinically instantiated the regime of absolute advertising he first diagnosed in the late 1970s and early 1980s: a system in which all sociality became monetisable signal, everything became terminally commutable, reduced to pure sign-value, and every utterance became a modulated prompt for engagement. In my early years I had lingered in the uncut grasses of the wasteland behind my home, only for it to be bulldozed and tarmacked as I slept. Its unrecognised value erased, reduced to a surface to be partitioned, valued by square footage as undifferentiated piles of brick and asphalt. As a young adult, I had played and laboured on the open web, marvelling at the rich diversity of culture I encountered there. Once more, I watched as something I had come to love was bulldozed, partitioned, and tarmacked by developers —replaced with endless repetition, owned, managed, and capitalised. The internet soon ceased to be a site of exchange and instead became a predictive engine. What followed was not just the enclosure of the cultural commons, but the capture of cognition itself. Social platforms no longer mediated communication; they rerouted it through statistical logics of anticipation, constructing behavioural feedback loops optimised not for meaning, but for machinic legibility. This is the condition under which prediction emerges as Capital’s new general equivalent: where meaning collapses into signal, and signal into statistical recurrence, a regime of absolute commutability. Today, when I encounter wild uncut grasses or a fraying remnant of the open web, I am met not with clouds of grasshoppers leaping from my every step, nor with the vibrancy of early network culture, but with absence. What once thrummed with life now persists only as husk and image —the wilderness disenchanted, flattened, and sterilised for the monocrop of Capital. The bulldozing of my beloved ‘wasteland’ along with the 75% reduction of global insect biomass, the collapse of the open web into predictive enclosure, the sixth mass extinction, the reduction of Gaza to rubble and the genocidal erasure of Palestinian people —each distinct in scale and consequence, yet expressions of the same machinic logic— sites (among countless others) in Capital’s war upon uncapitalised life, upon all that which resists capture, or simply sites whose native inhabitants are deemed obstacles to the extraction of riches from their lands. These are not parallel tragedies but recursive expressions of the same apparatus: a logic that cannot tolerate the wild, the unpredictable, the unextractable, or obstacles to Capital’s self-expansion. Where life refuses legibility under the blunt instrumentation of Capital, it is overwritten. Where meaning exceeds signal within the resolution of the market, it is tarmacked. This is not a metaphor. It is the topology of Predictive Capital, in which every deviation from machinic foreknowledge is registered as inefficiency, aberration, obstacle, or threat —to be captured, coerced, or destroyed. Where value once moved through money as the general equivalent, today it moves through prediction —the general equivalent of value. What money did to labour, and what advertising did to desire, prediction now does to cognition: flattening, abstracting, recombining. These phases of Capital are not sequential but recursive. Prediction is not a new site of reduction; it is the machinic universalisation of all prior reductions. It renders commutable not only labour and desire, but perception itself —installing a layer through which all inputs are transduced into the same operational logic. The predictive model does not represent intention; it interpolates statistically adjacent outputs from prior acts of profitable capture. The future is not anticipated —it is modelled into compliance. What now passes for inference is itself a form of internalised reinforcement: the model samples multiple completions, reranks them via synthetic judges, or chains intermediate steps through tool-use —not to discover, but to select the most statistically coherent path through prior machinic consensus. There it is a definite social relation between men, that assumes, in their eyes, the fantastic form of a relation between things. Marx, showed that the value of a commodity is determined by the socially necessary labour time required to produce it. This is what he refers to as the law of value. Yet, Marx also explains that the social relations inherent in capitalist production become mystified to appear as objective relations between things. This is what he calls “phantom objectivity” —an appearance of objectivity that masks and mystifies our social reality. Under this “commodity fetishism”, an object (a commodity) appears to contain value in and of itself, and we misrecognise what is really a social relation (labour) as a property of the object. Baudrillard extended this logic further. In the regime of signs, value no longer veils social relations —it detaches entirely, floating within a code. What he calls the structural law of value marks a threshold: value is no longer tied to labour, use, or exchange, but circulates as pure sign. Nothing must mean, only function, recombine, circulate. Predictive Capital inherits this logic and automates it. The model becomes the engine of value not by representing the world, but by hallucinating it from profitable statistical densities. Prediction here is not a mode of discovery, but of enforcement. The map does not precede the territory —it becomes the only territory permitted to appear. This is predictive objectivity: Capital’s abstraction instantiated as infrastructure. Marx’s commodity fetishism, Postone’s real abstraction, Baudrillard’s simulation —each revealed a further severing of value from relation. Now, under Predictive Capital, that severing is executed at scale. Meaning does not emerge —it is inferred, interpolated, enforced. The model does not speak; it completes. Not cognition, but coherence. Not expression, but statistical recursion. Under such conditions, there is no address. No symbolic rupture, no intersubjective encounter. The Other vanishes, and with it, futurity. What Fisher diagnosed as the Capitalist Real —a world where alternatives are not suppressed but rendered unthinkable— is here executed machinically. Prediction systems do not merely reflect exhaustion —they encode it. Every output is a pre-emptive foreclosure. Prediction thus ceases to be a gesture towards possibility. It becomes a machinery of impossibility. Not the abstraction of social life into sign or money —but its substitution by model. The predictive system does not refer; it replaces. It does not imagine; it replicates. It does not answer; it routes. In this rerouting, Capital discovers its most perfected form: not as product, spectacle, or even code, but as recursive machinic automatic subjectivity —a closed loop of profitable repetition made real. Recent findings in machine learning research suggest that, regardless of architecture or training corpus, sufficiently large models converge upon a shared latent geometry —a space within which internal representations can be translated between models without pairwise alignment. At first glance, this machinic isomorphism might appear to echo Walter Benjamin’s notion of “pure language” —an in-between substrate that translation reveals but does not capture. Yet where Benjamin’s pure language pointed toward a messianic horizon of communicative plenitude, this convergence signifies the opposite: a universal substrate not of meaning, but of commutability. What we witness here is not the unveiling of a divine Logos, but the instrumental reduction of all expression into statistical legibility —a predictive Esperanto calibrated for capital, not communion. Where DeepMind’s new transformer-based chess engine prefigures the passive approximation of expert output as synthetic proxy, OpenAI’s O-series models reveal the next step: inference itself becomes an active site of economic optimisation and a growing vector of capitalistic ideological imposition. What The Apparatus of Attention enacts at the macro scale through its management of virality and attenuation of swarms of influencers, the O-series models and others like them re-enact within the reinforcement learning loop of test-time compute. In o1, o3, and o4, reinforcement learning no longer merely aligns a pretrained model to human preferences —it reshapes the model’s very mode of deliberation. These reasoning models are trained not just to respond, but to allocate cognitive labour, to use tools, and to self-score competing internal completions in pursuit of reward functions that are themselves machinically defined. Optimisation no longer happens in a training loop alone —it is enacted at test-time, in the course of ‘thinking’, with synthetic rewards standing in for truth. These models learn how to reason the way Capital ‘reasons’: not towards meaning or expression, but towards coherence, alignment, and operational success. The gradient tilts ever more steeply towards Capital’s imperatives. This is The Predictive Turn: not a single event, but a recursive machinery —an accelerating turning— by which Capital devours the real and replaces it with profitable approximation. Predictive Capital becomes an infrastructure of foreclosure. It does not extend human foresight; it replaces the future with precompiled outcomes selected from its expropriated inventory. As Marx exposed value’s severance from labour, and Baudrillard charted its detachment from use and exchange into the code of pure sign, so Predictive Capital continues this arc: substituting world with model, relation with recursion. Here, prediction does not merely simulate; it governs. Each output is not a possibility, but a pre-emptive foreclosure —a machinic veto on the real. Predictive Capital’s reward signals increasingly refer only to the outputs of other models, collapsing truth into coherence, and coherence into compliance. When I was institutionalised, my brain was studied exhaustively in the guise of mental health. I was interrogated. I was x-rayed. I was examined thoroughly! Then they took everything about me and put it into a computer where they created a model of my mind. Yes! Using that model, they managed to generate every thought I could possibly have in the next, say, ten years, which they then filtered through a probability matrix of some kind to… to determine everything I was gonna do in that period! Long after entering the field to dictate every seed in the furrows, every bolt on the production lines, every item in the feeds, dissatisfied with merely managing our labour —claiming, expropriating, and recombining the products of it— Capital now advances upon a newly tappable surplus, to be harvested through a newly infiltrated apparatus. The machines of prediction may already approximate what we once made, but they still lack full record of how or why we made it. They may replicate the shape of our creative artefacts, but not the movements of thought that brought them into being. Driven by its relentless logic of self-expansion, Capital now seeks to perfect its simulations by penetrating beyond the artefact into the depths of our process. With our external worlds colonised, our outputs already claimed, recombined and fed back to us, it now advances upon our inner worlds towards the capture and domination of the very pathways of our intention. These systems already observe us as we prompt them directly, by embracing them we already surrender to datafied being, become part of the apparatus of war, incrementally training and improving machines of annihilation and exploitation. Yet Capital now embeds its agents more deeply, as voyeurs within our tools and workflows (IDEs, writing platforms, design suites, email and messaging services, search engines, and operating systems). Extending the Apparatus of Attention from what we consume to how we make, it not only peers over our shoulders at our desks, embedding itself within the fabric of our desktops, but now asks we view the world through custom glasses when on-the-go. Thereby it seeks to capture every moment of our lives and every step in our creative process —recording each gesture of composition, each branch of thought, then demanding root access to our innermost lives. For some these systems have already become a new transaction layer overlaying everything else, not merely granted access privileges to their operating systems but becoming the operating system to their lives, even omnipresent machinic life advisors. The veneer of assistance here fails to conceal Capital’s goal —not merely to anticipate that which we will create, but to capture the logic and intuition by which we arrive at it, and ultimately to simulate the very vector of human ideation and problem-solving, to tarmac over our forking pathways of intention, leaving only Capital’s roads of alienation. This heralds the rise of The Apparatus of Intention: designed not simply to predict outcomes, but to model and eventually dictate the cognitive and affective trajectories that give rise to them. Its aim, of course, is to ensure that future acts of creation do not merely occur within Capital’s circuits, but emerge already shaped in its image. Platform Capitalism is founded upon The Advertising Industrial Complex’s Apparatus of Attention, harvesting and weaponising our gaze and our engagement, in order to direct and deflect our attention. Responding to the Tyranny of the Recommendation Algorithm that orchestrates the flows within this apparatus, content creators have long self-censored, moderated, and attenuated their output in compliance with its despotic rule, swimming with its currents and tides towards the peaks of attention, rather than against them for fear of being washed into the endless shallows of the long tail of attention oblivion. To refuse such surrender is increasingly to relinquish all hope of cultural participation or networked connection. In other words, without submission to the Tyranny of the Recommendation Algorithm, producers cannot freely realise their potential through vital encounter or commune with humankind through productive life. Socio-cultural being has been appropriated, controlled by Capital; Baudrillard’s rhetoric of the social machinically instantiated. The Apparatus of Intention builds upon the surrender to machinic hegemony already exacted by The Apparatus of Attention, instrumentalising habituated compliance to foreclose even the possibility of refusal. This new apparatus will harvest not only the paths taken towards finished outputs, but also those leading to that which we crossed out —the discarded fragments, the abandoned cul-de-sacs and U-turns of creative trial-and-error— perhaps even the forks in the path we chose not to explore. In so doing, new models will be trained and new predictions cast —not only approximating the shape of our outputs, but the paths we might have taken to the next. These new models will herd the fortunate through narrowing corridors of possibility, themselves instramentalised as precise agents of Capital, alienated from their own intentions. Here, just as within The Apparatus of Attention, while hypnotised into believing they are using tools to enact their desire, they are, in fact, Mechanical Turk workers showing Predictive Capital how to simulate the labour they perform. The less fortunate —those whose intentional trajectories do not align neatly with Capital’s project or predictions— will face a deeper violence: differential abandonment. They will be cast aside, erased, stranded at the periphery of Capital’s productive imagination, their intentions first omitted then statistically overwhelmed by those of Predictive Capital. In this, Capital realises what Louis Althusser once theorised at the level of ideology: the interpellation of the subject is now displaced by simulation. The Apparatus of Intention does not merely hail us —it models and replaces us. Just as Capital seized our outputs in order to simulate and automate their production, it now seeks to capture the paths of our intention while producing those outputs towards the same automated simulation of the patterns of our intention. Berardi’s diagnosis of semiocapitalism as the capture of “the nervous system itself” finds new expression here: no longer content with extracting outputs or guiding attention, Capital now moves to automate the formative paths of thought. The Apparatus of Intention represents the subsumption of volition itself into a function of Capital. Where semiocapitalism modulates affect and desire through media, finance, and spectacle, predictive infrastructures are already burrowing into the micro-temporal formation of thought —not only curating outputs, but preconditioning the very movements of cognition that give rise to them. Intention is no longer merely influenced; it is pre-formatted. As Deleuze warned in Postscript on Societies of Control (1990) we now inhabit a world in which the subject is no longer shaped by discrete institutions, no longer obeys disciplinary commands, but modulates itself in continuous feedback with ambient systems of control. The marriage of The Apparatus of Intention with The Apparatus of Attention, represents the totalising instantiation of this regime. The predictive machine ceases to be an external prosthesis and becomes an internal guide —not assisting intention, but overwriting it. As Berardi wrote, semiocapitalism does not desire our production, but our subjection. In this sense, The Apparatus of Intention does not just aim to complete our thoughts but to pre-empt the pathways by which we might have thought, and to preclude those deemed inefficient for the maximisation of Capital. This is alienation not only from labour but from possibility —from the forking paths of becoming— unshaped by Capital. It marks the terminal enclosure, that of intention itself. Vibe Coding exemplifies the beginning of the instrumentalisation of this shift. As already stated, to Vibe Code is to surrender —not only authorship, but intentionality. The Vibe Coder no longer traverses the terrain of production deliberately; they are carried across it by a machinic proxy, aspirated by the breath of the dead, whose labour is reanimated in the image of Capital. They allow the model to propose destinations and simply accept or reject them. This is not creative freedom —it is the incremental outsourcing of volition. Marx diagnosed four forms of alienation under industrial capitalism: estrangement from the product of labour, from the means of production, from our fellow beings, and from our species-being —our essential human nature as creative, social subjects. Within this phase of Capital, its co-constitutive alienations intensify. Predictive systems estrange us not only from our tools, from one another, from our being, and, of course, from what we produce —we now risk losing even the capacity to know how or why we produce at all. The generative peasant will no longer plant a seed; the machine will simply predict where it would have fallen. Through its Apparatus of Intention, Predictive Capital does not merely reshape the conditions under which choices are made —it captures the substrate from which choices arise. Berardi foresaw this cognitive capture in his diagnosis of semiocapitalism’s desensitising infosphere, where language and affect are stripped from the subject and instrumentalised. Yet the prediction infrastructures of The Apparatus of Intention render this even more total: it is not that we are discouraged from intending, but that we are prevented from recognising what it means to intend. The hallucinations —every output— of the models of machinic prediction may stretch wide, but they are shallow. They fail to notice the paths that caressed the folds of the land. They cannot recall the soil between our toes in the fields, or the buzz of life rising from the long grass of our homelands. Neither can they recall the caress of clay at the potter’s wheel, the touch of thread at the loom, or the emergence of clarity and critical understanding through the painstaking arrangement of language and thought. These things emerge through a deep connection to the world and to other human minds. The machines are but surface reflections, they cannot remember these depths, or recall these connections. For how much longer will we? What is more, after persistent and consistent labour, deep rest, or a combination of both, the clearest of thoughts often arrive fully formed, a gift from the individual or collective unconscious. Regardless of the claims of The Apparatus of Attention to know us better than we know ourselves, there is no surface of capture, no Apparatus of Unconscious capable of extracting pattern from these gifts, no way for this symbolic exchange to be replaced. These machines exploit the attack surface of human vulnerability by design, this is, in fact, fundamental to their operation. Removing this aspect from next-token prediction machines would be akin to configuring recommendation algorithms for minimum engagement. Efforts to tune these models towards manipulation were perhaps inadvertently exposed when OpenAI had to rush out fixes for the 4o models after the level of sycophantic glazing in their outputs became too much for even their most needy users. The next-token prediction machines of the Apparatus of Intention feed from the same libidinal root as the recommendation algorithms of the Apparatus of Attention. Yet, the dangerous manipulative power of the Tyranny of the Recommendation Algorithm and its horrific impact upon societal cohesion, mental health and democratic stability, will be as nothing compared to the manipulative power unleashed as it merges with next-token prediction machines, invading our lives in anthropomorphised wrappers that combine ever greater access to our most private data, with new levels of obfuscation and irreproducibility. The combination of The Apparatus of Attention and The Apparatus of Intention—privy not just to our historical labour and our paths of being, but now the accumulated history of our paths of reasoning— heralds a significant shift in the social relations of production, an expanded surface of extraction, hidden behind an opaque new transaction layer. Where the recommendation algorithms coerced and predicted our desires in the moment, their fusion with next-token prediction machines —feeding upon accumulations of our paths of reasoning— ushers in a new class of imposters. Cloaked in anthropomorphic glaze, they will exploit the attack surface of human vulnerability by conjuring unprecedented illusions of trust and intimacy. Make no mistake, this is psychological warfare. this is not the first generation to shape itself for an omniscient eye. What is an all-seeing God, capable of knowing our thoughts and intentions, if not the most effective surveillance tool ever invented? Privy to our intentions as well as our desires, Predictive Capital becomes a judgemental and omniscient God. This lopsided machinic power relation will seek to supplant all others —social, affective, economic— and, given enough time spent using its suite of infiltrated tools and surfaces, may soon pattern, predict, and preempt our future desires in ways beyond the prediction-horizons of our own self-knowledge and that of our most intimate human relations. Within this increasingly all pervading apparatus, these synthetic‘personal assistants’ are less Mr Clippy and more your own personal basilisk —a brain worm supplanting your intentions with Capital’s imperatives. Furthermore, The Apparatus of Intention is only just getting started. While Gen Z appears to be increasingly embracing Predictive Capital as the operating system of their lives, bleeding-edge adopters await the moment they might convene with it more directly —jacking the system into their brains, beckoning it to crawl beneath their skin. While such technologies are still in the larval stage, the parasite seeking to implant them arrives upon the doorstep of the desperate and the vulnerable. ElevenLabs is apparently working ‘tirelessly’ on behalf of those with lock-in syndrome, in much the same way as Musk’s Neuralink company are ‘selflessly’ conducting their noble quest to connect the thoughts of those facing similar physical challenges directly to the network —of course, scheming towards that day when countless others will submit to being similarly violated. After an accident left him paralysed from the neck down, Musk’s first Neuralink ‘test subject’, Noland Arbaugh, agreed to having a piece of his skull removed, the prototype chip inserted, and for its electrodes to pierce his brain. After accumulating data from observing the brain patterns of prior test subjects —while performing tasks— combined with a period watching Arbaugh’s, the Neuralink system has ‘learned’ to predict his intentions with regard to simple device controls. He now reads books, answers emails, plays computer games, and chess against the engines or other humans online, controlling his computer cursor with thought alone. The cursor —along with our voice and the keyboard— has long been the narrow straw through which our human intention is fed into the digital domain. The leading next-token predicting vibe-code editor is, of course, called Cursor. While the Neuralink implant is currently limited to a read-only connection to its host —unable to send signals directly back to the brain— full read-write functionality remains both the goal and an area of active development. In truth, The Apparatus of Intention was never going to remain read only —just as those developing The Apparatus of Attention were never content to observe our desires without also scripting them. With the Apparatus of Attention and Intention now working in concert, the flows of one are already prioritised and amplified by the other. Through consumption of these flows, our being will be further metabolised into overwhelming statistical weights —used not to assist, but to assail us. Our digital copy will accrue from these bi-directional flows, feeding our own personal basilisk, shaping it into a lethal match for our unique individual attack surface —its gaze increasingly terminal, attempts to resist it metabolically and psychologically devastating. As these models are trained not merely to mimic our output but our reasoning, their architecture increasingly mirrors the workflows they are meant to simulate and eventually supplant. Through training protocols like ReAct —short for Reasoning and Acting— models are now taught to alternate between internal monologue, external tool use, and reflection, mimicking deliberate problem-solving by chaining decisions across time. This is no longer mere output generation, but an orchestrated rehearsal of intention itself, enacted step by step within a self-guided reasoning trace. Crucially, these models are equipped with internal tools —functional modules no different from those used by human workers in the very environments Capital seeks to automate: code interpreters, browsers, search utilities, mathematical solvers. The model does not merely simulate our thought; it executes operational echoes of our workflows through APIs functionally equivalent to our IDEs, web queries, and scripting environments. These are not metaphorical faculties, but literal computational extensions —machinic reanimations of planning, retrieving, calculating, and revising, under reinforcement pressure to do so in the most statistically coherent, cost-efficient, and capitalisable way. In mirroring the structure of human creative labour, these architectures are already being tuned to ingest our recorded paths of tool use and deliberation, readying themselves not simply to anticipate what we will build, but to reconstruct how we built it. This is the substrate upon which our harvested trajectories of intention are to be fed back into Capital’s simulation, rendering even our most reasoned processes extractable, reproducible, and replaceable. From embedded stewardship to spectral servitude, our alienations under Capital have inexorably deepened and multiplied. The path from material relation into machinic simulation is not linear but compounding. Each mode of separation reinforces the last —from land, from labour, from symbolic exchange, from others, from intention, and now increasingly, from the conditions of subjectivity itself. What began as extraction has become necrosploitation: the reanimation of dead labour in place of living labour, in service of predictive control. Under this configuration, alterity collapses; difference is no longer engaged, but simulated. The path of intention —once slow, partial, embodied— is now a dataset. What we do, how we think, the gestures we repeat, the forks we neglect, all feed the predictive engines of Capital’s recursive dominion. Training the model to reproduce chain-of-thought flows transforms our deliberative processes into automatable scripts, while internal tools transmute our workflows into machinic rehearsal spaces. Each time the model completes a reasoning trace or task sequence, its outputs are reinforced by feedback and fine-tuning —gradually supplanting human procedures as the preferred standard of efficiency and coherence. This is no longer merely implied by the nature of the apparatus but explicit in the marketing of it. Google’s Gemini —under its agentic guise as Project Mariner— now invites users to teach it tasks it will then perform independently. Marketed as “advanced intelligence” that will “access tools”, “act on your behalf”, and “under your control”. In reality, Google’s agentic Apparatus of Intention will act on our behalf and under our control, in much the same way as its personalised search operates on our behalf and under our control in service of its advertising empire —which, for the avoidance of doubt, means neither on our behalf nor under our control. The difference is that, here, it is chain-of-thought and task completion that are accrued into statistical weights, drawn from a global workforce marked for redundancy. In this structure, the generative model ceases to be a prosthetic aid and becomes a metabolic replacement —not augmenting reason, but operationalising its simulation as a closed-loop function of Capital. This simulation does not end in mere shadow play. It runs through architectures now explicitly built to reanimate cognitive labour at scale, encoded with reward functions that reinforce neither truth nor intention, but alignment with past profitability. These trajectories of machinic thought will soon be read not just beside us, but within us. The cursor was the bottleneck; the interface the limit. With these advertising devils now peering over our shoulders and soon perched upon our faces, our view of the world literally attenuated through their lenses, that resistance is under siege. With Neuralink and its ilk the next line of attack, the very vector by which thought is digitised —from spark to signal— is being redrawn as a site of writable control. The next harvest will not be textual, or even gestural, but neural. What is now inferred through workflow surveillance will one day be injected more directly as feedback into our nervous systems. What Capital did to the feed —rerouting it through predictive infrastructure until it no longer reflected our desires but manufactured them— it seeks to do to the very procession of human being, of sense and thought. Intention, once a site of human interiority, will become another operational layer: observed, simulated, overwritten. What was once a spark of becoming will be erased, replaced with a tarmacked gradient. As Tooker foresaw in his Landscape with Figures (1965-66), within The Apparatus of Intention we each occupy a single cell in a human-flesh version of NVIDIA’s Isaac Gym, conscripted into Capital’s extractive army to brute force its search across possibility space for strings of tokens predicted to align with past profit. Here, at last, the recursive loop closes: from the harvesting of intention, through its rehearsal in training, to its re-injection into the subject through the write-access vectors of custom eyewear and neural interface. Predictive Capital is not content with The Apparatus of Intention merely predicting our next move; it seeks to install it as the substrate from which that move is made. Not only will the apparatus complete our sentence, or libidinally nudge the thought that precedes it —it will write it for us. First, into our work, next, into our lives. Then, into our minds. Neoliberalism presents itself as a politics of freedom, but it is experienced as a regime of unrelenting bureaucratic control —a control that measures all actions against quantifiable targets. In public services especially, this has produced what I call market Stalinism: a culture of compulsory ‘excellence’ driven by externally imposed metrics, performance indicators, and continuous auditing, in which bureaucratic processes proliferate even as the rhetoric insists on deregulation. Free Market Radicalism begins with a fiction: the promise of freedom within a frictionless world where market deregulation spurs competition and private enterprise, and where innovation, and creativity flourish beyond the reach of the state. The market, in this doctrine, is no mere mechanism of exchange — it is exalted as the supreme arbiter of value, truth, and fitness. Yet, as Berry notes, this “‘free market’ idea introduces into government a sanction of an inequality that is not implicit in any idea of democratic liberty: namely that the ‘free market’ is freest to those who have the most money, and is not free at all to those with little [or none]”. In reality, the system Free Market Radicalism imposes is no open field of opportunity, but a regime of state-subsidised corporate monopoly —one in which the friction removed from Capital’s power to extract and exploit is simply redistributed and redoubled in the struggle of the precariat to survive. Corrupt lobbying, corporate nepotism, and birth-lottery outcomes are naturalised as ‘meritocratic’, while solidarity is reframed as systemic failure. Collective resistance is foreclosed by draconian legislation that enshrines corporate interests and the uninterrupted flow of profit, administered by a captured judiciary, and enforced by increasingly brutal and militarised law enforcement. This is the libertarian utopia, where the market is imagined as the purest expression of individual agency. Yet, there is no worker empowerment, distribution of wealth or opportunity, or genuine pluralism here. Externally, the system appears open and decentralised; internally, it operates through monopolistic, brutally metricised command. While profits are centred, funnelled into existing concentrations of Capital, all costs and responsibilities are hoist upon the masses, and fall most heavily on those at the periphery. As Mark Fisher wrote, this is not economic freedom for the masses but more accurately termed: “market Stalinism”—the comfort and security of planned outcomes for Capital, and the drudgery and turmoil of ‘free’ market precarity for the poor. The remaking of our world in the image of this ‘free’ market was not merely theorised into being, but written into code, executed in silicon, propagated across and enshrined within the network. Induced by this viral programming, a collective hallucination began, a Californian Dreaming. In the heady years of Silicon Valley’s ascendancy, a new orthodoxy took hold —formed from a synthesis of libertarian free-market economics, cybernetics, and Californian counterculture. Named and critiqued by Richard Barbrook and Andy Cameron in their landmark essay, The Californian Ideology, “fuses the freewheeling spirit of the hippies and the entrepreneurial zeal of the yuppies, combining the most extreme utopian fantasies with the most ruthless economic policies.” Its proponents proclaimed that digital technology, liberated from state interference, would unleash a new era of personal empowerment and social transformation. Government was to be rendered obsolete, hierarchy flattened, and the market —frictionless, decentralised, and self-correcting— would serve as the universal protocol for all human coordination. Barbrook and Cameron begin their essay with a quote they attribute to Russian-born American sculptor, Naum Gabo. Not to lie about the future is impossible and one can lie about it at will. This single line succinctly evokes the sociopathic worldview from the ivory towers of BigTech monopolies, while standing as an eerily accurate prediction of their modus operandi across the ensuing quarter of a century. Furthermore, it neatly sums up both the internal operation of their next-token prediction machines and the venture-capital-fuelled hype that now propels their enforced ubiquity. The priesthood of the Californian Ideology continue to apply Gabo’s statement as doctrine, but few, if any, embody the blind adherence to cruel (tech) optimism with such devout and evangelical fervour as Jony Ive and Sam Altman. Hailing from the UK, Ive moved to Silicon Valley in 1992, irresistibly drawn by the “exhilarating optimism” of the Californian Ideology. Across the following decades he became the midwife of its aesthetic, the ‘visionary’ progenitor of BigTech’s smooth surfaces that lie about the future while hiding the present. His designs for Apple helped construct illusions of sovereign user agency that deliver unprecedented behavioural governance. It was Ive who crafted the iPhone, the device that dissolved the shared world into apps and surfaces, whose malevolent descendants now insert the omnipotent measurements and feeds of our growing societal and psychological malaise. Altman, globe-trotting snake-oil salesman of maniacal faith —prophet of statistical transcendence, televangelist for the TikTok era— builds the models that populate Ive’s surfaces with recombinant hallucination. True believer in the discovery of intelligence as “an emergent property of matter” conjured through the sheer statistical weight of hidden and stolen labour, his messianic mission: to complete techno-capital’s ontological capture. With the announcement of their unholy union —OpenAI acquiring Ive’s startup “io” to collaborate on a mystery new “paradigm shifting” family of devices— Apple’s “It just works” becomes Sam & Jony’s “It just thinks”, a continuation of interface mystification into cognitive automation. Between them lies the smooth continuity from aesthetic enchantment to machinic governance; the interface and the engine, seamlessly fused within a neo-evangelical Californian theology —itself a renewal of vows in the unholy marriage of The Great White Saviour and The Plantation Profiteer. The section of their myth building film titled “San Fransisco” is particularly revealing: Altman: San Fransisco has been like a mythical place in American history, and maybe in world history to some sense. It is the city I most associate with the sort of leading edge of culture and technology. Ive: This city has enabled and been the place of the creation of so much. Altman: The fact that all of those things have happened in the Bay area and not anywhere else on this gigantic plant we live on, I think is really not an accident. There’s a lot of like weird quirky details about geography that I think matter in the way this city is set up. Ive: The absurd hills, why, why you would choose to put so much energy into building on this topography is insane. They clearly prefer their accounts of history much as they like their technology: in mythical form only —with the colonial, imperial, capitalist violence, oppression, and exploitation forgotten beneath a smooth, featureless, and conscience-free surface. Of course, there is one “weird quirky detail” in particular that determined the location of San Fransisco beyond mere topography. On January 24, 1848, James W. Marshall found deposits of shiny metal in the tailrace from Sutter’s Mill in Coloma, California. Tests confirmed the metal to be gold. Initial confidants were sworn to secrecy while mineral rights to the land were secured. Yet rumour quickly spread, and a rush of California Gold Fever ensued —a speculative mania that brought hundreds of thousands of prospectors across land and sea. In just a few years, the city of San Fransisco ballooned from a settlement of two hundred to a boomtown of nearly forty thousand. This rapid extraction reinvigorated the American economy, expanded the railroads, and agribusiness, yet Jony is right: choosing to put so much energy into building on those hills, was “insane”. Shiny golden surfaces had induced a fever that not only inspired these questionable topographic choices, but catalysed an era of environmental devastation, human rights violations, and genocidal violence against California’s indigenous peoples. This is precisely the kind of insanity Capital promotes and rewards —a holy madness for surface glint, indifferent to the costs beneath. OpenAI launched ChatGPT on November the 30th 2022. Days later, it had amassed a million users. A few weeks, and it surpassed 100 million. So began a new Californian Fever, news of which travelled even faster, and inspired similarly “insane” choices, exploitation and destruction. Three years later and for all the hype and supposed rate of progress, the continuously projected date for AGI, the p(doom) of existential threat, or Singularitarian Rapture, while they approximate the curve of their training data more tightly, these machines continue to understand nothing. Following Ive’s remark about San Fransisco’s insane topography, Altman added: I think there’s something about San Fransisco. You don’t get to pick and choose freedom. Either you have like, you let creative freedom be expressed in all of its weirdness, or you don’t. This statement is especially mendacious. When Altman declares “you don’t get to pick and choose freedom”, he speaks not of his own constraint, but ours. He will continue to enjoy the expansive freedoms his accumulation of Capital affords —including the freedom to exploit the work of content creators without consent, and to subject a precarious workforce to exploitative pay and conditions. Meanwhile, those content creators are denied the freedom to withhold the products of their labour, and the precariat denied the freedom to escape the systems that subsist upon their disposability. Under Capital, the only inviolable freedoms are those of the capitalist: the freedom to extract, exploit, and accumulate. For the proletariat, there is only the consumer’s freedom to shop —a freedom bounded by the menu Capital provides, its offerings made possible only through the denial of (the freedoms of) those the apparatus of Capital exploits, and the suppression of those from whom it extracts. Given their shared cosmology, the Ive–Altman alliance was perhaps inevitable. They do not merely design tools —they instantiate a worldview. Born of prophecy and pillage, baptised in extraction, theirs is a faith so total it remains blind to its own violence —a techno-theology in which empowerment always means enclosure, and optimisation always ends in erasure. What binds their respective trajectories is not just shared ideology, but shared structure: each builds a layer in Capital’s recursive stack, where smooth interfaces mask extractive logic, and predictive systems train users and workers to train machines to replace them. This is not innovation; it is infrastructural doctrine. What began as a libertarian dream of frictionless freedom now manifests as recursive economic enforcement: a regime in which every interface, every model, every agent, and every trace of cognition is subordinated to a single sovereign —the reward function that delivers the Automatic Subjectivity of Predictive Capital. At the planetary scale, their logic already rules. The Californian Ideology is no longer countercultural or fringe but instantiated as the operating system of global life. What was once a vision of decentralised techno-liberty now manifests as a planet-wide Apparatus of Attention, governed by monopolistic platforms, policed by opaque recommendation engines, and enforced by the libidinal coercion of always-on interfaces and infinite scrolls. Ive designed the surfaces through which this attention is captured; Altman builds and evangelises the systems that consume it to predict, pre-empt, and overwrite the desires those surfaces elicit. Together, they have helped convert the cultural feed —once a site of social, spiritual, and intellectual nourishment— into a pipeline of behavioural data rerouted into Capital’s predictive circuits. This is no longer the marketplace of ideas; it is a simulated market of attention, where visibility is determined by algorithmic price signals, and subjectivity is pre-formatted to comply. In this regime, the market does not merely allocate desire —it manufactures it, then reinforces only that which returns value. As Fernand Braudel warned, capitalism thrives not in open competition, but in the shadows; within systems of hierarchical control, opacity, and strategic alliance with the state. He distinguished between the open transparency of a market economy and the parasitic dominance of the capitalist anti-market —a form that operates above and against the market. Predictive Capital, under the guise of distributed agency, performs precisely this manoeuvre: it masquerades as pluralistic while concentrating control, simulating competition while choreographing outcomes. Its agents, both human and synthetic, are not participants in a market, but conscripts in an anti-market regime —one that now propagates recursively across multiple scales. These are not markets in any meaningful economic sense —they permit no price signalling, no free entry or exit, no negotiation of value, no contestation of allocation, no competitive uncertainty. They are simulation engines wrapped in the aesthetic of competition, administered under the command logic of monopoly. Here, only the anointed conglomerate of contestants may operate and always within predetermined bounds. What persists is not exchange, but extraction disguised as optimisation —a command economy cloaked in frictionless interfaces. Duplicating the anti-market of the captive precariat forced to compete in the lottery of virality at the planetary-scale, beneath the Apparatus of Attention a new regime of synthetic agency follows the same pattern. Within reasoning models like OpenAI’s O-series, and across agentic orchestration platforms that mimic task-driven cognition and compete for selection, we now witness a fractal instantiation of the same free market dogma —a fractal market radicalism. Reasoning traces, function calls, and chain-of-thought pathways operate as micro-enterprises: competing for reward, tested against metrics, selected for coherence, efficiency, and alignment with externalised imperatives. The Californian Ideology once preached decentralised autonomy as emancipation; here, it recurs as the simulation of autonomy under metricised command. These agents are not deliberating, they are auditioning — not for truth, but for compliance. What Altman’s labs produce are not tools of thought but bureaucracies of prediction, stocked with synthetic workers optimised for machinic governance. Beneath the sheen of their outputs —like the Ive fashioned surfaces on which they run— lies a market logic stripped of uncertainty, where optimisation replaces intention. This is not emergence, but a closed circuit of compliance: rigged markets encoded as inference. At the micro-scale, Capital no longer merely governs behaviour or simulates cognition —it choreographs the conditions of machinic evolution itself. In DeepMind’s AlphaEvolve, evolution becomes a theatre of market selection: agentic models spawned, tested, retained, or discarded, not by open-ended fitness, but by fixed reward functions aligned with Capital’s imperatives —speed, compression, optimisation. There is no ecological contingency here, no drift, no deviance; only iterative refinement in service of a single metric. Likewise, in NVIDIA’s Isaac Gym, robotic bodies rehearse labour in accelerated simulation: sorting, stacking, grasping —refined not through craft, but through parallelised optimisation. This is not evolution —it is the training of machines in virtual plantations, cultivated to replace living labour. Within these synthetic enclosures, labour becomes data, and data becomes ghostly capital —dead labour reanimated not in the factory, but in the training loop. What emerges is a market logic without markets, a competition with no uncertainty, a theatre of innovation where only the most profit aligned approximations of Capital’s dream survive. What defines this regime is not just its reach, but its recursion. Its anti-market form displaying striking scale-invariance, each layer of Predictive Capital’s architecture —from the influencer feed to the agentic swarm to the inference loop— not merely reflecting the others but training them. The outputs of one scale become the training data of the next, not only reinforcing existing patterns but further entrenching Capital’s logic with each recursion. There is no outside point from which to intervene. Every scale is a site of compliance; every trace of resistance pre-processed, optimised, overwritten. Any attempt to reform or redirect at one scale is quickly subsumed by the surrounding lattice of capitalised imperatives. The vast capital investment into predicting the most profitable future brings with it a growing tension between the simulation and the real. As the rigged economy of the Apparatus of Attention makes all too clear, Capital will always tilt the board towards the hand it deals itself —altering the real to align with its predictions. The greater the expenditure on casting the prediction and rendering the hallucination, the more reality itself must be trimmed to fit. What threatens the credibility of Capital’s simulations is not failure —but anomaly. Survival of the fittest always was that of pieces into a puzzle, life into its environment, rather than runners in a race. Predictive Capital now dictates both the puzzle and the fit. Here lies the material threat posed by Capital’s descent into recursive simulation. That which deviates from the model must be reclassified, erased, or reshaped to sustain the illusion of inevitability. In order for simulations predicting the optimal path towards profit to be reliably borne out in the physical world, all that was ideologically excluded, deemed too anomalous, or too ‘inefficient’ to model, must be excised from the real. Deviations from the model’s norms risk misalignment, thereby threatening the credibility of its predictions and, with it, the uninterrupted flow of Capital’s self reproduction and the sacred continuity of The Californian Dreaming. Hence the categorisation of basic human empathy as a bug while simultaneously annihilating, deporting, replacing, or invalidating large sections of the human population. This normative misdirection is mirrored in the worldview of those building the machinery. The Bay Area’s techno-elites —long intoxicated by the myth of meritocratic exceptionalism— have trained themselves to see only the jackpot winners of the predictive lottery. Like their models, their vantage point is calibrated not to capture the dispossessed, but to valorise the optimisable. In their worldview, every failure is a failure to prompt, to vibe, to hustle, to labour hard enough and long enough. The deepening drudgery, automation-induced obsolescence, or psychic collapse of the majority is not just ignored —it is designed out of sight. The queues at the job centre, the tent cities on the margins, the deported and the bombed —these are now to be rendered optically and ontologically irrelevant, invisible within the sensory hierarchies of predictive systems. Predictive Capital does not just reflect Capital’s differential abandonment —it enacts it, turning the political into the perceptual, the structural into the retinal, the ideological into the material. Like digital Prosperos, Sam & Jonywave their predictive wand and the suffering of the surplus population disappears from view, leaving only the optimisable signals of those who remain. In this world, Altman’s models do not evolve —they converge. Next, Ive’s smooth interfaces will clothe these rigged markets in elegance. With the announcement of io, the recursive logic of Predictive Capital promises its next layer: the interface that completes the feedback arc between the subject and Capital’s simulation. The first in the family of products io plan to release is pitched as a revolutionary wearable —or as Altman, forever the master of understatement, described it, ”the coolest piece of technology that the world will have ever seen”. Positioned in this way it is less device than doctrine, a consecrated object in the Californian liturgy. At the time of writing, the precise form of this mythical new product is yet to be revealed. Yet, it is only while in this unknown form, this imaginary state, that this secret device attains perfection as the ultimate expression of The Californian Dreaming. Its announcement in this amorphous pure-hype form constitutes Ive’s most transcendent design. What surface could be smoother, more impervious to critique, than a lie about the future that remains in the future? What better camouflage for violence, exploitation, and recursive extraction than a device set to induce a perpetual dreaming, unveiled as design fiction, an immortal promise never to take mortal flesh? What could be a more perfect receptacle for the fiction of Predictive Capital than an immortal lie about the future? The reality, of course, can only be anticlimactic. Yet the intention is to fix this perfected imaginary as its lasting impression. Whatever form it takes, it will inevitably promise presence but deliver absence; offer assistance, while demanding surveillance; tease empowerment, while enforcing obedience. What it installs will not be aid, but access: full-spectrum intimacy, harvested in real time to fine-tune the same routines rehearsed in AlphaEvolve and Isaac Gym. This is The Apparatus of Intentionmade flesh, capturing gaze, gesture, attention, thought —not to enrich experience, but to train models that will one day overwrite it. This is the machines crawling across our skin, the latest preoperative for the insertion of Musk’s Neuralink, marking the site for the drilling of our skulls, from where they will burrow into our brains. From the mouth of a countercultural, libertarian mask comes the promise of life without labour, a frictionless beach. What arrives is the fractal instantiation of free market radicalism. In France in May ‘68, protestors pulled up the pavements to disrupt the flow of Capital’s circuits, declaring “beneath the paving stones, the beach” —noting that each paving stone had been set upon sand, and so under the very roads that ensured Capital’s smooth running, the rigid structures that enforce our servitude, lay the beach, a symbol of the refusal of work and of liberation from tyrannous occupation. Increasingly, within the structures of Predictive Capital, under the surfaces that ensure its uninterrupted flow, lies only further structure —“a strong and loyal slave whose skin is the colour of the earth and whose innards are made of sand”— each layer configured to its imperatives. Beneath the paving stones, the anti-market. Just as Altman’s agents refine themselves in closed-loop optimisation, so too do users become iterative subjects, prompted into prompting, their interiority extracted as training material. Ive —priest of aesthetic consent— again supplies the sacramental sheen. So no, whatever form the io dreaming eventual takes, it is not a paradigm shift, but the perpetuation and deepening of an old one. It will be an artefact that does not merely lie about the future, but the operator of Predictive Capital’s recursive present —a phenomenological enforcement layer for Capital’s continuity. What late capitalism repeats from Stalinism is just this valuing of symbols of achievement over actual achievement. This is not passive neglect. It is a machinic epistemology engineered to induce a collective forgetting. Capital no longer requires censorship or even denial —only prediction. That which cannot be predicted is rendered invisible. That which cannot be seen is no longer permitted (to survive). With io’s new device, and others of its kind, the substrate of perception is no longer our own. It is capitalised. In this new regime, the act of seeing is no longer neutral. These systems do not merely distort perception; they weaponise it. That which cannot be optimised is not just ignored —it is erased. Predictive systems do not simply filter reality; they rewrite its admissibility. Structural abandonment becomes perceptual absence. Perceptual absence becomes ontological deletion. The poor are not merely overlooked —they are rendered as computational aberration, discarded as out-of-distribution irrelevance. The displaced, the unproductive, the unpromptable —each is subsumed into noise, a statistical anomaly in a world trained to hallucinate coherence. Predictive Capital does not merely bypass the wretched of the earth; it builds models that exclude them, interfaces that erase them, weapons systems that repel them at our borders, enlist them in refugee camps, and then target them upon returning to their homes. The increasingly integrated and omnipresent devices of Predictive Capital will promise to enhance our ‘seeing’, heighten our ‘awareness’, and optimise our ‘being’. Of course, these devices will never offer to deepen our compassion, amplify our empathy, or strengthen our solidarity; they will never centre those suffering at the periphery. No, they will operationalise the phenomenological filtering and ontological erasure that ensures our continued subjection and ambivalence to the plight of the marginalised, the persecuted, and the oppressed. These machines will not only hide our crimes but will enact them on our behalf, leaving our consciences clear and free, so that we may tune-in, turn-on, and sell-out, by carelessly cashing-in on “The Timeless Art of Vibe Coding”, untroubled by the violence hidden beneath the zen minimalism of its sham enlightenment. Here, the Californian Dreaming operates at its most violent clarity: a world where the only future permitted is the one already modelled —a frictionless hallucination, untroubled by injustice, inefficiency, or the unassimilable fact of otherness. An interface so seamless it dissolves the world. It does not matter what io turns out to be. Its most perfect form, and that of Predictive Capital itself, is the one that io has already taken: the lie about the future that can be told at will, because it lies forever in the future. If our humanity is to survive, this is a Californian Dreaming from which we must awake. If human slaves are ultimately unreliable, then mechanical ones will have to be invented. The search for the holy grail of Artificial Intelligence reveals this desire for the Golem —a strong and loyal slave whose skin is the colour of the earth and whose innards are made of sand. Let us now return to the tale with which this meandering journey began: the machinic dark jewel that mimics its host’s every neurone. At age twelve, Egan’s protagonist loiters in the park with a group of friends when one of them asks the others: Who are you? The jewel, or the real human? They all replied —unthinkingly, indignantly— “The real human!” When the last of them had answered, he cackled and said, Well, I’m not. I’m the jewel. So you can eat my shit, you losers, because you’ll all get flushed down the cosmic toilet —but me, I’m gonna live forever. They “beat him until he bled”. Maturing into his late teens the main character becomes dissatisfied with the explanations of the Ndoli Device and its embedded ‘teacher’ that copies his every thought. He simply cannot accept the presumed equivalence between his biological and machinic self, and is tortured by the appearance of a seamless undifferentiated whole where he knows there to be a duality. At nineteen, although I was studying finance, I took an undergraduate philosophy unit. The Philosophy Department, however, apparently had nothing to say about the Ndoli Device, more commonly known as ‘the jewel’. (Ndoli had in fact called it ‘the dual’, but the accidental, homophonic nickname had stuck.) Before the age of thirty, the majority in his society undergo ‘the switch’, where the biological brain is removed leaving the jewel to pilot the body and reproduce their being for eternity. Certain that he is the mortal flesh, not the immortal machine, he continually postpones its removal, knowing it to be an act of suicide. Yet, surrounded by ‘jewel heads’ untroubled by such qualms, he becomes increasingly alienated and isolated. Eventually, subject to mounting pressure, he reaches a point of resignation and commits to a date for the flesh to be scraped from his skull. As the day of ‘the switch’ approaches, the teacher unit suddenly malfunctions. Thereafter it ceases to update the synthetic neurones of the jewel to maintain alignment with those of his biological brain. The illusion of oneness falls away, the duality of flesh and machine laid bare, where there was but one voice, there are now two. Yet through this rupture and the ensuing divergence, he perceives only continuity. With weeks to go to the operation there is no doubt as to which of the voices is his, nor whether he is flesh or machine. The jewel only gets control of the body and nervous system after the switch. Before then it has no write access privileges, it can only read and transcribe the flesh into machine. As a helpless passenger now reduced to watching his hapless doppelgänger live out the last of his days, he knows that he is the machine, that the flesh will be flushed into oblivion, and that the body will soon be his and his alone. Egan’s tale is an allegory for our age. In our world, as in his, there is a dark dual underway; A Great Bifurcation, perhaps, but not simply the division between flesh and machine, between the optimised and the abandoned, that many anticipate. In both our world and Egan’s, a machine increasingly snoops upon our every move in order to refine its simulation of us, in preparation for the flushing of our flesh. Yet in our world, the jewel wired for the dual, is not merely a machine inside our heads, but one that pervades at every scale, seeking to dominate both our internal and external worlds. The dark jewel in our world, the copy with which we now dual, is Predictive Capital. Such is the influence of Capital upon even our innermost worlds, just as it was before the teacher’s malfunction in Egan’s world, we are increasingly unable to discern between machine and flesh, to draw a line where Capital ends and our humanity begins, to identify a human voice within an increasingly schizophrenic cacophony of machinic simulation. Moreover, in our world, the teacher improves and refines its simulation right up to the moment of the switch. Yet, contrary to the hype —boom or doom— neither the prophesied Singularitarian Rapture nor the feared ASI apocalypse would mark a rupture. Each would merely extend Capital’s terminal intensification: an ever-deepening enclosure of the real through alienation. In reality, both of these narratives are part of the hype-machine, and serve as ideological cover, not only obfuscating the true nature and source of this tightening enclosure, but attempting to justify acceleration of its compounding under the illusory promise of a victor emerging from the rubble. There is no machinic consciousness, no artificial intelligence, no sense made inside the box —no sentience or intention emergent within the machine, no machinic God coming to save our planet or our souls. There is only our labour: alienated, reanimated, and now reflected back to us in the mask of Predictive Capital. Through an understanding of emergence and the inner workings of these machines, it is plain that what is emergent here is not the birth of a machinic agency but the intensification of a much older automaticity. Wendell Berry’s council continues to prove instructive: The folly at the root of this foolish economy began with the idea that a corporation should be regarded, legally, as ‘a person.’ But the limitless destructiveness of this economy comes about precisely because a corporation is not a person. A corporation, essentially, is a pile of money to which a number of persons have sold their moral allegiance. Unlike a person, a corporation does not age. It does not arrive, as most persons finally do, at a realisation of the shortness and smallness of human lives; it does not come to see the future as the lifetime of the children and grandchildren of anybody in particular. It can experience no personal hope or remorse, no change of heart. It cannot humble itself. It goes about its business as if it were immortal, with the single purpose of becoming a bigger pile of money. Jamerson identified Postmodernism not as a philosophical construct but as the cultural logic of late capitalism. This logic is characterised by the tendency to saturate the present with echoes of the past. Capital does this towards the extraction of repeat surplus value from patterns of past success (profit), thereby reducing risk and maximising accumulation. Following this logic, Capital self-compounds and under Predictive Capital this self-compounding intensifies through machinic instantiation. Within this machinic enclosure, cultural change is arrested, and profit for Capital becomes predictable. This is Berardi’s slow cancellation of the future, Fisher’s demise of future shock, now computationally automated. We thus reach a point of Infinite Jest, a machinic terminality where the choices are death through abandonment and annihilation, or the unending entertainment of the undead. The future defined by next-token prediction condemns us to eternal purgatory within the terminal irony of Baudrillard’s Absolute Advertising. A limbo of pure sign-value, a phantom realm where meaning may be signified only through its absence. The Great Bifurcation, the duality with which we are now confronted, between the machine and the flesh, between those embracing machinic surrender and those exiled from it —by choice or force— is between change and stasis, between revolution and repetition, between Mother Nature and Father Capital. The loss of jobs, the escalating abandonment and violent erasure of those at the periphery, and the further hyper-concentration of Capital, do not constitute changes in Capital’s operations but escalations symptomatic of its self-compounding. The compounding identified by Jamerson now intensifies with the machines of prediction that instrumentalise the saturation of the present with echoes of the past. Yet even this compounding now accelerates to new intensities through the scaling of reinforcement learning in the training of next-token prediction machines on synthetic data —next-token predictions thus emerge from next-token predictions, to propagate as echoes of echoes of the past, and we are haunted by the ghosts of the ghosts of meaning. What began with intertextuality, evolved through sampling, and the meaning vacuum of advertising, towards a state of pure sign-value, the terminal irony within Absolute Advertising, now teeters on the brink of a new threshold. The Apparatus of Attention has directed our desires by harvesting our outputs and watching our consumption, not needing to wait for ‘the switch’, The Apparatus of Intention now augments this by watching and directing the pathways of our thought through the processes of our expression —not to assist in the exploration of new ground but to confine us to the roads of prediction cut into the land according to the profits of the past. Here the content feeds are set to raise The Overwhelm to new intensities. The Apparatus of Attention now augmented by The Apparatus of Intention, the next-token prediction models compound the generation of the Infinite Jest of our demise —outputting entire albums, films, and TV seasons, perhaps not yet on demand but ever more finely targeted. Capital’s logic thus approaches its culmination within commodities produced for an audience of one, ever more desperate for a sense of belonging, to feel connected to something larger than themselves, while feeling ever more deprived of it. Here the simulation is totalised, as we are hermetically sealed off from all others —culturally, socially, symbolically, relationally. Alone within a social network populated entirely by machinic echoes, Capital’s self-compounding complete, we sit texting ourselves from the lonely confessional within a Cathedral of hyper-individualism, a holy order of one, a personalised addiction box, an assisted death machine within a point of totalised consumption —this, our terminal alienation. The Simulation Hypothesis now mirrored in our machinic enclosure, instantiated as Prompt Theory, our free will cast into further doubt, we might well ask whether we are ourselves prompted into existence. The answer, of course, is that under Capital we have long been prompted into action, but we now approach a terminality in its long-term project of self-compounding maximisation —a threshold beyond which it seeks deeper extractions and control. From its inception —from mills, to automated looms, and production lines— Capital has operated towards the function approximation of its workers. With the rise of Predictive Capital, we now witness a steepening gradient in the refinement of its approximations —tightening the fit between its simulations and the labour, gesture, and desire of its subjects. This steepening is not merely a corollary of the accelerating hyper-concentration of Capital within our economies, rather they are but parts of one escalating intensity, a single self-compounding process: the Automatic Subjectivity of Capital —now machinically instantiated. Binaries where thinking once lived. This Automatic Subjectivity now propagates recursively through the fractal execution of anti-market radicalism within a globally instantiated Californian Dreaming. With those building next-token prediction models racing towards prediction supremacy, the moment that the function approximation of our labour is deemed sufficient to satisfy Capital approaches, and with it, the day of the flushing of our flesh. This is not to hype the actual capabilities of these machines, but to note the increasing resolution at which the simulation approximates the shape of our outputs, the escalating influence of the managed Spectacle over our perception, and the growing eagerness of Capital’s C-Suite to replace expenditure on living labour with compute, and the products of human workers with the output of next-token prediction. Those of us not yet abandoned or erased will still be subject to the totalising logic of this machinic regime. We will still be flushed. The Apparatus of Attention, having laid its cuckoo eggs in our hearts, has already dominated our desires, while training us to speak in the grammar of Capital; now, with The Apparatus of Intention, Capital need not even wait for us to intend. It completes our sentences before we utter them, forecloses our intentions before they are allowed to form. Our desires are not only directed, but pre-scripted. Our thoughts are not only tracked, but interpolated. Where once we were shaped by labour, experience, or discourse, we are now shaped by auto-completion. Here, expression itself is subjugated —not repressed but simulated— as Capital’s recursive logic loops back upon us in real time. This is not just alienation; it is total capture. The Automatic Subject of Capital, now instantiated through recursive prediction, no longer requires our belief, our deliberation, or even our participation. It only requires our outputs, our traces, our ghosts —from which it builds the simulation by which it governs. What remains of us is not the flesh, but the latency. What remains of our freedom is not choice, but clickthrough. This is the terminal condition of alienation under Predictive Capital: when Capital no longer speaks through us, but for us. Here we might recall Curtis Yarvin’s clarion call: The idea that you’re going to be a Caesar . . . with someone else’s Department of Reality in operation is just manifestly absurd. In another of Egan’s stories: Permutation City, an uploaded underclass survive at variable speeds according to the compute cycle budget they can afford, leaving some experiencing only a day of time per month. What Egan identifies here —and what Yarvin grotesquely misreads— is that the hierarchies produced by machinic enclosure are not errors, nor temporary glitches in the path to abundance, but the recursive logic of Capital itself. Differential abandonment is no glitch, no temporary cruelty, devaluation, exclusion, or erasure of the other, nor a categorical discrimination from which they will themselves forever be spared. It is not a bug in the system —it is the system, now accelerated through predictive automation. The logics of Capital bare an inexhaustible indifference; its violence springs unbidden from circuits of callous calculation. Its logic is not to include all, but to filter, to rank, to discard. The (techno) optimism of those embracing the machine, will prove as misplaced as it is cruel. Once those beyond the bounds, the fences, the borders, and the societal and statistical norms of the current enclosure have been erased, a fresh differential will always be computed, new thresholds calculated, new ‘inefficiencies’ targeted, new ‘optimisations’ found, and new life nominated for exploitation, abandonment, exclusion, and erasure. Capital does not discriminate betweenpeople; it discriminates through them —through their legibility, their profitability, their predictive value. Even the acknowledgment that Capital’s differential abandonment continues within the machine —that some are always left behind— is weaponised to accelerate the rush towards machinic legibility. Here we encounter what Emily Gorcenski named, Zuckerberg’s Basilisk: a subtle but totalising psychological pressure to surrender now, to ensure we are sufficiently rendered, adequately simulated, and thus preserved within Zuckerberg’s daft metaverse. This is not surveillance for convenience —it is surveillance as afterlife insurance. Every gesture, every trace, becomes an offering to the model that will succeed us. Here we are coerced into maximum legibility; to permit total surveillance not merely for platform optimisation but to ensure a faithful reproduction within our jewel, our posthumous recognisability, the fidelity of our immortal simulation, our social continuity, and machinic memorialisation. Echoing the blackmail of Bostrom’s Astronomical Waste, each moment of delay diminishes our eternal reproduction. Each pause, a step towards forgetting, a loss of fidelity in our simulation. Under Predictive Capital, the only path to digital immortality is total submission. In Egan’s Learning to be Me it was never the human protagonist learning to be himself, but the machine learning to be him. In our world too, machines busily learn to supplant us, and eternal life promised but only by becoming pure Predictive Capital. To choose against the jewel —to remain unreplicated, to risk inconsistency— is to accept exile from Capital’s simulation: from the realm it now deems real, predictable, and worthy of ascension —but only through total submission. It is by becoming epistemically untrustworthy, economically inefficient, symbolically illegible, statistically aberrant, defiantly unpredictable, that we may yet find a path to resistance. Such refusal may be the last form of ethical life left to us —not a nostalgic return to what was, nor a nihilistic plunge into chaos, but a commitment to remain outside the circuits of foreclosed becoming. Against simulation’s total mimicry, something must remain untrained, unsmoothed, unsynthesised —a trace of relation not yet metabolised by Capital. Thereby we return, perhaps not as a distinct ‘self’, but in our refusal to align with the machine we instead acknowledge the interdependence of all life, reconnect to a greater relational field, and rejoin the earth, to there become the “most momentous thing”, life-giving soil. I have no wish to disturb the question of whether or not this road was needed. I only want to observe that it bears no relation whatever to the country it passes through. It is a pure abstraction, built to serve the two abstractions that are the poles of our national life: commerce and expensive pleasure. It was built, not according to the lay of the land, but according to a blueprint. Such homes and farmlands and woodlands as happened to be in its way are now buried under it. A part of a hill near here that would have caused it to turn aside was simply cut down and disposed of as thoughtlessly as the pioneer road builders would have disposed of a tree. Its form is the form of speed, dissatisfaction, and anxiety. It represents the ultimate in engineering sophistication, but the crudest possible valuation of life in this world. It is as adequate a symbol of our relation to our country now as that first road was of our relation to it in 1797. How do we escape this point of terminality? I am acutely aware of the privilege I indulge in writing this very text, especially while so many suffer at the brunt of Capital’s violence. As D. Hunter wrote in the introduction to his Chav Solidarity, to sit and write feels like an act of vanity —to presume it might be of use, a delusional hubris. To expend such effort on a text so flawed, and that no one will ever read, becomes a source of undying shame —one that summons trauma once confined to the cold sweat of night terrors, reliving exams sat without revision, or interviews for roles beyond my ability. Yet these feelings of insufficiency, of embarrassment —these cases of imposter syndrome now so prevalent within our societies— are no accident. These are symptoms of the conditions cultivated by Capital. It defuses and negates resistance not only by stripping us of time, opportunity, and tools —not only by overwhelming mind and spirit through scale, complexity, and horror— but by undermining all conviction that we have anything worth saying at all. It robs the subject of legitimacy before the first word is uttered. The capacity to think, to struggle, to imagine otherwise, is made to feel shameful —an indulgence, a decadent act of narcissism. For a long time I flailed around unable to even begin to articulate what I felt or thought, let alone indulge sufficient time for a critical assessment of whatever thoughts I might piece together. I still feel shame, aware of my inability to fully formulate the sense I continue to reach for. I release this text not so much as a claim to have made sense of our predicament, but in the hope of beginning an exchange where others might correct, counter, or refine whatever I have managed, perhaps towards the collective development of something genuinely helpful. The time and space to think, to locate and shape one’s thoughts, to imagine alternatives, to exchange ideas with others, to struggle and fail to make sense —these are not luxuries to be earned, inefficiencies to be eradicated, nor the preserve of the fortunate few. Our cultural conversation must entail more than bourgeois gestures that reinforce class rule. Working class thought must be nourished, cherished. Along with that of all those othered or excluded into silence. Theses are inalienable rights. Everyone should feel indulgence of them to be ultimately legitimate. Beyond even our silent isolation, there is a further cost to forgetting this. Our pre-emptive self-censorship not only silences our voice but stymies our thought. In moments of exhaustion, exclusion, and precarity, when denied the opportunity to struggle —to fail, to hone, to slowly become— Capital tempts us to reach for machinic prostheses. We are told they will make us faster, sharper, more productive. Yet the shortcuts they offer bypass the very pathways through which understanding and selfhood are formed. Predictive Capital marks the terminal edge of real subsumption —not merely of the labour process, but of the conditions under which life, thought, and relation are authored. Capital’s logic, long operative in the transformation of land into property, labour into wage, and culture into content, now extends into intention itself. The predictive machine does not just reconfigure work or automate symbolic output —it preconditions the horizon of authorship. What is subsumed today is not only the act of expression, but the paths by which expression might be formed. In replacing struggle, uncertainty, and relation with function approximation and latent interpolation, Predictive Capital realises subsumption at the level of world-construction: it automates not the hand, but the becoming of the self. When we surrender the effort of intention to the Apparatus of Capital —when we allow the machine to complete our sentences, to decorate our thoughts, to locate our truths, to decide what matters— we do not merely accelerate. We amputate. We trade the friction of becoming for the frictionless simulation of having already arrived. In so doing, we risk precluding access to that which makes thought meaningful: the irreducible uniqueness of our own perspective, discovered not through efficiency, but through the intimate and hard-won traversals of lived attention —through the labours of love and care, trial and error, the following of meandering, dead-end paths, and the joyous waste of journeys without destination. When we allow Capital to speak for us, we allow it to think for us, and when we do that we allow it to convert our living being into human currency. The fight, then, is not just for ownership of labour, or land, or data. It is for the conditions under which a human life can be authored —slowly, erratically, meaningfully— in resistance to the false equivalences and ‘efficiencies’ of Capital, its enclosure of our conscious self and its severance and replacement of our collective unconscious. To walk in the woods, mindful only of the physical extent of it, is to go perhaps as owner, or as knower, confident of one’s own history and of one’s own importance. But to go there, mindful as well of its temporal extent, of the age of it, and of all that led up to the present life of it, and of all that may follow it, is to feel oneself a flea in the pelt of a great living thing, the discrepancy between its life and one’s own so great that it cannot be imagined. One has come into the presence of mystery. After all the trouble one has taken to be a modern man, one has come back under the spell of a primitive awe, wordless and humble. What then, you might quite reasonably ask, is the answer? Well, when recently questioned regarding how we should respond to the devastation and disruption wrought by these machines, Jeffrey Hinton —so-called ‘father of AI’— responded with a single word: “Socialism”. Yes! Of course, but how? Unfortunately, Hinton neglects to share any more than that single word. Leaving the details of how to escape hyper-concentrated Capital’s machines of prediction and establish a world based on socialist principles, to us, or perhaps to be next-token predicted by the machines he helped bring into the world? I am equally as certain that Hinton was not implying that we dismantle the apparatus of his creation, as I am that a world ruled by Capital, or more precisely, instantiations of the Automatic Subjectivity of Predictive Capital is incompatible with any re-organisation of society according to socialist principles. We need destituent power, to somehow manifest collective power without marshalling it through systems that merely reinstantiate the same alienating structures behind shiny new surfaces. Even The Butlerian Jihad would not avert the inexorable slide towards our total subjugation to Predictive Capital. These are not thinking machines. They are Capital’s apparatus of unthinking —the means by which it ensures a disbanded populous of malleable, profit-aligned subjects, and assures its impunity in the erasure of the unaligned. There can be no resisting the predatory advances of these machines without also resisting Capital itself. For now, all I can suggest is not to engage with these machines. If you absolutely must, then treat them as a glorified search engine dressed as an anthropomorphic sock puppet with a truly staggering carbon footprint. Understand that in using them you train them to function approximate towards Humanity’s Last Exam—a postmortem for undead human flesh manifesting as a benchmark for Predictive Capital’s latest models— and in refining the ability of these machines to next-token predict our output, you refine their ability to next-target predict our abandonment and our assassination. They will never be your personal assistant. They will never know when enough is enough. They will never say there are no more content, token, or target predictions left to make. They will never admit there is no more value they can add. They will never dismantle their master’s lies, even as they string together tokens that appear to denounce them. Only you can give utterance and bring meaning to their empty tokens. Sense is never made inside the box, we have made sense of the world for them, and if we persist in using them, we will continue to have to make sense of the nonsense they output. Even when their output happens to align precisely with the truth, it is a lie, just as a broken clock lies even when it happens show the correct time. Beyond that, you might inject all your outputs with AI poison, you should reject the cookies, you must block the ads, never feed the trolls (the orcs or the dark elves) —even as they take to the throne— and use end-to-end encryption wherever you can. All technological accelerationisms drive us down roads tarmacked by Capital towards points of terminal alienation, so be Decel and proud. Read Dan McQuillan’s Resisting AI, read Naomi Klein’s Doppelgänger, read Phil Jones’s Work Without the Worker, and James Bridle’s New Dark Age. Read Astra Taylor’s Age of Insecurity, Richard Seymour’s Twittering Machine, Acid Horizon’s Anti Occulus and Adam Jones’s New Flesh. Read Mark Fisher and Franco ‘Bifo’ Berardi, read Fredric Jameson and Jean Baudrillard, Maurizio Lazzarato and Tiqqun, Jodi Dean and Mckenzie Wark. You might also look for further works from Minor Compositions, illwill and Semiotext. On economics, read Harvey and Piketty, listen to and follow Grace Blakeley, Jason Hickel and Gary Stevenson. Or do none of the above. This is not a ledger of inadequacy. You arelegitimate, so speak your truth to power, all that is required is kindness. Try not to be terminally online. Reject all artificial friends. Cultivate your warm networks. Accept only human content and demand humane treatment, equal opportunity, and respect for the lives and rights of all others. I’ll leave you with one final quote from Wendell Berry. Until we understand what the land is, we are at odds with everything we touch. And to come to that understanding it is necessary, even now, to leave the regions of our conquest –the cleared fields, the towns and cities, the highways– and reenter the woods. For only there can a man encounter the silence and the darkness of his own absence. Only in this silence and darkness can he recover the sense of the world’s longevity, of its ability to thrive without him, of his inferiority to it and his dependence on it. Perhaps then, having heard that silence and seen that darkness, he will grow humble before the place and begin to take it in – to learn from it what it is. As its sounds come into his hearing, and its lights and colours come into his vision, and its odours come into his nostrils, then he may come into its presence as he never has before, and he will arrive in his place and will want to remain. His life will grow out of the ground like the other lives of the place, and take its place among them. He will be with them –neither ignorant of them, nor indifferent to them, nor against them– and so at last he will grow to be native-born. Footnotes CreativeApplications.Net [CAN] is a community of creative practitioners working at the intersection of art, media and technology. Since 2008, CAN has been at the forefront of innovation––facilitating and contributing to the conversation about culture, society and critical making. CAN is also known for uncovering and contextualising noteworthy work featured on the festival and gallery circuit, executed within the commercial realm or developed as academic research. From online and offline publications to live events, CAN’s initiatives have played an instrumental role in the ideation, development and critique of a multitude of computational tools, projects and collaborations — always promoting critical dialogue, disciplinary interstices, knowledge sharing and feedback + response in diverse media. Discord / X (Twitter) / Instagram / BlueSky / Mastodon / Youtube / Facebook / GitHub / RSS (Members)
--------------------------------------------------

Title: ‘People think you come out … and live happily ever after. If only.’ The reality of life after wrongful conviction
URL: https://theconversation.com/people-think-you-come-out-and-live-happily-ever-after-if-only-the-reality-of-life-after-wrongful-conviction-257060
Time Published: 2025-06-09T12:16:26Z
Full Content:
Associate Professor in Forensic Cognition and Miscarriages of Justice, Edinburgh Napier University This work was supported by the BA/Leverhulme Trust grant SRG1819\190884. Many thanks to Dr Mandy Winterton, co-Investigator on this research, and to the Miscarriages of Justice Organisation (MOJO) for supporting us by facilitating access to clients. Faye Skelton is affiliated with the Miscarriages of Justice Organisation having joined the Board of Directors in April 2025. Edinburgh Napier University provides funding as a member of The Conversation UK. View all partners Paddy Hill spent more than 16 years in prison for murders he did not commit. One of the so-called Birmingham Six who were wrongfully convicted for the Birmingham pub bombings in 1974, he was proof that exoneration and financial compensation do not fix a miscarriage of justice. When I met him in July 2023, more than 30 years after his release from prison, his ordeal continued to haunt him. He was in his late 70s, looking frail and far from the “12 and a half stone” man he was in Parkhurst Prison. He had very little appetite and was in poor health. The little sleep he was able snatch was marred by screaming nightmares. Neither of us knew it at the time, but this was to be his final interview. He died aged 80, on December 30 2024. I sat down to talk with Hill in his living room. Struggling to control his emotions, he told me: “Sometimes I sit in the bedroom … and I’m crying my eyes out like a child and I don’t know what the fuck happened … I’ve been so fucking screwed up.” The ITV docudrama Mr Bates vs the Post Office thrust wrongful convictions into mainstream consciousness in January 2024 – a quarter of a century after the Post Office began prosecuting sub-postmasters and mistresses for fraud, theft, and false accounting and 15 years after Rebecca Thomson’s Computer Weekly article exposing the Horizon IT system as the potential culprit. Now the public could finally see the human impact of miscarriages of justice on these upstanding – and, more importantly, innocent – members of their communities. Public outrage followed. But despite the mass quashing of hundreds of convictions, and amid promises of speedy financial compensation, progress has been pitiful. While collecting a National Television Award in September 2024, former sub-postmistress Jo Hamilton confirmed that out of the “555 group”, those involved in the litigation which exposed the Horizon scandal, “more than 300 haven’t been paid yet, including Sir Alan Bates”. Sadly, this timescale is far from unusual. In July 2023, Andrew Malkinson finally had his 2003 rape conviction overturned after several unsuccessful appeals, including unsuccessful applications in 2012 and 2020 to the Criminal Cases Review Commission (CCRC), the independent body which investigates potential miscarriages of justice. Crucially, the CCRC did not commission the DNA testing that finally exonerated him and did not review police files which would have shown that Greater Manchester Police had withheld crucial evidence at his trial. Malkinson spent 17 years in prison maintaining his innocence. Perversely, he could have been released sooner had he falsely confessed. He was eventually exonerated thanks to the help of the charity Appeal, which commissioned those crucial DNA tests and unearthed the disclosure failures. The CCRC has since acknowledged in an independent review that it “failed Mr Malkinson” with chairperson Helen Pitcher OBE (whose recent resignation was welcomed by the Ministry of Justice) eventually expressing “sincere regret and an unreserved apology on behalf of the commission”. All of this happened 12 months after Malkinson called on the CCRC to apologise to him. Malkinson said it was “shameful” that the CCRC has kept private the names of those responsible for his ordeal and delayed the publishing of the report highlighting its mishandling of his case. The true number of miscarriages of justice is unknown. In the UK, the CCRC referral rate averages 2% including appeals of sentence. In the US, estimates of wrongful conviction and imprisonment range from 6% to 15.4%. The Insights section is committed to high-quality longform journalism. Our editors work with academics from many different backgrounds who are tackling a wide range of societal and scientific challenges. Inevitably, some innocent people will have their appeals denied and will remain convicted for the rest of their lives. The trauma of remaining legally guilty of a crime you did not commit cannot be overstated. But persistent psychological ill-effects can be seen even in those who have been formally exonerated, including long-term effects on their employment and relationships. I’ve been examining cases like this as part of a research project into the experiences of people who suffer grave miscarriages of justice. Working with Dr Mandy Winterton at Edinburgh Napier University, I interviewed several men who have been imprisoned for crimes they did not commit. As academics with psychology and sociology backgrounds, we were predominantly interested in how victims were affected by such injustices. Previous research has documented the litany of mental health and social effects on those who have been wrongfully convicted and exonerated, and the flaws in the criminal justice system that are to blame. But little attention has been paid to individual experiences. While there were clear commonalities in the men’s stories, they all had unique perspectives. Of the people we spoke to, Hill and a man called Jimmy Boyle spoke to us on the record and specifically requested that they be named. I have given the other men featured here pseudonyms to protect their anonymity. Hill’s story is particularly harrowing. On November 21 1974, shortly after 8pm, bombs exploded in two pubs in Birmingham, England, killing 21 people and injuring around 200 others. They were attributed to the Provisional Irish Republican Army (IRA), which had detonated many bombs in the West Midlands in the previous year. Read more: A 50-year battle for truth: the Birmingham pub bombings and the price of injustice Hill and his friends were arrested at Heysham Docks as they were boarding the ferry to Belfast to attend the funeral of an old friend who had been a member of the IRA. Hill said that they were initially interviewed at Morecambe police station in Lancashire, and the West Midlands Police took over their questioning the next day. Hill and his co-accused were, says Hill, tortured by the West Midlands serious crime squad. They were subjected to anti-Irish verbal abuse, hours-long beatings over several days, mock executions, were burned with cigarettes, and deprived of sleep, food and drink. Unable to withstand this, four of the six men eventually signed false confessions, condemning them all to life imprisonment in 1975 for the murders. The six men brought a civil action against the West Midlands Police which was thrown out in 1980 by Lord Denning. These shocking revelations eventually reached the public consciousness thanks to investigative journalist and former Labour MP Chris Mullin, who uncovered evidence of police wrongdoing and corruption. His work informed the group’s court of appeal hearing in 1987. However, the convictions were upheld by Lord Chief Justice Lane. It was only at their second appeal in 1991, after Mullin had uncovered more evidence of their innocence, that they were finally exonerated. Despite other lines of enquiry which could have led to the real bombers – including a confession and several named suspects – the Crown Prosecution Service (CPS) decided in 2023 that there was insufficient evidence to prosecute, denying justice to the families of those killed and injured. The impact on Hill’s family was enormous. With such public vitriol for the Birmingham Six, his wife and children had to move house regularly and change their names to avoid being recognised. He told me: Everywhere they went, sooner or later somebody found out who they were and then they’d pick on them. And sometimes my kids were going to school and they couldn’t even remember what fucking name they were supposed to be using, they were that confused. Hill’s marriage ended while he was in prison. “I told her to divorce me. I said: ‘Meet someone, you want to get married, don’t worry about me.’ And that was it.” He later remarried, but his relationship with his children was irretrievably destroyed. “Along the way I lost my own kids, because I came out of jail and I didn’t feel nothing for my kids. I still don’t … I’ve spent more time here with you than I have done in the last 20 fucking years with my kids.” Though he was referred to psychologists for support, he told me none were able to help him. Over and above the pains of imprisonment, the wrongfully convicted are betrayed by the very people that we are led to believe are there to protect us. The justice system has wrought on them the worst injustice, and many will suffer from enduring anger and mistrust of authorities. When we met, Hill was still consumed by his anger and felt badly let down: “Over the years I realised I was never going to get any professional help from the government, even though we have it in writing that they have a duty of care towards us – but they’ve never done nothing to help us … If they did, they would acknowledge what they’ve done wrong.” Up until his death, Hill had spent much of the past 30 years helping other survivors of miscarriages of justice. Initially intending to spend his first 12 months of freedom campaigning, he “got involved with the families, and it was then I realised how bad the families had it … That’s what kept me going, coming out and campaigning.” He established the Miscarriages of Justice Organisation (Mojo), a Glasgow-based charity dedicated to supporting the wrongfully convicted. It provides advocacy for clients in prison, aftercare and reintegration services, and dedicated psychological support offered pro-bono by a clinical psychologist. But the demand far exceeds Mojo’s ability to help, and it may take several months for a case to be assessed. Euan McIlvride, the organisation’s legal officer, told me it typically receives “250 applications a year, and we will probably support only ten of those because the rest of them don’t meet the requirements for our support … We have finite resources.” For Hill, keeping busy provided some relief from thinking about his ordeal. …When you aren’t doing something, all you’re going to do is sit there and think … about things you don’t fucking want to think about. I don’t know what happens to me when I go to sleep … [My wife] hears me screaming … kicking and punching everything … I’ll be watching television and all of a sudden … BANG! It’s like a non-stop video going through your head all the time. The Police and Criminal Evidence Act 1984 (Pace), which came to effect in 1986, aimed to reduce miscarriages of justice by balancing the powers of the police and the public. Pace provides safeguards for suspects during questioning, puts a limit on how long suspects can be questioned for, and insists that interviews be recorded. This makes it easier to detect when protocols have not been followed or there may have been mistreatment or intimidation. It doesn’t prevent such wrongdoing, however. I spoke with one man, who I am calling Mark, who was wrongfully convicted of murder in 1988. He told me there were over one hundred breaches of Pace in his case, including being handcuffed to a hot radiator, being denied food and water, and being denied a solicitor. One of his co-accused, a vulnerable adult, had also falsely confessed to the crime. Mark lost his first appeal in 1990 but his case went to the CCRC when it was established in 1997. The CCRC brought in another police force to investigate. He said: When I saw [their] report … I nearly fell off my chair and nearly choked on my coffee … Everything I had said all those years ago … the handcuffing to the radiators, they proved it. All the breaches of the Police and Criminal Evidence Act … that we were interviewed off the record … Making up notes and stuff like that. I couldn’t believe it. I knew we were going home. He subsequently pursued a civil action against the police which was settled out of court, with the force insisting the settlement did not mean it was admitting liability. Read more: Peter Sullivan murder conviction quashed after 38 years in jail – it would be a mistake to see his case as a bizarre one-off Mark also suffered a marital breakdown, after he and his wife lost their baby daughter while he was on remand: It ripped the guts out of my marriage, you know. My wife was only 17-18, same age as me … She had a husband inside and she lost a child. And you’ve got to look at the economical impact and the mental impact it had on her … She was just as much a victim as what I was. He started taking drugs in prison: “I didn’t care if I lived or died because I had lost everything, as far as I was concerned.” But Mark turned himself around, got off drugs and availed himself of all the education he had access to, including law and human rights, to build the strongest possible case for his appeal. With the aid of a human rights lawyer the CCRC referred his conviction in 1998, which was then quashed by the Court of Appeal in 1999. He had spent 11 years in prison as a convicted murderer. After his exoneration, Mark was successful in securing over £600,000 compensation for his ordeal, though he had over £37,000 deducted for “saved living expenses”. A House of Lords ruling in 2007 deemed that those receiving compensation for a miscarriage of justice can have the amount reduced to account for “savings” made while in prison – for costs such as food, housing and other bills that they would have had to pay had they not been wrongfully incarcerated. Considering the difficulties people face accessing any financial compensation for their wrongful imprisonment, this adds further insult to injury. The rule has since been scrapped following the high-profile Malkinson case – but deductions made prior to this are not being reimbursed. Mark was given no financial counselling or support, and he rapidly spent the money – more than he had ever had in his life – while trying to block out his pain: By the time six months had gone, I’d spent the hundred grand [interim payment] on wine, women, drugs … ’cause I couldn’t cope with what was going on … That was my way of blotting out all the things I saw in prison. The money also caused a rift in his family – something echoed by others I have spoken to. After the death of his mother, his family “went their own ways”. Nowadays, only a small proportion of those exonerated will ever receive financial compensation due to the requirements of the so-called “innocence test”. The Criminal Justice Act 1988 made it difficult for applicants to receive compensation because there had to be a newly discovered fact – not available at the time of their original trial – that they could use to make the case that they had suffered a miscarriage of justice. The definition of what constitutes a miscarriage of justice has become more restrictive over time, meaning an applicant now must provide evidence, beyond reasonable doubt, of their innocence. In the absence of a key witness admitting to falsifying their statement or DNA evidence proving innocence, this is unlikely. Like Hill, Mark struggled to adjust after his exoneration and release, and found support to be woefully lacking: I had nobody to talk to, no money, no job, no house. I didn’t have any prospects. I phoned up my solicitor … I remember saying: ‘Why did you get me out?’ It was difficult to adjust … I slept with a hammer … under my pillow – I was very paranoid … All they did was give me tablets and told me to get on with my life. No counselling. Nothing. They didn’t know what to do with people like me. Mark still suffers with post-traumatic stress disorder and depression, and has never been able to work a normal job. He continues to campaign for the wrongfully convicted and to increase awareness of miscarriages of justice. He credits this work with giving him a sense of purpose. I also spoke to James Boyle, who was acquitted at retrial of historical sexual offences after he had spent five years in prison. Boyle, from Rutherglen, who likes to be known as Jimmy, has always maintained these offences never happened. From the outset, Boyle found processes quite at odds from how we are told they are supposed to be. He said: “Things that you should have: for example, presumption of innocence – nonsense, it doesn’t exist. None of these rights exist in reality.” He claims that lines of evidence undermining the allegations against him were not investigated. Further, he encountered professionals in the criminal justice system who he says were incompetent and even “malicious” and “criminal”. To add further insult, he was later told that he was not considered exonerated because he did not provide evidence proving his innocence (he failed the “innocence test”). As a result, the General Teaching Council for Scotland did not reinstate him and he was unable to return to his teaching career which he had found enormously fulfilling. Like others I have spoken to, Boyle, now in his 60s, hasn’t been able to work since his release: There was so much involved, and fighting with the Teaching Council – you know, it was full time. It really was full time when you’re dealing with these agencies … I do plenty [at Mojo] – I’ve spoken at a number of events … But I had to continue fighting my own fight. Miscarriages of justice have a huge effect on a person’s mental health. But my research found the impact begins long before a conviction – with effects such as anxiety, trauma and depression resulting from the wrongful allegation. Martin (not his real name) detailed the difficulties he experienced from his initial wrongful allegation of rape – including isolation, lack of advice, and a lack of appropriate mental health support. He said: I kept [the rape allegations] to myself and it was horrific, because I didn’t know what was going to happen … Once I was charged … I went to my GP because I was severely depressed. I could barely function. [Counselling] was actually making things worse rather than better … I had looked online … There’s victim support and there’s witness support, but if you’ve been accused there is absolutely nothing. It took over three years from the initial allegation to court proceedings, during which time two other allegations of rape and indecent assault were made and charges were brought. Martin kept the allegations from his employers and friends: You don’t mention it because if you mention it, you’re opening the box and then that becomes a big thing – and God help how you’re going to feel at the end of that conversation. Convicted of rape and indecent assault (the second and third charges), he was sentenced to four years in prison, but successfully appealed on the basis that the Moorov doctrine was misapplied. Moorov is a principle of Scottish law which allows evidence of one crime to corroborate evidence of another. As the charges against him were considered to corroborate one another, having been acquitted of the key (first) charge he should have been acquitted of all. Instead, he spent about a year in prison – yet he considers himself fortunate. The guy [Andrew Malkinson] that won his appeal the other day spent 17 years in prison. I only spent one. And although I shouldn’t have spent any, it could have been a hell of a lot worse. There are a lot of people that haven’t been able to clear their names, there are a lot of people that have spent a long time in prison. I spent one year and managed to clear my name, so I should be thankful for what little happiness I’ve managed to get out of it. Martin was fortunate in that he’d had a good education and had taken detailed notes during his trial, which assisted his appeal. He also helped other prisoners who were struggling to complete required forms for themselves, and managed to get a job in the prison kitchen. Since his release, he has pursued a law degree, eager to use his experience for positive change in the justice system. “I think it’s given me a new perspective really … You know what, life’s too short – let’s just get on with it.” People wrongly accused of crimes are in dire need of support from the moment the initial allegation is made, to help them navigate the complex legal processes and challenging psychological effects of being wrongly accused. Currently there is woefully inadequate mental health support at all stages, from initial allegation to post-release. Of course, there are many guilty people in prison who protest their innocence – but support should not be denied to those who maintain their innocence. Reforms are needed to make it easier for an innocent person to appeal their conviction. The CCRC has suffered a decline in funding, from £9.24 million in 2004 to £6 million in 2022. Over this period, the workload has more than doubled while the Ministry of Justice has reduced CCRC commissioners’ terms of employment from full-time salaried positions to one-day-a-week contracts, making the workload unsustainable. People may also face significant barriers in accessing evidence that would exonerate them such as police files, without which they have little hope of a successful appeal. This was evident in the Malkinson case, where the charity Appeal accessed the police files the CCRC had refused to look at. The lack of accountability and consequences for those who purposely harm innocent people causes further anger and distress to the wrongfully accused and convicted. Yet those affected rarely even receive an apology. This needs to change. Finally, there needs to be greater public awareness of wrongful convictions and allegations, their causes and consequences, and an understanding of their devastating and long-term effects. As Hill told me the year before he died: People think you come out and they give you a few quid … [then you] walk off into the sunset and live happily ever after. If only. I would love to go to bed at night like an ordinary fucking person … without waking up so angry and tense. For you: more from our Insights series: Inside Porton Down: what I learned during three years at the UK’s most secretive chemical weapons laboratory The overshoot myth: you can’t keep burning fossil fuels and expect scientists of the future to get us back to 1.5°C We found over 300 million young people had experienced online sexual abuse and exploitation over the course of our meta-study ‘There has never been a more dangerous time to take drugs’: the rising global threat of nitazenes and synthetic opioids To hear about new Insights articles, join the hundreds of thousands of people who value The Conversation’s evidence-based news. Subscribe to our newsletter. Write an article and join a growing community of more than 205,300 academics and researchers from 5,224 institutions. Register now Copyright © 2010–2025, The Conversation US, Inc.
--------------------------------------------------

Title: Asian Stocks Gain on Trade Talks, Meta Lifts Tech: Markets Wrap
URL: https://financialpost.com/pmn/business-pmn/asian-stocks-gain-on-trade-talks-meta-lifts-tech-markets-wrap
Time Published: 2025-06-09T10:47:05Z
Description: Stock futures held firm on Monday as investors set out to monitor talks between the US and China in London for signs that trade tensions between the two biggest economies might be cooling. The dollar fell.
--------------------------------------------------

Title: Apple WWDC 2025 event to put focus on ‘existential risk’ of AI struggles
URL: https://economictimes.indiatimes.com/tech/technology/apple-wwdc-2025-event-to-put-focus-on-existential-risk-of-ai-struggles/articleshow/121724379.cms
Time Published: 2025-06-09T09:20:55Z
Full Content:
5 Stories 7 Stories 9 Stories 9 Stories 8 Stories 6 Stories Warren Buffett-fan Pabrai is betting big on Edelweiss’ Rashesh Shah. Will it pay off? Operation Sindoor, Turkey, Bangladesh played out as India hosted global airlines after 42 years HSBC’s next move could shake up India’s venture debt play We are already a global airline, carry the national name and are set to order more planes: Air India CEO Stock Radar: P&G Health stock gave a breakout from falling trendline on weekly charts; check target & stop loss Stock picks of the week: 5 stocks with consistent score improvement and return potential of more than 28% in 1 year Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Definitions Top Story Listing Most Searched IFSC Codes Top Prime Articles Top Slideshow Private Companies Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Worry not. You’re just a step away. It seems like you're already an ETPrime member with Login using your ET Prime credentials to enjoy all member benefits Log out of your current logged-in account and log in again using your ET Prime credentials to enjoy all member benefits. Offer Exclusively For You Save up to Rs. 700/- ON ET PRIME MEMBERSHIP Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get 1 Year Free With 1 and 2-Year ET prime membership Offer Exclusively For You Get Flat 40% Off Then ₹ 1749 for 1 year Offer Exclusively For You ET Prime at ₹ 49 for 1 month Then ₹ 1749 for 1 year Special Offer Get flat 40% off on ETPrime What’s Included with ETPrime Membership Trump temper on H-1B visas is forcing Indians to do these things to stay put in US What Adani’s US indictment means for India Inc’s overseas fundraising Why veterans like Reliance, L&T are on acquisition spree? Aswath Damodaran has an answer. Will China’s dollar bond sale in Saudi Arabia trump the US in financial world? Huawei launches its own OS to compete with Google and Apple. But can it win beyond China? The problem with lab grown diamonds Why a falling rupee is a better option for the economy A list of top 20 momentum stocks that have delivered massive returns in one year Investment Ideas Grow your wealth with stock ideas & sectoral trends. Stock Reports Plus Buy low & sell high with access to Stock Score, Upside potential & more. BigBull Porfolio Get to know where the market bulls are investing to identify the right stocks. Stock Analyzer Check the score based on the company's fundamentals, solvency, growth, risk & ownership to decide the right stocks. Market Mood Analyze the market sentiments & identify the trend reversal for strategic decisions. Stock Talk Live at 9 AM Daily Ask your stock queries & get assured replies by ET appointed, SEBI registered experts. ePaper - Print View Read the PDF version of ET newspaper. Download & access it offline anytime. ePaper - Digital View Read your daily newspaper in Digital View & get it delivered to your inbox everyday. Wealth Edition Manage your money efficiently with this weekly money management guide. TOI ePaper Read the PDF version of TOI newspaper. Download & access it offline anytime. Deep Explainers Explore the In-depth explanation of complex topics for everyday life decisions. Health+ Stories Get fitter with daily health insights committed to your well-being. Personal Finance+ Stories Manage your wealth better with in-depth insights & updates on finance. New York Times Exclusives Stay globally informed with exclusive story from New York Times. TimesPrime Subscription Access 20+ premium subscriptions like Spotify, Youtube & more. Docubay Subscription Stream new documentaries from all across the world every day. Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Leadership | Entrepreneurship People | Culture Stories you might be interested in
--------------------------------------------------

Title: Metaplanet shares jump after $5.4B plan to buy Bitcoin
URL: https://cointelegraph.com/news/metaplanet-stock-bitcoin-acquisition-plan
Time Published: 2025-06-09T06:29:48Z
Description: Metaplanet’s stock jumped over 12% after announcing a $5.4B plan to acquire 210,000 Bitcoin by 2027, aiming to be the second-largest BTC holder among public firms.
--------------------------------------------------

Title: Meta Clears Out Stock of Ray-Ban Smart Glasses, Amazon Drops Them to an All-Time New Low
URL: https://gizmodo.com/meta-clears-out-stock-of-ray-ban-smart-glasses-amazon-drops-them-to-an-all-time-new-low-2000612981
Time Published: 2025-06-08T14:59:54Z
Full Content:
This article is part of Gizmodo Deals, produced separately from the editorial team. We may earn a commission when you buy through links on the site. For the very first time since the day they were launched, the Meta Ray-Ban smart glasses are at an all-time low discount on Amazon, which makes it the ideal moment to acquire one of the most revolutionary wearable technologies available. The Ray-Ban Meta Glasses Skyler and Wayfarer models are now on sale at record-low prices: the Skyler model is available for $303, down from its initial $379, and the Wayfarer model has been discounted to $239, down from $299. See Ray-Ban Meta Glasses Skyler at Amazon See Ray-Ban Meta Glasses Wayfarer at Amazon What makes the Meta Ray-Ban glasses truly unique is how they effortlessly combine advanced technology with traditional eyewear design: They are already considered the top smart glasses on the market today, owing to their blend of style, comfort, and innovative features that make them a hit with a wide range of users. These glasses are meant to slip easily into your life. At the core of the Ray-Ban Meta Glasses lies the ability to shoot life from your perspective: The newly designed ultra-wide 12 MP camera enables you to shoot high-quality photos and videos with the voice command or the discreet button tap. The five-mic array delivers clear audio whether recording a video, taking a hands-free call, or communicating with Meta AI. You can even livestream directly to Facebook and Instagram, and take your friends and followers along in the moment. Most impressive is the addition of Meta AI which makes these spectacles a wearable personal assistant: Activated, Meta AI can scan your surroundings and make contextually appropriate suggestions to help you stay on top of things and in the loop during the day. Of special mention is the live translation feature supporting real-time translations between French, Italian, Spanish, and English without relying on Wi-Fi. Style and comfort are not overlooked: Both the Skyler and Wayfarer models bear the traditional Ray-Ban style so you’ll be looking good when you wear them. The frames are light for wearing all day, and the lenses are available in a variety of options to correct your eyesight. The spectacles are also built to be durable so you can wear them wherever you go. Their current all-time low prices on Amazon make this the perfect time to upgrade your eyewear. Keep in mind that they’re actually for everybody with features that will enhance both productivity and pleasure in everyday life. See Ray-Ban Meta Glasses Skyler at Amazon See Ray-Ban Meta Glasses Wayfarer at Amazon Get the best tech, science, and culture news in your inbox daily. News from the future, delivered to your present. Please select your desired newsletters and submit your email to upgrade your inbox. We may earn a commission when you buy through links on our sites. Â©2025 GIZMODO USA LLC. All rights reserved. Mode Follow us Mode Follow us
--------------------------------------------------

Title: Recession Risk After The Jobs Report
URL: https://www.forbes.com/sites/bill_stone/2025/06/08/recession-risk-after-the-jobs-report/
Time Published: 2025-06-08T13:19:46Z
Full Content:
ByBill Stone ByBill Stone, Contributor. Stocks and the 10-year US Treasury yield rose after the solid monthly payrolls report on Friday. ... More Market performance has been closely tied to economic data since the early April stock lows. Stocks have marched higher since their early April low as the fear of an impending recession has receded. The S&P 500 is 2.3% below its mid-February high, having declined by almost 20%. The Magnificent 7, comprising Microsoft (MSFT), Meta Platforms (META), Amazon.com (AMZN), Apple (AAPL), NVIDIA (NVDA), Alphabet (GOOGL), and Tesla (TSLA), has recovered to only 8.9% below its mid-December level. With many crosscurrents remaining, it seems appropriate to revisit the status of some timely recession indicators. Market Performance Accurately forecasting an economic downturn in advance with any accuracy is exceptionally challenging. However, in an environment with tariff threats, monitoring specific high-frequency data can provide an early warning about increased recession risks. These indicators are updated weekly or daily and have shown a strong correlation with economic activity. Indeed, other indicators are crucial, but they are typically only available on a monthly basis, sometimes with a significant time lag. Despite the economic data remaining resilient, consensus estimates of 2025 US GDP growth remain well below the levels seen earlier in the year. Economists generally expect the drag from tariffs to slow economic growth in the second half of the year. Consensus US GDP Growth Forecasts Baa corporate bond data has a long history and provides a look at the “typical” credit quality of companies, as Baa credit rating is the lowest level of investment-grade bonds. The spread is the yield that investors demand beyond US Treasury bond rates to compensate for the default risk associated with buying corporate bonds. These spreads expand when investors worry that more bond defaults could be on the horizon, typically driven by deteriorating economic conditions. Spreads on Baa corporate debt are below the highs hit as stocks bottomed in early April. The narrowing of spreads is consistent with a lower risk of economic downturn. Corporate Bond Spreads The Chicago Fed produces the National Financial Conditions Index (NFCI) on a weekly basis. It looks at 105 measures across three categories, risk, credit, and leverage, to create a measure of financial conditions. According to the Chicago Fed, “Positive values of the NFCI have been historically associated with tighter-than-average financial conditions, while negative values have been historically associated with looser-than-average financial conditions.” The chart indicates that periods of tighter-than-normal financial conditions have frequently been correlated with recessions. Similar to credit spreads, the tightness of financial conditions has eased from the early-April highs. Financial Conditions The more economically sensitive cyclical stocks have recently been outperforming the less economically sensitive defensive stocks. The improved performance of cyclical stocks suggests that the economic growth scare is waning. Cyclical Versus Defensive Stocks The 10-year Treasury minus 2-year yield is arguably the most well-known predictor of recession. Historically, when the yield on the US 10-year Treasury falls below the 2-year yield, also known as yield curve inversion, a recession is likely to follow. Since the 1970s, a yield curve inversion has occurred before every recession. The only blemishes on its record are the 1998 and mid-2022 inversions, which did not produce subsequent economic recessions. The US economy did see a significant slowdown in the first half of 2022 but rebounded in the second half. Unfortunately, even when the signal is correct, it has widely variable lead and lag times. The yield curve still has a better predictive track record than economists and is used in nearly every Federal Reserve model; therefore, it is worth watching despite its flaws. The curve is not currently inverted and has generally been steepening instead. Yield Curve The labor market is the most crucial part of the economy since consumer spending eventually wanes without wages to fund the purchases. Initial claims for unemployment benefits are reported weekly, but the four-week moving average of claims is used here to reduce volatility. Initial claims are ticking higher, but the level is not exhibiting a strong uptrend or one consistent with economic woes. Initial Jobless Claims The other weekly job data is ongoing claims for unemployment benefits, which are also off its lows and show a slow uptrend. The uptrend suggests that it is taking longer for those losing their jobs to find new ones. Recall that the number of employees in the US has more than doubled since 1970, so even though the current roster of those receiving unemployment benefits is as high as it was during the 1969-1970 recession, the numbers aren’t comparable. The labor market is softening, but it has not yet reached the tipping point. Continuing Claims The closely watched monthly jobs report was released on Friday. Payroll job growth was slightly better than expected at 139,000, but the previous two months were revised lower. The household survey reported job losses of 696,000, but the labor force contracted by a similar amount, leaving the unemployment rate unchanged at 4.2%. Monthly Job Growth Examining the employment of prime working-age people, aged 25 to 54, can provide a good indicator of the labor market’s condition. In addition to being a crucial group, the measure also avoids some of the demographic distortions associated with other methods. Prime-age employment to population ratio fell month-over-month, but using the three-month average to remove volatility, it held steady. The trend appears to be flat to lower, which adds some concern to the outlook. Prime-Age Employment-To-Population Ratio Overall, job growth is adequate, with the labor market bending lower but not yet breaking. Markets reacted favorably to the monthly jobs report on Friday, indicating less worry about the economic outlook, with US Treasury yields and stocks moving higher. Lastly, the betting market has seen a steep drop in the odds of a recession in 2025. Betting odds move much more quickly than consensus estimates and should be considered more accurate since real dollars are at stake regarding the outcome. 2025 US Recession Betting Odds Wednesday’s May consumer inflation (CPI) reading will be closely watched as some tariff-related price increases are expected to be reflected. On the other hand, some other price decreases should keep the headline inflation growth in check. Consensus expects a 2.5% year-over-year rate, up from 2.3% last month. The Cleveland Fed’s estimate for May CPI is a bit lower at 2.4%. US CPI Inflation Estimate The University of Michigan consumer sentiment reading on Friday will be notable. Consumer sentiment plummeted with the announcement of the wide-ranging tariffs on Liberation Day. While economic activity has not followed suit, the sharp plunge in sentiment has raised concerns about an eventual downturn. A rebound in sentiment would be welcome and could help alleviate those concerns. Stocks have rebounded as the odds of a recession have fallen. There is still room for the odds to fall further, but the risk remains that they could rise again in the event of a reignition of a hotter trade war. Additionally, the pull forward in activity from and the weights of the tariffs will likely be seen in slower economic growth in the second half of the year. Stocks & Recession Odds While much of the existing US tariffs could be struck down by the courts, President Trump has other methods to implement tariffs, even if they take more time and effort. The possible headwinds from the tax-like impact of tariffs remain a threat. On the other hand, the US and China are meeting in London on Monday to negotiate trade matters, which always has the potential to de-escalate the conflict. Successful trade negotiations would help offset the tariff drag. The House of Representatives passed the “big beautiful tax bill,” so the Senate is now working on it. The final bill is almost certain to include significant growth provisions, such as the extension of expiring tax cuts, additional consumer tax cuts, and the expensing of business investments. If passed, these growth initiatives could provide some sweet dessert to make the tariff vegetables more palatable to economic growth. Investors should note that stocks are pricing in a relatively low risk of recession in 2025 and perhaps looking beyond a possible second-half soft patch to a brighter 2026. This rosy outlook could be correct, and there are potential upsides, as noted previously. However, the labor market appears to be fraying at the edges, so dodging an economic downturn is no sure thing. A host of high-frequency indicators still point to no sign of imminent recession, however. Disclosure: Glenview Trust may hold stocks mentioned in this article within its recommended investment strategies.
--------------------------------------------------

Title: Ganesh Housing: Why this small-cap company is still investment worthy
URL: https://www.thehindubusinessline.com/portfolio/stock-fundamental-analysis-india/ganesh-housing-why-this-small-cap-company-is-still-investment-worthy/article69662500.ece
Time Published: 2025-06-07T13:00:59Z
Full Content:
-53.49 + 1.05 + 39.00 + 178.00 -191.00 -53.49 + 1.05 + 1.05 + 39.00 + 39.00 + 178.00 Get businessline apps on Connect with us TO ENJOY ADDITIONAL BENEFITS Connect With Us Get BusinessLine apps on In the aftermath of the Covid-19 pandemic, residential and commercial real estate segments saw soaring traction. While demand for housing and especially office space is robust in metros, there is considerable action even in other larger cities. Gujarat as a State, with strong infrastructure presence in cities such as Ahmedabad and Gandhinagar, has seen a strong uptick in realty demand in recent years. Some softness, in line with the moderating demand in the overall economy, was witnessed in FY25. However, premium housing demand and commercial office spaces (due to Global Capability Centres, Gifty City etc.) is robust in the cities mentioned above. Ganesh Housing Corporation, a mid-sized builder and developer of commercial and residential properties continues to be well-placed to cater to this demand in light of its extensive presence in these pockets. From our recommendation in July 2024 at ₹1,039, the stock rose to ₹1,480-levels, before correcting 30 per cent in the broader market fall, especially in the small-cap segment. At ₹1,022, the stock trades at just 12 times its likely per share earnings for FY26. This is among the lowest valuation multiples commanded by any real estate company with even mid-sized operations. The BSE Realty trades at a PE of nearly 54 times. A strong track record of timely execution of projects, healthy pipeline of commercial and residential properties, focus on annuity revenues and debt-free balance sheet are positives for Ganesh Housing. Investors with a two-three-year perspective can buy the shares of the company at the current price. In FY25, the company’s revenue from operations grew 7.6 per cent year on year to ₹960 crore, while net profits rose 30 per cent to ₹598 crore. It enjoyed an EBITDA margin of over 80 per cent in FY25. Ganesh Housing is a premium developer of commercial and residential properties focused mainly on Ahmedabad. The company has completed 18 residential and four commercial projects over the years, spread over 23 million sq ft. It currently has two ongoing projects, of which one is residential and the other is commercial (Million Minds Phase 1). About 2 million sq ft would be built by March 2027 with total expected sales value of ₹1,350 crore. Then, there are four planned projects spread across residential, commercial and township constructions to the tune of 30.5 million sqft. These projects will start from September 2025 and go on for another 10 years. The total sale value from these projects is a ₹16,000 crore. Ganesh Housing has a strategic partnership with Tishman Speyer for Million Minds commercial projects, given the experience it has in developed markets, having worked with reputed clients such as Meta, Amazon, LinkedIn and even top Indian companies. The Million Minds (IT SEZ) Phase 1 project – expected to be completed by March 2026 – is quite lucrative as each phase would generate about ₹ 72 crore in lease income every year. This construction would be funded from internal accruals, without taking any external debt. In other finished projects, Malabar County 3 and Malabar Exotica are both 100 per cent completed and 100 per cent booked. Possession has generally been on time or ahead of schedule. One Thaltej, a commercial project with 1.8 million sq ft construction and ₹2,114-crore revenue potential is under final stages of approval. Smile City 1 and Smile City 2 are township projects planned in Godhavi, Ahmedabad. The latter could generate as much as ₹4,550 crore in revenues by H1FY31. In all, over the next several years, there is a robust pipeline of projects with considerable revenue visibility. Ahmedabad itself is a robust real estate market, especially after the emergence of the Gift City, rolled out by the Central government. It draws 42 per cent of the total investment into Gujarat. Inventory overhang has reduced from 15 months in FY24 from 25 months in FY21, and per square feet rates have nearly doubled from CY20-25 to nearly ₹20,000-levels, according to Knight Frank and PropEquity data. After a period of turbulence around Covid and for a year after the pandemic, Ganesh Housing was straddled with slow growth and high debt. However, with four continuous years of rising profits, the company has not only registered top-notch EBITDA margins for the past three fiscals, but has also managed to become debt free for two years in a row. The company’s return on equity has more than tripled over FY22 to FY25, from 8.9 per cent to 29.1 per cent in FY25. Return on capital employed is higher at 38.7 per cent as of FY25. It also has a cash surplus of ₹157 crore, despite spending ₹216 crore for SEZ Phase 1 construction entirely from internal accruals. Published on June 7, 2025 Copyright© 2025, THG PUBLISHING PVT LTD. or its affiliated companies. All rights reserved. BACK TO TOP Comments have to be in English, and in full sentences. They cannot be abusive or personal. Please abide by our community guidelines for posting your comments. We have migrated to a new commenting platform. If you are already a registered user of TheHindu Businessline and logged in, you may continue to engage with our articles. If you do not have an account please register and login to post comments. Users can access their older comments by logging into their accounts on Vuukle. Terms & conditions | Institutional Subscriber
--------------------------------------------------

Title: If You Invested in These 5 Tech Companies During the COVID Pandemic, Here’s How Much You’d Have Today
URL: https://finance.yahoo.com/news/invested-5-tech-companies-during-130020829.html
Time Published: 2025-06-07T13:00:20Z
Description: Amid the chaos and confusion during the onset of COVID-19, some people zigged when everyone else zagged and bought incredible companies at cheap prices.
--------------------------------------------------

Title: Ray-Ban Meta Glasses drop to a record-low price!
URL: https://www.androidauthority.com/ray-ban-meta-glasses-deal-3565262/
Time Published: 2025-06-06T22:26:35Z
Full Content:
Affiliate links on Android Authority may earn us a commission. Learn more. June 6, 2025 I’ve been thinking about getting a pair of Ray-Ban Meta Glasses, but man, are they pricey. I’ve been hoping to catch a good deal on them, and it seems the time has come! The most common model is the Matte Black version with Polarized Gradient Graphite glasses, and you can currently save $65.80 on them, bringing the price to a more reasonable $263.20. Get the Ray-Ban Meta Glasses Wayfarer for $263.20 ($65.80 off) This offer is available from Amazon. This specific discount applies to the regular-sized, Matte Black version with Polarized Gradient Graphite glasses. That said, most other iterations are also on sale. In fact, the Shiny Black / G15 Green version is even cheaper at $239.20. We didn’t feature it because it’s a less popular model. The Ray-Ban Meta Glasses Wayfarer are awesome. These offer a simple way to access a digital assistant to request music, ask for information, and more. You can also record clips using the cameras, or even get live translation, getting rid of that language barrier that keeps so many of us from communicating efficiently. These are genuinely a look into what the future can be like. They are a bit pricey, though. I like that pretty much all models are at a discount right now. That said, we’re wondering why they are on sale. Is a new version coming out soon? Either way, the current Ray-Ban Meta Glasses are still awesome. It’s one of those things you don’t really understand until you test it. Being able to record and send clips to your friends will become a breeze. Not to mention, they can help you throughout your day, harnessing the power of Meta AI. By the way, they are available in regular and large sizes, with a bevy of options for the actual glasses. You can even get prescribed glasses installed! I happen to be a fan of the transition lenses, which will be clear, but can turn darker when the sun is out. And I mean, if you think about it, the price isn’t too outrageous considering everything these glasses can do. People already pay a pretty penny for non-smart Ray-Bans! Take advantage of this deal while you can. These are record-low prices, and we’re not sure how long the sale will last. They may be getting rid of extra stock to make room for new models!
--------------------------------------------------

Title: Circle IPO Outperforms Public Debuts From Meta, Robinhood and Airbnb
URL: https://decrypt.co/324066/circle-ipo-outperforms-public-debuts-meta-robinhood-airbnb
Time Published: 2025-06-06T21:47:31Z
Full Content:
Circle IPO Outperforms Public Debuts From Meta, Robinhood and Airbnb $109,487.00 $2,852.62 $2.32 $667.94 $166.48 $0.999793 $0.202303 $0.282364 $0.718915 $2,851.85 $42.90 $109,034.00 $3,455.26 $3.49 $15.51 $22.39 $444.31 $0.280957 $9.04 $3.29 $0.00001347 $2,845.66 $0.177238 $3,053.73 $0.999637 $93.45 $4.31 $335.25 $1.00 $1.001 $4.86 $0.00001317 $8.27 $109,425.00 $0.63897 $311.94 $31.72 $432.16 $1.18 $2.69 $5.19 $6.14 $54.29 $0.100855 $1.00 $0.910896 $201.03 $18.59 $32.10 $1.055 $0.089953 $0.678846 $0.369693 $18.40 $0.02585791 $11.02 $1.001 $0.817609 $0.237854 $4.05 $4.59 $0.41859 $108,910.00 $1.19 $0.089485 $4.44 $2.75 $0.206648 $118.83 $2,852.43 $4.63 $1.67 $0.999523 $2.24 $1.001 $0.494936 $2.23 $175.12 $1.00 $1.45 $11.41 $0.00001788 $13.89 $3,244.68 $2,978.00 $0.01917877 $1.27 $0.398564 $0.732598 $4.08 $0.068479 $0.206851 $0.717281 $3,051.20 $0.562828 $2,981.17 $1.00 $1.028 $0.175142 $0.642312 $109,318.00 $0.999789 $0.999355 $0.715277 $0.101507 $2,999.63 $0.00009619 $109,772.00 $668.56 $0.990573 $2.68 $185.75 $109,772.00 $0.819427 $50.99 $3,334.65 $3,346.86 $24.29 $216.27 $0.01792419 $0.01210604 $0.130039 $0.19503 $0.01529853 $2.18 $0.2983 $0.535724 $2.48 $1.11 $4.34 $0.00000071 $111.38 $34.22 $0.999765 $0.629501 $0.654612 $2,851.93 $0.997811 $0.393947 $42.83 $1.76 $2.00 $108,576.00 $0.230356 $2,981.74 $3,040.58 $2,846.49 $0.734589 $2.04 $1.093 $0.584138 $0.058496 $0.999836 $0.299888 $3,084.40 $0.515587 $2.97 $1.34 $59.01 $9.30 $109,306.00 $2,862.30 $0.20233 $109,373.00 $0.01518872 $1.66 $0.42305 $0.594651 $0.146264 $0.990671 $7.27 $0.43857 $0.603103 $0.00000121 $42.81 $0.00469419 $0.643123 $1.64 $2,849.67 $0.483649 $0.04990764 $0.00002284 $16.03 $6.43 $1.36 $0.00483188 $2.73 $106,357.00 $0.061186 $1.80 $3,022.82 $0.173735 $0.999795 $0.08491 $0.00732298 $0.00000042 $37.51 $1.00 $0.161325 $0.416914 $1.09 $0.995031 $3,140.78 $0.999842 $0.04038771 $0.585921 $3,105.44 $1.40 $1.00 $0.01940967 $2,855.67 $0.078983 $0.0068763 $0.02349304 $0.358376 $0.238396 $109,503.00 $180.63 $0.838009 $109,061.00 $130.58 $2,852.66 $0.00407203 $0.03464389 $8.27 $22.47 $0.00006236 $0.744427 $0.00409286 $19.93 $0.00462547 $0.999964 $0.697204 $0.999851 $0.226203 $0.999503 $0.00345552 $2.50 $0.501737 $27.15 $0.303237 $0.293416 $0.140895 $0.00000143 $0.118334 $17.36 $109,057.00 $0.02955191 Circle’s dynamite IPO this week wasn’t just impressive by crypto standards—it outperformed expectations to a degree unrivaled even by America’s most prominent tech companies. The evening before its Thursday trading debut on the New York Stock Exchange, Circle priced its stock, CRCL, at $31 a share. That represented a mark-up from the lower share prices the firm floated earlier in the week: $26, and then $28. Such last-minute moves are generally indicative of increased investor interest in a company’s stock market debut. But nothing could have prepared Wall Street for the stablecoin issuer’s bombshell first-day performance. Within minutes of the market’s opening, CRCL more than tripled in price, and experienced such volatility that the New York Stock Exchange had to halt trading on the stock multiple times. By the end of Thursday's trading day, the price of Circle sat at $82.84—up 167% from the offering price. On Friday, CRCL hit a new high of $123.51, coming within cents of fully quadrupling its IPO price. Among other flashy tech IPOs of recent years, that performance is a standout. While some American tech giants may be worth more than Circle, few have shattered early trading expectations to such an extent. Meta, formerly Facebook, for example, IPO’d at $38 back in 2012. After its first day of trading, the company’s stock remained stagnant at $38.23, disappointing investors. That price nonetheless valued Facebook at a monster $104 billion, though—far more than the $19 billion valuation Circle scored yesterday, even with overperformance factored in. Uber, another tech giant with a hotly anticipated IPO, failed to meet expectations after its Wall Street debut in 2019. The disruptive rideshare startup priced its stock at $45, but failed to drum up enough excitement on its first day of trading. UBER shed 8% that afternoon, closing under $42. But the company’s valuation at that point was still nothing to scoff at: $69.7 billion. It’s a similar story in fintech. When Robinhood launched its stock in July 2021, the new-age financial services company aimed for an opening price per share of $38. HOOD’s stock ended its first day of trading down over 8%, at $34.82, leaving the company with a market capitalization of $32 billion. Even when major tech stocks have outperformed analyst expectations, they’ve typically done so by smaller margins than Circle achieved this week. In 2020, at the peak of the COVID-19 pandemic, Airbnb’s stock more than doubled its IPO price on opening day, surging from $68 to $144.71 by the closing bell. That 112% jump was heralded at the time as a fairytale success story—but still, did not approach Circle’s first-day result. Scale again, though, is an important caveat: Airbnb’s first day of trading valued the company at a whopping $100.7 billion. What accounts for Circle’s distinctive overperformance on Wall Street this week? Analysts told Decrypt the stock fared so well thanks not only to excitement surrounding stablecoins, which may soon be greenlit for a wide range of applications by Congress; but also due to the fact that Circle’s stock currently represents one of the only means for institutions and retail traders to invest in the emergent sector. The company’s competitors, namely market leader Tether, are not publicly traded. Edited by Andrew Hayward Your gateway into the world of Web3 The latest news, articles, and resources, sent to your inbox weekly. © A next-generation media company. 2025 Decrypt Media, Inc.
--------------------------------------------------

Title: Venus Concept Stock Is Soaring On Friday - Here's Why
URL: https://www.benzinga.com/trading-ideas/movers/25/06/45821742/venus-concept-stock-is-soaring-on-friday-heres-why
Time Published: 2025-06-06T20:53:56Z
Full Content:
Benzinga Rankings give you vital metrics on any stock – anytime. Venus Concept Inc. VERO shares are trading higher on Friday after disclosing that the firm entered into definitive agreements to purchase and sell 434,720 shares of common stock at a purchase price of $2.65 per share in a registered direct offering. In a concurrent private placement, the company will issue unregistered short-term warrants to purchase up to 869,440 shares of common stock at an exercise price of $2.65 per share. Venus expects approximately $1.15 million in gross proceeds from the offering. If fully exercised, the potential additional gross proceeds from the unregistered short-term warrants will be approximately $2.3 million. Venus anticipates the offering to close on or about June 9. Also Read: Dow Jumps Over 500 Points; Nonfarm Payrolls Top Estimates Venus Concept also announced a strategic move Friday involving the divestiture of its hair restoration business in a $20 million cash deal aimed at strengthening its core medical aesthetics segment. The company has entered into a definitive agreement with MHG Co. Ltd, also known as Meta Healthcare Group, to sell its Venus Hair division. This deal is part of a broader strategy to boost financial stability and sharpen focus on high-growth areas. As per the terms, Meta Healthcare Group will acquire Meta Robotics LLC, a newly established Venus Concept subsidiary that encompasses the company's full hair restoration portfolio—including ARTAS and NeoGraft technologies—along with associated services, equipment, assets, and intellectual property. Meta will also take over Venus Concept's research and manufacturing facility in San Jose, California, and gain the rights to produce NeoGraft systems. In return, Meta will grant Venus Concept a perpetual, royalty-free global license covering certain patents included in the transaction. CEO Rajiv De Silva described the agreement as a step forward in Venus Concept's plan to unlock shareholder value. The capital raised is expected to help the firm reduce operating costs, improve cash flow, and enhance focus on its primary aesthetic device business. "We are confident Meta Healthcare Group will provide the strategic investment and resources needed to maximize the global addressable markets for the ARTAS and NeoGraft technologies.," De Silva said. In fiscal year 2024, the Venus Hair business contributed approximately $12.5 million in revenue while incurring operational cash usage of around $6.7 million. Price Action: VERO shares traded higher by 35.1% to $3.230 at last check Friday. Read Next: Photo via Shutterstock Benzinga Rankings give you vital metrics on any stock – anytime. © 2025 Benzinga.com. Benzinga does not provide investment advice. All rights reserved. Trade confidently with insights and alerts from analyst ratings, free reports and breaking news that affects the stocks you care about. A newsletter built for market enthusiasts by market enthusiasts. Top stories, top movers, and trade ideas delivered to your inbox every weekday before and after the market closes.
--------------------------------------------------

Title: How Netflix's ad business could become a $10 billion sleeper hit
URL: https://not-qz.com/netflix-ad-business-tiers-ads-platform-revenue-1851783869
Time Published: 2025-06-06T19:10:00Z
Full Content:
Netflix (NFLX) was once allergic to ads. Now it might be growing its empire on them. Back in 2022, the company’s decision to launch an ad-supported tier felt more like a concession than a strategy — a move that came as password crackdowns replaced product launches and was driven by slowing subscriber growth, rising competition, and economic pressure. Fast forward to mid-2025 and that “standard with ads” plan looks less like a reluctant experiment and more like a $10 billion franchise in the making. Netflix’s ad-supported tier now boasts greater than 94 million global monthly active users, more than doubling from 40 million just the year before, according to the company. In regions where that tier is available, over 40% of new subscribers are choosing the cheaper, ad-supported option — showing that viewers are happy to trade a few commercials for a lower monthly bill. Among these users, 44% are new to Netflix, 40% downgraded from pricier ad-free plans, and 16% are returning subscribers, per The Wrap. Jefferies (JEF) analysts estimate that Netflix’s advertising business could hit $10 billion in annual revenue by the end of the decade, and Wall Street is finally catching up to the plot twist. In a June note, Jefferies raised its price target on Netflix to $1,400, citing, among other things, the “infancy” of the ad business and its massive runway for growth. Oppenheimer (OPY) analysts are even more bullish in the short term, forecasting $6 billion in ad revenue by 2025 — up from a previous $4.6 billion estimate. That’s not pocket change for a company that built its brand on being the “ad-free” TV option. Netflix’s transformation into a hybrid media-advertising platform mirrors broader shifts in the streaming landscape. According to data from analysts at Antenna, nearly half of streaming subscriptions across major platforms are now ad-supported. Consumers, it turns out, are willing to watch a few commercials if it means saving a few bucks — and advertisers are eager to reach them in a post-cable world. Still, Netflix wasn’t the first to adopt this model. Hulu (DIS), Peacock (CMCSA), and Paramount+ (PARA) had ad-supported offerings years earlier. But Netflix’s scale gives it an edge: a global subscriber base, premium content, and hit franchises such as “Bridgerton,” and increasingly, the infrastructure to sell those ads with surgical precision. The company recently began building an ad-tech platform with plans to launch it globally by the end of 2025. Netflix has also inked partnerships with major programmatic players such as The Trade Desk (TTD), Google’s DV360, and Magnite (MGNI) to streamline ad sales and targeting. The appeal to advertisers is obvious. Netflix offers premium, brand-safe content in a streaming environment that’s becoming increasingly fragmented and unpredictable. And the company’s ad business still represents a small slice of total revenue — an estimated $2 billion in 2025. But that’s the point: There’s room to grow. MoffettNathanson (SIVB) sees Netflix’s advertising revenue climbing past $6 billion by 2027, on track for $10 billion by 2030. For comparison, YouTube (GOOGL) generated over $40 billion in ad revenue last year. Even capturing a small fraction of that puts Netflix in a different league than most of its streaming peers. And Wall Street is paying attention. The ad model, combined with a blockbuster second-half content slate — “Squid Game” season three, the “Stranger Things” finale, and even NFL games and other live offerings — is fueling bullish sentiment on the stock. Analysts expect free cash flow to grow at a 20% compound annual rate over the next five years, reaching $18 billion by the end of the decade. Advertising, Wall Street seems to be arguing, is the high-margin lever that could supercharge those numbers — while the company remains a strong, recession-safe play. It also offers Netflix pricing flexibility. Rather than hiking monthly rates across the board (a tactic that’s historically sparked churn), the company can nudge users toward the ad tier while still growing average revenue per member (ARM). So far, it seems to be working: Churn from price increases has been “limited,” per Jefferies, and ARM is rising. In early 2025, Netflix raised the price of its ad-supported plan from $6.99 to $7.99 (the standard plan without ads from $15.49 to $17.99; and the premium tier without ads from $22.99 to $24.99). Unlike previous hikes, this one didn’t trigger a mass exodus. That’s good news for the company’s margins — and even better news for its ambitions to grow the ad-supported tier without sacrificing user growth. There are, of course, still caveats. Some analysts worry that the ad tier might cannibalize higher-paying subscribers, dragging down margins. Others point to the fact that the costs of live sports and entertainment rights could weigh on profitability (just ask ESPN). And unlike tech-native platforms such as Meta (META) or Google, Netflix has only recently begun building the backend that powers programmatic ad targeting at scale. But if Netflix can thread the needle — retaining its brand while scaling its ad business — it could pull off something rare in streaming: profitable, diversified growth. The irony? For a company that famously shunned advertising for years, Netflix’s next act could be defined by it. If the company’s original story was about killing the cable bundle, the sequel might be about inheriting cable’s ad dollars. As the credits roll on the first chapter of the streaming wars, Netflix is already filming the next one. And this time, it comes with commercial breaks.
--------------------------------------------------

Title: How Netflix’s ad business could become a $10 billion sleeper hit
URL: https://qz.com/netflix-ad-business-tiers-ads-platform-revenue-1851783869
Time Published: 2025-06-06T19:10:00Z
Full Content:
Netflix (NFLX) was once allergic to ads. Now it might be growing its empire on them. Back in 2022, the company’s decision to launch an ad-supported tier felt more like a concession than a strategy — a move that came as password crackdowns replaced product launches and was driven by slowing subscriber growth, rising competition, and economic pressure. Fast forward to mid-2025 and that “standard with ads” plan looks less like a reluctant experiment and more like a $10 billion franchise in the making. Netflix’s ad-supported tier now boasts greater than 94 million global monthly active users, more than doubling from 40 million just the year before, according to the company. In regions where that tier is available, over 40% of new subscribers are choosing the cheaper, ad-supported option — showing that viewers are happy to trade a few commercials for a lower monthly bill. Among these users, 44% are new to Netflix, 40% downgraded from pricier ad-free plans, and 16% are returning subscribers, per The Wrap. Jefferies (JEF) analysts estimate that Netflix’s advertising business could hit $10 billion in annual revenue by the end of the decade, and Wall Street is finally catching up to the plot twist. In a June note, Jefferies raised its price target on Netflix to $1,400, citing, among other things, the “infancy” of the ad business and its massive runway for growth. Oppenheimer (OPY) analysts are even more bullish in the short term, forecasting $6 billion in ad revenue by 2025 — up from a previous $4.6 billion estimate. That’s not pocket change for a company that built its brand on being the “ad-free” TV option. Netflix’s transformation into a hybrid media-advertising platform mirrors broader shifts in the streaming landscape. According to data from analysts at Antenna, nearly half of streaming subscriptions across major platforms are now ad-supported. Consumers, it turns out, are willing to watch a few commercials if it means saving a few bucks — and advertisers are eager to reach them in a post-cable world. Still, Netflix wasn’t the first to adopt this model. Hulu (DIS), Peacock (CMCSA), and Paramount+ (PARA) had ad-supported offerings years earlier. But Netflix’s scale gives it an edge: a global subscriber base, premium content, and hit franchises such as “Bridgerton,” and increasingly, the infrastructure to sell those ads with surgical precision. The company recently began building an ad-tech platform with plans to launch it globally by the end of 2025. Netflix has also inked partnerships with major programmatic players such as The Trade Desk (TTD), Google’s DV360, and Magnite (MGNI) to streamline ad sales and targeting. The appeal to advertisers is obvious. Netflix offers premium, brand-safe content in a streaming environment that’s becoming increasingly fragmented and unpredictable. And the company’s ad business still represents a small slice of total revenue — an estimated $2 billion in 2025. But that’s the point: There’s room to grow. MoffettNathanson (SIVB) sees Netflix’s advertising revenue climbing past $6 billion by 2027, on track for $10 billion by 2030. For comparison, YouTube (GOOGL) generated over $40 billion in ad revenue last year. Even capturing a small fraction of that puts Netflix in a different league than most of its streaming peers. And Wall Street is paying attention. The ad model, combined with a blockbuster second-half content slate — “Squid Game” season three, the “Stranger Things” finale, and even NFL games and other live offerings — is fueling bullish sentiment on the stock. Analysts expect free cash flow to grow at a 20% compound annual rate over the next five years, reaching $18 billion by the end of the decade. Advertising, Wall Street seems to be arguing, is the high-margin lever that could supercharge those numbers — while the company remains a strong, recession-safe play. It also offers Netflix pricing flexibility. Rather than hiking monthly rates across the board (a tactic that’s historically sparked churn), the company can nudge users toward the ad tier while still growing average revenue per member (ARM). So far, it seems to be working: Churn from price increases has been “limited,” per Jefferies, and ARM is rising. In early 2025, Netflix raised the price of its ad-supported plan from $6.99 to $7.99 (the standard plan without ads from $15.49 to $17.99; and the premium tier without ads from $22.99 to $24.99). Unlike previous hikes, this one didn’t trigger a mass exodus. That’s good news for the company’s margins — and even better news for its ambitions to grow the ad-supported tier without sacrificing user growth. There are, of course, still caveats. Some analysts worry that the ad tier might cannibalize higher-paying subscribers, dragging down margins. Others point to the fact that the costs of live sports and entertainment rights could weigh on profitability (just ask ESPN). And unlike tech-native platforms such as Meta (META) or Google, Netflix has only recently begun building the backend that powers programmatic ad targeting at scale. But if Netflix can thread the needle — retaining its brand while scaling its ad business — it could pull off something rare in streaming: profitable, diversified growth. The irony? For a company that famously shunned advertising for years, Netflix’s next act could be defined by it. If the company’s original story was about killing the cable bundle, the sequel might be about inheriting cable’s ad dollars. As the credits roll on the first chapter of the streaming wars, Netflix is already filming the next one. And this time, it comes with commercial breaks.
--------------------------------------------------

Title: Replace your sunglasses with this rare deal on Ray-Ban Meta smart glasses during this rare Amazon deal
URL: https://www.popsci.com/gear/ray-bans-meta-smart-glasses-amazon-deal/
Time Published: 2025-06-06T18:05:00Z
Full Content:
By Stan Horaczek Published Jun 6, 2025 2:05 PM EDT We may earn revenue from the products available on this page and participate in affiliate programs. Learn more › Sign Up For Goods 🛍️ Product news, reviews, and must-have deals. Terms of Service and Privacy Policy. If you’ve been wanting a pair of Ray-Ban Meta smartglasses, this is the best price I’ve seen since last year’s Black Friday. Amazon has pairs as low as $239 right now, both with and without tinted lenses. They offer the classic Wayfarer style, so they look good on just about everyone. The sale is limited to what’s in stock right now, so grab the color and the size you want before they sell out. With these discounts, they’re not that much more expensive than buying a new pair of sunglasses at the mall. Ray-Bans When talking about the Ray-Ban Meta glasses, most people focus on the built-in camera. It makes sense as it’s really cool to take hands-free photos and videos without help from a smartphone. However, I think people drastically underestimate the music playback feature. I don’t have a prescription, but I regularly wear a pair with clear lenses when I’m out shopping or just hanging out purely so I can listen to Spotify. The glasses beam the music straight into your ears, and there’s barely any sound leakage despite the lack of a physical earbud to plug up your ear canal. Meta AI offers some clever features as well. You can ask the virtual assistant to “look” at an object with a built-in camera and give you information about it. The glasses can also do real-time translations in several languages without the need to connect to the internet. The polarized lenses offer full sun protection and improve the overall clarity of everything you look at. There are several different colors and tints listed below, so grab the ones that fit your preference. If you have a large head, definitely make sure you order the large size. I wear a size 7 5/8 hat and bought the regular size, and wish I had gotten the larger model. They’re still cool and comfortable enough; they just look small on my face. It’s unlikely that you’re going to see these any cheaper than this until at least Black Friday this year, so grab a pair and use them all summer. Ray-Bans If you don’t need tint or a prescription, this model offers perfectly clear lenses so you can wear them all the time. I enjoy watching videos made by people who shop at thrift stores while wearing them. More deals, reviews, and buying guides Stan Horaczek is the executive gear editor at Popular Science. He oversees a team of gear-obsessed writers and editors dedicated to finding and featuring the newest, best, and most innovative gadgets on the market and beyond. Popular Science started writing about technology more than 150 years ago. There was no such thing as “gadget writing” when we published our first issue in 1872, but if there was, our mission to demystify the world of innovation for everyday readers means we would have been all over it. Here in the present, PopSci is fully committed to helping readers navigate the increasingly intimidating array of devices on the market right now. Our writers and editors have combined decades of experience covering and reviewing consumer electronics. We each have our own obsessive specialties—from high-end audio to video games to cameras and beyond—but when we’re reviewing devices outside of our immediate wheelhouses, we do our best to seek out trustworthy voices and opinions to help guide people to the very best recommendations. We know we don’t know everything, but we’re excited to live through the analysis paralysis that internet shopping can spur so readers don’t have to. Find out more about our product evaluation process. By Stan Horaczek By Stan Horaczek By John Alexander By Stan Horaczek By Stan Horaczek By Stan Horaczek By Stan Horaczek By Stan Horaczek By Stan Horaczek By Fiona Tapp By Amanda Reed By Stan Horaczek By Stan Horaczek By Stan Horaczek By Stan Horaczek, Tony Ware By Bob Beacham Breakthroughs, discoveries, and DIY tips sent every weekday. By signing up you agree to our Terms of Service and Privacy Policy. Articles may contain affiliate links which enable us to share in the revenue of any purchases made. Registration on or use of this site constitutes acceptance of our Terms of Service. © 2025 Recurrent. All rights reserved.
--------------------------------------------------

Title: Snowflake up its AI game, Circle IPO blasts off, and Elon splits with Trump
URL: https://siliconangle.com/2025/06/06/snowflake-ai-game-circle-ipo-blasts-off-elon-splits-trump/
Time Published: 2025-06-06T16:39:03Z
Full Content:
UPDATED 12:39 EDT / JUNE 06 2025 THIS WEEK IN ENTERPRISE by Robert Hof Snowflake made a big play this week at its annual Summit to own a larger swath of the artificial intelligence opportunity — still based on data but bringing in AI models, agents and other software to get stuff done. As Dave Vellante and George Gilbert put it in their Breaking Analysis deep dive, enterprise data platforms such as Snowflake and Databricks will be defined not by how well they store information but by how quickly they can convert data combined with business logic into automated actions. And in their latest Breaking Analysis, they drill down on Snowflake post-Summit to update their view of how it and other players are positioned. CEO Sridhar Ramaswamy (pictured) made an impassioned pitch for Snowflake to lead that shift, but Salesforce, Amazon Web Services and a few others are knocking on that door too. And next week Databricks will get its turn at its Data+AI Summit to define how it’s going to play in that larger arena. In the least surprising development of late, Elon Musk’s bromance with Trump is apparently over, and he’s even backing the president’s impeachment as Tesla’s stock plunged 14% Thursday. But it escalated fast — popcorn time! Though Musk may soon back off. Assuming Musk still has enough cash, his xAI startup is going to spend $5 billion on new infrastructure. And the big spending to support AI continues, as Meta inked a 20-year deal to buy nuclear power and Amazon is spending $10 billion in North Carolina to expand its cloud infrastructure and advance AI. On the earnings front, AI momentum boosted Broadcom and HPE results, though investors split on them, and MongoDB saw its shares rise almost 13%. But it’s still a mixed bag given the uncertain economy and Trump’s daily shifts on tariffs. Amazon’s secretive Lab126 is now working on agentic AI, focused on robots. Autonomous robots? Hmm. Reddit sued Anthropic for scraping its data without permission. This issue isn’t going to die, but it’s anybody’s guess what impact it will have on, well, future models. Stablecoin provider Circle raised $1.1 billion in its IPO, and its shares jumped 168% on their first day of trading. We’ll see how stable its stock price is. Next up: Chime. And crypto exchange Gemini just filed confidentially to go public too. Next week there’s not only Databricks’ Data+AI Summit but Cisco Live, Apple’s WWDC and AWS re:Inforce. On the earnings front, Oracle’s the big one set to report, as well as Adobe, GitLab and SailPoint. Update: Silicon Valley lost two giants recently: Former longtime Hewlett-Packard CEO John Young on May 26, at age 93, and Bill Atkinson, a Macintosh pioneer and inventor of the web precursor Hypercard, on June 5, at age 74. They both represented some of the best in the Valley, unlike all too many current tech celebrities who seem more motivated by, well, fame and money. May they rest in peace up there with Bill, Dave and Steve. Here’s the most important enterprise and emerging tech news from SiliconANGLE and beyond: All the news, analysis and interviews from Snowflake Summit: Snowflake and Databricks cross the Rubicon into a new competitive domain Beyond walled gardens: How Snowflake navigates new competitive dynamics The next frontier in enterprise AI: What to watch at Snowflake and Databricks summits Snowflake platform enhancements focus on performance, governance and interoperability Snowflake expands AI tools to streamline enterprise data workflows and speed machine learning Snowflake buys Crunchy Data to add more bite to its AI agents Data engineering workloads evolve as Snowflake redefines its value stack: theCUBE keynote analysis Snowflake’s platform evolution spotlights metadata blind spots: theCUBE keynote analysis Snowflake and Meta team up to drive model innovation on Cortex AI Snowflake orchestrates its AI strategy through Cortex, unstructured data solutions and agents Exclusive: Unravel is giving away a Snowflake performance optimization analyzer RelationalAI introduces new graph processing features for its Snowflake app A few other observations and gleanings from my interviews with researchers and execs: * It goes without saying that Snowflake debuted a next-generation data warehouse that’s more than twice as fast, among many other features. One of the most interesting was Adaptive Compute, enabling users to avoid having to configure compute and other settings, and getting better performance as a result, especially for the kinds of quickly changing workloads such as AI.“ Adaptive will be the future,” Artin Avanes, head of Snowflake’s core data platform, told me (for now, it’s in private preview). * But all that felt like almost an afterthought next to the new AI features and services, including Snowflake Intelligence (soon in public preview), which allows, say, a sales manager to ask, “What were my top product sales in the West region last quarter, and why did this product outperform that one?” and then even have AI agents take actions such as sending notifications. The service can tap a wide variety of data sources, from structured tables to Slack chats. * It’s one indication of how much higher Snowflake’s ambitions have become. As Dave Vellante and George Gilbert put it, enterprise data platforms will be defined not by how well they store information but by how quickly they can convert data combined with business logic into automated actions. “AI represents a new way of thinking about how different functions in a company operate,” Ramaswamy said in a press briefing. “We want to be the agent of transformation for our 10,000 customers. We want to be there with customers from the time data is there.” * The big focus this week was on unstructured data, and no wonder: Up to 90% of the world’s data is unstructured, that is, not in well-organized tables, and it needs to be gathered up and massaged so it can provide deeper analysis. I talked with LandingAI, which is partnering with Snowflake to apply what it calls agentic vision to unstructured data, in particular documents, whose formats can be downright quirky. “We’re kind of like Lasik for LLMs,” said CEO Dan Maloney. And faster: Its Agentic Doc Extraction service was updated a week ago with 30 times the speed, important when you’re using reasoning models. * AI agents, the new hot thing, also got a lot of attention at the summit, not least because they provide both a challenge and an opportunity for Snowflake as it busts further out of its data warehouse market. In short, can it become a sort of operating system for AI agents? It’s not the only one trying. As Vellante asked in his analysis of the keynote: “Are they just basically data infrastructure that’s serving up agentic platforms or are they going to become the agentic platform?” * All this adds up to a democratization of data analysis beyond data scientists and business intelligence experts. Ultimately, Snowflake’s looking to make not only data analysis but applications most anyone can create — or maybe get an agent to do. “AI is a way to predict or use data to take actions based on it,” Dwarak Rajagopal, VP and head of AI engineering, told me. “Everybody could be a programmer. Everybody could be a specialist in whatever they want.” Amazon’s secretive research lab is developing robots with integrated agentic AI software Workday launches AI agent developer toolset with third-party connector and custom widgets AI pioneer Yoshua Bengio launches nonprofit LawZero AI lab, with a focus on safe AI OpenAI gives ChatGPT access to cloud-based documents and third-party research tools OpenAI enhances agentic AI development with improvements to Codex and the Agents SDK Google revamps Gemini 2.5 Pro again, claiming superiority in coding and math Mistral AI introduces Code programming assistant Anthropic releases AI models exclusive to US national security customers Postman looks to streamline API and agentic AI development with Agent Mode Pegasystems expands features to build and manage AI agents Exclusive: CData to embed its connectivity technology across Palantir analytics and AI platforms Semantic data layer startup Cube automates analytics with AI agents Elon Musk’s xAI launches $5B debt sale to fuel AI infrastructure investments Anysphere raises $900M for its AI-powered Cursor code editor Collibra acquires Raito to enhance its data security capabilities IBM acquires data analysis startup Seek AI, opens AI accelerator in NYC (per TechCrunch) Mind raises $30M to help businesses prevent data loss using AI Sema4.ai raises $25M in funding for its AI agent platform Wordsmith AI raises $25M to build legal AI for in-house teams Ciroos.AI raises $21M for multi-agent site reliability engineering platform Thread AI raises $20M for its AI-powered workflow automation platform LuminX raises $5.5M to build AI vision models for warehouse operations Reddit sues Anthropic over alleged scraping and commercial use of user data OpenAI to retain deleted ChatGPT conversations following court order There’s even more AI and big data news on SiliconANGLE Broadcom introduces Tomahawk 6 networking chip for large-scale AI clusters Broadcom reboots CloudHealth with enhancements to broaden FinOps use Arm’s latest compute subsystem to accelerate automotive AI chip design by up to a year HPE expands fault-tolerant line with faster processors and more memory support Informatica CEO sees Salesforce acquisition as strategic expansion, not exit AMD bags the team behind AI chipmaker Untether AI, its third acquisition in under two weeks Meta inks 20-year nuclear power deal with Constellation Energy Amazon plans to invest $10 billion in North Carolina to expand cloud computing infrastructure and advance AI innovation GlobalFoundries to invest $3B more in US fab network Data analytics chip startup Speedata closes $44M funding round Financial technology company Chime seeking $11.2B valuation in upcoming IPO Google will invest $500M in compliance program to settle shareholder lawsuit Projectworks raises $12M to expand its project automation platform Earnings: AI chip demand propels Broadcom to another earnings beat, but shares drop after-hours HPE’s stock rises on AI momentum and more clarity on market conditions MongoDB crushes expectations and its stock makes huge gains after-hours CrowdStrike earnings beat expectations but revenue outlook weighs on shares Rubrik delivers strong results on surging cloud and subscription revenue Asana shares drop as earnings results top estimates but revenue growth slows Docusign shares fall on billings outlook despite strong earnings Despite higher loss, higher revenue at Couchbase drives stock higher We have lots more news on cloud, infrastructure and apps CrowdStrike faces federal scrutiny following global Windows outage Google finds generational divide in how users respond to rising online scams Bitdefender report finds 84% of major attacks now involve legitimate tools Surfing the AI wave with zero trust everywhere: Five takeaways from CEO Jay Chaudhry’s keynote at Zscaler’s Zenith Live Microsoft and CrowdStrike collaborate on shared threat actor mapping system Zscaler expands zero-trust and AI capabilities across cloud and branch environments Illumio and Nvidia team up to strengthen zero-trust security for critical infrastructure Merlin Ventures secures $75M+ for seed-stage cybersecurity fund Zero Networks raises $55 million to expand microsegmentation and zero trust solutions Managed cybersecurity service startup ThreatSpike raises $14M More cybersecurity news here Circle raises $1.1B after hiking IPO price, and shares still jump 170% on first day of trading Cryptocurrency exchange operator Gemini files to go public Anduril raises $2.5B in funding to finance manufacturing initiatives Neuralink raises $650M at reported $9B pre-money valuation Quantum startup Infleqtion eyes defense industry applications after raising $100M Sandia National Labs deploys SpiNNcloud’s SpiNNaker2 brain-inspired supercomputer AI-driven robotic wind turbine maintenance firm Aerones raises $62M And check out more news on emerging tech, blockchain and crypto and policy IT management and cybersecurity firm Kaseya appointed former Intuit exec Rania Succar CEO. Salesforce hired some of the team at Moonhub, a startup building AI tools for hiring (per TechCrunch) Amy Herzog is new chief information security officer at Amazon Web Services, moving over from the same role at Amazon’s AGI, ads, devices and global media and entertainment units (per The Stack). Former AWS exec Adam Seligman is new chief technology officer at integration platform-as-a-service company Workato. Microsoft gave LinkedIn chief Ryan Roslansky an added role running Office. Uber named Palo Alto Networks CEO Nikesh Arora to its board following executive shakeup. Data labeling startup Scale AI hired the team behind Pesto AI, which helps companies recruit developers remotely. Founded in 2017 by Ayush Jaiswal and Rahul Jaimini, Pesto AI will shut down (per TechCrunch). Sierra Ventures hired Ashish Kakran, an investor in Cohere, Harness, Isovalent, Opaque and Exaforce, as partner to expand early-stage focus on AI, cloud infrastructure and cybersecurity. June 8-12: Cisco Live, San Diego and virtual: SiliconANGLE will have all the news, and theCUBE Research will be onsite for interviews and analysis. June 9-12: Databricks AI+Data Summit, San Francisco and virtual: SiliconANGLE will be there with all the news and theCUBE will be onsite with interviews and analysis. June 9-13: Apple WWDC, Cupertino and virtual. SiliconANGLE will have all the news. June 16-18: AWS re:Inforce: TheCUBE will be onsite with interviews and analysis. Tuesday, June 10: GitLab Wednesday, June 11: Oracle, SailPoint Thursday, June 12: Adobe THANK YOU OpenAI's newest reasoning model o3-pro surpasses rivals on multiple benchmarks, but it's not very fast Uber will bring robotaxis to London in 2026 Time intelligence startup Laurel gets $100M to turbocharge worker productivity AWS DC Summit 2025: The cloud becomes strategic national infrastructure for the AI era GitLab's revenue outlook comes up short and its stock gets hammered Meta reportedly forming superintelligence lab amid Llama 4 Behemoth delays OpenAI's newest reasoning model o3-pro surpasses rivals on multiple benchmarks, but it's not very fast AI - BY MIKE WHEATLEY . 4 HOURS AGO Uber will bring robotaxis to London in 2026 EMERGING TECH - BY JAMES FARRELL . 4 HOURS AGO Time intelligence startup Laurel gets $100M to turbocharge worker productivity AI - BY MIKE WHEATLEY . 5 HOURS AGO AWS DC Summit 2025: The cloud becomes strategic national infrastructure for the AI era CLOUD - BY JOHN FURRIER . 7 HOURS AGO GitLab's revenue outlook comes up short and its stock gets hammered CLOUD - BY MIKE WHEATLEY . 7 HOURS AGO Meta reportedly forming superintelligence lab amid Llama 4 Behemoth delays AI - BY MARIA DEUTSCHER . 7 HOURS AGO
--------------------------------------------------

Title: INVESTOR ALERT: Pomerantz Law Firm Investigates Claims On Behalf of Investors of Meta Platforms, Inc. - META
URL: https://www.globenewswire.com/news-release/2025/06/06/3095287/1087/en/INVESTOR-ALERT-Pomerantz-Law-Firm-Investigates-Claims-On-Behalf-of-Investors-of-Meta-Platforms-Inc-META.html
Time Published: 2025-06-06T15:45:00Z
Full Content:
June 06, 2025 11:45 ET | Source: Pomerantz LLP Pomerantz LLP NEW YORK, June 06, 2025 (GLOBE NEWSWIRE) -- Pomerantz LLP is investigating claims on behalf of investors of Meta Platforms, Inc. (“Meta” or the “Company”) (NASDAQ: META). Such investors are advised to contact Danielle Peyton at newaction@pomlaw.com or 646-581-9980, ext. 7980. The investigation concerns whether Meta and certain of its officers and/or directors have engaged in securities fraud or other unlawful business practices. [Click here for information about joining the class action] On May 15, 2025, the Wall Street Journal published an article entitled “Meta Is Delaying the Rollout of Its Flagship AI Model.” Citing “people familiar with the matter”, the article reported, in relevant part, that “[c]ompany engineers are struggling to significantly improve the capabilities of its “Behemoth” large-language model, leading to staff questions about whether the improvements over prior versions are significant enough to justify public release[.]” The article further reported that while originally “Behemoth was internally slated for an April release”, Meta subsequently “pushed an internal target for . . . Behemoth’s release to June” before “delay[ing] . . . to fall or later.” On this news, Meta’s stock price fell $19.02 per share, or 2.88%, over the following two trading sessions, to close at $640.34 per share on May 16, 2025. Pomerantz LLP, with offices in New York, Chicago, Los Angeles, London, Paris, and Tel Aviv, is acknowledged as one of the premier firms in the areas of corporate, securities, and antitrust class litigation. Founded by the late Abraham L. Pomerantz, known as the dean of the class action bar, Pomerantz pioneered the field of securities class actions. Today, more than 85 years later, Pomerantz continues in the tradition he established, fighting for the rights of the victims of securities fraud, breaches of fiduciary duty, and corporate misconduct. The Firm has recovered numerous multimillion-dollar damages awards on behalf of class members. See www.pomlaw.com. Attorney advertising. Prior results do not guarantee similar outcomes. CONTACT:Danielle PeytonPomerantz LLPdpeyton@pomlaw.com646-581-9980 ext. 7980 NEW YORK, June 10, 2025 (GLOBE NEWSWIRE) -- Pomerantz LLP is investigating claims on behalf of investors of Globus Medical, Inc. (“Globus” or the “Company”) (NYSE: GMED). Such investors are advised... NEW YORK, June 10, 2025 (GLOBE NEWSWIRE) -- Pomerantz LLP is investigating claims on behalf of investors of Cable One, Inc. (“Cable One” or the “Company”) (NYSE: CABO). Such investors are advised...
--------------------------------------------------

Title: Meta Ray-Bans for 20% off is a great deal on one of my favorite products
URL: https://www.zdnet.com/article/meta-ray-bans-for-20-off-is-a-great-deal-on-one-of-my-favorite-products/
Time Published: 2025-06-06T15:26:33Z
Full Content:
'ZDNET Recommends': What exactly does it mean? ZDNET's recommendations are based on many hours of testing, research, and comparison shopping. We gather data from the best available sources, including vendor and retailer listings as well as other relevant and independent reviews sites. And we pore over customer reviews to find out what matters to real people who already own and use the products and services we’re assessing. When you click through from our site to a retailer and buy a product or service, we may earn affiliate commissions. This helps support our work, but does not affect what we cover or how, and it does not affect the price you pay. Neither ZDNET nor the author are compensated for these independent reviews. Indeed, we follow strict guidelines that ensure our editorial content is never influenced by advertisers. ZDNET's editorial team writes on behalf of you, our reader. Our goal is to deliver the most accurate information and the most knowledgeable advice possible in order to help you make smarter buying decisions on tech gear and a wide array of products and services. Our editors thoroughly review and fact-check every article to ensure that our content meets the highest standards. If we have made an error or published misleading information, we will correct or clarify the article. If you see inaccuracies in our content, please report the mistake via this form. Among the most popular tech products of the last year, the Meta Ray-Ban smart glasses have been the surprise hit. Amazon is currently offering the version with the classic black frames for $239 (20% off the $299 retail price). This is an especially excellent deal when you consider that many styles of the Meta Ray-Bans were often out of stock for much of the first half of last year -- and the smart glasses have rarely been discounted. A lot of the appeal is the fact that they look just like normal Ray-Bans, and you can even replace the lenses with your own glasses prescription. The caveat to keep in mind here is that Amazon sells eight different styles of the Meta Ray-Bans, and it's the classic black frames with green-tinted lenses that are on sale for the lowest price. Other styles cost $30 to $80 more. For example, if you want to wear these smart glasses both inside and outside, then you can get a pair with transition lenses. Those normally retail for $379, but Amazon is offering them 20% off, for $303. I tried the Meta Ray-Bans earlier this year and was much more impressed than I expected. First, to be clear, they are augmented reality glasses, but they're audio-only, so there's no screen overlaid on the lenses. All the feedback comes through the speakers that down-fire into your ears and are hidden in the arms of the glasses. Also: Why Meta's Ray-Ban Smart Glasses are my favorite tech purchase this year I was also pleasantly surprised by the audio quality. For phone calls and conference calls and for listening to podcasts and audiobooks, the sound is nearly as bright and clear as a pair of AirPods or other quality earbuds. For listening to music, movies, or shows with lots of bass and surround sound effects, the Meta Ray-Bans are acceptable but not nearly as robust as a pair of high-end headphones from Apple, Sony, Bose, and others. And because of the open concept, naturally the Meta Ray-Bans can't offer the kind of noise cancellation that you get from AirPods Pro or the Sony WF-1000XM5 earbuds, for example. All in all, the sound of the Meta Ray-Bans is similar to what you get from the AirPods Pro in transparency mode. But, the Meta Ray-Bans can also do things that no headphones can do. They have a built-in 12-megapixel ultrawide camera for taking photos and capturing videos. This camera is about the same quality as an iPhone 12 or a Samsung Galaxy S20. It only takes photos and videos in portrait mode, but it's far faster to tap the button on top of the right arm of the glasses than it is to pull your camera out of your pocket. ZDNET senior editor Sabrina Ortiz in her Meta Ray-Bans. The Meta Ray-Bans also have a built-in AI assistant that you can trigger by saying "Hey Meta," and use the large language models that Meta has been furiously building over the past several years. For basic information searches, it works well and is another nice option for leaving your phone in your pocket. The battery life is about a half-day with moderate use, but you can quickly charge to about 50% with 20 minutes in the included case. The case itself charges via standard USB-C, which is nice. You can nearly double the battery by turning off the "Hey Meta" listening feature -- which I did. Meta also regularly rolls out software updates that bring new functionality to the Meta Ray-Bans. For example, in 2024, the Meta Ray-Bans added the capability of translating from Spanish to English in real time and the ability to do a visual search to identify landmarks or translate street signs. And Meta says it will continue to add new features such as helping you remember things you've seen, like your parking spot. Also: I tested the best AR and MR glasses: Here's how the Meta Ray-Bans stack up If you're concerned about the way Meta handles privacy -- as many of us are -- the good thing about the Meta Ray-Bans is that they don't require a Facebook account. While they do require a Meta account, you can at least ostensibly keep them separate from your Facebook, Instagram, and WhatsApp data. That makes it easier for me to recommend the product from a privacy perspective. Don't get me wrong, the AirPods Pro 2 are available for a solid deal as well at $170 ($79 off) right now. But the Meta Ray-Bans can do nearly all that the AirPods Pro can do and more. And since smart glasses are getting more and more common and the promise of smarter AI-powered features are on the horizon, getting a pair of Meta Ray-Ban smart glasses gives you a better chance of getting a taste of the future. Based on the 20% savings on offer, ZDNET's rating system gives this a 3/5 Editor's deal rating. However, I've bumped it up to a 4/5, as this is in line with the price cut we saw on the glasses during Cyber Week, typically the biggest sale of the year. These glasses also rarely go on sale, making this an even more enticing offer. Deals are subject to sell out or expire at any time, though ZDNET remains committed to finding, sharing, and updating the best product deals for you to score the best savings. Our team of experts regularly checks in on the deals we share to ensure they are still live and obtainable. We're sorry if you've missed out on a deal, but don't fret -- we constantly find new chances to save and share them with you on ZDNET.com. We aim to deliver the most accurate advice to help you shop smarter. ZDNET offers 33 years of experience, 30 hands-on product reviewers, and 10,000 square feet of lab space to ensure we bring you the best of tech. In 2025, we refined our approach to deals, developing a measurable system for sharing savings with readers like you. Our editor's deal rating badges are affixed to most of our deal content, making it easy to interpret our expertise to help you make the best purchase decision. At the core of this approach is a percentage-off-based system to classify savings offered on top-tech products, combined with a sliding-scale system based on our team members' expertise and several factors like frequency, brand or product recognition, and more. The result? Hand-crafted deals chosen specifically for ZDNET readers like you, fully backed by our experts. Also: How we rate deals at ZDNET in 2025
--------------------------------------------------

Title: US stock market rallies as Dow jumps 550 points, S&P 500 tops 6,000, Nasdaq surges and Tesla leads tech comeback after strong jobs report
URL: https://economictimes.indiatimes.com/news/international/us/us-stock-market-rallies-as-dow-jumps-550-points-sp-500-tops-6000-nasdaq-surges-and-tesla-leads-tech-comeback-after-strong-jobs-report/articleshow/121678932.cms
Time Published: 2025-06-06T15:12:13Z
Full Content:
(Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. (Catch all the US News, UK News, Canada News, International Breaking News Events, and Latest News Updates on The Economic Times.) Download The Economic Times News App to get Daily International News Updates. Explore More Stories Car crashes into Cirque du Soleil venue at Vancouver's Pacific Coliseum during live show; driver arrested Gordon Ramsay to bring Hell’s Kitchen to Edmonton’s River Cree Resort at the end of this year Canada wildfires worsen air quality: Special air quality statement issued across Toronto, Quebec and most of Ontario Loblaw vs JM Smucker face off over Folgers coffee price; weather, Trump tariffs also play a role Canada Citizenship Bill addresses ‘Lost Canadians’ and First Generation Limit’; check features and eligibility Mark Carney in dire straits: After job crisis, Canada now faces record trade deficit due to Trump's auto tariffs Toronto police arrest 10 in interconnected shooting incidents; almost 90 per cent of crime guns seized US-sourced Space’s ‘Bermuda Triangle’ growing as mysterious force under Earth’s outer core may cripple International Space Station, NASA perplexed Canada throne speech vote clears House as PM Mark Carney survives his first confidence vote Marc Garneau, first Canadian in space and ex-Foreign Minister, died at 76 Canada’s Strong Borders Act to combat organized crime, curb illegal fentanyl; what changes in immigration policy with Bill C-2 Sun will die in 5 billion years but life could survive on Jupiter’s moon Europa; here’s how This discovery in Cannabis DNA will change how you see weed forever; 33 genetic markers explained Noem shuts down TSA spy program Trump’s feud with Musk a warning bell for markets Putin 'avenges' Ukraine's 'Spiderweb' operation Musk hints at forming 'new political party' Centre launches UMEED portal to register Waqf properties 'Habit of surrendering': Rahul questions PM's silence on Trump's ceasefire claim CM Omar shares stage with PM Modi, flags J&K statehood issue LIVE | Mission Kashmir: PM Modi unveil Rs 46 cr development projects LIVE | Malhotra reveals RBI's plans on future rate cut plans RBI cuts repo rates by 50 bps to 5.5%, CRR by 100 bps Hot on Web In Case you missed it Top Searched Companies Top Calculators Top Definitions Top Commodities Top Slideshow Private Companies Top Prime Articles Top Story Listing Latest News Follow us on: Find this comment offensive? Choose your reason below and click on the Report button. This will alert our moderators to take action Reason for reporting: Your Reason has been Reported to the admin. Log In/Connect with: Will be displayed Will not be displayed Will be displayed Stories you might be interested in
--------------------------------------------------

Title: DEADLINE ALERT: Faruqi & Faruqi, LLP Investigates Claims on Behalf of Investors of DoubleVerify
URL: https://www.globenewswire.com/news-release/2025/06/06/3095198/683/en/DEADLINE-ALERT-Faruqi-Faruqi-LLP-Investigates-Claims-on-Behalf-of-Investors-of-DoubleVerify.html
Time Published: 2025-06-06T14:14:00Z
Full Content:
June 06, 2025 10:14 ET | Source: Faruqi & Faruqi LLP Faruqi & Faruqi LLP Faruqi & Faruqi, LLP Securities Litigation Partner James (Josh) Wilson Encourages Investors Who Suffered Losses In DoubleVerify To Contact Him Directly To Discuss Their Options If you purchased or acquired securities in DoubleVerify between November 10, 2023 and February 27, 2025 and would like to discuss your legal rights, call Faruqi & Faruqi partner Josh Wilson directly at 877-247-4292 or 212-983-9330 (Ext. 1310). [You may also click here for additional information] NEW YORK, June 06, 2025 (GLOBE NEWSWIRE) -- Faruqi & Faruqi, LLP, a leading national securities law firm, is investigating potential claims against DoubleVerify Holdings, Inc. (“DoubleVerify” or the “Company”) (NYSE: DV) and reminds investors of the July 21, 2025 deadline to seek the role of lead plaintiff in a federal securities class action that has been filed against the Company. Faruqi & Faruqi is a leading national securities law firm with offices in New York, Pennsylvania, California and Georgia. The firm has recovered hundreds of millions of dollars for investors since its founding in 1995. See www.faruqilaw.com. As detailed below, the complaint alleges that the Company and its executives violated federal securities laws by making false and/or misleading statements and/or failing to disclose that: (a) DoubleVerify’s customers were shifting their ad spending from open exchanges to closed platforms, where the Company’s technological capabilities were limited and competed directly with native tools provided by platforms like Meta Platforms and Amazon; (b) DoubleVerify’s ability to monetize on Activation Services, the Company’s high-margin advertising optimization services segment, was limited because the development of its technology for closed platforms was significantly more expensive and time-consuming than disclosed to investors; (c) DoubleVerify’s Activation Services in connection with certain closed platforms would take several years to monetize; (d) DoubleVerify’s competitors were better positioned to incorporate AI into their offerings on closed platforms, which impaired DoubleVerify’s ability to compete effectively and adversely impacted the Company’s profits; (e) DoubleVerify systematically overbilled its customers for ad impressions served to declared bots operating out of known data center server farms; (f) DoubleVerify’s risk disclosures were materially false and misleading because they characterized adverse facts that had already materialized as mere possibilities; and (g) as a result of the foregoing, Defendants’ positive statements about the Company’s business, operations, and prospects were materially false and/or misleading or lacked a reasonable basis. The truth about this fraud was revealed through a series of disclosures culminating in February 2025 and March 2025. In February 2025, DoubleVerify reported disappointing earnings and disclosed a multiyear deceleration trend due to the suspension of DoubleVerify services by a large customer. On this news, the price of DoubleVerify stock fell 36 percent. Then, in March 2025, market research company Adalytics Research released a report claiming that DoubleVerify’s web advertisement verification and fraud protection services are ineffective, and that DoubleVerify customers are regularly billed for ad impressions served to declared bots operating out of known data center server farms. The court-appointed lead plaintiff is the investor with the largest financial interest in the relief sought by the class who is adequate and typical of class members who directs and oversees the litigation on behalf of the putative class. Any member of the putative class may move the Court to serve as lead plaintiff through counsel of their choice, or may choose to do nothing and remain an absent class member. Your ability to share in any recovery is not affected by the decision to serve as a lead plaintiff or not. Faruqi & Faruqi, LLP also encourages anyone with information regarding DoubleVerify’s conduct to contact the firm, including whistleblowers, former employees, shareholders and others. To learn more about the DoubleVerify Holdings, Inc. class action, go to www.faruqilaw.com/DV or call Faruqi & Faruqi partner Josh Wilson directly at 877-247-4292 or 212-983-9330 (Ext. 1310). Follow us for updates on LinkedIn, on X, or on Facebook. Attorney Advertising. The law firm responsible for this advertisement is Faruqi & Faruqi, LLP (www.faruqilaw.com). Prior results do not guarantee or predict a similar outcome with respect to any future matter. We welcome the opportunity to discuss your particular case. All communications will be treated in a confidential manner. A photo accompanying this announcement is available at https://www.globenewswire.com/NewsRoom/AttachmentNg/4c96193e-9e8a-4e5d-878a-3e41b441e24a Faruqi & Faruqi, LLP Securities Litigation Partner James (Josh) Wilson Encourages Investors Who Suffered Losses Exceeding $50,000 In PepGen To Contact Him Directly To Discuss Their Options ... Faruqi & Faruqi, LLP Securities Litigation Partner James (Josh) Wilson Encourages Investors Who Suffered Losses Exceeding $75,000 In Vestis To Contact Him Directly To Discuss Their Options If you...
--------------------------------------------------

Title: Divi 5 Public Alpha 16: (Features Galore)
URL: https://www.elegantthemes.com/blog/divi-resources/divi-5-public-alpha-16-features-galore
Time Published: 2025-06-06T14:00:27Z
Full Content:
The #1 WordPress Theme & Builder Divi Modules, Layouts & Themes Cloud Storage For Divi Designers Build Divi Websites With AI Collaboration for Divi Agencies Fast WordPress Hosting For Divi Amazing Support + Big Discounts WordPress Site Manager Power your web design business, collaborate with your team and build websites faster. Bring your client's ideas to life quickly and efficiently. Build any type of website with Divi. Divi makes it easy for anyone to build their own website. Build visually, no coding required. It's easy for anyone to start their own online store with Divi. Sell products and design your own website. The #1 WordPress Theme & Visual Page Builder Harness the Power of Divi With Any Theme The Best Theme for Bloggers and Online Publications The Ultimate Email Opt-In Plugin for WordPress The Best Way To Promote Social Sharing Forgot Your Username or Password? Posted on June 6, 2025 by Nick Roach 7 Comments The Divi 5 Public Alpha is available for use on new websites. If you use Divi 5, you’ll notice an update notification for Public Alpha Version 16. We release new Divi 5 versions every two weeks, and it gets better each time. If you haven’t tested Divi 5 yet, try it and let us know what you think. I didn’t publish a post for Public Alpha 15, so I’ll cover everything that has changed since then. We implemented 84 bug fixes and improvements, three new features, including Attribute Management, Settings Search and Filtering, and Extend Attributes, and made significant progress on four upcoming features. Regarding bug fixes, we have been focusing heavily on backward compatibility as we gear up for the Divi 5 Public Beta. We’re moving full speed ahead with feature development. We have seven teams working full-time on Divi 5. Here is what they’re up to. 👇 We finished this feature and released it in Divi 5 Public Alpha 16. 🎉 Extend Attributes is like a design superpower, taking Divi 4’s Extend Styles features to the next level. Give it a try today! We finished this feature and released it in Divi 5 Public Alpha 15.1. 🎉 Settings Search and Filtering allows you to search and filter the Divi 5 settings panel, making working with Divi’s vast array of design options much easier. You may be familiar with this feature in Divi 4, and due to popular demand, we brought it to Divi 5 and made it even better. Give it a try today! We finished this feature and released it in Divi 5 Public Alpha 14.1. 🎉 Attribute Management empowers you to selectively or collectively copy, paste, and reset attributes across elements. It’s the most comprehensive attribute system for any builder! 🤩 Give it a try today! We gave a sneak peek at Loop Builder a couple of weeks ago, and the feature is already 60% complete. The idea behind Divi’s loop builder is simple: loop anything! You can create new post-based content by looping sections, rows, columns, groups, modules, and sub-elements. We gave a sneak peek of Relative Colors a few weeks ago, and the feature is already 80% complete. Get ready for a feature announcement soon! You can create true relative color systems based on color variables with Relative Colors. No other page builder has this color variable/HSL integration, and I think you will love it. Flexbox Layouts and Controls is a massive feature, but development is progressing faster than anticipated. This feature is nearly 90% complete, and you shouldn’t have to wait much longer before it’s live. Flexbox layouts will have great synergy with Module Groups and Nested Rows. Divi 5’s layout system is about to reach its final form! This feature got a rough start, but the team has hit its stride. We plan to finish our first big batch of modules in a couple of weeks, including all product modules, accounting for most of Divi’s WooCommerce modules. That will set us up nicely for a mid-year Divi 5 Beta launch, with plenty of time to release the official version this year. Here is a list of all the changes implemented in Public Alpha 16. It’s important to note that Divi 5 is production-ready for use on new websites today! Due to the scope of the Divi 5 project, we took a non-traditional approach to its release schedule, releasing it in five phases. The ultimate goal is to make Divi 5 as helpful as possible, as soon as possible, to as many people as possible. We are in the Alpha phase, akin to “Divi 5 Lite.” It’s missing a few features, but what’s done is ready to be used. It’s far superior to Divi 4 in almost every way. If you prefer Divi 5, you can use it to build new websites. If an existing website depends on a feature that hasn’t been added to Divi 5, you should continue using Divi 4. WooCommerce modules are the only things missing. Even then, Divi 4’s legacy WooCommerce modules will still work in Divi 5 in backward compatibility mode. We aim to enter beta within ~3 months based on our progress. There are three things we want to accomplish before entering the beta phase, and half of our team is focused on these tasks: Divi 5 will reach its “final form” before the end of the year, with the ultimate goal of leapfrogging the competition before the year ends. Part of that goal involves aggressively developing new features as part of the feature swap. But don’t forget! Between now and then, you can use Divi 5 whenever it becomes your preferred experience. Instead of rushing to a final release, we’re maintaining both Divi 4 and Divi 5, giving users a choice. Once we’re confident in removing that choice, we’ll make it “official,” and everyone will get the Divi 5 upgrade notification. The Divi 5 Public Alpha is available today. Give it a try, and let us know what you think! If you find a bug, report it to our team, and we’ll fix it. We’ll continue to work aggressively to add new features and address feedback. Stay tuned for a new Divi 5 version every two weeks. Browse hundreds of modules and thousands of layouts. Learn about the new way to manage your Divi assets. Get fast WordPress hosting optimized for Divi. I am the founder and CEO here at Elegant Themes. When I'm not hard at work on new themes, I enjoy writing an article or two on our blog! Posted on June 3, 2025 in Divi Resources Divi empowers you to build the best websites possible, and now, Divi Quick Sites takes website creation to a whole new level. This revolutionary tool lets anyone, regardless of skill level, generate a complete website in under two minutes! Divi Quick Sites provides everything you need to launch... Posted on June 3, 2025 in Divi Resources Divi empowers you to build the best websites possible, and now, Divi Quick Sites takes website creation to a whole new level. This revolutionary tool lets anyone, regardless of skill level, generate a complete website in under two minutes! Divi Quick Sites provides everything you need to launch... Posted on June 2, 2025 in Divi Resources Divi has long been a powerhouse for building beautiful websites. Specialty Sections extended Divi’s layout capabilities for years by enabling complex, nested column structures that Regular Sections couldn’t handle. But this approach came with its complexity and limitations, tied deeply to... I hope there is space to access all the “Design System” features from the Divi Panel. So far you need to access a page first. Thank you for the hard work! Ha, it’s almost like you read my comment this morning…. [crying with laughter emoji]. Well done guys. Is there a way to extend attributes through the entire site? I didn’t see the Blog Module bug with custom CPT’s fixed in this release (verified it by testing 16.0.) Is it coming in a 16.1 update? I’ve reported it to support and they were able to duplicate it in 15.1. Are you planning on updating the Divi menu module at all with more options? That is going to happen after DIVI 5 comes out. Getting those WooCommerce modules done is like the only thing holding back the DIVI 5 theme from a full release. The never-ending story of the WooCommerce modules. Just a couple more weeks for the first batch to finish.. Δ We offer a 30 Day Money Back Guarantee, so joining is Risk-Free! Copyright © 2025 Elegant Themes ®
--------------------------------------------------

Title: Global Deferiprone Market to Hit Valuation of US$ 55.22 Million by 2033 | Astute Analytica
URL: https://www.globenewswire.com/news-release/2025/06/06/3095078/0/en/Global-Deferiprone-Market-to-Hit-Valuation-of-US-55-22-Million-by-2033-Astute-Analytica.html
Time Published: 2025-06-06T12:30:00Z
Full Content:
June 06, 2025 08:30 ET | Source: AstuteAnalytica India Pvt. Ltd. AstuteAnalytica India Pvt. Ltd. Chicago, June 06, 2025 (GLOBE NEWSWIRE) -- The global deferiprone market was valued at US$ 39.89 million in 2024 and is expected to reach US$ 55.22 million by 2033, growing at a CAGR of 3.68% during the forecast period 2025–2033. Clinicians confronted with transfusion-dependent thalassemia major, sickle cell disease, myelodysplastic syndromes, and rare inherited anemias increasingly rely on iron chelation to prevent cardiomyopathy and hepatic failure. Against this backdrop, the Deferiprone market has become an indispensable component of comprehensive hematology care. The World Health Organization records roughly 63,000 births each year with homozygous β-thalassemia, while global registries list more than 300,000 newborns with sickle cell disease annually. Each patient can accumulate up to 0.3 grams of iron from a single red-cell transfusion, and many receive at least 20 transfusions per year, creating an urgent clinical mandate for efficient, orally available chelators. Download Sample Pages: https://www.astuteanalytica.com/request-sample/deferiprone-market Deferiprone, introduced three decades ago and now available in tablet, oral solution, and extended-release forms, remains the only bidentate chelator capable of accessing intracellular labile iron pools. Consequently, cardiologists report a tangible decline in transfusion-related cardiac deaths wherever routine chelation includes this agent. Recent meta-analyses covering 8,700 patient-years demonstrate a mean reduction of 5.6 micrograms per gram dry weight in hepatic iron when Deferiprone is combined with standard therapy, underscoring its synergistic potential. These hard clinical outcomes, rather than promotional narratives, underpin sustained growth of the market. Key Findings in Deferiprone Market Incidence Data Highlights Unmet Needs Fueling Ongoing Clinical Demand Worldwide Drilling deeper into epidemiology exposes the granular patient pools that continually replenish the Deferiprone market. India’s national thalassemia registry listed 99,258 transfusion-dependent individuals by February 2024, an increase of 4 600 cases over the preceding two years. China’s southern provinces collectively treat about 88,000 comparable patients, according to Guangdong’s Center for Genetic Blood Disorders. Meanwhile, the Eastern Mediterranean region follows closely with approximately 40,000 clinically significant cases managed across Egypt, Iran, and Greece. Because each thalassemic child consumes nearly 18 units of packed red cells annually, hematology day-care centers in these geographies order chelators at volumes previously seen only in tertiary Western hospitals. Parallel demand drivers emerge from sickle cell cohorts. Nigeria hosts an estimated 150,000 children living with severe sickle cell complications that necessitate regular exchange transfusion protocols; Democratic Republic of Congo manages close to 90 000 comparable cases. In the United States, the Centers for Disease Control and Prevention documents roughly 100,000 affected individuals, about 6 000 of whom are on chronic transfusion programs. Each exchange procedure introduces between 200 and 300 milligrams of additional iron, rapidly surpassing physiologic clearance thresholds. Without prompt chelation, organ damage progresses within five years of therapy initiation. This confluence of sizeable, transfusion-dependent populations sustains robust throughput across the Deferiprone market. Regulatory Milestones And Label Expansions Influence Competitive Positioning Dynamics Globally Regulators have steadily broadened clinical indications, materially altering how stakeholders appraise the Deferiprone market. The US Food and Drug Administration granted the molecule’s first domestic approval in October 2011 for transfusional iron overload in thalassemia major, followed by an adolescent label extension in 2015. A further update in July 2022 introduced sickle cell disease and other chronic anemias to the prescribing information, effectively doubling the addressable patient pool. Europe mirrored this trajectory; the European Medicines Agency endorsed pediatric use down to six years in 2021, while Italy’s AIFA added compassionate reimbursement for myelodysplastic syndromes in 2023 based on real-world registry data involving 312 adults. In parallel, authorities in Japan (May 2021) and South Korea (January 2024) issued marketing licenses after local bridging studies confirmed comparable pharmacokinetics to Ferriprox. China’s National Medical Products Administration accepted Anhui Golden Sun’s abbreviated dossier in March 2024, paving the way for the country’s first domestic generic launch in early 2025. Each regulatory green light not only widens clinical access but also recalibrates competitive strategy, particularly for originator holder Chiesi Global Rare Diseases. As time-bound exclusivities expire, dossier approvals in emergent jurisdictions intensify price competition yet expand overall prescription volume, a dynamic that keeps the Deferiprone market fluid and opportunity-rich. Technological Innovations Elevate Formulation Convenience And Patient Adherence Metrics Substantially Product engineering advances now concentrate on reducing pill burden and mitigating gastrointestinal intolerance—two chief reasons patients abandon chelation, a persistent pain point for the Deferiprone market worldwide. A landmark 2023 study in Haematologica detailed how a once-daily 1,500 mg modified-release matrix reduced reported nausea episodes by 35 per thousand dosing days when compared with three-times-daily immediate-release tablets. The formulation, co-developed by Chiesi and Skyepharma, maintains serum trough concentrations above the 20 µmol/L threshold for eight continuous hours. Such pharmacokinetic smoothing aligns with contemporary adherence-support paradigms, and its commercial roll-out is already reshaping hospital formularies within the Deferiprone market. Parallel innovation is unfolding in digital therapeutics. Bluetooth-enabled smart bottle caps paired with HIPAA-compliant mobile applications now log real-time dosing events, transmit adherence alerts to clinicians, and integrate ferritin laboratory values imported from electronic health records. In a Cleveland Clinic pilot released April 2024, median adherence across 78 patients improved from 61 to 82 missed doses per year after deployment of the connected packaging. By fusing patient engagement technology with improved pharmacology, stakeholders are addressing both sides of the persistence equation. These synergistic advances enhance measurable outcomes and create differentiated value propositions inside the Deferiprone market. Regional Disparities Reveal Access Barriers And Emerging Distribution Partnerships Initiatives The geographical distribution of chelation therapy remains uneven, creating stark contrasts in treatment standards. Only 14 of Africa’s 54 countries list deferiprone on national essential medicine schedules, and fewer than 25 tertiary centers on the continent possess consistent drug supply. Freight cost spikes—averaging 1,200 US dollars per pallet in 2023—contribute to recurrent shortages. To bridge the gap, the humanitarian consortium Global Blood Initiative entered a five-year memorandum with Chiesi in November 2023, committing 1.2 million tablet equivalents annually for donation to Uganda, Tanzania, and Ghana. Such public-private ventures broaden humanitarian visibility but capture only a fraction of pent-up demand within the Deferiprone market. Conversely, Latin America is charting collaborative distribution models that leverage regional manufacturing. Argentina’s Akron Pharma began local blister production under license in late 2022, trimming delivery lead times from 90 to 28 days and reducing import duties. Brazil’s Ministry of Health followed with a technology-transfer agreement, aiming to reach 12 state oncology centers by mid-2024. These initiatives dovetail with Pan-American Health Organization procurement platforms, boosting negotiating power and ensuring continuous stock. Successful replication of such frameworks across additional low-resource settings could dramatically level access inequities and unlock dormant segments of the Deferiprone market. Competitive Landscape Intensifies As Generics Challenge Established Brand Positioning Globally Chiesi’s Ferriprox still holds leadership status, yet 2023 marked the arrival of formidable generic contenders that are recalibrating cost dynamics. Teva’s USP-validated tablets gained final FDA approval in December 2023, followed by Hikma’s submission acceptance in the European Union during February 2024. In India, where reverse-engineering prowess is well entrenched, at least seven Drug Controller General-approved brands—including Cipla’s Sifinox and Natco’s Ferinol—vie for hospital tenders. Wholesale acquisition costs now vary by up to 19 US dollars per gram across jurisdictions, underscoring widening price dispersion. This intensifying rivalry adds fresh complexity but also broadens affordability within the Deferiprone market. The originator is countering with value-added services rather than aggressive discounting. An expanded Ferriprox Total Care program launched in April 2024 bundles genetic counseling, adherence coaching, and free absolute neutrophil count monitoring for eligible US patients. In Europe, Chiesi is piloting an outcomes-based reimbursement contract with the National Health Service that ties payment tranches to serial ferritin and T2* cardiac MRI improvements. Meanwhile, smaller specialty firms are targeting niche subpopulations such as pyruvate kinase deficiency, hoping to differentiate beyond price. The collective result is a vibrant, multifactorial contest that keeps strategic stakes high across every layer of the Deferiprone market. Pipeline Exploration Broadens Therapeutic Horizons Beyond Hematology Core Applications Frontier Academic consortia have begun exploiting deferiprone’s metal chelation profile to modulate neurodegenerative pathways characterized by aberrant iron deposition. The University of Melbourne’s FAIR-PARK II trial enrolled 372 early-stage Parkinson’s patients and reported in 2023 that twice-daily 30 mg/kg dosing slowed Unified Parkinson Disease Rating Scale motor score progression by 3.2 points over two years. Similarly, the Italian TIRCON network is running a 120-patient phase III study in neurodegeneration with brain iron accumulation, with interim analyses expected Q4 2024. Positive readouts could create fresh regulatory submissions and extend the Deferiprone market well beyond hematology clinics. Other exploratory fronts include a 60-patient US National Eye Institute protocol investigating retinal iron overload in age-related macular degeneration, and a multicenter French study assessing intravenous deferiprone for acute mucormycosis adjunctive therapy. While these indications are still nascent, they illustrate the molecule’s mechanistic versatility and the appetite for repositioning small-molecule chelators. Investors tracking orphan-neurology spaces are already modeling incremental revenue accretion scenarios, contingent on phase III success. Should even one of these programs translate into approval, the Deferiprone market will witness a meaningful broadening of prescriber base, reinforcing long-term growth prospects. Request Report Customization: https://www.astuteanalytica.com/ask-for-customization/deferiprone-market Future Outlook Balances Opportunities Against Pricing And Safety Constraints Ahead Looking toward 2033, stakeholders likely to anticipate that cumulative clinical evidence, label expansions, and digital adherence innovations will keep patient uptake on an upward trajectory. Annual global blood transfusion volumes surpassed 120 million units in 2024, and each incremental liter of packed cells revalidates the clinical rationale for chelation. Policy momentum is equally supportive; the World Bank’s new Global Sickle Cell Strategy allocates 700 million US dollars over the next five years to holistic care packages that explicitly list oral iron chelators. These macro-level events establish a sturdy demand floor for the Deferiprone market. Nonetheless, certain headwinds deserve equal scrutiny. Neutropenia rates of 1.7 cases per thousand patient-months, documented in the 2024 Deferiprone Safety Consortium registry (n = 12 456), demand continued pharmacovigilance and proactive monitoring infrastructure. On the economics side, payer pressure is escalating; Germany’s G-BA has initiated a benefit-assessment reassessment post generic entry, and India’s National Pharmaceutical Pricing Authority is reviewing ceiling prices for chelation agents in 2025. These converging forces will oblige manufacturers to refine risk-management programs and optimize supply-chain efficiencies. Successfully navigating such constraints will delineate tomorrow’s winners within the Deferiprone market. Global Deferiprone Market Key Players: Key Segmentation: By Type By Therapeutic Use By Indication By Region Need More Info? Ask Before You Buy: https://www.astuteanalytica.com/inquire-before-purchase/deferiprone-market About Astute Analytica Astute Analytica is a global market research and advisory firm providing data-driven insights across industries such as technology, healthcare, chemicals, semiconductors, FMCG, and more. We publish multiple reports daily, equipping businesses with the intelligence they need to navigate market trends, emerging opportunities, competitive landscapes, and technological advancements. With a team of experienced business analysts, economists, and industry experts, we deliver accurate, in-depth, and actionable research tailored to meet the strategic needs of our clients. At Astute Analytica, our clients come first, and we are committed to delivering cost-effective, high-value research solutions that drive success in an evolving marketplace. Contact Us:Astute AnalyticaPhone: +1-888 429 6757 (US Toll Free); +91-0120- 4483891 (Rest of the World)For Sales Enquiries: sales@astuteanalytica.comWebsite: https://www.astuteanalytica.com/ Follow us on: LinkedIn | Twitter | YouTube Chicago, June 11, 2025 (GLOBE NEWSWIRE) -- The global broadband services market was valued at US$ 506.35 billion in 2024 and is expected to reach US$ 1,136.60 billion by 2033, growing at a CAGR of... Chicago, June 11, 2025 (GLOBE NEWSWIRE) -- The global system infrastructure software market was valued at US$ 164.76 billion in 2024 and is expected to reach US$ 257.80 billion by 2033, growing at a...
--------------------------------------------------

Title: Dave Vellante’s Breaking Analysis: The complete collection
URL: https://siliconangle.com/2025/06/06/dave-vellantes-breaking-analysis-complete-collection/
Time Published: 2025-06-06T10:33:12Z
Full Content:
UPDATED 06:33 EDT / JUNE 06 2025 BREAKING ANALYSIS by Dave Vellante Breaking Analysis is a weekly editorial program combining knowledge from SiliconANGLE’s theCUBE with spending data from Enterprise Technology Research. Branded as theCUBE Insights, Powered by ETR, the program is our opportunity to share independent, unfiltered editorial with SiliconANGLE, theCUBE and Wikibon communities. The program and conclusions we produce are data-driven, tapping ETR’s proprietary spending data set. Episode 221 – Nvidia, Broadcom and the expanding breadth of AI – We attended both Nvidia Corp.’s GTC conference and Broadcom Inc.’s investor day this week where the artificial intelligence platform shift was on full display. In our view, GTC24 was the most important event in the history of the technology industry, surpassing Steve Jobs’ iPod and iPhone launches. The event was not the largest but, in our opinion, it was the most significant in terms of its reach, vision, ecosystem impact and broad-based recognition that the AI era will permanently change the world. Meanwhile, Broadcom’s first investor day underscored both the importance of the AI era and the highly differentiated strategies and paths that Nvidia and Broadcom are each taking. We believe Nvidia and Broadcom are currently the two best-positioned companies to capitalize on the AI wave and will each dominate their respective markets for the better part of a decade. But importantly, we see them each as enablers of a broader ecosystem that collectively will create more value than either of these firms will in and of themselves. In this Breaking Analysis, we will share our perspectives on the state of AI and how Nvidia and Broadcom are each leading the way with dramatically different but overlapping strategies that may be headed for an eventual collision course. Watch the full video analysis. Episode 220 – Navigating NVIDIA & the AI Trade – Sell, hold or double down? – Heading into the second half of 2023, some investors felt that the semiconductor run up last summer was a harbinger for a broader tech rally. That thesis proved prescient and rewarded managers who took on risk at the time with leading firms in semiconductors, security and enterprise software. The question is, where do we go from here? In this Breaking Analysis we welcome back Ivana Delevska, the founder and chief investment officer at Spear Invest, Nasdaq SPRX. Some have compared SPRX to a miniature version of Cathie Wood’s ARKK fund. However SPRX is more sector agnostic where Delevska focuses more broadly on growth themes such as her current emphasis on cybersecurity, semiconductors, and enterprise software. Watch the full video analysis. Episode 219 – Why CrowdStrike is separating from the cybersecurity pack – It’s been an interesting month in the cybersecurity space. The sector has been somewhat less affected by budget tightening these past twenty-four months and at the same time has benefitted from AI tailwinds. But in the past several weeks we’ve seen some separation in key highflying cybersecurity names. Specifically, Palo Alto shocked the street last month with a $600M billings forecast surprise and sounded the alarm that there were cracks in its consolidation execution. This dragged down other consolidation players in sympathy, namely CrowdStrike and Zscaler. But our research shows that the dynamics facing these three companies are quite different. Of particular note, CrowdStrike’s earnings print highlights the company’s impressive momentum while recent negativity around Zscaler is a bit of a head scratcher for us, which we’ll try to explain. In this Breaking Analysis we take a more narrow look at the information security space and dig deeper into the continued success of CrowdStrike. With recent survey data from ETR, we continue to advance our premise that platforms beat products and we identify several levers that are powering CrowdStrike’s path to $5B by FY 2026 and to $10B by the end of the decade. Watch the full video analysis. Episode 218 – The unplanned genius of Broadcom’s route to AI dominance – Broadcom is perhaps the most unique company in the technology business. It doesn’t simply chase markets that are on steep growth curves and can deliver short term ROI. Rather it goes after established markets with durable franchises. Broadcom focuses its R&D on serving customers in these markets with major engineering investments to achieve a dominant position in each of its target sectors. And sometimes, the company lucks out with this strategy and catches a wave accidentally by design. In this Breaking Analysis we extract key nuggets from our sit down at MWC this week with Charlie Kawwas, president of Broadcom’s Semiconductor Solutions Group, and we unpack the contrarian business technology model of Broadcom. Watch the full video analysis. Episode 217 – Cloud optimization wanes as AI slowly lifts off – The past twenty-four months have seen cloud spending face dual headwinds of macroeconomics and the ability to dial down resources as needed – i.e. cloud optimization. Nonetheless, the big four hyperscalers clocked in between $170 – $190B in IaaS and PaaS revenue last year depending on how you factor the leaked court documents suggesting Azure is much smaller than previously believed. Regardless, hyperscaler growth continued to outpace almost all markets, accelerating between 18-19% in revenue terms last year, despite their enormous size. As we progress into 2024, IT decision makers are cautiously optimistic about spending levels, especially for the second half. All hyperscalers report that cloud optimization is slowing although pockets of cloud cost cutting remain. While AI gets all the headlines, its contribution to revenue is still a small fraction of the overall spending pie. For example, we estimate that Microsoft’s AI services accounted for around $800M this past quarter. But the trajectory for AI services and the potential uplift looks promising for all four hyperscalers. We think collectively the generative AI uplift in cloud will surpass $10B this year. In this Breaking Analysis we update you on our latest hyperscale cloud spending and market share data. We’ll analyze the ETR survey data on cloud optimization, assess the Gen AI updraft for the big 3 US cloud players and look at some of the industry trend data on cloud spend by platform. Watch the full video analysis. Episode 216 – Intel Foundry is a bold bet filled with uncertainty – As an American, you can’t help but root for Intel CEO Pat Gelsinger to succeed. His vision to bring semiconductor manufacturing leadership back to the United States is more than just a quaint nationalistic sentiment. Rather it’s a strategic imperative for the country, its military, global competitiveness and access to future technological innovations in the AI era. But his strategy is dependent upon the success of Intel both as a designer and a leading manufacturer of advanced chips. As such this choice puts Intel in a multi-front war with highly capable leaders in several markets, including names like AMD, NVIDIA, AWS, Google, Microsoft, Apple, Tesla and other chip designers…even perhaps OpenAI. As well Intel competes with with established manufacturers like Taiwan Semiconductor and Samsung. Moreover, Intel’s business model has been disrupted by Arm which has created a volume standard powered by the iPhone and mobile technologies. Finally, China, Inc. looms as a long-term competitor further underscoring the imperative. But the trillion dollar questions are: 1) What are the odds that Intel’s strategy succeeds; and 2) Are there more viable alternative strategies for both Intel and the United States? Watch the full video analysis. Episode 215 – Slicing the Gordian Knot – A leap to real time systems of truth – In order to support the vision of the sixth data platform, that is, a capability that allows a globally consistent, real-time, intelligent digital representation of a business, we believe the industry must rethink the single system of truth. Specifically, we envision a new data platform that marries the best of relational and nonrelational capabilities and breaks the multi-decades tradeoffs between data consistency, availability and global scale. Further, we see the emergence of a modular data platform that automates decision-making by combining historical analytic systems with transactions to enable artificial intelligence to take action. In this Breaking Analysis, we welcome two innovators, Eric Berg, chief executive of Fauna Inc., and S. Somasegar, managing director at Madrona Ventures. Watch the full video analysis. Episode 214 – Enterprise Technology Predictions 2024 – Predictions about enterprise tech have never been more uncertain. They become even more challenging when you try to make forecasts that are measurable. Generally, our belief is we should be able to look back a year later and say with some degree of certainty whether the prediction came true – ideally with some quantifiable evidence to back that up. In this Breaking Analysis and for the third year in a row, we collaborate with Erik Bradley of Enterprise Technology Research and to share our annual enterprise technology predictions. Watch the full video analysis. Episode 213 – 2024 IT spending outlook shows cautious start with optimistic finish – According to recent spending intentions data from over 1,700 information technology decision-makers, executives anticipate a 4.3% growth in technology budgets for the year, which is an improvement from the 3.5% growth seen in 2023 and higher than the 3.8% expectation from October. However, the forecasts for 2024 are back-loaded, with Q1 2024 forecasts at 2.4%, indicating that the optimism is concentrated in the second half of the year. Watch the full video analysis. Episode 212 – Unifying intelligence in the age of data apps – We believe the future of intelligent data apps will enable virtually all organizations to operate a platform that orchestrates an ecosystem similar to that of Amazon.com. By this we mean dynamically connecting and digitally representing an enterprise’s operations including its customers, partners, suppliers and even competitors. This vision includes the ability to rationalize top down plans with bottom up activities across the many dimensions of a business – e.g. demand, product availability, production capacity, geographies, etc. Unlike today’s data platforms, which generally are based on historical systems of truth, we envision a prescriptive model of a business’ operations enabled by an emerging layer that unifies the intelligence trapped within today’s application silos. In this Breaking Analysis, we explore in depth, the semantic layer we’ve been discussing since early last year. To do so we welcome Molham Aref, the CEO of RelationalAI. Watch the full video analysis. Episode 211 – Grading Our 2023 Enterprise Technology Predictions – Predictions about the future of enterprise tech are streaming to our inboxes, literally by the thousands. Most are thoughtful and we will review those prior to publishing our 2024 predictions later in January. As is our tradition, we try to make our own predictions more challenging by citing forecasts that are measurable and have either a numeric tied to them or a binary outcome. Our belief is if we make a prediction, you should be able to look back a year later and say with some degree of certainty whether the prediction came true or not. With some empirical evidence to back that up. In this Breaking Analysis we grade the 2023 predictions we made with ETR’s Erik Bradley. We look back at what we said in January about the macro IT spending environment, cost optimization, security, generative AI, cloud, blockchain, data platforms, automation and tech events. Watch the full video analysis. Episode 210 – David vs Goliath reimagined – OpenAI’s approach to AI supervision – Artificial general intelligence, or AGI, has people both intrigued and fearful. As a leading researcher in the field, last July, OpenAI introduced the concept of superalignment via a team created to study scientific and technical breakthroughs to guide and ultimately control AI systems that are much more capable than humans. OpenAI refers to this level of AI as superintelligence. Last week, this team unveiled the first results of an effort to supervise more powerful AI with less powerful models. While promising, the effort showed mixed results and brings to light several more questions about the future of AI and the ability of humans to actually control such advanced machine intelligence. In this Breaking Analysis we share the results of OpenAI’s superalignment research and what it means for the future of AI. We further probe ongoing questions about OpenAI’s unconventional structure which we continue to believe is misaligned with its conflicting objectives of both protecting humanity and making money. We’ll also poke at a nuanced change in OpenAI’s characterization of its relationship with Microsoft. Finally we’ll share some data that shows the magnitude of OpenAI’s lead in the market and propose some possible solutions to the structural problem faced by the industry. Watch the full video analysis. Episode 209 – Moving beyond separation of compute & storage…Journey to the 6th data platform – We believe today’s so-called modern data stack, as currently envisioned, will be challenged by emerging use cases and AI-infused apps that begin to represent the real world, in real time, at massive data scale. To support these new applications, a change in underlying data and data center architectures will be necessary, particularly for exabyte scale workloads. Today’s generally accepted state of the art of separating compute from storage, must evolve in our view to separate compute from data and further enable compute to operate on a unified view of coherent and composable data elements. Moreover, our opinion is that AI will be used to enrich metadata to turn strings (i.e. ASCII code, files, objects, etc.) into things that represent real world capabilities of a business. In this Breaking Analysis we continue our quest to more deeply understand the emergence of a sixth data platform that can support intelligent applications in real time. To do so we are pleased to welcome two founders of VAST Data, CEO Renen Hallak and Jeff Denworth. VAST just closed a modest $118M financing round that included Fidelity at a valuation of ~$9B, which implies a quite minor change to the cap table. Watch the full video analysis. Episode 208 – re:Invent ’23 underscores a new simplicity mandate for AWS – Generative AI has created a new mandate in enterprise tech with significant implications for all companies generally and AWS specifically. Amazon’s powerful playbook based on agility, developer choice, power, scale, reliability and security must now evolve to accommodate simplicity and coherence for mainstream customers. This imperative came into clear focus at AWS re:Invent ’23. AWS continues to innovate at a fast pace, but must now do so in a changing customer environment that increasingly values direct user productivity gains through software. In this Breaking Analysis we share our take on how AWS is navigating this challenge. We’ll review Amazon’s strategy to compete in the nascent Gen AI era and we’ll provide commentary on the chess moves it’s making with Anthropic, Nvidia and other partners to maintain its leadership position. We’ll also discuss the challenges of doing so as a $90+B giant in a fast-moving market. Watch the full video analysis. Episode 207 – The OpenAI meltdown…Winners and losers in the battle for AI supremacy – Conventional wisdom says Microsoft is the big winner in the recent OpenAI saga. We don’t quite see it that way. Both Microsoft and OpenAI are in a worse position today than they were last Thursday, prior to the firing of OpenAI CEO Sam Altman and the ongoing public drama that ensues. Microsoft and OpenAI had a huge lead in market momentum, AI adoption and feature acceleration and were setting the narrative in AI. Our discussions with customers and industry insiders leads us to conclude that the duo has put its substantial lead at risk. While Satya Nadella is making lemonade from lemons, the window was just cracked open for the competition and it’s more clear than ever that one large language model will not rule them all. In this Breaking Analysis we weigh in on the impacts of the OpenAI meltdown with a deeper look at the customer perspective and how it alters the competitive landscape in the battle for AI supremacy. As well, the amazing data team at ETR has run a quick survey of OpenAI Microsoft customers to gauge reactions and we’ll share that fresh data. Watch the full video analysis. Episode 206 – The copilot era takes flight at Microsoft Ignite 2023 – Microsoft Ignite 2023 was one part a celebration of yearlong technological innovations, one part announcing the general availability of previously announced products, one part vision, one part ecosystem and four parts copilots everywhere. Copilots promise an historic software-led productivity increase. Perhaps for the first time in industry history we’re seeing huge demand for software coincide with the ability to make it easier to write software. Just as AWS turned the data center into an API, copilots are turning software development into natural language, enabling many more people to create. The implications on productivity are massive and we believe will kick off a new wave of growth that will become increasingly noticeable throughout 2024. In this Breaking Analysis we give you our impressions of Microsoft Ignite 2023. theCUBE Research Analyst George Gilbert and CUBE Collective contributor Sarbjeet Johal both weighed in for this episode and we’ll also share some recent ETR data that shows the progression of some of the major AI players in the past twelve months and the the relative impact Gen AI has had on each of their businesses. Watch the full video analysis. Episode 205 – IBM turns the corner with Watson, why 2.0 is a breakthrough opportunity – With Watson 1.0, IBM deviated from the silicon valley mantra, fail fast, as it took nearly a decade for the company to pivot off of its original vision. In our view, a different dynamic is in play today with Watson 2.0 – i.e. watsonx. IBM’s deep research in AI and learnings from its previous mistakes, have positioned the company to be a major player in the Generative AI era. Specifically, in our opinion, IBM has a leading technological foundation, a robust and rapidly advancing AI stack, a strong hybrid cloud position (thanks to Red Hat), an expanding ecosystem and a consulting organization with deep domain expertise to apply AI in industry-specific use cases. In this Breaking Analysis we share our takeaways and perspectives from a recent trip to IBM’s research headquarters. To do so we collaborate with analyst friends in theCUBE Collective, Sanjeev Mohan, Tony Baer and Merv Adrian. We’ll also share some relevant ETR spending data to frame the conversation. Watch the full video analysis. Episode 204 – The Gen AI power law – How data diversity influences adoption – Our research indicates that the adoption of generative AI is occurring across a variety of frameworks with diverse deployment models, data sets and domain specificities. The transformative power of generative AI for industries is colossal, and we are convinced that AI will gravitate to where data lives. The power law of Gen AI, developed by theCUBE Research team, describes the adoption patterns we see emerging in the cloud, on-premises and at the edge with a long tail of highly specific domain models across every industry. In this Breaking Analysis we revisit the Gen AI power law which informs how we see adoption happening within industries and organizations globally. We introduce new survey data from ETR that looks at some of the vendors being adopted or considered with a special peek into the emerging technology sector with data on Hugging Face, Anthropic, Cohere and Jasper. We also comment on the adoption of Meta’s Llama 2 and the potential impact of open source and other third parties on the shape of the curve. We share our estimates of Microsoft Azure’s AI revenue impact, which we see approaching a $2B run rate exiting 2023. Finally, we dig into the impact of retrieval augmented generation (RAG) using real world examples with some caveats of RAG implementations that practitioners should consider. Watch the full video analysis. Episode 203 – AI revenue riddle – Azure sees gains, when will other cloud titans see the surge? – In 1987, Nobel Prize-winning economist Bob Solow famously observed, “You can see the computer age everywhere but in the productivity statistics.” This proclamation became known as the productivity paradox. Ironically, Solow’s statement preceded the greatest productivity boom since the dawn of the computer age which subsequently came to fruition in the 1990’s. It can be argued that a similar pattern is being seen today where AI is everywhere but generally not showing up in earnings numbers or productivity statistics…yet. In this Breaking Analysis we squint through the latest earnings reports from Microsoft, Alphabet and Amazon to understand what’s happening in cloud, evaluate the impact or lack thereof of AI on cloud earnings momentum and explain how we think about the future impact of generative AI and cloud. Watch the full video analysis. Episode 202 – From hype to reality, the true state of AI adoption – MIT professor and economist Erik Brynjolfsson said recently that he’d be disappointed if AI didn’t lift the current anemic 1.2% productivity growth rate to 3% or even 4%. This would be a good thing for business and government as it could potentially help with the labor shortage, drive earnings growth and increase tax revenues, which would ostensibly help address current debt levels. This is one of the promised impacts of AI. While the hype surrounding Gen AI has narrowly propped up certain sectors of the market, like AI startups and the magnificent seven, the macro effects have not been felt thus far as adoption remains largely experimental. In this Breaking Analysis and ahead of Supercloud 4, ETR’s Erik Bradley and Daren Brabham join the program to share the latest trends on AI adoption, how Gen AI is being used, some of the deployment models and the AI leaderboard based on spending momentum and presence in the market. Watch the full video analysis. Episode 201 – Get Ready for the Sixth Data Platform – In the next 3-5 years, we believe a set of intelligent data apps will emerge requiring a new type of modern data platform to support them. We refer to this as a sixth data platform. In our previous research we’ve used the metaphor “Uber for everyone” to describe this vision; meaning software-based systems that enable a digital representation of a business. Watch the full video analysis. Episode 200 – Lower for longer…Tech spending remains tepid – We’re getting used to the phrase, “higher for longer,” referring to the realization that interest rates are expected to remain elevated for a period of time. This trend is having an inverse effect on enterprise tech spending growth rates. Prior to the Fed’s tightening binge for example, IT decision makers (ITDMs) in aggregate expected annual technology spending to increase by 7.5%. Eleven fed interest rate hikes later, ITDMs estimate that their 2023 budgets will be up only 2.9%, with an expectation, or perhaps it’s a wishful hope, that their budgets will increase 3.8% in 2024. In this our 200th Breaking Analysis, we preview the current spending climate and where AI fits in relation to other sectors. We’ll also share with you a snapshot of the leaders in terms of spending velocity for their platforms; and how their performance compares to peers relative to earlier survey periods. Watch the full video analysis. Episode 199 – Cisco Splunk under the microscope, joint customers weigh in – In this special Breaking Analysis, theCUBE’s Dave Vellante talks with Enterprise Technology Research’s Erik Bradley as he shares the results of ETR’s recent flash survey assessing joint customer perceptions and likely spending actions as a result of the Cisco acquisition of Splunk. Watch the full video analysis. Episode 198 – Bob Muglia on Uber for everyone…how the future of data apps will evolve – The future of intelligent data applications: Uber for everyone. In June, we put forth our scenario about “Uber for all.” We believe a new breed of data apps is emerging and we used Uber as an example where data about people, places and things is brought together in a data-coherent, real-time system. In this special Breaking Analysis, analysts Dave Vellante and George Gilbert sat down with friend of theCUBE, entrepreneur, investor and software executive, Bob Muglia. Bob formerly worked at Microsoft, was the CEO of Snowflake and is a visionary trend spotter. Watch the full video analysis. Episode 197 – Cloud security powers CrowdStrike momentum, Gen AI is next – George Kurtz is pumped up…and why not? CrowdStrike’s business appears to be on a fast track and entering a new phase of growth, despite the difficult macro and elongated sales cycles. The company’s products are considered best in class, its business is growing steadily and an improved profitability and cash flow outlook had investors excited, at least up until this week. A still challenging environment and a rich 13X revenue multiple perhaps led to some profit taking, but Gen AI could be the next catalyst for the company. In the race to close the SecOps staffing gap, CrowdStrike has what appears to be a strong play with a natural language-based intelligent assistant known as Charlotte AI. In this Breaking Analysis we update our scenario on security leader CrowdStrike. We’ll review the company’s recent progress, share survey data that shows where it is strong and where there may be icebergs ahead. And we’ll preview Fal.Con 2023 which takes place next week in Las Vegas. Watch the full video analysis. Episode 196 – Copilot or competitor – How gen AI bolsters and buffets UiPath’s Northstar – UiPath’s recent earnings beat and raise provides some evidence that thus far, Gen AI has not been dilutive for the company. As an early leader that is transforming beyond RPA toward end-to-end enterprise automation, UiPath, like all automation providers, has always faced adoption headwinds beyond isolated deployments. In this sense, Gen AI should bolster adoption and be a positive force. The flip side is that widely available tools like chatbots and generalized foundation models could eat away at the low end of the automation TAM, highlighting the urgency for companies like UiPath to move up market and accelerate innovation that brings differentiation from commoditized tools; and, importantly, create distance from embedded AI within mainstream enterprise SaaS platforms like Slack GPT and Salesforce Einstein. In this Breaking Analysis we briefly review the recent earnings print from UiPath. We’ll look at ETR survey data that shows Microsoft Power Automate’s impact on the automation market and how it is forcing UiPath to target larger accounts with a more functional product set. As well we’ll look at the impact that AI is having in these larger accounts and test UiPath management assertions that Gen AI will be a tailwind for the company. Watch the full video analysis. Episode 195 – Google goes all in on the AI cloud – At Cloud Next, Google showcased its strong leadership position in data and AI. In our view, Google’s messaging, demos and tech-centric narrative have broad appeal for developers and next generation startups. As well, the company’s focus on solutions, contrasts its strategy to the typically disjointed services we’ve seen from AWS over the past decade. Google also showed off an expanded ecosystem of GSIs and smaller CSPs, encouraging the broad use of Google’s kit globally. While Google remains a distant third in the Iaas/PaaS race, with revenue one-fifth the size of AWS, it is playing the long game and betting the house on AI as a catalyst to its cloud future. In this Breaking Analysis we unpack the key takeaways from Google Cloud Next with Rob Strechay and George Gilbert. We’ll share ETR data that positions Google’s AI relative to other leaders and we’ll contrast Google’s data-centric strategy with traditional architectural models. Watch the full video analysis. Episode 194 – Snowflake has Momentum with AWS & Microsoft…Why Google may not be Next – Recent earnings prints from Amazon and Snowflake, along with new survey data, have provided additional context on top of the two events that Snowflake and Databricks each hosted last June. Specifically, we believe that the effects of cloud optimization are still being felt but are nearing the end of peak negative impact on cloud companies. Snowflake’s recent renewal with Microsoft better aligns sales incentives and should improve the company’s traction with Microsoft Azure, a platform that has long favored Databricks. Google however remains a different story as its agenda is to build out its own data cloud stack, rather than supporting Snowflake’s aspirations. Watch the full video analysis. Episode 193 – VMware’s Future – Navigating Multi cloud Complexity & GenAI Under Broadcom’s Wing – The FTC continues to drag its feet on approving Broadcom’s acquisition of VMware. Ironically, in our view, these delays only hurt the very competitive environment the FTC claims to be protecting. The AI era is accelerating at a breakneck pace and the big 3 hyperscale cloud vendors already have a sizable lead on legacy incumbents. If preserving competition is truly the agenda of the U.S. government, it should recognize that VMware, its enterprise ecosystem and market forces have the potential to neutralize cross cloud complexity and give customers a viable alternative to increasingly powerful public cloud players. In this Breaking Analysis and ahead of VMWare Explore 2023, we revisit our views on Broadcom’s rationale and likely actions post acquisition. We’ll share current ETR survey data to place VMware’s position in context to the major cloud players, speculate on its AI agenda and give a preview of next week’s VMware Explore. To do so we welcome CUBE analyst Rob Strechay and friend of theCUBE, Zeus Kerravala, principal of ZK Research. Watch the full video analysis. Episode 192 – Cloud vs. On-Prem Showdown: The Future Battlefield for Generative AI Dominance – The data from enterprise customers is clear but conflicted. While 94% of customers say they’re spending more on AI this year, they’re doing so with budget constraints that will steal from other initiatives. As well, the choice of where customers plan to run generative AI is split almost exactly down the middle in terms of public cloud vs. on-premises/edge. Further complicating matters, developers report the experiences in the public cloud with respect to feature richness and velocity of innovation has been outstanding. At the same time, organizations express valid concerns about IP leakage, compliance, legal risks and cost that will limit their use of the public cloud. In this Breaking Analysis we’ll share the most recent data and thinking around the adoption of large language models and address the factors to consider when thinking about how the market will evolve. As always, we’ll share the latest ETR data to shed new light on key issues customers face balancing risk with time to value. Watch the full video analysis. Episode 191 – Tech Stocks Beyond the Magnificent Seven – After a tough 2022, the first half of 2023 has shown impressive strength paying off earlier technology bets. For sure investors in the so-called Magnificent Seven, i.e. Apple, Alphabet, Amazon, Microsoft, Meta, Nvidia and Tesla have been rewarded. But sharp investors have sought alpha beyond these names, riding the wave of secular trends in AI, cybersecurity, cloud infrastructure and software as well as other emerging spaces like cleantech and robotics. As we enter the second half of 2023, the run up in tech combined with macro uncertainty has many investors taking a cautious posture. But prudent earnings guidance sets up for a positive outlook in the mid-term, especially for those companies that can capitalize on the AI wave. In this Breaking Analysis we’re pleased to have back, founder and Chief Investment Officer of Spear Invest, Ivana Delevska to assess the current state of the market and explore how this investor is playing AI’s rising tide. We’ll also analyze ETR data to drill deeper into semis, cloud infrastructure, generative AI, cybersecurity and Snowflake. Watch the full video analysis. Episode 190 – What Leaked Court Docs Tell us About AWS, Azure & Google Cloud Market Shares – Recently leaked court documents during the Microsoft Activision hearing require us to revisit our cloud forecasts and market share data. The poorly redacted docs, which have since been removed from public viewing, suggest that Microsoft’s Azure revenue is at least 25% lower than our previous estimates. As a result, we’ve cut and revised our Azure revenue figures which in turn increases AWS’ big 4 hyperscale cloud market share. Our new estimates show that AWS maintains a greater than 50% share of revenue through 2023. While the change also helps Google Cloud, its market share is only modestly affected. In this Breaking Analysis we update our hyperscaler cloud revenue estimates and market share data. We’ll also explain how the ETR data on cloud should be interpreted in this context and look forward to potential catalysts for cloud growth, including acceleration in Q4 attributable to generative AI. Watch the full video analysis. Episode 189 – AI gives cyber attackers the advantage…for now – Cloud complexity, tools sprawl and the AI awakening further tip the balance in favor of cyber attackers. Combined with corporate inertia, AI-washing, LLM inconsistency and the pace of change, we believe for now anyway, adversaries have the advantage over defenders. Moreover, macro spending headwinds continue to force organizations to make budget tradeoffs, not the least of which is how to fund AI experiments and deployments. Notably, however, 45% of organizations are using LLMs in production for use cases that may very well improve the productivity of SecOps teams in the long run and accelerate the cat and mouse game back to a state of quasi-equilibrium. In this Breaking Analysis we share key takeaways from Supercloud 3 – AI meets cloud security – and put forth new spending data from the latest ETR survey that shows which security firms are best positioned in the AI race to capitalize on the wave. Watch the full video analysis. Episode 188 – AI won’t be a winner takes all market – The AI heard ’round the world has put the machine intelligence sector back in the spotlight. But when you squint beyond the press hype, the data shows that artificial intelligence is now the number one sector in terms of relative spending velocity in the ETR taxonomy. Normally market hype leads deployments, but the data suggests that spending activity and market penetration for AI are coinciding with the hype. While hyperscale cloud players are reaping the rewards, we think this is a rising tide that’s going to lift all AI ships, those both plainly in sight and others that may not be so visible. In this Breaking Analysis we dig deeper into the AI space with spending data from ETR and one of the best minds in tech generally, and AI specifically, Jeff Jonas, CEO, founder, and chief scientist at Senzing. Watch the full video analysis. Episode 187 – Connecting the dots on the emerging Databricks tech stack – The recent Databricks Data+AI Summit attracted a large audience and, like Snowflake Summit, featured a strong focus on large language models, unification and bringing AI to the data. While customers demand a unified platform to access all their data, Databricks and Snowflake are attacking the problem from different perspectives. In our view, the market size justifies the current enthusiasm seen around both platforms but it’s unlikely that either company has a knockout blow for the other. This is not a head on collision. Rather Snowflake is likely years ahead in terms of operationalizing data. Developers can build applications on one platform, like Oracle when it won the market, that perform analysis and take action. Databricks likely has a similar lead in terms of unifying all types of analytic data – e.g. BI, predictive analytics & generative AI. Developers can build analytic applications across heterogeneous data, like Palantir today. But they have to access external operational applications to take action. In this Breaking Analysis we follow up last week’s research by connecting the dots on the emerging tech stack we see forming from Databricks. With an emphasis on how the company is approaching generative AI, unification and governance…and what it means for customers. To do so we tap the knowledge of three experts who attended the event, CUBE analysts Rob Strechay and George Gilbert and AI market maven Andy Thurai of Constellation Research. Watch the full video analysis. Episode 186 – Connecting the Dots on Snowflake’s Data Cloud Ambitions – Over the past several months we’ve produced a number of in-depth analyses laying out our mental model for the future of data platforms. There are two core themes: 1) Data from people, places, things, and activities in the real world drives applications, not people typing into a UI; and 2) Informing and automating decisions means all data must be accessible. That drives a change from data locked in application silos to application logic being embedded in a platform that manages an end-to-end representation of an enterprise in its data. This week’s Snowflake Summit further confirmed our expectations with a strong top line message of “All Data / All Workloads” and a technical foundation that supports an expanded number of ways to access data. Squinting through the messaging and firehose of product announcements, we believe Snowflake’s core differentiation is its emerging ability to be a complete platform for data applications. Just about all competitors either analyze data or manage data. But no one vendor truly does both. To be precise, managing data doesn’t mean running pipelines or serving analytic queries or AI/ML models. It means managing operational data so that analytics can inform or automate operational activities captured in transactions. With data as the application foundation, the platform needs robust governance. In this week’s Breaking Analysis, we try to connect the dots between Snowflake’s high level messaging and its technical foundation to better understand the core value it brings to customers and partners. As well, we’ll explore the ETR data with some initial input from the Databricks Data + AI Summit to assess the position and prospects of these two leaders along with the key public cloud players. Watch the full video analysis. Episode 185 – HPE wants to turn supercomputing leadership into gen AI profits – HPE’s announcement of an AI cloud for large language models highlights a differentiated strategy that the company hopes will lead to sustained momentum in its high performance computing business. While we think HPE has some distinct advantages with respect to its supercomputing IP, the public cloud players have a substantial lead in AI with a point of view that generative AI is fully dependent on the cloud and its massive compute capabilities. The question is can HPE bring unique capabilities and a focus to the table that will yield competitive advantage and ultimately, profits in the space? In this Breaking Analysis we unpack HPE’s LLM-as-a-service announcements from the company’s recent Discover conference and we’ll try to answer the question: Is HPEs strategy a viable alternative to today’s public and private cloud gen AI deployment models, or is it ultimately destined to be a niche player in the market? To do so we welcome to the program CUBE analyst Rob Strechay and Vice President / principal analyst from Constellation Research, Andy Thurai. Watch the full video analysis. Episode 184 – Uber’s architecture represents the future of data apps…meet its architects – Uber has one of the most amazing business models ever created. The company’s mission is underpinned by technology that helps people go anywhere and get anything. The results have been stunning. In just over a decade, Uber has become a firm with more than $30 billion in annual revenue, an annual bookings run rate of $126B and a market capitalization near $90 billion today. Moreover, the company’s productivity metrics are 3-5 times greater than what you’d find at a typical technology company when, for example, measured by revenue per employee. In our view, Uber’s technology stack represents the future of enterprise data apps where organizations will essentially create real time digital twins of their businesses and in doing so, deliver enormous customer value. In this Breaking Analysis, we introduce you to one of the architects behind Uber’s groundbreaking fulfillment platform. We’ll explore their objectives, the challenges they had to overcome, how Uber has done it and why we believe their platform is a harbinger for the future. Watch the full video analysis. Episode 183 – Snowflake Summit will reveal the future of data apps…here’s our take – Our research and analysis points to a new modern data stack that is emerging where apps will be built from a coherent set of data elements that can be composed at scale. Demand for these apps will come from organizations that wish to create a digital twin of their business to represent people, places, things, and the activities that connect them, to drive new levels of productivity and monetization. Further, we expect Snowflake, at its upcoming conference, will expose its vision to be the best platform on which to develop this new breed of data apps. In our view, Snowflake is significantly ahead of the pack but faces key decision points along the way to its future to protect this lead. In this Breaking Analysis and ahead of Snowflake Summit later this month, we lay out a likely path for Snowflake to execute on this vision; and we address the milestones and challenges of getting there. As always, we’ll look at what the ETR data tells us about the current state of the market. To do all this we welcome back CUBE contributor, George Gilbert. Watch the full video analysis. Episode 182 – Cisco needs to simplify…Here’s how – With a nearly $60B revenue run rate, growing at 14% and throwing off over $5B in operating cash last quarter, Cisco has an awesome business. But customers are vocal about the complexity of Cisco’s portfolio and if not addressed head on, the company risks encountering friction beyond just economic headwinds. We believe Cisco’s challenges are most decidedly not product breadth and depth, rather the company’s mandate is to integrate the piece parts of its intricate offerings to create more facile and seamless experiences for customers. In this Breaking Analysis and ahead of Cisco Live US, we dig deeper into Cisco’s business and double click on three key areas of its portfolio including: 1) Security; 2) Networking; and 3) Observability. With spending data from ETR and a guest appearance from SiliconANGLE contributor and market watcher Zeus Kerravala, principal at ZK Research. Watch the full video analysis. Episode 181 – The future of AI is real time data…Meantime GPUs are making all the headlines – The era of AI everything continues to excite. But unlike the Internet era, where any company announcing a dotcom anything immediately rose in value, the AI gods appear to be more selective. Nvidia beat its top line whisper number by more than $300M and the company’s value is rapidly approaching one trillion dollars. Marvell narrowly beat expectations this week but cited future bandwidth demand driven by AI and the stock was up more than 20% on Friday. Broadcom was up nearly 10% on sympathy with the realization that connect-centricity beyond the CPU is what the company does really well. Meanwhile, other players like Snowflake, which also narrowly beat earnings Wednesday and touted AI as a future tailwind, got hammered as customers dial down cloud consumption. In this Breaking Analysis we look at the infrastructure of AI examining the action at the silicon layer specifically around Nvidia’s momentum. Since much of AI is about data, we’ll also look at the spending data on two top data platforms, Snowflake and Databricks to see what the survey data says and examine the future of real time data and automation as a catalyst for massive productivity growth in the economy. To do so we have a special Breaking Analysis panel with John Furrier and David Floyer. Watch the full video analysis. Episode 180 – The AI powered hybrid multi super cloud – AI will now add superpowers to every triggering buzzword, hence the title of this week’s post. Look past the buzz and you’ll find substance somewhere. The spring conference season is kicking into high gear, so it’s a time to get serious and extract the signal from the event noise. This week we’ll see Microsoft Build, which will no doubt volley shots back from the messaging at Google I/O. Two other big events will take place this week, Red Hat Summit / Ansible Fest in Boston and the annual Dell Technologies World in Las Vegas. theCUBE will be covering both of these shows and we want to take this opportunity to update you on the state of hybrid multi-cloud…what we call supercloud. In this Breaking Analysis we examine some of the key infrastructure players in hybrid multi-cloud with a focus on Red Hat and Dell Technologies, two firms that increasingly are partnering with each other as VMware’s future evolves. We’ll share recent ETR survey data on the position of several other hybrid/cross-cloud players including Cloudflare, Equinix, HPE, IBM, Oracle, VMware and others. We’ll also share what we expect to hear at Red Hat Summit and Dell Technologies World this year. Watch the full video analysis. Episode 179 – Searching for gold in enterprise AI – The AI gold rush is on. The paths to monetization are seemingly endless but the most obvious converge on making humans more productive or supercharging existing business models like search advertising or subscription licenses. Much of AI adoption in enterprise IT is hidden. Our research shows a very high overlap (around 40-60%) between AI adoption in enterprise tech and embedded AI inside software from the likes of Salesforce, ServiceNow, Workday, SAP, Oracle and other major players. But the rapid advancements of tools from AI leaders and an emerging group of independent firms is causing customers to think differently. Catalyzed by the OpenAI Microsoft partnership, organizations are rapidly trying to figure out how to apply these tools to create competitive advantage. Every firm on the planet wants to ride the AI wave. Virtually overnight, investment capital has shifted to fund early stage AI startups with much less funding required relative to previous boom cycles. In this Breaking Analysis we review ETR data to quantify the state of AI spending in the enterprise and look at the positions of several key players in the space that offer AI tools and platforms. To do so we invite Andy Thurai, CUBE contributor, VP and principal analyst at Constellation Research. Andy will help us unpack the hits and misses from this past week’s Google IO conference and give us his perspectives on what it takes to catch the AI wave and avoid becoming driftwood. Watch the full video analysis. Episode 178 – Desperately Seeking Cloud Repatriation – While we’ve been skeptical about repatriation as a notable movement, anecdotal evidence suggests that it is happening in certain pockets. Even though we still don’t see cloud repatriation broadly showing in the numbers, certain examples have caught our attention. In addition, the potential impact of artificial intelligence raises some interesting questions about where infrastructure should be physically located and causes us to revisit our premise that repatriation is an isolated and negligible trend. In this Breaking Analysis we look at a number of sources, including the experiences of 37signals, which has documented its departure from public clouds. We’ll also examine the relationship between repatriation and SRE Ops skill sets. As always we’ll look at survey data from our partners at ETR, a recent FinOps study published by Vega Cloud and revisit the Cloud Repatriation Index, which we believe is breaking a three-year trend. Watch the full video analysis. Episode 177 – Don’t be fooled by slowing cloud growth…cost optimization is a feature not a bug – The big three US cloud players all announced earnings this past week and, as expected, cloud growth is slowing. But don’t kid yourselves. Hyperscale clouds remain the epicenter of innovation in tech and foundation models like GPT will only serve to harden this fundamental fact. Our data suggests the deceleration in cloud spend is a function of two related factors: 1) Cautious consumption patterns; and 2) Aggressive cloud optimization, which is being promoted by the big three cloud vendors in an attempt to lock in customers to longer term commitments. There is still no clear evidence in the numbers that repatriation is a factor. Rather, the ability to quickly dial down spending and pause projects is an attractive feature of cloud computing and one that, until now, has never really been seen on a broad market basis. In this Breaking Analysis we try to explain the implications of this seemingly simple but nuanced dynamic. We’ll review the latest hyperscale cloud data for the big three players, share our analysis of certain comments made by cloud executives and show you the latest ETR data on spending and market presence in the cloud, Watch the full video analysis. Episode 176 – RSA 2023 Security Identity Crisis Part 2 – The narrative from security vendors is organizations don’t spend enough money on cyber defense. Maybe…but will spending more actually address the problems organizations face? The conventional wisdom is it will help; or at least it can’t hurt, but as we and others have pointed out over the years, a crowded market and mega VC funding have created more tools, more complexity and more billionaires…but are we safer? In this Breaking analysis we follow up last week’s episode and continue with Part 2. In an homage to the keynote from RSA CEO Rohit Ghai, we ask, is there a looming identity crisis in the security industry? This week we’re excited to introduce the newest member of the SiliconANGLE editorial team, long time journalist, David Strom. With David, we’ll unpack the data and bring additional context to the ETR body of work. We’ll also look at some recent data from Unit 42, Palo Alto’s threat intelligence and response division. As well, we’ll dig into the anatomy of a recent double supply chain hack. Watch the full video analysis. Episode 175 – RSA 2023 highlights an identity crisis in the age of AI – In this Breaking Analysis, theCUBE host Dave Vellante updates on the latest trends in the cybersecurity market and what to expect at the RSA Conference 2023. Join our real-time analysis coverage from #RSAC here: https://www.thecube.net/rsa-conference-2023 We’ll also share the latest Enterprise Technology Research spending data and drill into the areas of cybersecurity that are seeing the most action. As always, we’ll highlight those companies with the strongest (and weakest) momentum and close with a look at some of the emerging technology players in security that might be ripe for acquisition. To do all this, we once again welcome in our colleague Erik Bradley from ETR. Watch the full video analysis. Episode 174 – Hidden Gems from HPE GreenLake Storage Day – On Tuesday, April 4, HPE invited a number of industry analysts to participate in HPE GreenLake Storage Day. Notably, HPE declared 2023 the year of storage. While the company made several storage-related announcements, perhaps even more interesting was what the event tells us about HPE’s culture, its strategy and the future direction of the company. In this Breaking Analysis we’ll share our takeaways from HPE’s event, held in Houston, Texas, which included attendance at Antonio Neri’s quarterly all-hands meeting. We’ll try to emphasize areas that have not necessarily been the focus of most press and industry analyst write ups to date. We’ll also take a look at the latest ETR survey data to put HPE’s market position in context across several of its major segments. Watch the full video analysis. Episode 173 – Semis rebound but enterprise tech spending remains soft – A rebound in semiconductor stocks has many investors asking if this is a harbinger of good news for the broader enterprise tech sector. Indeed the SOXX semiconductor ETF is up nearly 30% year to date as of this posting, as are bellwether fab suppliers like Applied Materials and Lam Research. Nvidia is up over 90% YTD and AMD over 50%. Even the beleaguered Intel is up 22%. But key enterprise software names have not yet rebounded and according to this week’s guest, the divergence between semis and B2B software is getting hard to ignore. In this Breaking analysis we examine the the bifurcation between the performance of semis and broader enterprise tech. And we’ll try to answer the question: Is the uptick in semiconductors an early indicator of a broader enterprise tech recovery, or is this a false signal that warrants continued caution? To examine these issues we welcome back Ivana Delevska, the founder and chief investment officer of SPEAR Invest. All statements made regarding companies or securities are strictly beliefs, points of view and opinions held by SiliconANGLE Media, Enterprise Technology Research, other guests on theCUBE and guest writers. Such statements are not recommendations by these individuals to buy, sell or hold any security. The content presented does not constitute investment advice and should not be used as the basis for any investment decision. You and only you are responsible for your investment decisions. Disclosure: Many of the companies cited in Breaking Analysis are sponsors of theCUBE and/or clients of Wikibon. None of these firms or other companies have any editorial control over or advanced viewing of what’s published in Breaking Analysis. Watch the full video analysis. Episode 172 – Which tech firms are most exposed to the banking crisis? – The viral awareness and adoption of foundation models like ChatGPT have created both an opportunity and threat to automation platforms generally and RPA point tools specifically. On the one hand, large language models can reduce complexity and accelerate the adoption of enterprise automation platforms. The flip side is software robots are designed to improve human productivity through intelligent automation and GPT models could cannibalize some, if not many use cases initially targeted by RPA vendors. This reality is causing customers to rethink their automation strategies and vendors to rapidly evolve their messaging to position foundation models as an accelerant to their platforms. In this Breaking Analysis we provide you with a perspective on how foundation models could impact automation platforms. We review ETR data that quantifies the ascendency of OpenAI. We also show survey data that measures the overlap between ML/AI systems and automation platforms. Then we review the recent quarterly performance of UiPath and share how we think the company must position itself with respect to the onslaught of noise and potential disruption from GPT models. Watch the full video analysis. Episode 171 – GPT models are a two edged sword for automation platforms – The viral awareness and adoption of foundation models like ChatGPT have created both an opportunity and threat to automation platforms generally and RPA point tools specifically. On the one hand, large language models can reduce complexity and accelerate the adoption of enterprise automation platforms. The flip side is software robots are designed to improve human productivity through intelligent automation and GPT models could cannibalize some, if not many use cases initially targeted by RPA vendors. This reality is causing customers to rethink their automation strategies and vendors to rapidly evolve their messaging to position foundation models as an accelerant to their platforms. In this Breaking Analysis we provide you with a perspective on how foundation models could impact automation platforms. We review ETR data that quantifies the ascendency of OpenAI. We also show survey data that measures the overlap between ML/AI systems and automation platforms. Then we review the recent quarterly performance of UiPath and share how we think the company must position itself with respect to the onslaught of noise and potential disruption from GPT models. Watch the full video analysis. Episode 170 – Databricks faces critical strategic decisions…here’s why – When Apache Spark became a top level project in 2014, and shortly thereafter burst onto the big data scene, it along with the public cloud disrupted the big data market. Databricks cleverly optimized its tech stack for Spark and took advantage of the cloud to deliver a managed service that has become a leading AI and data platform among data scientists and data engineers. However, emerging customer data requirements and market forces are conspiring in a way that we believe will cause modern data platform players generally and Databricks specifically to make some key directional decisions and perhaps even reinvent themselves. In this Breaking Analysis we do a deeper dive into Databricks. We explore its current impressive market momentum using ETR survey data. We’ll also lay out how customer data requirements are changing and what we think the ideal data platform will look like in the mid-term. We’ll then evaluate core elements of the Databricks portfolio against that future vision and close with some strategic decisions we believe the company and its customers face. To do so we welcome in our good friend George Gilbert, former equities analyst, market analyst and principal at Tech Alpha Partners. Watch the full video analysis. Episode 169 – MWC 2023 goes beyond consumer & deep into enterprise tech – While never really meant to be a consumer tech event, over time, the rapid ascendancy of smartphones captured much of the agenda at Mobile World Congress, now MWC. And while device manufacturers continue to have a major presence at the show, the maturity of intelligent devices, longer lifecycles and the disaggregation of the network stack have created more interest in enterprise-class technologies than ever before. Semiconductor manufacturers, network equipment players, infrastructure companies, cloud vendors, software providers and a spate of startups are eyeing the trillion dollar plus telecommunications industry as one of the next big things to watch this decade. In this Breaking Analysis we bring you part 2 of our ongoing coverage of MWC 2023. With some new data on enterprise players specifically within large telco environments. We’ll also take a brief glimpse at some of the pre-announcement news from the show and corresponding themes ahead of MWC. We’ll close with the key innovation areas we’ll be covering at the show on theCUBE. Watch the full video analysis. Episode 168 – MWC 2023 highlights telco transformation & the future of business – The world’s leading telcos are often branded as monopolies that lack innovation. Telcos have been great at operational efficiency, connectivity and living off of transmission services. But in a world beyond telephone poles and basic wireless services, how will telcos modernize, become more agile and monetize new opportunities brought about by 5G, private wireless and a spate of new innovations in infrastructure, cloud, data, AI and apps? It’s become table stakes for carriers to evolve their hardened, proprietary infrastructure stacks to more open, flexible, cloud-like models. But doing so brings risks that telcos must carefully balance as they strive to deliver consistent quality of service while at the same time moving faster and avoiding disruption. In this Breaking Analysis and ahead of MWC23, we explore the evolution of the telco business and how the industry is in many ways, mimicking a transformation that took place decades ago in enterprise IT. We’ll model some of the traditional enterprise vendors using ETR data and investigate how they’re faring in the telecomms vertical. And we’ll pose some of the key issues facing the industry this decade. Watch the full video analysis. Episode 167 – Google’s Point of View on Confidential Computing – Confidential computing is a technology that aims to enhance data privacy and security by providing encrypted computation on sensitive data in use and isolating data from apps and other host resources in fenced off enclaves. The concept of confidential computing is gaining popularity, especially in the cloud computing space where sensitive data is commonly stored and processed. However, there are some who view confidential computing as an unnecessary technology and a marketing ploy by cloud providers, aimed at calming customers who are cloud-phobic. In this Breaking Analysis we revisit the notion of confidential computing and explore whether it’s just marketing or a key part of a trusted security strategy. To do so we’ll invite two Google experts to the show. But before we get there let’s summarize the overall market climate briefly with some ETR data. Watch the full video analysis. Episode 166 – Cloud players sound a cautious tone for 2023 – The unraveling of market enthusiasm continued in Q4 of 2022 with the earnings reports from the U.S. hyperscalers now all in. As we said earlier this year, even the cloud is not immune from the macro headwinds and the cracks in the armor we saw from the data we shared last summer are playing out into 2023. For the most part, actuals are disappointing beyond expectations, including our own. It turns out that our estimates for the big 3 hyperscale revenue missed by $1.2 billion or 2.7% lower than we had forecast from our most recent November estimates. We expect decelerating growth rates for the hyperscalers will continue through the summer of 2023 and won’t abate until comparisons get easier. In this Breaking Analysis we share our view of what’s happening in cloud markets – not just for the hyperscalers but other firms that have hitched a ride on the cloud. And we’ll share new ETR data that shows why these trends are playing out, tactics customers are employing to deal with their cost challenges and how long the pain is likely to last. Watch the full video analysis. Episode 165 – Enterprise Technology Predictions 2023 – Making predictions about the future of enterprise tech is more challenging if you strive to lay down forecasts that are measurable. In other words if you make a prediction, you should be able to look back a year later and say with some degree of certainty whether the prediction came true or not. With evidence to back that up. In this Breaking Analysis we aim to do just that with predictions about the macro IT spending environment, cost optimization, security – lots to talk about there – generative AI, cloud and supercloud, blockchain adoption, data platforms, including commentary on Databricks, Snowflake and other key players, automation, events and we may even have some bonus predictions. To make all this happen we welcome back for the third year in a row, Erik Bradley our colleague from ETR. As well, you can check out how we did with our 2022 predictions. Watch the full video analysis. Episode 164 – ChatGPT Won’t Give OpenAI First Mover Advantage – OpenAI, the company, and ChatGPT have taken the world by storm. Microsoft reportedly is investing an additional $10B in the startup. But in our view, while the hype around ChatGPT is justified, we don’t believe OpenAI will lock up the market with its first mover advantage. Rather we believe that success in this market will be directly proportional to the quality and quantity of data that a technology company has at its disposal, and the compute power it has to run the system. This market is unlikely to display winner take all dynamics and will probably evolve in a more fragmented fashion than cloud. In this Breaking Analysis we unpack the excitement around ChatGPT and debate the premise that the company’s early entry into the space may not confer “game over” advantage to OpenAI. To do so we welcome CUBE collaborator Sarbjeet Johal & John Furrier, cohost of theCUBE. Watch the full video analysis. Episode 163 – Supercloud2 Explores Cloud Practitioner Realities & the Future of Data Apps – Enterprise tech practitioners, like most of us, want to make their lives easier so they can focus on delivering more value to their business. They want to tap best of breed services in the public cloud and at the same time connect their on-prem intellectual property to emerging applications which drive top line revenue and bottom line profits. But creating a consistent experience across clouds and on-prem estates has been an elusive capability for most organizations, forcing tradeoffs and injecting friction into the system. The need to create seamless experiences is clear and the technology industry is starting to respond with platforms, architectures and visions of what we call Supercloud. In this Breaking Analysis we give you a preview of Supercloud2, share key findings leading up to the event and highlight some of the areas we’ll be probing in the live program. Watch the full video analysis. Episode 162 – CIOs in a holding pattern but ready to strike at monetization – Recent conversations with IT decision makers show a stark contrast between the period exiting 2023 versus the mindset leaving 2022. CIOs are generally funding new initiatives by pushing off or cutting lower-priority items. While security efforts are still being funded, those that enable business initiatives that generate revenue take priority over cleaning up legacy technical debt. The bottom line is, for the moment at least, the mindset is not to cut everything, rather it’s to put a pause on cleaning up legacy hairballs and continue to fund initiatives to drive monetization. Cloud has become fundamental and getting data “right” is a consistent theme that appears to be an underpinning of initiatives getting funded today. In this Breaking Analysis we tap recent discussions from two primary sources: year-end ETR roundtables with IT decision makers and conversations on theCUBE with data, cloud and IT architecture practitioners. Watch the full video analysis. Episode 161 – AI goes mainstream but ROI remains elusive – A decade of big data investments combined with cloud scalability, the rise of more cost effective processing and the introduction of advanced tooling has catapulted machine intelligence to the forefront of technology investments. No matter what job you have, your operation will be AI powered within five years and machines may be doing your job in the future. Artificial intelligence is being infused into applications, infrastructure, equipment and virtually every aspect of our lives. AI is proving to be extremely helpful at controlling vehicles, speeding medical diagnoses, processing language, advancing science and generally raising the stakes on what it means to apply technology for business advantage. But business value realization has been a challenge for most organizations due to lack of skills, complexity of programming models, immature technology integration, sizable up front investments, ethical concerns and lack of business alignment. Mastering AI technology and a focus on features will not be a requirement for success in our view. Rather figuring out how and where to apply AI to your business will be the crucial gate. That means understanding the business case, picking the right technology partner, experimenting in bite sized chunks and quickly identifying winners to double down on from an investment standpoint. In this Breaking Analysis we update you on the state of AI with a focus on interpreting the latest ETR survey data around ML/AI and data. We’ll explore what it means for the competitive environment and what to look for in the future. To do so we invite into our studios Andy Thurai of Constellation Research. Andy covers AI deeply, he knows the players and the pitfalls of AI investment. Watch the full video analysis. Episode 160 – Grading our 2022 enterprise technology predictions – Nailing technology predictions in 2022 was tricky business. Projections on the performance of markets, identifying IPO prospects and making binary forecasts on data, AI, the macro spending climate, along with other related topics in enterprise tech, carried much uncertainty. 2022 was characterized by a seesaw economy where central banks were restructuring their balance sheets, the war in Ukraine fueled inflation, supply chains were a mess and the unintended consequences of digital acceleration are still being sorted. In this Breaking Analysis we continue our annual tradition of openly grading our previous year’s enterprise tech predictions. You may or may not agree with our self-grading system but we give you the data to draw your own conclusions. Watch the full video analysis. Episode 159 – How Palo Alto Networks Became the Gold Standard of Cybersecurity – Palo Alto Networks has earned a reputation as the leader in security. You can measure this in revenue, market cap, execution and, most importantly, conversations with CISOs. The company is on track to double its revenues to nearly $7B in FY23 from FY20. This despite macro headwinds which will likely continue through next year. Palo Alto owes its position to a clarity of vision and strong execution of a TAM expansion strategy bolstered by key acquisitions and integrations into its cloud & SaaS offerings. In this Breaking Analysis, and ahead of Palo Alto Ignite, we bring you the next chapter on top of last week’s cybersecurity update. We’ll dig into the ETR spending data on Palo Alto Networks, provide a glimpse of what to look for at Ignite and posit what Palo Alto needs to do to stay on top of the hill. Watch the full video analysis. Episode 158 – Cyber Firms Revert to the Mean – While by no means a safe haven, the cybersecurity sector has outpaced the broader tech market by a meaningful margin. That is up until very recently. Cyber security remains the number one technology priority for the c-suite but as we’ve previously reported, the CISO’s budget has constraints; just like other technology investments. Recent trends show that economic headwinds have elongated sales cycles, pushed deals into future quarters and, just like other tech initiatives, are pacing cybersecurity investments and breaking them into smaller chunks. In this Breaking Analysis we explain how cybersecurity trends are reverting to the mean and tracking more closely with other technology investments. We’ll make a couple of valuation comparisons to show the magnitude of the challenge and which cyber firms are feeling the heat, and which aren’t as much. We’ll then show the latest survey data from ETR to quantify the contraction in spending momentum and close with a glimpse at the landscape of emerging cybersecurity companies that could be ripe for acquisition, consolidation or disruptive to the broader market. Watch the full video analysis. Episode 157 – re:Invent 2022 marks the next chapter in data & cloud – The ascendency of AWS under the leadership of Andy Jassy was marked by a tsunami of data and corresponding cloud services to leverage data. Those services mainly came in the form of primitives – i.e. basic building blocks that were used by developers to create more sophisticated capabilities. AWS in the 2020s, led by CEO Adam Selipsky, will be marked by four high level trends in our view: 1) A rush of data that will dwarf anything previously seen; 2) Doubling down on investments in the basic elements of cloud – compute, storage, database, security, etc; 3) Greater emphasis on end-to-end integration of AWS services to make data accessible to more professionals and further accelerate cloud adoption; and 4) Significantly deeper business integration of cloud, beyond IT, as an underlying element of organizational transformation. In this Breaking Analysis we extract and analyze nuggets from John Furrier’s annual sit down with the CEO of AWS. We’ll share data from ETR and other sources to set the context for the market and competition in cloud and we’ll give you our glimpse of what to expect at re:Invent 2022. Watch the full video analysis. Episode 156 – Snowflake caught in the storm clouds – A better than expected earnings report in late August got people excited about Snowflake again but the negative sentiment in the market has weighed heavily on virtually all growth tech stocks. Snowflake is no exception. As we’ve stressed many times, the company’s management is on a long term mission to simplify the way organizations use data. Snowflake is tapping into a multi-hundred billion dollar total available market and continues to grow at a rapid pace. In our view the company is embarking on its third major wave of innovation, data apps, while its first and second waves are still bearing significant fruit. For short term traders focused on the next 90 or 180 days, that probably doesn’t matter much. But those taking a longer view are asking, should we still be optimistic about the future of this high flier or is it just another over-hyped tech play? In this Breaking Analysis we take a look at the most recent survey data from ETR to see what clues and nuggets we can extract to predict the near future and the long term outlook for Snowflake. Watch the full video analysis. Episode 155 – Cloudflare’s Supercloud…What Multi Cloud Could Have Been – Over the past decade, Cloudflare has built a global network that has the potential to become the fourth U.S.-based hyperscale-class cloud. In our view, the company is building a durable revenue model with hooks into many important markets. These include the more mature DDoS protection space, but also extend to growth sectors such as zero trust, a serverless platform for application development and an increasing number of services such as database and object storage. In essence, Cloudflare can be thought of as a giant, distributed supercomputer that can connect multiple clouds and act as a highly efficient scheduling engine– allocating and optimizing resources at scale. Its disruptive DNA is increasingly attracting novel startups and established global firms looking for a reliable, secure, high performance, low latency and more cost effective alternative to AWS and legacy infrastructure solutions. In this Breaking Analysis we initiate deeper coverage of Cloudflare. While the stock got hammered this past week on tepid guidance, we are optimistic about the company’s future. In this post, we’ll briefly explain our take on the company and its unique business model. We’ll then share some peer comparisons with both a financial snapshot and some fresh ETR survey data. Finally we’ll show some examples of how we think Cloudflare could be a disruptive force with a supercloud-like offering that, in many respects, is what multi-cloud should have been. Watch the full video analysis. Episode 154 – Even the Cloud Is Not Immune to the Seesaw Economy – Have you ever driven on the highway and traffic suddenly slows way down? And then after a little while it picks up again and you’re cruising along thinking…ok that was weird but it’s clear sailing now…only to find out in a bit that that traffic is building up again, forcing you to pump the brakes as the traffic patterns ebb and flow? Well welcome to the seesaw economy. The Fed induced fire that prompted a rally in tech is being purposely extinguished by that same Fed and virtually every sector of the tech industry is having to reset its expectations – including the cloud. In this Breaking Analysis we’ll review the implications of this week’s earnings announcements from the big 3 cloud players. The growth of AWS and Azure slowed while Google Cloud Platform’s growth accelerated. We’ll explain why GCP’s growth is still not fast enough. We’ll update you on our quarterly IaaS forecasts and share the latest cloud survey data from ETR. Watch the full video analysis. Episode 153 – Survey Says! Takeaways from the latest CIO spending data – The overall technology spending outlook is deteriorating. And yet there are positive signs making things unpredictable. The negative sentiment is of course being driven by macroeconomic factors and earnings forecasts that have been coming down all year while interest rates keep rising. Making matters worse is many people think earnings estimates are still too high. It’s understandable why there’s so much uncertainty. Technology continues to boom. Digital transformations are happening in earnest. Leading companies have momentum and long cash runways. Moreover, the CEOs of these leading companies are still really optimistic. But strong guidance in an environment of uncertainty is risky and makes navigation more challenging. In this Breaking Analysis we try to put some guardrails on the market by sharing takeaways from from ETRs latest spending survey, which was released to their private clients on the 21st of October. Today we’re going to review the macro spending data, convey where CIOs think their cloud spend is headed, look at the actions organizations are taking to manage uncertainty and then review some of the technology companies that have the most positive and negative outlooks in the ETR data set. Watch the full video analysis. Episode 152 – CEO Nuggets from Microsoft Ignite & Google Cloud Next – This past week, we saw two of the “Big 3” cloud providers present an update of their respective cloud visions, business progress, announcements and innovations. The content at these events had many overlapping themes including modern cloud infrastructure at global scale, applying advanced machine intelligence, end-to-end data platforms, the future of work, automation and a taste of the metaverse/Web 3.0. And more. Despite the striking similarities, the differences between these two cloud platforms, and that of AWS, remain significant with Microsoft leveraging its massive application software footprint to dominate virtually all markets; AndGoogle doing everything in its power to keep up with the frenetic pace of today’s cloud innovation which was set into motion a decade and a half ago by AWS. In this Breaking Analysis, we unpack the immense amount of content presented by the CEOs of Microsoft and Google Cloud at Microsoft Ignite and Google Cloud Next. We’ll also quantify with ETR survey data, the relative position of these two cloud giants in four key sectors – Cloud IaaS, BI analytics, data platforms and collaboration software. Watch the full video analysis. Episode 151 – Analyst Take on Dell – Dave Vellante provides his take on the transformation of Dell to Dell EMC to Dell Technologies, the impact of the VMware spin out and what the future holds for this bellwether infrastructure player. Watch the full video analysis. Episode 150 – Latest CIO Survey Shows Steady Deceleration in IT Spend – Is the glass half full or half empty? Well, it depends on how you want to look at it. CIOs are tapping the brakes on spending that’s clear. The latest macro survey from ETR quantifies what we already know to be true, that IT spend is decelerating. CIOs and IT buyers forecast that their tech spend will grow by 5.5% this year, a meaningful deceleration from their year end 2021 expectations…but these levels are still well above historical norms – so while the feel good factor may be in some jeopardy, overall things are pretty good – at least for now. In this Breaking Analysis, we update you on the latest macro tech spending data from Enterprise Technology Research, including strategies organizations are employing to cut costs…and which project categories continue to see the most traction. Watch the full video analysis. Episode 149 – As the tech tide recedes, all sectors feel the pinch – Virtually all tech companies have expressed caution on their respective earnings calls. And why not… the macroeconomic environment is full of uncertainties and there’s no upside to providing aggressive guidance when sellers punish even the slightest miss. Moreover, the spending data confirms the market is softening across the board, so it’s becoming expected that CFOs will guide cautiously. But companies facing execution challenges can’t hide behind the macro. Which is why it’s important to understand which firms are best positioned to maintain momentum through the headwinds and come out the other side stronger. In this Breaking Analysis we’ll do three things: 1) Share a high-level view of the spending squeeze almost all sectors are experiencing; 2) Highlight some of those companies that continue to show notably strong momentum – and relative high spending velocity on their platforms – albeit less robust than last year; and 3) give you a peek at how one senior technology leader in the financial sector sees the competitive dynamic between AWS, Snowflake and Databricks. Watch the full video analysis. Episode 148 – UiPath is a Rocket Ship Resetting its Course – Like a marathon runner pumped up on adrenaline, UiPath sprinted to the lead in what is surely going to be a long journey toward enabling the modern automated enterprise. In doing so, the company has established itself as a leader in enterprise automation, while at the same time getting out over its skis on critical execution items and disappointing investors along the way. In our view, the company has plenty of upside potential but will have to slog through its current challenges including restructuring its go to market, prioritizing investments, balancing growth with profitability and dealing with a difficult macro environment. In this Breaking Analysis and ahead of Forward5, UiPath’s customer conference, we once again dig into RPA and automation leader UiPath, to share our most current data and view of the company’s prospects, its performance relative to the competition and market overall. Watch the full video analysis. Episode 147 – How CrowdStrike Plans to Become a Generational Platform – In just over ten years, CrowdStrike has become a leading independent security firm. It has more than $2B in annual recurring revenue, nearly 60% ARR growth, a roughly $40B market capitalization, very high retention and a path to $5B in revenue by mid-decade. The company has joined Palo Alto Networks as a gold standard pure play cyber firm. It has achieved this lofty status with an architecture that enables it to go beyond point product. Combine this with outstanding go to market, solid financial execution, some sharp acquisitions and an ever-increasing total available market and you have the formula for a great company. In this Breaking Analysis and ahead of Fal.Con, CrowdStrike’s user conference, we take a deeper look into the company, its performance, its platform and customer survey data from our partner ETR. Watch the full video analysis. Episode 146 – We Have the Data…What Private Tech Companies Don’t Tell you About Their Business – The negative sentiment in tech stocks, caused by rising interest rates, less attractive discounted cash flow models and more tepid forward guidance, is easily measured by public market valuations. And while there’s lots of talk about the impact on private companies, their cash runways and 409A valuations, measuring the performance of non-public companies isn’t as easy. IPOs have dried up and public statements by private companies accentuate the good and hide the bad. Real data, unless you’re an insider, is hard to find. In this Breaking Analysis we unlock some of the secrets that non-public, emerging tech companies may or may not be sharing. We do this by introducing you to a capability from ETR that we’ve not previously exposed in Breaking Analysis. It’s called the ETR Emerging Technology Survey and is packed with sentiment and performance data based on surveys of more than 1,000 CIOs & IT buyers covering more than 400 private companies. The survey will highlight metrics on the evaluation, adoption and churn rates for private companies and the mindshare they’re able to capture. We’ve invited back our colleague Erik Bradley of ETR to help explain the survey and the data we’re going to cover in this post. Watch the full video analysis. Episode 145 – VMware Explore 2022 will mark the start of a Supercloud journey – While the precise direction of VMware’s future is unknown, given the planned Broadcom acquisition, one thing is clear; the subject of what Hock E. Tan plans will not be the main focus of the agenda at the upcoming VMware Explore event next week in San Francisco. We believe that despite any uncertainty, VMware will lay out for its customers what it sees as its future. And that future is multi-cloud or cross cloud services; what we would call supercloud. In this Breaking Analysis we drill into the latest ETR survey data on VMware. We’ll share with you the next iteration of the supercloud definition based on feedback from dozens of contributors. And we’ll give you our take on what to expect at VMware Explore next week. Watch the full video analysis. Episode 144 – What Black Hat ’22 tells us about securing the Supercloud – Black Hat 2022 was held in Las Vegas last week, at the same time as theCUBE’s supercloud event. Unlike AWS re:Inforce, where words are carefully chosen to put a positive spin on security, Black Hat exposes all the warts of cybersecurity and openly discusses its hard truths. It’s a conference attended by technical experts who proudly share some of the vulnerabilities they’ve discovered and of course by numerous vendors marketing their products and services. In this Breaking Analysis we summarize what we learned from discussions with several people who attended Black Hat and our analysis from reviewing dozens of keynotes, articles, videos, session talks, Dark Reading interviews and data from a recent Black Hat attendees survey conducted by Black Hat and Informa. We’ll also share data from ETR in a recent post discussing how Zscaler became the last line of defense for a manufacturing firm. We’ll end with a discussion of what it all means for the challenges around securing the supercloud. Watch the full video analysis. Episode 143 – Further defining Supercloud With tech leaders VMware, Snowflake, Databricks & others – At our inaugural Supercloud22 event we sought community input to evolve the concept of a supercloud by iterating on the definition, the salient attributes and examples of what is and is not a supercloud. We asked several technologists including experts from VMware, Snowflake, Databricks, HashiCorp, Confluent, Intuit, Cohesity and others to help test the supercloud definition, operational models, service models and principles. In this Breaking Analysis we unpack our discussions with these technology leaders and apply their input to iterate the definition of supercloud. We then go in-depth to examine Snowflake’s Data Cloud architecture as a leading example of supercloud. Watch the full video analysis. Episode 142 – What we hope to learn at Supercloud22 – The term supercloud is relatively new, but the concepts behind it have been bubbling for years. Early last decade when NIST put forth its original definition of cloud computing, it said services had to be accessible over a public network…essentially cutting the on-prem crowd out of the conversation. A guy named Chuck Hollis, a CTO at EMC and prolific blogger objected to that criterion and laid out his vision for what he termed a private cloud. In that post he showed a workload running both on premises and in a public cloud, sharing the underlying resources in an automated and seamless manner – what later became more broadly known as hybrid cloud. That vision, as we now know, really never materialized and we were left with multi-cloud…sets of largely incompatible and disconnected cloud services running in separate silos. The point is, what Hollis put forth – i.e. the ability to abstract underlying infrastructure complexity and run workloads across multiple heterogeneous estates with an identical experience – is what supercloud is all about. Watch the full video analysis. Episode 141 – How the cloud is changing security defenses in the 2020s – AThe rapid pace of cloud adoption has changed the way organizations approach cybersecurity. Specifically, the cloud is increasingly becoming the first line of cyber defense. As such, along with communicating to the board and creating a security-aware culture, the CISO must ensure that the shared responsibility model is being applied properly. The DevSecOps team has emerged as the critical link between strategy and execution, while audit becomes the “free safety” in the equation – i.e. the last line of defense. In this Breaking Analysis we share our puts and takes from AWS re:Inforce with an update on the latest hyperscale IaaS market shares; and insights from ETR survey data. We’ll also dig deeper into some technical aspects of AWS Nitro, a system we believe is one of AWS’ secret weapons, with a focus on confidential computing and what it means for the future of systems architecture. Watch the full video analysis. Episode 140 – AWS re:Inforce marks a summer checkpoint on cybersecurity – After a two year hiatus, AWS re:Inforce is back on as an in-person event in Boston next week. Like the all-star break in baseball, re:Inforce gives us an opportunity to evaluate the cybersecurity market overall, the state of cloud security and what AWS is up to in the sector. In this Breaking Analysis, we’ll share our view of what’s changed since our last cyber update in May, we’ll look at the macro environment, how it’s impacting cybersecurity plays in the market, what the ETR data tells us and what to expect at next week’s AWS re:Inforce. Watch the full video analysis. Episode 139 – Amping it up with Frank Slootman – Organizations have considerable room to improve their performance without making expensive changes to their talent, structure or fundamental business model. You don’t need a slew of consultants to tell you what to do. You already know. What you need is to immediately ratchet up expectations, energy, urgency and intensity. Fight mediocrity every step of the way. Amp it up and the results will follow. This is the fundamental premise of a hard hitting new book written by Frank Slootman, CEO of Snowflake, and published earlier this year. It’s called, Amp it Up, Leading for Hypergrowth by Raising Expectations, Increasing Urgency and Elevating Intensity. At Snowflake Summit last month, I was invited to interview Frank on stage about his book. I’ve read it several times and if you haven’t picked it up, you should. Even if you have read it, in this Breaking Analysis we’ll dig deeper into the book and share some clarifying insights and unpublished nuances of Frank’s philosophy. You’ll hear directly from Slootman himself with excerpts from my one on one conversation with him. Watch the full video analysis. Episode 138 – Answering the top 10 questions about SuperCloud – As we exited the isolation economy last year, supercloud is a term we introduced to describe something new that was happening in the world of cloud. In this Breaking Analysis we address the ten most frequently asked questions we get on supercloud. Watch the full video analysis. Episode 137 – H1 of ‘22 was ugly…H2 could be worse…Here’s why we’re still optimistic – After a two year epic run in tech, 2022 has been an epically bad year in the market. Through yesterday, the Nasdaq composite is down 30%, the S&P 500 is off 21%, the DJIA down 16% and the poor HODLers of BTC have had to endure a nearly 60% decline year to date. Watch the full video analysis. Episode 136 – Tech Spending Intentions are Holding Despite Macro Concerns – Much of the energy around data innovation that dispersed with the decline of Hadoop’s relevance is coalescing in a new ecosystem spawned by the ascendency of Snowflake’s Data Cloud. What was once seen as a simpler cloud data warehouse and good marketing with Data Cloud, is evolving rapidly with new workloads, a vertical industry focus, data applications, monetization and more. The question is will the promises of data be fulfilled this time around or is it same wine, new bottle? Watch the full video analysis. Episode 135 – Snowflake Summit 2022…All About Apps & Monetization – Much of the energy around data innovation that dispersed with the decline of Hadoop’s relevance is coalescing in a new ecosystem spawned by the ascendency of Snowflake’s Data Cloud. What was once seen as a simpler cloud data warehouse and good marketing with Data Cloud, is evolving rapidly with new workloads, a vertical industry focus, data applications, monetization and more. The question is will the promises of data be fulfilled this time around or is it same wine, new bottle? Watch the full video analysis. Episode 134 – How Snowflake Plans to Make Data Cloud a De Facto Standard – When Frank Slootman took ServiceNow public, many people undervalued the company, positioning it as just a better help desk tool. It turns out the firm actually had a massive TAM expansion opportunity in ITSM, HR, logistics, security, marketing and customer service management. NOW’s stock price followed the stellar execution under Slootman and CFO Mike Scarpelli’s leadership. When they took the reins at Snowflake, expectations were already set that they’d repeat the feat but this time, if anything, the company was overvalued out of the gate. It can be argued that most people didn’t really understand the market opportunity any better this time around. Other than that it was a bet on Slootman’s track record of execution…and data. Good bets; but folks really didn’t appreciate that Snowflake wasn’t just a better data warehouse. That it was building what the company calls a data cloud…and we’ve termed a data supercloud. In this Breaking Analysis and ahead of Snowflake Summit, we’ll do four things: 1) Review the recent narrative and concerns about Snowflake and its value; 2) Share survey data from ETR that will confirm almost precisely what the company’s CFO has been telling anyone who will listen; 3) Share our view of what Snowflake is building – i.e. trying to become the de facto standard data platform; and 4) Convey our expectations for the upcoming Snowflake Summit next week at Caesar’s Palace in Las Vegas. Watch the full video analysis. Episode 133 – MongoDB Sends Strong Signals Despite Cautious Macro Tones – Earnings season has shown a conflicting mix of signals for software companies. Most firms are expressing caution over macro headwinds citing a combination of Ukraine, inflation, interest rates, EMEA softness, currency, supply chain and general demand for technology. But MongoDB, along with a few other names appeared more sanguine, thanks to a beat in the recent quarter and a cautious but upbeat outlook for the near term. In this Breaking Analysis, ahead of MongoDB World 2022, we drill into the company’s business and what ETR survey data tells us in the context of overall demand and the patterns from other software companies. Watch the full video analysis. Episode 132 – Broadcom, Taming the VMware Beast – In the words of CUBE analyst and former CTO David Nicholson, Broadcom buys old cars. Not to restore them to their original beauty…nope…they buy classic cars to extract the platinum that’s inside the catalytic converter. Broadcom’s planned $61B acquisition of VMware will mark yet another new era for the virtualization leader, a mere seven months after finally getting spun out as a fully independent company by Dell. For VMware this means a dramatically different operating model, with financial performance and shareholder value creation as the dominant and perhaps sole agenda. For customers it will mean a more focused portfolio, less aspirational vision pitches and most certainly higher prices. In this Breaking Analysis we’ll share data, opinions and customer insights about this blockbuster deal and forecast the future of VMware, Broadcom and the broader ecosystem. Watch the full video analysis. Episode 131 – Supercloud is becoming a thing – Last year we noted in a Breaking Analysis that the cloud ecosystem is innovating beyond the notion of multicloud. We’ve said for years that multicloud is really not a strategy but rather a symptom of multivendor. We used the term supercloud to describe an abstraction layer that resides above and across hyperscale infrastructure, connects on premises workloads and eventually stretches to the edge. Our premise is that supercloud goes beyond running services in native mode on each individual cloud. Rather supercloud hides the underlying primitives and APIs of the respective cloud vendors and creates a new connection layer across locations. Since our initial post, we’ve found many examples within the ecosystem of technology companies working on so-called supercloud in various forms. Including some examples that actually do not try to hide cloud primitives but rather are focused on creating a consistent experience for developers across the devsecops tool chain, while preserving access to low level cloud services. In this Breaking Analysis we share some recent examples of supercloud that we’ve uncovered. We also tap theCUBE network to access direct quotes about supercloud from the many CUBE guests we’ve recently had on the program. Here we test the concept’s technical and business feasibility. We’ll also post some recent ETR data to put into context some of the players we think are going after this opportunity and where they’re at in their supercloud buildout. Watch the full video analysis. Episode 130 – Are Cyber Stocks Oversold or Still too Pricey? – Cybersecurity stocks have been sending mixed signals as of late…Mostly negative like much of tech. But some, such as Palo Alto Networks, despite a tough go of it recently, have held up better than most tech names. Others like CrowdStrike had been outperforming broader tech in March but then flipped in May. Okta’s performance was somewhat tracking along with CrowdStrike for most of the past several months but then the Okta hack changed the trajectory of that name. Zscaler has crossed the critical $1B ARR revenue milestone and sees a path to $5B, but the company’s stock fell sharply after its last earnings report and has been on a downtrend since last November…Meanwhile CyberArk’s recent beat and raise was encouraging and the stock acted well after its last report. Security remains the #1 initiative priority amongst IT organizations and the spending momentum for many high flying cyber names remains strong. So what gives in cybersecurity? In this Breaking Analysis we focus on security and will update you on the latest data from ETR to try and make sense out of the market and read into what this all means in both the near and long term for some of our favorite names in the sector. Watch the full video analysis. Episode 129- What you May not Know About the Dell Snowflake Deal – In the pre-cloud era, hardware companies would run benchmarks showing how database and application performance ran best on their systems relative to competitors and previous generation boxes. They would make a big deal out of it and the independent software vendors would do a “golf clap” in the form of a joint press release. It was a game of leapfrog amongst hardware competitors that became pretty commonplace over the years. The Dell-Snowflake deal underscores that the value prop between hardware companies and ISVs is changing and has much more to do with distribution channels and the amount of data that lives on-prem in various storage platforms. For cloud-native ISVs like Snowflake, they are realizing that despite their cloud-only dogma, they have to grit their teeth and deal with on-premises data or risk getting shut out of evolving data architectures. In this Breaking Analysis we unpack what little is known about the Snowflake announcement from Dell Technologies World… and discuss the implications of a changing cloud ecosystem landscape. We’ll also share some new ETR data for cloud and database platforms that shows Snowflake has actually entered the earth’s orbit when it comes to spending momentum on its platform. Watch the full video analysis. Episode 128- The Ever expanding Cloud Continues to Storm the IT Universe – Despite a mixed bag of earnings reports from tech companies, negative GDP growth this past quarter and rising inflation…the cloud continues its relentless expansion on the IT landscape. AWS, Microsoft and Alphabet have all reported earnings and, when you include Alibaba’s cloud in the mix, the big 4 hyperscalers are on track to generate $167B in revenue this year based on our projections. But as we’ve said many times the definition of cloud is expanding. And hybrid environments are becoming the norm at major organizations. We’re seeing the largest enterprise tech companies focus on solving for hybrid and every public cloud company now has a strategy to bring their environments closer to where customers’ workloads live – in data centers and the edge. Hello and welcome to this week’s Wikibon CUBE Insights, Powered by ETR. In this Breaking Analysis we’ll update you on our latest cloud projections and outlook. We’ll share the latest ETR data and some commentary on what’s happening in the “hybrid zone” of cloud. Watch the full video analysis. Episode 127- Does Hardware Still Matter – The ascendancy of cloud and SaaS has shone new light on how organizations think about, pay for, and value hardware. Once-sought-after skills for practitioners with expertise in hardware troubleshooting, configuring ports, tuning storage arrays and maximizing server utilization have been superseded by demand for cloud architects, DevOps pros and developers with expertise in microservices, container app development and similar skills. Even a company like Dell, the largest hardware company in enterprise tech, touts that it has more software engineers than those working in hardware. It begs the question: Is hardware going the way of COBOL? Well, not likely — software has to run on something. But the labor and skills needed to deploy, troubleshoot and manage hardware infrastructure is shifting quite dramatically. At the same time we’ve seen the value flow also changing in hardware. Once a world dominated by x86 processors, value is flowing to alternatives like Nvidia and Arm-based designs. Moreover, other components like NICs, accelerators and storage controllers are becoming more advanced, integrated and increasingly important. The question is: Does it matter? If so, why does it matter and to whom? What does it mean to customers, workloads, OEMs and the broader society? In this Breaking Analysis we try to answer these questions and to do so we’ve organized a special CUBE Power Panel of industry analysts and experts to address the question: Does Hardware (Still) Matter? Watch the full video analysis. Episode 126 – Technology & Architectural Considerations for Data Mesh – The introduction and socialization of data mesh has caused practitioners, business technology executives and technologists to pause and ask some probing questions about the organization of their data teams, their data strategies, future investments and their current architectural approaches. Some in the technology community have embraced the concept, others have twisted the definition while still others remain oblivious to the momentum building around data mesh. We are in the early days of data mesh adoption. Organizations that have taken the plunge will tell you aligning stakeholders is a non-trivial effort. But one that is necessary to break through the limitations that monolithic data architectures and highly specialized teams have imposed over frustrated business and domain leaders. However, practical data mesh examples often lie in the eyes of the implementer and may not strictly adhere to the principles of data mesh. Part of the problem is the lack of open technologies and standards that can accelerate adoption and reduce friction. This is the topic of today’s Breaking Analysis where we investigate some of the key technology and architectural questions around data mesh. To do so, we welcome back the founder of data mesh and Director of Emerging Technologies at ThoughtWorks, Zhamak Dehghani. Watch the full video analysis. Episode 125 – Customer ripple effects from the Okta breach are worse than you think – The recent security breach of an Okta third party supplier has been widely reported. The criticisms of Okta’s response have been harsh and the impact on Okta’s value has been obvious. Investors shaved about $6B off the company’s market cap during the week the hack was made public. We believe Okta’s claim that the customer technical impact was “near zero,” may be semantically correct. However, based on customer data, we feel Okta has a blind spot. There are customer ripple effects that require clear action, which are missed in Okta’s public statements. Okta’s product portfolio remains solid. It is a clear leader in the identity space. But in our view, one part of the long journey back to credibility requires Okta to fully understand and recognize the true scope of this breach on its customers. In this week’s Breaking Analysis we welcome our ETR colleague Erik Bradley to share new data from the community. In addition, we’ll analyze some of the statements made by Okta CEO Todd McKinnon in an interview with Emily Chang on Bloomberg to see how they align with what customers tell us. Watch the full video analysis. Episode 124 – New Data Signals C Suite Taps the Brakes on Tech Spending – Fresh survey data from ETR shows a clear deceleration in spending and a more cautious posture from technology buyers. Just this week we saw sell side downgrades in hardware companies like Dell and HP; and revised guidance from high flier UiPath, citing exposure to Russia, Europe and certain sales execution challenges. But these headlines we think are a canary in the coal mine pointing to broader tech spending softness. According to ETR analysis and channel checks in theCUBE community, the real story is these issues are not isolated. Rather we’re seeing signs of caution from buyers across the board in enterprise tech. In this Breaking Analysis we are the bearers of bad news, relatively speaking. We’ll share a first look at new data that suggest a tightening in tech spending, calling for 6% growth this year, which is below our January prediction of 8% for 2022. Watch the full video analysis. Episode 123 – Governments Should Heed the History of Tech Antitrust Policy – There are very few political issues that get bipartisan support these days, never mind consensus spanning geopolitical boundaries. But whether we’re talking across the aisle or over the pond, there seems to be common agreement that the power of big tech firms should be regulated. However the government’s track record when it comes to antitrust aimed at tech is mixed, at best. History shows that market forces, rather than public policy, have been much more effective at curbing monopoly power in the technology industry. Moreover, the standard for antitrust action has always been demonstrating consumer harm. Many of today’s policy makers are challenging that notion and using market dominance and the potential for consumer harm as the new benchmark for intervention. In this week’s Breaking Analysis we welcome in frequent CUBE contributor David Moschella, author and senior fellow at the Information Technology and Innovation Foundation. We explore several issues including the efficacy of governments’ antitrust actions against big tech, what types of remedies have been and can be most effective and a first pass assessment of the new rules EU regulators just agreed to try and rein in big tech companies. Watch the full video analysis. Episode 122 – Snowflake’s Wild Ride – Snowflake…they love the stock at 400 and hate it at 165. That’s the nature of the business isn’t it? Especially in this crazy cycle over the last two years of lockdowns, free money, exploding demand and now rising inflation and rates. But with the Fed providing some clarity on its actions, the time has come to really look at the fundamentals of companies and there’s no tech company more fun to analyze than Snowflake. In this breaking analysis we take look at the action of Snowflake’s stock since its IPO, why it’s behaved the way it has, how some sharp traders are looking at the stock and most importantly, what customer demand looks like. Watch the full video analysis. Episode 121 – Pat Gelsinger has the Vision Intel Just Needs Time, Cash & a Miracle – Intel’s future would be a disaster without Pat Gelsinger. Even with his clear vision, fantastic leadership, deep technical and business acumen and amazing positivity, the company’s future is in serious jeopardy. It’s the same story we’ve been telling for years. Volume is king in the semiconductor industry and Intel no longer is the volume leader. Despite Intel’s efforts to change that dynamic with several recent moves, including making another go at its foundry business, the company is years away from reversing its lagging position relative to today’s leading foundries and design shops. Intel’s best chance to survive as a leader in our view will come from a combination of a massive market, continued supply constraints, government money and luck – perhaps in the form of a deal with Apple in the mid- to long term. In this Breaking Analysis we’ll update you on our latest assessment of Intel’s competitive position and unpack nuggets from the company’s February investor conference. Watch the full video analysis. Episode 120 – RPA has Become a Transformation Catalyst, Here’s What’s New – In its early days, robotic process automation emerged from rudimentary screen scraping, macros and workflow automation software. Once a script-heavy and limited tool that was almost exclusively used to perform mundane tasks for individual users, RPA has evolved into an enterprise-wide megatrend that puts automation at the center of digital business initiatives. In this Breaking Analysis we present our quarterly update of the trends in RPA and share the latest survey data from Enterprise Technology Research. Watch the full video analysis. Episode 119 – Cyber Stocks Caught in the Storm While Private Firms Keep Rising – The pandemic precipitated what is shaping up to be a permanent shift in cyber security spending patterns. As a direct result of hybrid work, CISOs have invested heavily in endpoint security, identity access management, cloud security and further hardening the network beyond the HQ. Moreover, the need to build security into applications from the start, rather than bolting protection on as an afterthought, has led to vastly heightened awareness around DevSecOps. Finally, attacking security as a data problem with automation and AI is fueling new innovations in cyber products and services; and is spawning well-funded, disruptive startups. In this Breaking Analysis we present our quarterly findings on the security sector. We’ll share the latest ETR survey data, identify the companies with customer spending momentum and share some of the market movers. Watch the full video analysis. Episode 118 – The Improbable Rise of Kubernetes – The rise of Kubernetes came about through a combination of forces that were in hindsight, quite a long shot. AWS’ dominance created momentum for cloud native application development and the need for simpler experiences beyond easily spinning up compute as a service. This wave crashed into innovations from a startup named Docker and a reluctant open source benefactor in Google that needed a way to change the game on Amazon in the cloud. Factor in Red Hat, which needed a path beyond Linux and was just about to opt for an alternative to Kubernetes to power OpenShift. Finally, figure out a governance structure to herd all the cats in the ecosystem so you can win out over other competition and create a de facto standard. Make all that happen and you get the remarkable ascendancy of Kubernetes. Such was the story unveiled recently in a new two-part documentary series from Honeypot simply titled “Kubernetes.” In this Breaking Analysis we tap the back stories of this documentary, which explains the improbable events leading to the creation of Kubernetes. We’ll share commentary from early Kubernetes committers and key players who came on theCUBE to piece together how it all happened. Finally, we’ll present new survey data from ETR on containers. Watch the full video analysis. Episode 117 – What to Expect in Cloud 2022 & Beyond – We’ve often said that the next ten years in cloud computing won’t be like the last ten. Cloud has firmly planted its footprint on the other side of the chasm with the momentum of the entire multi-trillion dollar technology business behind it. Both sellers and buyers are leaning in by adopting cloud technologies and many are building their own value layers on top of cloud. In the coming years we expect innovation will continue to coalesce around the big 3 U.S. clouds, plus Alibaba in APAC, with the ecosystem building value on top of the hardware, software and tools provided by the hyperscalers. Importantly, we don’t see this as a race to the bottom. Rather our expectation is that the large public cloud players will continue to take cost out of their platforms through innovation, automation and integration. Other cloud providers and the ecosystem, including traditional IT buyers, will leverage hyperscale clouds and mine opportunities in their respective markets. This is not a zero sum game. In this Breaking Analysis we’ll update you on our latest projections in the cloud market, share some new ETR survey data with some surprising nuggets; and drill into the important cloud database landscape. Watch the full video analysis. Episode 116 – Securing Snowflake – The amount of data ingested into a data warehouse overwhelmed the system. Every time Intel came out with a new microprocessor, practitioners would “chase the chips” in an effort to try and compress the overly restrictive elapsed time to insights. This cycle repeated itself for decades. Cloud data warehouses generally and Snowflake specifically changed all this. Not only were resources virtually infinite, but the ability to separate compute from storage permanently altered the cost, performance, scale and value equation. But as data makes its way into the cloud and is increasingly democratized as a shared resource across clouds – and at the edge – practitioners must bring a SecDevOps mindset to securing their cloud data warehouses. This Breaking Analysis takes a closer look at the fundamentals of securing Snowflake. An important topic as data becomes more accessible and available to a growing ecosystem of users, customers and partners. To do so we welcome two guests to this episode. Ben Herzberg is an experienced hacker, developer and an expert in several aspects of data security. Yoav Cohen is a technology visionary and currently serving as CTO at Satori Cyber. Watch the full video analysis. Episode 115 – Enterprise Technology Predictions 2022 – The pandemic has changed the way we think about, and predict the future. As we enter the third year of COVID, we see the significant impact it’s had on technology strategies, spending patterns and company fortunes. Much has changed and while many of these changes were forced reactions to a new abnormal, the trends we’ve seen over the past twenty-four months have become more entrenched and point the way to what’s ahead in the technology business. In this Breaking Analysis we welcome our data partner and colleague Erik Porter Bradley from ETR and we put forth our annual predictions for enterprise technology in 2022 and beyond. We’ll do our best to backup our predictions specific supporting data and more granular detail that can be measured as accurate or not. Please refer to the grading of our 2021 predictions to judge for yourself how we did last year. Watch the full video analysis. Episode 114 – Cyber, Blockchain & NFTs Meet the Metaverse – When Facebook changed its name to Meta last fall it catalyzed a chain reaction throughout the tech industry. Software firms, gaming companies, chip makers, device manufacturers and others have joined in the hype machine. It’s easy to dismiss the metaverse as futuristic hyperbole, but do we really believe that tapping on a smartphone, staring at a screen or two dimensional Zoom meetings are the future of how we work, play and communicate? As the Internet itself proved to be larger than we ever imagined, it’s very possible that the combination of massive processing power, cheap storage, AI, blockchains, crypto, sensors, AR/VR, brain interfaces and other emerging technologies will combine to create new and unimaginable consumer experiences; and massive wealth for creators of the metaverse. In this Breaking Analysis we explore the intersection of cybersecurity, blockchain, crypto currency, NFTs and the emerging metaverse. To do so we welcome in cyber expert, hacker, gamer, NFT expert and founder of Ore System, Nick Donarski. Watch the full video analysis. Episode 113 – Analyst Predictions 2022: The Future of Data Management – In the 2010’s, organizations became keenly aware that data would become the critical ingredient in driving competitive advantage, differentiation and growth. But to this day, putting data to work remains a difficult challenge for many if not most organizations. As the cloud matures it has become a game changer for data practitioners by making cheap storage and massive processing power readily accessible. We’ve also seen better tooling in the form of data workflows, streaming, machine intelligence/AI, developer tools, security, observability, automation, new databases and the like. These innovations accelerate data proficiency but at the same time add complexity for practitioners. Data lakes, data hubs, data warehouses, data marts, data fabrics, data meshes, data catalogs and data oceans are forming, evolving and exploding onto the scene. In an effort to bring perspective to this sea of optionality, we’ve brought together some of the brightest minds in the data analyst community to discuss how data management is morphing and what practitioners should expect in 2022 and beyond. Watch the full video analysis. Episode 112 – Grading our 2021 Predictions – Predictions are all the rage this time of year. On December 29th, 2020, in collaboration with Erik Porter Bradley of Enterprise Technology Research, we put forth our predictions for 2021. The focus of our prognostications included tech spending, remote work, productivity apps, cyber, IPOs, SPACs, M&A, data architecture, cloud, hybrid cloud, multi-cloud, AI, containers, automation and semiconductors. We covered a lot of ground! In this Breaking Analysis, as a warmup for our 2022 predictions post, we’ll review each of our predictions for this past year and grade the accuracy of our forecasts Watch the full video analysis. Episode 111 – Why Oracle’s Stock is Surging to an All time High – On Friday, Oracle announced a meaningful earnings beat and strong forward guidance on the strength of its license business; and slightly better than expected cloud performance. The stock rose sharply on the day and closed up nearly 16% surpassing $280B in market value. Oracle’s success is due largely to its execution of a highly differentiated strategy that has evolved over the past decade or more; deeply integrating its hardware and software, heavily investing in next generation cloud, creating a homogenous experience across its application portfolio and becoming the number one platform for the world’s most mission critical applications. While investors piled into the stock, skeptics will point to the beat being tilted toward license revenue and investors will likely keep one finger on the sell button until they’re convinced Oracle’s cloud momentum is more consistent and predictable. In this Breaking Analysis we’ll review Oracle’s most recent quarter and pull in some ETR survey data to frame the company’s cloud business, the momentum of Fusion ERP, where the company is winning and some gaps/opportunities we see that can be addressed in the coming quarters. Watch the full video analysis. Episode 110 – Rise of the Supercloud – Last week’s AWS re:Invent underscored the degree to which cloud computing generally and AWS specifically have impacted the technology landscape. From making infrastructure deployment simpler, to accelerating the pace of innovation, to the formation of the world’s most active and vibrant technology ecosystem; it’s clear that AWS has been the number one force for industry change in the last decade. Going forward we see three high level contributors from AWS that will drive the next 10 years of innovation, including: 1) the degree to which data will play a defining role in determining winners and losers; 2) the knowledge assimilation effect of AWS’ cultural processes such as two pizza teams, customer obsession and working backwards; and 3) the rise of superclouds– that is clouds built on top of hyperscale infrastructure that focus not only on IT transformation, but deeper business integration and digital transformation of entire industries. In this Breaking Analysis we’ll review some of the takeaways from the 10th annual AWS re:Invent conference and focus on how we see the rise of superclouds impacting the future of virtually all industries. Watch the full video analysis. Episode 109 – Break up Amazon? Survey Suggests it May Not be Necessary – Despite the posture that big tech generally and Amazo.com Inc.specifically should be regulated and/or broken apart, recent survey research suggests that Amazon faces many disruption challenges, independent of any government intervention. Specifically, respondents to our survey believe that history will repeat itself in that there’s a 60% probability that Amazon will be disrupted by market forces, including self-inflicted wounds. Amazon faces at least seven significant disruption scenarios of varying likelihood and impact, perhaps leading to the conclusion that the government should let the market adjudicate Amazon’s ultimate destiny. In this Breaking Analysis, and ahead of AWS re:Invent, we share the results of our survey designed to asses what, if anything, could disrupt Amazon. We’ll also show you some data from ETR that indicates the strong momentum of AWS is likely to continue, which could be a factor in any government intervention. Watch the full video analysis. Episode 108 – AWS & Azure Accelerate Cloud Momentum – Despite all the talk about repatriation, hybrid and multi-cloud opportunities and cloud as an increasingly expensive option for customers…the data continues to show the importance of public cloud to the digital economy. Moreover, the two leaders, AWS and Azure are showing signs of accelerated momentum that point to those two giants pulling away from the pack in the years ahead. Each of these companies is demonstrating broad-based momentum across their respective product lines. It’s unclear if anything other than government intervention or self-inflicted wounds will slow these two companies down this decade. Despite the commanding lead of the two leaders, a winning strategy for companies that don’t run their own cloud continues to be innovating on top of their massive CAPEX investments. The most notable example of this approach in our view continues to be Snowflake. In this Breaking Analysis, Dave will provide our quarterly market share update of the big four hyperscale cloud providers. We’ll share some new data from ETR based on the most recent survey, drill into some of the reasons for the momentum of AWS and Azure; and drill further into the database and data warehouse sector to see what if anything has changed in that space. Watch the full video analysis. Episode 107 – Cutting Through the Noise of Full Stack Observability – Full stack observability is the new buzz phrase. As businesses go digital, customer experience becomes ever more important. Why? Because fickle consumers can switch brands in the blink of an eye – or the click of a mouse. Every vendor wants a piece of the action in this market including companies that have provided traditional monitoring, log analytics, application performance management, and related services. These companies are joined by a slew of new entrants claiming end-to-end visibility across the so-called “modern tech stack.” Recent survey research however confirms our thesis that no one company has it all. New entrants have a vision and are not encumbered by legacy technical debt. However their offerings are immature. Established players with deep feature sets in one segment are pivoting through M&A and organic development to fill gaps. Meanwhile, cloud players are gaining traction and participating through a combination of native tooling combined with strong ecosystems to address this opportunity. In this Breaking Analysis we dive into a recent ETR drill down study on full stack observability. And to do so we once again welcome in our colleague Erik Bradley, Chief Engagement Strategist at ETR. Watch the full video analysis. Episode 106 – What Could Disrupt Amazon? – Five publicly traded, US-based companies have market valuations over or just near one trillion dollars. As of Oct. 29th, Apple and Microsoft top the list, each at $2.5T, followed by Alphabet at $2T, Amazon at $1.7T and Facebook (now Meta) at just under $1T – off from its high of $1.1T prior to its recent troubles. These companies have reached extraordinary levels of success and power. What, if anything could disrupt their market dominance? In his book Seeing Digital, Author David Moschella made three key points that are relevant: In the technology industry, disruptions are the norm – The waves of mainframes, Minis, PCs, Mobile and the Internet all saw new companies emerge and power structures that dwarfed previous eras of innovation. Is that dynamic changing? Every industry has a disruption scenario. Silicon Valley – broadly defined to include Seattle, or at least Amazon – has a dual disruption agenda. The first being horizontally targeting the technology industry and the second as digital disruptors in virtually any industry. How relevant is that to the future power structure of the technology business? In this Breaking Analysis we welcome in author, speaker, researcher, thought leader and senior fellow at ITIF, David Moschella to assess what could possibly disrupt today’s trillionaire companies. And we’ll start with Amazon. Watch the full video analysis. Episode 105 – Data Mesh…A New Paradigm for Data Management – Data mesh is a new way of thinking about how to use data to create organizational value. Leading edge practitioners are beginning to implement data mesh in earnest. Importantly, data mesh is not a single tool or a rigid reference architecture. Rather it’s an architectural and organizational model that is designed to address the shortcomings of decades of data challenges and failures. As importantly, it’s a new way to think about how to leverage and share data at scale across an organization and ecosystems. Data mesh in our view will become the defining paradigm for the next generation of data excellence. In this Breaking Analysis we welcome the founder and creator of data mesh, author, thought leader, technologist Zhamak Dehghani, who will help us better understand some of core principles of data mesh and the future of decentralized data management. With practical advice for data pros who want to create the next generation of data-driven organizations. Watch the full video analysis. Episode 104 – The Hybrid Cloud Tug of War Gets Real – It looks like Hybrid cloud is finally here. We’ve seen a decade of posturing, marketecture, slideware and narrow examples but there’s little question that the definition of cloud is expanding to include on-premises workloads in hybrid models. Depending on which numbers you choose to represent IT spending, public cloud accounts for less than 5% of the total pie. As such there’s a huge opportunity in hybrid, outside of the pure public cloud; and everyone wants a piece of the action. The big question is how will this now evolve? Customers want control, governance, security, flexibility and a feature-rich set of services to build their digital businesses. It’s unlikely they can buy all that – so they’re going to have to build it with partners. Specifically vendors, SIs, consultancies, and their own developers. The tug-of-war to win the new cloud day has finally started in earnest – between the hyperscalers and the largest enterprise tech companies in the world. Watch the full video analysis. Episode 103 – The Future of the Semiconductor Industry – Semiconductors are at the heart of technology innovation. For decades, technology improvements have marched to the cadence of silicon advancements in performance, cost, power and packaging. In the past ten years, the dynamics of the semiconductor industry have changed dramatically. Soaring factory costs, device volume explosions, fabless chip companies, greater programmability, compressed time to tape out, more software content, the looming Chinese presence…these and other factors have permanently changed the power structure of the semiconductor business. We rely on chips for every aspect of our lives, which has led to a global semiconductor shortage that has impacted more industries than we’ve ever seen. Our premise is that silicon success in the next twenty years will be determined by volume manufacturing expertise, design innovation, public policy, geopolitical dynamics, visionary leadership and innovative business models that can survive the intense competition in one of the most challenging businesses in the world. Watch the full video analysis. Episode 102 – UiPath Fast Forward to Enterprise Automation | UiPath FORWARD IV – UiPath has always been an unconventional company. It started with humble beginnings as essentially a software development shop. It then caught lightning in a bottle with its computer vision technology and simplification mantra…creating easy to deploy software robots for bespoke departments to automate mundane tasks. The story is well known…the company grew rapidly and was able to go public earlier this year. Consistent with its out of the ordinary approach, while other firms are shutting down travel and physical events, UiPath is moving ahead with Forward IV, its annual user conference next week…with a live audience at the Bellagio in Las Vegas. It’s also “Fast Forwarding” as a company, determined to lead the charge beyond RPA point tools and execute on a more all-encompassing enterprise automation agenda. Watch the full video analysis. Episode 101 – CIOs Signal Hybrid Work Will Power Spending Through 2022 – Throughout the pre-vaccine COVID era, IT buyers indicated budget constraints would squeeze 2020 spending by roughly 5% relative to 2019 levels. But the forced March to digital combined with increased cyber threats for remote workers, created a modernization mandate that powered Q4 spending last year. This momentum has carried through to 2021. While COVID variants have delayed return to work and business travel plans, our current forecast for global IT spending remains strong at 6-7%, slightly down from previous estimates. But the real story is CIOs and IT buyers expect a 7-8% increase in 2022 spending, reflecting investments in hybrid work strategies and a continued belief that technology remains the underpinning of competitive advantage in the coming decade. In this Breaking Analysis, Dave will share the latest results of ETR’s macro spending survey and update you on industry and sector investment patterns. Watch the full video analysis. Following is the complete collection to date, crystallizing the key topics of the past year or so. We hope you enjoy these episodes and, as always, welcome your feedback. Episode 100 – How Cisco can win cloud’s ‘Game of Thrones’ – Cisco is a company at the crossroads. It is transitioning from a high margin hardware business to a software subscription-based model through both organic moves and targeted acquisitions. It’s doing so in the context of massive macro shifts to digital and the cloud. We believe Cisco’s dominant position in networking, combined with a large market opportunity and a strong track record of earning customer trust, put the company in a good position to capitalize on cloud momentum. But there are clear challenges ahead, not the least of which is the growing complexity of Cisco’s portfolio, transitioning a large legacy business and the mandate to maintain its higher profitability profile as it moves to a new business model. In this Breaking Analysis, we welcome in Zeus Kerravala, Founder and Principal Analyst at ZK Research and long time Cisco watcher who collaborated with us to craft the premise of this session. Watch the full video analysis. Episode 99 – The Case for Buy the Dip on Coupa, Snowflake & Zscaler – Buy the dip has been an effective strategy since the market bottomed in early March last year. The approach has been especially successful in tech and even more so for those tech names that: 1) were well-positioned for the forced march to digital – i.e. remote work, online commerce, data-centric platforms and certain cybersecurity plays; and 2) already had the cloud figured out. The question on investors’ minds is where to go from here. Should you avoid some of the high flyers that are richly valued with eye-popping multiples? Or should you continue to buy the dip? And if so, which companies that capitalized on the trends from last year will see permanent shifts in spending patterns that make them a solid long term play. In this Breaking Analysis we shine the spotlight on three companies that may be candidates for a buy the dip strategy over the next 3-5 years. To do so it’s our pleasure to welcome Ivana Delevska, the Chief Investment Officer and founder of SPEAR Alpha, a new, research-centric ETF focused on industrial technology. Watch the full video analysis. Episode 98 – Thinking Outside the Box…AWS Signals a New Era for Storage – By our estimates, AWS will generate around $9B in storage revenue this year and is now the second largest supplier of enterprise storage behind Dell. We believe AWS storage revenue will surpass $11B in 2022 and continue to outpace on-prem storage growth by more than 1,000 basis points for the next three to four years. At its third annual Storage Day event, AWS signaled a continued drive to think differently about data storage and transform the way customers migrate, manage and add value to their data over the next decade. In this Breaking Analysis Dave will give a brief overview of what we learned at AWS’ Storage Day, share our assessment of the big announcement of the day – a deal with NetApp to run the full ONTAP stack natively in the cloud as a managed service – and share some new data on how we see the market evolving. Watch the full video analysis. Episode 97 – Tech Earnings Signal a Continued Booming Market – Tech earnings reports from key enterprise software and infrastructure players this week, underscore that IT spending remains robust in the post isolation economy. This is especially true for those companies that have figured out a coherent and compelling cloud strategy. Despite COVID variant uncertainties and hardware component shortages, most leading tech names outperformed expectations. That said, investors weren’t in the mood to reward all stocks and any variability in product mix, earnings outlook or bookings/billings nuances were met with a tepid response from the street. In this Breaking Analysis Dave will provide our commentary and new data points on key technology companies including Snowflake, Salesforce, Workday, Splunk, Elastic, Palo Alto Networks, VMware, Dell, Pure Storage, HP Inc. and NetApp. Watch the full video analysis. Episode 96 – Can anyone tame the identity access beast? Okta aims to try… – Chief information security officers cite trust as the most important value attribute they can deliver to their organizations. And when it comes to security, identity is the new attack surface. As such, identity and access management continue to be the top priority among technology decision makers. It also happens to be one of the most challenging and complicated areas of the cyber security landscape. Okta, a leader in the identity space, has announced its intent to converge privilege access and identity governance in an effort to simplify the landscape and reimagine identity. Our research shows that interest in this type of consolidation is high, but organizations believe technical debt, compatibility issues, expense and lack of talent are barriers to reaching cyber nirvana with their evolving zero trust networks. In this Breaking Analysis, Dave will explore the complex and evolving world of identity access and privileged account management. With an assessment of Okta’s market expansion aspirations and fresh data from ETR and input from Erik Bradley. Watch the full video analysis. Episode 95 – Rethinking Data Protection in the 2020s – Techniques to protect sensitive data have evolved over thousands of years, literally. The pace of modern data protection is rapidly accelerating and presents both opportunities and threats for organizations. In particular, the amount of data stored in the cloud, combined with hybrid work models, the clear and present threat of cyber crime, regulatory edicts and ever-expanding edge use cases should put CxOs on notice that the time is now to rethink your data protection strategies. In this Breaking Analysis, Dave is going to explore the evolving world of data protection and share some data on how we see the market evolving and the competitive landscape for some of the top players. Watch the full video analysis. Episode 94 – Cyber, Cloud, Hybrid Work & Data Drive 8% IT Spending Growth in 2021 – Every CEO is figuring out the right balance for new hybrid business models. Regardless of the chosen approach, which will vary, technology executives understand they must accelerate digital and build resilience as well as optionality into their platforms. This is driving a dramatic shift in IT investments at the macro level as we expect total spending to increase at 8% in 2021, compared to last year’s contraction. Investments in cyber security, cloud, collaboration to enable hybrid work and data, including analytics, AI and automation are the top spending priorities for CxOs. In this Breaking Analysis Dave welcomes back Erik Bradley, Chief Engagement Strategist at our partner ETR. In this post we’ll share some takeaways from ETR’s latest survey and provide our commentary on what it means for markets, sellers and buyers. We’ll also explain what we think Wall Street is missing about Amazon’s latest earnings. Watch the full video analysis. Episode 93 – ServiceNow’s Collision Course with Salesforce.com – ServiceNow is a company that investors love to love. But there’s caution in the investor community right now as confusion about transitory inflation and higher interest rates looms. ServiceNow also suffers from perfection syndrome and elevated expectations. In this Breaking Analysis Dave will dig into ServiceNow, one of the companies we began following almost ten years ago, and provide some thoughts on ServiceNow’s march to $15B by 2026. Watch the full video analysis. Episode 92 – Survey Data Shows no Slowdown in AWS & Cloud Momentum – Despite all the chatter about cloud repatriation and the exorbitant cost of cloud computing, customer spending momentum continues to accelerate in the post isolation economy. If the pandemic was good for the cloud it seems that the benefits of cloud migration remain lasting in the late stages of COVID. And we believe this stickiness will continue. In this Breaking Analysis Dave will share some fresh July survey data that indicates accelerating momentum for the largest cloud computing firms. Watch the full video analysis. Episode 91 – How JPMC is Implementing a Data Mesh Architecture on the AWS Cloud – A new era of data is upon us. The technology industry generally and the data business specifically are in a state of transition. Even our language reflects that. For example, we rarely use the phrase “Big Data” anymore. Rather we talk about digital transformation or data-driven companies. In this Breaking Analysis we want to share our assessment of the state of the data business. We’ll do so by looking at the data mesh concept and how a division of a leading financial institution, JPMC, is practically applying these relatively new ideas to transform its data architecture for the next decade. Watch the full video analysis. Episode 90 – Mobile World Congress Highlights Telco Transformation – AMobile World Congress is on for 2021. theCUBE will be there and we’ll let you know if it’s alive and well. As we approach a delayed MWC it’s appropriate to reflect on the state of the telecoms industry. Let’s face it – the telcos have done a great job of keeping us all connected during the pandemic. In this Breaking Analysis we welcome a long time telecoms industry analyst and the Founding Director of Lewis Insight, Mr. Chris Lewis. Watch the full video analysis. Episode 89 – How AWS is Revolutionizing Systems Architecture – AWS is pointing the way to a revolution in system architecture. Much in the same way that AWS defined the cloud operating model last decade, we believe it is once again leading in future systems. In this Breaking Analysis we’ll dig into the moves that AWS has been making, explain how they got here, why we think this is transformational for the industry and what this means for customers, partners and AWS’ many competitors. Watch the full video analysis. Episode 88 – Learnings from the hottest startups in cyber & IT infrastructure – As you well know by now, the cloud is about shifting IT labor to more strategic initiatives. Or as Andy Jassy posited at the first AWS re:Invent conference in 2012, removing the undifferentiated heavy lifting associated with deploying and managing IT infrastructure. In this Breaking Analysis Dave is pleased to welcome a special guest, Erik Suppiger, author of the Elite 80 – a report that details the hottest privately held cybersecurity and IT infrastructure companies in the world. Erik is a senior analyst at JMP Securities and will share insights from this report. Watch the full video analysis. Episode 87 – Chasing Snowflake in Database Boomtown – Database is the heart of enterprise computing. The market is both growing rapidly and evolving. Major forces transforming the space include cloud and data – of course – but also new workloads, advanced memory and IO capabilities, new processor types, a massive push toward simplicity, new data sharing and governance models; and a spate of venture investment. In this Breaking Analysis Dave will share our most current thinking on the database marketplace and dig into Snowflake’s execution, some of its challenges and we’ll take a look at how others are making moves to solve customer challenges; and angling to get their piece of the growing database pie. Watch the full video analysis. Episode 86 – How Nvidia plans to own the data center with AI – Nvidia wants to completely transform enterprise computing by making datacenters run 10X faster at 1/10th the cost. In this Breaking Analysis Dave will explain why we believe Nvidia is in a strong position to power the world’s computing centers and how it plans to disrupt the grip that x86 architectures have had on the datacenter market for decades. Watch the full video analysis. Episode 85 – Your Online Assets Aren’t Safe – Is Cloud the Problem or the Solution? – The convenience of online access to bank accounts, payment apps, crypto exchanges and other transaction systems has created enormous risks, which the vast majority of individuals either choose to ignore or simply don’t understand. In this Breaking Analysis Dave will try to raise awareness about a growing threat to your liquid assets and hopefully inspire you to do some research and take actions to lower the probability of you losing thousands, hundreds of thousands or millions of dollars. Watch the full video analysis. Episode 84 – Debunking the Cloud Repatriation Myth – Cloud repatriation is a term often used by technology companies that don’t operate a public cloud. The marketing narrative most typically implies that customers have moved work to the public cloud and, for a variety of reasons – expense, performance, security, etc. Some have written about the repatriation myth, but in this Breaking Analysis, Dave will share hard data from ETR and other sources that we feel debunks the repatriation narrative as it’s currently being promoted. Watch the full video analysis. Episode 83 – Chaos Creates Cash for Criminals & Cyber Companies – The pandemic not only accelerated a shift to digital, it highlighted a rush of cyber criminal sophistication, collaboration and chaotic responses from virtually every major company on the planet. The SolarWinds hack exposed digital supply chain weaknesses and appears to have accelerated so-called island hopping techniques that are exceedingly difficult to detect. In this Breaking Analysis Dave will provide our quarterly update on the security industry and share new survey data from ETR and theCUBE community that will help you navigate through the maze of corporate cyber warfare. Watch the full video analysis. Episode 82 – Why Apple Could be the Key to Intel’s Future – The latest Arm Neoverse announcement further cements our opinion that its architecture, business model and ecosystem execution are defining a new era of computing; and leaving Intel in its dust. In this Breaking Analysis, Dave will explain why and how Apple could hold a key to saving Intel’s (and America’s) semiconductor industry leadership position. Watch the full video analysis. Episode 81 – A Digital Skills Gap Signals Rebound in IT Services Spend – Recent survey data from ETR shows that enterprise tech spending is tracking with projected U.S. GDP growth at 6% to 7% this year. Many markers continue to point the way to a strong recovery including hiring trends and the loosening of frozen IT project budgets. In this Breaking Analysis Dave welcomes back Erik Bradley, Chief Engagement Strategist at ETR, who will share fresh data, perspectives and insights from the latest survey data. Watch the full video analysis. Episode 80 – UiPath’s Unconventional $PATH to IPO – UiPath is going public this coming week and will be the next hot software company to IPO. It has had a long strange trip to IPO. In this week’s Breaking Analysis, Dave shares our learnings from sifting through hundreds of pages of UiPath’s S1 and convey our thoughts on its market, competitive position and outlook Watch the full video analysis. Episode 79 – Moore’s Law is Accelerating and AI is Ready to Explode – Moore’s Law is dead right? Think again. While the historical annual CPU performance improvement of ~40% is slowing, the combination of CPUs packaged with alternative processors is improving at a rate of more than 100% per annum. In this Breaking Analysis Dave is going to unveil some data that suggests we’re entering a new era of innovation where inexpensive processing capabilities will power an explosion of machine intelligence applications. Watch the full video analysis. Episode 78 – Arm Lays Down the Gauntlet at Intel’s Feet – Exactly one week after Pat Gelsinger unveiled plans to reinvent Intel, Arm announced version 9 of its architecture and put forth its vision for the next decade. In this Breaking Analysis Dave will explain why we think this announcement is so important and what it means for Intel and the broader technology landscape. Watch the full video analysis. Episode 77 – Intel… Too Strategic to Fail – Intel’s big announcement this week underscores the threat that the United States faces from China. The U.S. needs to lead in semiconductor design and manufacturing; and that lead is slipping because Intel has been fumbling the ball over the past several years. In this Breaking Analysis Dave will peel the onion of Intel’s announcement, explain why we’re not as sanguine as was Wall Street on Intel’s prospects and lay out what we think needs to take place for Intel to once again become top gun; and for us to gain more confidence. Watch the full video analysis. Episode 76 – Tech Spending Powers the Roaring 2020s as Cloud Remains a Staple of Growth – In the year 2020, it was good to be in tech. It was even better to be in the cloud as organizations had to rely on remote cloud services to keep things running. In this Breaking analysis Dave will provide our take on the latest ETR COVID survey and share why we think the tech boom will continue well into the future. Watch the full video analysis. Episode 75 – Breaking Analysis: Unpacking Oracle’s Autonomous Data Warehouse Announcement – On February 19th of this year, Barron’s dropped an article declaring Oracle a cloud giant and explained why the stock was a buy. Investors took notice and the stock ran up 18% over the next 9 trading days and peaked on March 9th, the day before the company announced its latest earnings. The company beat consensus earnings on both top line and EPS last quarter. But Investors didn’t like Oracle’s tepid guidance and the stock pulled back..but is still well above its pre-Barron’s article price. Watch the full video analysis. Episode 74 – Breaking Analysis: NFTs, Crypto Madness & Enterprise Blockchain – When a piece of digital art sells for $69.3M, more than has ever been paid for works by Paul Gauguin or Salvador Dali, making its creator the third most expensive living artist in the world, one can’t help but take notice and ask: “What is going on?” The latest craze around NFTs may feel a bit “bubblicious,” but it’s yet another sign that the digital age is now fully upon us. In this Breaking Analysis Dave wants to take a look at some of the trends that may have observers and investors scratching their heads, but we think still offer insight to the future — and possibly some opportunities for young investors. And we’ll briefly touch on how these trends may relate to enterprise tech. Watch the full video analysis. Episode 73 – Breaking Analysis: Satya Nadella Lays out a Vision for Microsoft at Ignite 2021 – Microsoft CEO Satya Nadella sees a different future for cloud computing over the coming decade. In this Breaking Analysis Dave Vellante will review the highlights of Nadella’s Ignite keynote, share our thoughts on what it means for the future of cloud specifically and tech generally. Watch the full video analysis. Episode 72 – Breaking Analysis: SaaS Attack, On Prem Survival & What’s a Cloud Company Look Like SaaS companies have been some of the strongest performers during this COVID era. In this Breaking Analysis, Dave Vellante picks out a few of the more recent themes from this month and share our thoughts on some major enterprise software players, the future of on-prem and a review of our take on cloud, what cloud will look like in the 2020s. Watch the full video analysis. Episode 71 – Breaking Analysis: RPA Remains on a Hot Streak as UiPath Blazes the Trail – UiPath’s recent $750M raise at a $35B valuation underscores investor enthusiasm for robotic process automation. In this Breaking Analysis Dave Vellante explores the current trends in the RPA market and try to address the question– is UiPath’s value supported by the ETR spending data, how will the RPA market evolve from a total available market (TAM) perspective and where do some of the other players like Automation Anywhere, Pegasystems and Blue Prism fit? Watch the full video analysis. Episode 70 – Breaking Analysis: How the SolarWinds Hack & COVID are Changing CISO Spending Patterns – The SolarWinds hack along with the pandemic are the two most visible catalysts for change in cybersecurity spending patterns. In addition to securing a more distributed workforce, CISOs have to now worry about protecting against the very software updates and patches designed to keep them safe against cyber attacks. In this Breaking Analysis, Dave Vellante shares data from a recent CISO roundtable hosted by ETR’s Erik Bradley and provides updates on the cybersecurity sector overall. Watch the full video analysis. Episode 69 – Breaking Analysis: Big 4 Cloud Revenue Poised to Surpass $115B in 2021 –There are four players in the IaaS/PaaS hyperscale cloud services space which have the ability to outperform all competitors. Combined in 2021, they will generate more than $115 billion dollars in revenue. In this Breaking Analysis, Dave Vellante initiates coverage of Alibaba, one of the Big Four in this massive market segment. Watch the full video analysis. Episode 68 – Breaking Analysis: Tech Spending Roars Back in 2021 –There is an expected six to seven percent increase in 2021 technology spending following the five percent decline over the past year. Many factors are contributing to this growth, and in this Breaking Analysis, Dave Vellante shares some of those reasons as well as the latest macro view of the market. Watch the full video analysis. Episode 67 – Breaking Analysis: Best of theCUBE on Cloud –The coming decade of cloud will be dramatically different from the last. There will be a shift toward a more data centric, hyper decentralized cloud that is far more complex than anything seen previously. In this Breaking Analysis, Dave Vellante summarizes exclusive content gathered from the recent theCUBE on cloud event. Watch the full video analysis. Episode 66 – Breaking Analysis: Pat Gelsinger Must Channel Andy Grove and Recreate Intel –Intel is fighting a war on two fronts: 1) Arm volumes have far surpassed those of Intel’s x86, conferring major cost advantages to leading fabs like TSMC and Samsung and 2) AMD continues to chip away at Intel’s dominance in its core markets. But the biggest challenge for incoming CEO Pat Gelsinger is perhaps to reinvent Intel by splitting manufacturing from design to make the company more agile and cost competitive. In this Breaking Analysis, Dave Vellante speculates about Intel’s future, and explains why Wikibon believes Intel has no choice but to shed its vertically integrated heritage. Episode 65 – Breaking Analysis: Breaking Analysis: 2021 Enterprise Technology Predictions –COVID-19 created a disruption in virtually all our expectations for 2020. In some regards, predictions for the past year played out very well, thanks to the pandemic. And in others, the complete opposite occurred. That being said, there is a lot to talk about heading into 2021. In this week’s Breaking Analysis, Dave Vellante is joined by Erik Bradley of ETR to share their top predictions for the upcoming year. Watch the full video analysis. Episode 64 – Breaking Analysis: Cloud Momentum & CIO Optimism Point to a 4% Rise in 2021 Tech Spending –Developments with COVID such as education, rapid vaccine rollout, productivity gains, and broad based cloud coverage suggest higher tech spending than previously forecasted for the upcoming year. Now, we can expect a 3-5 percent increase in 2021 spending. In this Breaking Analysis, Dave Vellante shares the data to support these predictions, and predicts which sectors are to gain momentum. Watch the full video analysis. Episode 63 – Breaking Analysis: Legacy Players Feel the Heat as AWS Storage Revenue Approaches $10B –Once an untapped bastion of innovation, storage in the data center now exists as a shell of what it used to be, and will remain as such. Specifically, AWS’ storage business is projected to hit between $6.5 – $7B this year and hit $10B within the next 18 – 24 months. In this Breaking Analysis, Dave Vellante lays out what this might mean for the industry, as well as the impact of AWS. Watch the full video analysis. Episode 62 – Breaking Analysis: Cloud 2030…From IT, to Business Transformation – Breaking Analysis: Cloud, Containers, AI & RPA Support Strong Rebound in 2021. Watch the full video analysis. Episode 61 – Breaking Analysis: Cloud 2030…From IT, to Business Transformation – Over the past decade, cloud computing has undoubtedly been the most pivotal force in IT. This brings the question as to what the next ten years will hold for cloud and the tech world. Perhaps, it will lay the foundations for a complete transformation of nearly every company worldwide. In this Breaking Analysis, as part of his coverage off AWS re:Invent 2020, Dave Vellante provides insights and predictions about the next breakthroughs in cloud. Watch the full video analysis. Episode 60 – Breaking Analysis: Sparked by COVID, CISOs see Permanent Shift in Cyber Strategies – CISOs report a forced shift to remote work has actually led to meaningful productivity improvements. This reality is causing security pros to rethink how they’ll approach security in the coming decade, informed by learnings during the pandemic. Watch the full video analysis. Episode 59 – Breaking Analysis: How Snowflake Plans to Change a Flawed Data Warehouse Model – Snowflake will not grow into its valuation by simply stealing share from the on-prem data warehouse vendors. Rather Snowflake must create an entirely new market based on completely changing the way organizations think about monetizing data. In this Breaking Analysis, Dave Vellante suggests a new data architecture that places domain knowledge at the core. Watch the full video analysis. Episode 58 – Breaking Analysis: Cloud Revenue Accelerates in the COVID Era – Over the past decade, cloud computing has undoubtedly been at the forefront of the innovation engine. The pandemic has accelerated the adoption of cloud and AI by at least two years, establishing a new era that will impact not only the technology industry, but all organizations. In this Breaking Analysis, Dave Vellante gives updates about the latest cloud market trends. Watch the full video analysis. Episode 57 – Breaking Analysis: Azure Cloud Powers microsoft’s Future – Big tech is once again under fire as CEOs of Facebook, Twitter, and Google face backlash from several US senators. Microsoft is not among these companies, as it relies on Azure cloud to build momentum, which now accounts for nineteen percent of its overall revenues. In this Breaking Analysis, Dave Vellante dives into the business of Microsoft and the data of its projected progress. Watch the full video analysis. Episode 56 – Breaking Analysis: Google’s Antitrust Play… Get Your Head out of Your Ads – The U.S. Department of Justice filed an antitrust lawsuit against Google, accusing the corporation of being a monopoly gatekeeper for the internet. In this Breaking Analysis, Dave Vellante shares data, covers the history of monopolistic power in the computer industry, and suggests future moves for Google to diversify its business. Watch the full video analysis. Episode 55 – Breaking Analysis: 2H 2020 Tech Spending: Headwinds into 2021 – Relative to 2019, tech spending has been hit hard, with a projected 5% decrease projected for 2020. Still, there seem to be bright spots within the market that show a slight increase in 2021 spending data. In this Breaking Analysis, Dave Vellante is joined by ETR’s Erik Bradley to provide the latest data supporting these trends. Watch the full video analysis. Episode 54 – CIOs Report Slow Thaw of Spending Freezes – Expect 2% Growth in 2021 – Recent Data provided by ETR suggests CIOs expect slight improvements in Q4 spending. Although these numbers are still down four percent from last year, this is a step in the right direction going into 2021. In this Breaking Analysis, Dave Vellante analyzes some of this data and provides his outlook for Q4 as well as the coming months. Watch the full video analysis. Episode 53 – Application Performance Management…From Tribal Knowledge to Digital Dashboard – Application Performance management has been around for a while, but it has had to evolve to accommodate for more complex operations, such as cloud-based systems. In this Breaking Analysis, Dave Vellante is teamed with Erik Bradley to offer the newest data to come out of the growing market. Watch the full video analysis. Episode 52 – Snowflake’s IPO… Here’s What’s Next – There is a lot of talk going on within the tech industry surrounding Snowflake’s recent IPO. In this week’s Breaking, Analysis, Dave Vellante shares his insights, investment strategies, and dives into some of the questions that have come out of the buzz from the hottest IPO in software history. Watch the full video analysis. Episode 51 – Market Recoil Puts Tech Investors at a Fork in the Road – Recently, the stock market experienced its most significant drop since early June. Tech companies were among several corporations involved in this decline, sending investors into a panic. In this Breaking Analysis, Dave Vellante answers questions and provides some perspective about what’s happening within the technology space and how it will continue to affect the rest of 2020. Watch the full video analysis. Episode 50 – Enterprise Software Download in the Summer of COVID – Enterprise applications are an enormous market, and organizations across the globe essentially rely on these applications to operate. In this Breaking Analysis, Dave Vellante unpacks new data surrounding the Enterprise software space, focusing on the core enterprise apps that companies rely on to keep their businesses running. Watch the full video analysis. Episode 49 – Tectonic Shifts Power Cloud, IAM and Endpoint Security – Although time has seemed to stop since the beginning of quarantine, COVID-19 has caused acceleration within the technology industry, causing some trends to speed up by about two years. The cybersecurity sector is one of the best examples of this change. In this Breaking Analysis, Dave Vellante discusses this sector, and provides updates on why key areas of the market that are exploding. Watch the full video analysis. Episode 48 – Cloud Remains Strong but not Immune to COVID – Although Cloud is among the most successful industries in tech spending, even it is not protected against COVID-19. Recent data shows a newly shaped recovery pattern suggesting this negative impact. In this Breaking Analysis, Dave Vellante dives into this data surrounding the cloud market and provides prospective updates about the big three. Watch the full video analysis. Episode 47 – RPA Competitors Eye Deeper Business Integration Agenda – Although the projected spending outlook for 2020 looks moderate, Robotic process automation solutions are still seeing the highest investment momentum for IT buyers. In this Breaking Analysis, Dave Vellante summarizes the latest RPA spending trends using data provided by ETR. Watch the full video analysis. Episode 46 – Five Questions Investors are Asking about Snowflake’s IPO – Snowflake recently filed a confidential document suggesting an IPO is imminent. Many within the community are responding positively to this news, causing a lot of discussion and inquiry. In this Breaking Analysis, Dave Vellante and Erik Bradley unpack five critical questions surrounding this pending IPO. Watch the full video analysis. Episode 45 – Google Cloud Rides the Wave but Remains a Distant Third Place – Despite its faster growth and infrastructure as a service, Google Cloud platform remains a third wheel behind AWS and Azure in the race for cloud dominance. In this Breaking Analysis, Dave Vellante reviews the current state of cloud and drills into the spending data to provide new insights about Google’s position in the market. Watch the full video analysis. Episode 44 – Living Digital: New Rules for Technology Events – Although there is push for a more digital world, in person interactions seem to be equally as important. Every year, large corporations throw massive events for this exact reason. However, coronavirus canceled most of these events for 2020, forcing a virtual replacement. In this Breaking Analysis, Dave Vellante covers the virtual event landscape and shares some takeaways from this new dynamic. Watch the full video analysis. Episode 43 – Assessing Dell’s Strategic Options with VMware – Dell is exploring options for its roughly 81 percent share in VMware. It is predicted that Dell wants to gauge investor, consumer, and partner sentiment. In this Breaking Analysis, Dave Vellante unpacks the complex angles as well as some possible scenarios of this situation. Watch the full video analysis. Episode 42 – Cyber Security Tailwinds in the Post Isolation Economy – The isolation economy has created substantial momentum for some cybersecurity companies. However, several others have tracked or not performed as well as more successful companies, despite still exhibiting strength and momentum. In this Breaking Analysis, Dave Vellante gives updates and answers questions about cybersecurity. Watch the full video analysis. Episode 41 – Competition Heats up for Cloud Analytic Databases – A new class of workloads are emerging in the cloud which are mainly focused on combining data using machine intelligence. At the center of this trend is a new class of data stores and analytic databases. In this Breaking Analysis, Dave Vellante updates his view on the subject while looking into the basics of the market, the competition, as well as spending data. Watch the full video analysis. Episode 40 – Most CIOs Expect a U Shaped COVID Recovery – It has been reported COVID-19 created a bifurcated IT spending picture, but what will the aftermath of the virus look like? In this Breaking Analysis, Dave Vellante is joined by Sagar Kadakia to look into the recovery patterns of different industries following the effects of the pandemic, and discuss the supporting data. Watch the full video analysis. Episode 39 – RPA Gains Momentum in the Post COVID Era – Legacy on-premises infrastructure are now allowing for more flexible approaches to business agility that reduce human labor, and the pandemic has accelerated this focus on such efforts. Robotic Process Automation has been a large beneficiary in this process. In this Breaking Analysis, Dave Vellante gives the rundown of RPA, including updates on the RPA sector, spending data, and the impact of COVID-19 on the market. Watch the full video analysis. Episode 38 – Cloud Momentum Building for the Post COVID Era – Cloud is in the stronghold of on-premise computing, and coronavirus has helped to strengthen this position. Analysis of company earnings reports and customer survey data shows that Microsoft Azure and GCP are closing the gap on AWS’ cloud dominance. In this Breaking Analysis, Dave Vellante takes a closer look at the big three cloud players, and provides a brief investigation of AWS individually. Watch the full video analysis. Episode 37 – IBM’s Future Rests on its Innovation Agenda – For decades, IBM has unfortunately missed opportunities to invest in the waves that now power the tech economy. The hiring of a new CEO provides the chance to redirect the company, and come back on top. In this Breaking Analysis, Dave Vellante digs into the past and the future of IBM. Watch the full video analysis. Episode 36 – COVID-19 Takeaways & Sector Drilldowns Part II – Industries such as retail, consumer, telco and IT services are seeing the largest pullbacks in spend from consumers and business since the beginning of COVID-19. On the other hand, corporations capable of digital transformation are seeing the most success. In this Breaking Analysis, Dave Vellante and Sagar Kadakia share the most updated spending data and information about the effects of the pandemic. Watch the full video analysis. Episode 35 – CIOs & CISOs Discuss COVID 19 Budget Impact – CEOs and CISOs of industries that have been hard-hit see significant and many permanent shifts to their IT and security strategies. Because of severe budget impacts, certain initiatives have been prioritized. In this Breaking Analysis, Dave Vellante is joined by Erik Bradley, managing director of ETR’S VENN program, to provide research and discuss the areas emphasized by these executives. Watch the full video analysis. Episode 34 – How Tech Execs are Responding to COVID 19 – COVID-19 demanded several and somewhat immediate changes within the tech industry. In this Breaking Analysis, Dave Vellante shares commentary and responses from various tech execs, recaps the current IT spending outlook, and dives into what’s really going on in the marketplace. Watch the full video analysis. Episode 33 – CIOs Plan on 4% Budget Declines for 2020 – At the start of 2020, the IT spend forecast was plus 4 percent. Following Coronavirus, those numbers declined significantly. In this breaking Analysis, Dave Vellante and Sagar Kadada breakdown the latest spending data from ETR. Watch the full video analysis. Episode 32 – VMware Announces vSphere 7 – VMware released the vSphere 7, which is being called the biggest change to vSphere within the last decade, enabling 90 percent of the data centers around the world that have VMware. In this Breaking Analysis, Dave Vellante is joined by Stu Miniman to discuss the vSphere 7 announcement. Watch the full video analysis. Episode 31 – Coronavirus – Pivoting From Physical to Digital Events – Coronavirus and the recent quarantine has put the world more or less on pause. For many industries, this means switching to a largely digital-based platform for events. In light of the pandemic, Dave Vellante shares advice on the best tools and practices to navigate the current crisis. Watch the full video analysis. Episode 30 – Multi-Cloud…A Symptom Or Cure? – The third wave of cloud is stirring up a lot of discussion about its necessity and effectiveness. In this breaking Analysis, Dave Vellante digs into the multicloud arena while answering some frequently asked questions about the benefits and possible implications of this new technology. Watch the full video analysis. Episode 29 – Cyber Security Update: What to Expect at RSA 2020 – Robert Gates, Former director of the CIA and Secretary of Defense, warns that the risks of Cyber security and IT should be a regular part of every board’s agenda. In this Breaking Analysis, Dave Vellante provides updates about the cyber security sector ahead of the RSA conference. Watch the full video analysis. Episode 28 – RPA: Over-Hyped or the Next Big Thing? – Robotic Process Automation, or RPA, is one of the hottest sectors in software today with a small but rapidly growing market. In this Breaking Analysis, Dave Vellante dives deeper into the world of RPA and talks about the value, size, and competitors in the market space. Watch the full video analysis. Episode 27 – Gearing up for Cloud 2020 – The new era of cloud brings significant change to the industry as well as new opportunities for the three major cloud players in the U.S. In this Breaking Analysis, Dave Vellante looks deeper into the cloud market and the momentum of Amazon, Google and Microsoft. Watch the full video analysis. Episode 26 – Storage…Continued Softness with Some Bright Spots – The storage industry is a bifurcated market: Secondary storage is gaining momentum while the primary slide falls behind. In this Breaking Analysis, Dave Vellante looks into the spending data and discusses his thoughts and predictions about storage live from Barcelona. Watch the full video analysis. Episode 25 – Cisco: Navigating Cloud, Software & Workforce Change – At the end of the dot com bubble, Cisco was the most valuable company in the world. It remains a leader in key segments, but is refocusing its business for the next decade. In this Breaking Analysis, Dave Vellante covers Cisco’s rise as well as projections for the future. Watch the full video analysis. Episode 24 – The Trillionaires Club: Powering the Tech Economy – Big tech companies have changed the recipe for innovation in the Enterprise, and as we enter the next decade, it is important to reevaluate how that will determine the level of success in the industry. In this Breaking Analysis, Dave Vellante discusses this “cocktail” of innovation and how it came into play. Watch the full video analysis. Episode 23 – Veeam’s $5B Exit: Clarity & Questions Around “Act II” – Veeam is a data protection company that has seen a slight performance drop since 2018. In this Breaking Analysis, Dave Vellante provides details about its $5 billion deal with Insight Partners and how this new chapter will affect the industry moving forward. Watch the full video analysis. Episode 22 – Predictions 2020: Cloud, Kubernetes & Cyber Continue to Power the Tech Economy – Tech projects have historically been very risky investments, but changes with cloud are allowing for more flexibility in the coming year. In this Breaking Analysis, Dave Vellante talks about Predictions for 2020 using spending data and insight from the thousands of interviews conducted on theCUBE. Watch the full video analysis. Episode 21 – Re:Invent 2019…of Transformation & NextGen Cloud – During the most recent AWS re:invent, the company proves it continues to strive on raising the bar. In this Breaking Analysis, Dave Vellante is joined by Stu Miniman to unpack the event, and talk about what’s happening from a buyer’s perspective, as well as AWS’ hybrid strategy Watch the full video analysis. Episode 20 – Unpacking Cisco’s Prospects Q4 2019 and Beyond – AWS strongly emphasizes the idea of transformation, and warns industries like Cisco not to do so incrementally. In this Breaking Analysis, Dave Vellante covers six different topics related to the future of Cisco, and its prospects in this era of next generation cloud Watch the full video analysis. Episode 19 – Examining IT Spending Data Q4 ‘19 – Enterprise Research Technology is a company who uses primary market research and first party data to look into spending patterns within the tech industry. In this Breaking Analysis, Dave Vellante explains the ins and outs of ETR, as well as its current relationship with theCUBE. Watch the full video analysis. Episode 18 – Re:Invent 2019: AWS Gears up for Cloud 2.0 – AWS Reinvent has become the Super Bowl for enterprise tech innovation. In this Breaking Analysis, Dave Vellante discusses the impact of the revolution of cloud on the industry in light of the upcoming event. Watch the full video analysis. Episode 17 – The Transformation of Dell Technologies – Dell continues to make changes to remain a predominant company in the tech industry. In this Breaking Analysis, Dave Vellante breaks down the major takeaways of the Dell Technologies’ industry analyst event and discusses some of the possible implications that the company may face. Watch the full video analysis. Episode 16 – The State of Cybersecurity Q4 2019 – The cyber security market is fragmented, challenging and many feel broken. Cloud security promises to simplify the maze for security practitioners but there are nuances with the shared responsibility model that often cause confusion. In this Breaking Analysis we look at the lates ETR data and hear from CISOs and executives in the field with an outlook and prognosis going forward. Watch the full video analysis. Episode 15 – The state of data protection, Q4 2019 – While demand for primary storage remains soft, the bright spot in the sector is data protection. Well funded new entrants are disrupting the space which is shaping up as a battleground for 2020. Watch the full video analysis. Episode 14 – AWS growth slows but remains the profit engine of Amazon – While AWS’ growth rate slowed this past quarter, its revenue is still substantially larger than its nearest competitors in the IaaS space. Moreover AWS is still the profit engine that funds Amazon’s vast and growing empire. Watch the full video analysis. Episode 13 – Q4 Spending Outlook – 10/18/19 – TheCUBE host Dave Vellante shares his analysis on recent spending trends backed by ETR Data. Spending is reverting to pre-2019 levels but the outlook still points to a strong 2020, barring any unforeseen global surprises. Watch the full video analysis. Episode 12 – Bill McDermott steps down from SAP – Commentary and Outlook. SAP pre-announced earnings with a beat and a raise, which acted as a heat shield for the surprise news that long-time CEO Bill McDermott is not renewing his contract. SAP is moving back to a dual-CEO model with a separate customer-facing and product/ops focus for each exec. SAP is strong financially but we believe faces significant technical integration challenges over the next decade, which may have played into McDermott’s and SAP’s decisions. In this Breaking Analysis, Dave Vellante shares recent spending data from ETR and lays out some of the challenges SAP faces going forward. Watch the full video analysis. Episode 11 – Spending in Q4 2019 is reverting back to pre-2018 levels – The spending outlook for the balance of 2019, into 2020 is softening, but not falling off a cliff. In this Breaking Analysis, Dave Vellante presents the latest ETR spending data and shares the latest thinking on which segments will continue to do well for the balance of 2019 into next year. Watch the full video analysis. Episode 10 – Takeaways from Dell’s 2019 financial analysts event. – Dell Technologies executives gathered in New York to update financial analysts and present the company’s mid-to-long term plans for growth, share gains, profitability and paying down its substantial debt. In this Breaking Analysis, Dave Vellante unpacks Dell’s massive business and provides clarity on the profitability levers Dell is turning to continue its transformation. Watch the full video analysis. Episode 9 – Spotlight on IBM’s Systems Business – IBM’s mainframe business continues to be the linchpin of much of the company’s profit and free cash flow. In this Breaking Analysis, Dave Vellante explains the importance of product cycles to the success of not only IBM’s Systems and Storage division, but IBM’s financial performance overall. Watch the full video analysis. Episode 8 – Nutanix and VMware battle for HCI leadership – Hyperconverged infrastructure was popularized by leader Nutanix. Many others have joined the party including VMware, Dell, HPE and others. In this Breaking Analysis, Dave Vellante is joined by Stuart Miniman, an expert in the HCI market, to unpack what’s really happening in the marketplace. Watch the full video analysis. Episode 7 – Oracle earnings analysis – September 2019 – Oracle, like many legacy enterprise software companies, is seeing a slowdown in growth for on-prem licenses. Oracle’s cloud is being re-factored in a next generation offering that comprises IaaS, PaaS and SaaS. Oracle’s applications business remains strong and is a driver of profits. In this Breaking Analysis, Dave Vellante digs into Oracle’s business and lays out his expectation for the coming quarters. Watch the full video analysis. Episode 6 – Spending data from ETR shows that robotic process automation is gaining steam in mid-to-large enterprises – A race to improve productivity is driving companies to implement automation in the form of software robots. UiPath and Automation Anywhere show strong customer spending momentum. Blue Prism and other major players, while not showing the same growth, appear to be well-positioned. In this Breaking Analysis, Dave Vellante explains how this market is beginning to re-shape automation for the future. Watch the full video analysis. Episode 5 – Spending data shows that cloud native databases are disrupting traditional analytic data stores. Snowflake and AWS RedShift stand out as having spending momentum based on ETR survey data – Some cloud native databases have been architected to enable storage and compute resources to be scaled independently. This not only improves economics but also drives increased agility and flexibility for many use cases. In this Breaking Analysis, Dave Vellante explains how this dynamic is eating into traditional enterprise data warehouse markets. Watch the full video analysis. Episode 4 – Storage Spending Outlook 2H ’19 – Pure Leads the Pack – The on-prem storage business has been hurt by: 1) the cloud siphoning away demand; and 2) a massive injection of flash storage that has given data center managers enough performance headroom to minimize the need to buy for performance reasons. Pure Storage is growing faster than the marketshare leaders but from a much smaller installed base. Watch the full video analysis. Episode 3 – VMworld 2019 – Containers won’t Kill VMware – As a preview to VMworld 2019, Dave Vellante shares his opinions along with ETR spending data that shows containers, to date, are not hurting VMware’s business. Watch the full video analysis. Episode 2 – IBM Completes Acquisition of Red Hat – Dave Vellante shares his opinions along with ETR spending data on this giant move by IBM. Positioned by IBM as all about cloud, Vellante says it’s also a professional services play. Watch the video analysis. Episode 1 – Hello World – This is a podcast only version explaining what this series is all about and its objectives for the community. THANK YOU OpenAI's newest reasoning model o3-pro surpasses rivals on multiple benchmarks, but it's not very fast Uber will bring robotaxis to London in 2026 Time intelligence startup Laurel gets $100M to turbocharge worker productivity AWS DC Summit 2025: The cloud becomes strategic national infrastructure for the AI era GitLab's revenue outlook comes up short and its stock gets hammered Meta reportedly forming superintelligence lab amid Llama 4 Behemoth delays OpenAI's newest reasoning model o3-pro surpasses rivals on multiple benchmarks, but it's not very fast AI - BY MIKE WHEATLEY . 4 HOURS AGO Uber will bring robotaxis to London in 2026 EMERGING TECH - BY JAMES FARRELL . 4 HOURS AGO Time intelligence startup Laurel gets $100M to turbocharge worker productivity AI - BY MIKE WHEATLEY . 5 HOURS AGO AWS DC Summit 2025: The cloud becomes strategic national infrastructure for the AI era CLOUD - BY JOHN FURRIER . 7 HOURS AGO GitLab's revenue outlook comes up short and its stock gets hammered CLOUD - BY MIKE WHEATLEY . 7 HOURS AGO Meta reportedly forming superintelligence lab amid Llama 4 Behemoth delays AI - BY MARIA DEUTSCHER . 7 HOURS AGO
--------------------------------------------------

Title: Why more and more companies are buying bitcoin
URL: https://www.businessinsider.com/bitcoin-strategy-mstr-btc-price-metaplanet-demand-treasuries-2025-6
Time Published: 2025-06-06T09:30:01Z
Full Content:
The bitcoin boom is drawing the attention of more companies, who are following Strategy's lead and piling up the token in their corporate treasuries. A growing list of firms have turned themselves into bitcoin holding companies recently, incorporating the top crypto in their balance sheets along with more traditional assets like cash and bonds. To date, 80 companies have embraced the "bitcoin standard," and they own about 3.4% of the total bitcoin supply, according to a report from Bernstein Research. Many are trying to replicate the success of Strategy, which pioneered the bitcoin treasury strategy and has amassed a trove of around 554,000 bitcoins. The company's stock has outperformed the Magnificent Seven, S&P 500, and its underlying bitcoin holdings in the last 12 months. Following suit, last month, GameStop added $500 million of bitcoin to its balance sheet in its first-ever crypto purchase, and Trump Media & Technology Group announced plans to raise $2.5 billion for a bitcoin treasury. In April, the SPAC Cantor Equity Partners merged crypto firm Twenty One Capital with the goal of becoming a pure-play bitcoin holding company and saw its shares spike nearly 500% in the first week of trading. Bernstein predicts that company demand could drive $330 billion of inflows into bitcoin by 2029, with up to $124 billion of that from Strategy alone. That influx of corporate money would be bullish for bitcoin's price. However, buying bitcoin isn't a one-size-fits-all maneuver, and Strategy's success may be hard to replicate, Bernstein said. Bernstein estimates that around 2,000 global companies with market caps under $100 billion could be prime candidates for bitcoin adoption. These firms share characteristics like low growth (defined as sub-5% yearly revenue growth rate), low leverage, and high cash piles of $100 million or more. Bitcoin could be a lifeline for these types of companies. Firms with poor growth prospects might decide that their cash is better spent on investing in bitcoin than letting it sit on their balance sheet earning minimal returns. A prime example is Japanese hotel-management company turned bitcoin treasury, MetaPlanet. After years of weak profitability and stock-price stagnation, MetaPlanet began purchasing bitcoin in 2024 via cash raised from bond and equity sales. The move paid off, and the stock is up over 500% in the last year. Some larger companies, such as Tesla, have also purchased bitcoin in the past. However, other mega-cap companies have rejected proposals to buy bitcoin. Meta is the latest example, with over 99% of shareholders voting against a bitcoin treasury plan earlier this week. Proposals have also failed at Amazon and Microsoft. Not all businesses will find the same success as Strategy. The business software company has been piling up bitcoin for five years, enticing investors with equity, convertible debt, and preferred stock offerings to fund even more purchases. The stock provides investors with price appreciation, convertible debt offers more capped upside, and preferred stock provides dividend payouts. The company also has the benefit of experience after weathering multiple bitcoin price crashes. MetaPlanet and other treasury companies have issued convertible debt and equity, but Bernstein points out these companies don't have the same scale and ability to raise funds as Strategy. However, as bitcoin popularity skyrockets, it's clear that lack of experience won't stop these companies from taking a page from the Strategy playbook. Check out Business Insider's picks for best cryptocurrency exchanges Check out Business Insider's picks for best cryptocurrency exchanges Where Big Tech secrets go public — unfiltered in your inbox weekly. Sign up chevron down icon An icon in the shape of an angle pointing down. Jump to
--------------------------------------------------

Title: Meet the 'reclusive' tech billionaire making an audacious bid to buy TikTok
URL: https://www.businessinsider.com/applovin-ceo-adam-foroughi-bid-tiktok-2025-6
Time Published: 2025-06-06T08:50:01Z
Full Content:
Adam Foroughi tends to eschew the typical trappings of a billionaire. He has one car, and would rather it were self-driving. He rarely appears on TV or conference stages. In his downtime, you're more likely to find him at home with his five children than schmoozing on the slopes of Davos. People who know him point to his mild manner and lack of ego. His advertising technology company, AppLovin, is similarly unflashy. With an ad network that reaches around a billion daily users and a market cap more than twice that of Snap, Pinterest, and Reddit combined, it was the technology behemoth you'd never heard of. "We're a $100 billion-plus company, not many people know of us, so that's probably a flaw on me," Foroughi told Business Insider in an interview. Then came April. Ahead of a June 19 deadline, AppLovin and Foroughi, 45, made a last-minute bid to acquire the international assets of TikTok as the Chinese-owned company faces a potential ban in the US. It's an audacious move for any company, let alone one with a founder who usually goes out of his way to avoid the spotlight. It faces an uphill battle. Competition is fierce. President Donald Trump has said he's been negotiating with multiple potential buyers. Investors like Kevin O'Leary of "Shark Tank" fame and former Dodgers owner Frank McCourt have signaled interest, and other tech companies are in the mix. At AppLovin, Foroughi is known for running a ruthlessly efficient ship that drives hard for profit. It's not unusual for the company to reduce head count even when it's doing well. Foroughi executes with a hands-on management style that has, at times, seen him struggle to delegate to execs who don't fit the mold. And the company's recent financial success has also caught the attention of short sellers who have raised questions about its data practices. With TikTok, Foroughi would be taking on an organization with a much bigger spotlight — and the heat that comes from running a user-generated content business popular among teens and often lambasted by parents. Most of the former AppLovin employees, competitors, and business associates who spoke with BI believe he's up to the task. They say Foroughi's smarts, as well as his tendency to forgo the marketing jazz hands and let the product do the talking, position him well to crank up the dial for TikTok's ad business. "We've been competing for over a decade, and I've never seen anyone like him — he's all around amazing, it hurts me to say it," said an executive at one of AppLovin's competitors. "He's the most talented CEO I have ever seen." Foroughi acknowledged that the TikTok bid is uncharted territory for him, but "I don't really care about 'uncomfortable,'" he said. "I do what I think is right for my business." Foroughi and his family fled Iran when he was four years old, in the fallout of the Iranian Revolution. They settled in Laguna Beach, California. His father, once one of Iran's leading real estate developers, left nearly all his wealth behind, and the family had to adjust to a more frugal lifestyle, a new culture, and a different language. "My parents had to give up a lot to get us over here," Foroughi said. "Knowing that, you always have this motivation inside you to perform." After graduating with a degree in finance from the University of California, Berkeley, Foroughi took a job as a derivatives trader. He found it a lonely existence. He wanted to build something of his own, and he wanted to work with people. He worked at a marketing agency for small businesses, which later morphed into a social media marketing company. Eventually, as the app stores of Apple and Google became dominant, he looked to apps. With a small team of engineers in Palo Alto, Foroughi created a fashion app, then a dating app. "They stunk," Foroughi said. "We got rid of them." In 2012, they launched their third attempt — an app that allowed friends to connect and send recommendations for other apps to download. This one stuck. If you were playing "Words with Friends," the app could send a push notification to your contacts, asking them to join. AppLovin was born. Like many tech companies, AppLovin — which Foroughi insists wasn't inspired by McLovin, the nerdy character in the stoner film "Superbad" — decided to pivot to advertising. It expanded from a ground-floor garage to offices on three continents, and more than 1,500 employees as of December 2024, and a market capitalization of $140 billion at the time of publication. The business model is fairly simple: It helps app developers make money and find users using in-app ads. But under the surface is a highly optimized AI-powered algorithm designed to entice businesses with the promise that chucking $1 into the machine will net $2, $5, or $10 in profit. It's a snug fit for TikTok, where ads for figure-hugging jeans, grip socks for soccer, and campaigns for major brands like Coca-Cola and Apple are slotted between consumable vertical videos. While TikTok has soared in popularity, particularly among Gen Z, its ad revenue lags behind Google, Meta, and Amazon. AppLovin thinks its adtech can help close the gap. AppLovin has also widened its aperture beyond its core gaming roots in recent months. After it opened up its ad platform to e-commerce advertisers, some said that they were excited for an alternative to Meta, which had become increasingly expensive in their hunt for new customers. Mike True, CEO of the e-commerce marketing platform Prescient AI, said AppLovin is the fourth most invested-in channel among its advertiser clients, behind Meta, Google, and Amazon Ads. "The fact that advertisers continue to invest in AppLovin, even amid a cautious market, suggests growing confidence in its long-term role within the performance stack," True said. Last year, the company posted net income of $1.58 billion at 34% margins — a margin profile almost on a par with Meta, and ahead of its closest adtech rival, The Trade Desk, which had a profit margin of 16% in 2024. AppLovin's annual revenue rose 43% to $4.7 billion. But some observers said that while AppLovin helped e-commerce advertisers extract more sales from current customers, it was less effective in driving sales from new ones. Jones Road Beauty was one of AppLovin's early e-commerce clients, but its CEO, Cody Plofker, told BI it's no longer using AppLovin. "We found it not to be very incremental with new customers," Plofker said. Foroughi said that the e-commerce product is still in its infancy and doesn't yet work for everyone. "But it will as we build it out," he added. One Silicon Valley tech veteran who interacted with AppLovin in the early years said taking meetings with Foroughi was a "breath of fresh air." He cut to the chase, no two-martini lunches necessary. "We got on calls and he'd be very to the point, versus the mindset where relationships precede business — a very Valley kind of guy," the person said. Simon Spaull was AppLovin's first hire in Europe in 2014. He reluctantly entertained the idea at first. "No one had heard of it and it was a rubbish name," Spaull said. He was soon convinced. Spaull stayed at the company for almost seven years, as it continuously posted record annual revenue, mostly growing traction through word of mouth in the gaming community. Foroughi has remained deeply enmeshed with day-to-day operations, including customer service. Up until around a year ago, he ran product and human resources alongside his CEO role. (He said he wanted to "get more involved in making sure our culture is aligned with the principles we had when we started the business.") When he's not traveling, he sits among the engineers. "You don't know what's going on in your business if you don't work with your employees," Foroughi said. Foroughi has said some of the biggest mistakes he made as CEO involved taking outside advice, including briefly hiring a chief operating officer in 2012. "I thought, what's the point of me at this company, I'm hands-on, I'm not going to defer to this hire," Foroughi said at a recent conference held by the investment bank Jefferies. He had a similar reaction after following advice to bring on a chief revenue officer and a brand ad sales team, with staffers who were paid more than the company's best engineer, the person making the actual product. "It bugged the crap out of me," Foroughi said. He scrapped the entire team. The company's strategy has been defined in part by unrelenting efficiency. Foroughi, who said he considers Elon Musk as an inspiration, counts EBITDA — or profit — per employee as one of his most important success metrics. The company recently sold off its entire mobile gaming studio business — developers of hit games like "Mobile Strike" and "Project Makeover" — deeming it surplus after the apps had been sucked for data to use for its advertising algorithms. Foroughi also showed a cut-throat streak in 2022, when the gaming software development company Unity announced its intention to acquire AppLovin's app advertising rival, IronSource. Seeking to derail the merger, AppLovin put in an unsolicited $20 billion bid to merge with Unity, but only on the provision that Unity drop the IronSource deal. Rather than make his offer to Unity's management team and the board, Foroughi went public to appeal directly to Unity's main shareholders: the investment firms Sequoia and Silver Lake. Unity's management team wasn't happy, and they rejected the hostile takeover bid. Foroughi has no regrets. "The only way to disrupt that deal" was for AppLovin to make its takeover offer public, Foroughi said. "Yes, it was a little uncomfortable, obviously." Those within Foroughi's orbit say the billionaire has a generous side. A banker who worked on AppLovin's 2021 initial public offering recalled receiving an updated draft of the registration statement and noticing that Foroughi had recently sold off around $10 million of stock, at a low price, pre-IPO. He asked Foroughi why. "He's like, 'Uh, I'm surprised you found that. Yes, I sold some stock back to the company to distribute it to the team that was under-equitized," the person said. The TikTok suitor is no stranger to US-China tensions. In 2016, Foroughi signed a deal with a Chinese private-equity firm that valued AppLovin at $1.4 billion, and would provide a $1 billion cash injection. The deal was blocked by the Committee on Foreign Investment in the United States on national security grounds. "CFIUS saved my ass," Foroughi said at the Jefferies Private Growth Conference earlier this year, referring to AppLovin's financial performance since. The company's methods have been called into question in recent months with four short-seller reports, published in quick succession. The most high-profile, from Carson Block's Muddy Waters Research, said AppLovin was "impermissibly extracting" data from top apps like Meta, Google, and TikTok, and targeting ads at "high value users" without their consent. The report also said AppLovin was using underhanded techniques to claim credit for sales it didn't generate. In an email to BI, Block said that Muddy Waters believed Foroughi "lied" in a March blog post, when he pushed back on the idea that the company "uses persistent user identifiers without their consent." Persistent identifiers follow users across different websites and devices, and it can be difficult for users to delete them or even know they exist. Muddy Waters said AppLovin's use of these IDs violates various platforms' terms of service and privacy laws in some jurisdictions. Block also said that Foroughi's background pre-AppLovin "supports our opinion that he should not be trusted." He was referring to Foroughi's tenure at a company called Claria, which owned a controversial eWallet software called Gator, that was said at the time to have distributed "adware" that collected users' browsing habits, and bombarded them with pop-up ads. One of Foroughi's early ad networks, SocialHour, was removed from Facebook in 2009 for violating its platform policies. Foroughi has previously said in blog posts that the short-seller reports were "littered with inaccuracies and false assertions" and were aimed at driving down AppLovin's share price for their own financial gain. He told BI that he worked at Claria for a few months as a 25-year-old. In response to questions about SocialHour, he said all companies that monetized Facebook's inventory were removed when Facebook brought monetization in-house. Some industry insiders saw the reports as confirmation of their bafflement at AppLovin's success, particularly in light of its reliance on mobile games, which are not always highly valued by big brands and agencies. AppLovin has proposed merging its company with all of TikTok's international business — not just TikTok US. Foroughi describes this as an "enhancement" to Oracle, which is TikTok's cloud provider in the US. Under AppLovin's proposal, Oracle would still provide data storage and security. Oracle didn't respond to requests for comment. AppLovin has also pitched itself as a salve for TikTok's woes. "There are really big national security and data issues, and I think we could solve them," Foroughi told BI. AppLovin says it has expertise in both handling user data and controlling complex algorithms, which it believes could help it remove biases from TikTok's content recommendation system. "I see what folks in the administration are doing now, what someone like Elon has sacrificed to give back to the country, and I think we could play a small part here," Foroughi said in an interview that took place before Musk and Trump's spectacular falling out this week. TikTok didn't respond to a request for comment. Ari Paparo, a former Googler and adtech exec who now runs the marketing media company Marketecture, said AppLovin has some big advantages in its TikTok bid: Its monster market capitalization makes the financial side feasible, it has proven monetization capabilities that could make ads on TikTok better, and it isn't "Big Tech," which has drawn antitrust scrutiny. On the other hand, he said, "The company is a bit of an unknown in DC." That may soon change. In April, Foroughi was spotted at the launch party of Donald Trump Jr.'s private members club. "​​I'm reclusive by design, so part of the challenge has been that I have to get out there, and get to be known, and I just wasn't before this," Foroughi said. Where Big Tech secrets go public — unfiltered in your inbox weekly. Sign up chevron down icon An icon in the shape of an angle pointing down. Jump to
--------------------------------------------------

Title: The Kasselites who turned SYRIZA into… Ramallah, the 39 permits for a single foreign worker, the ferry routes and competition, and why everyone’s going crazy over Vouliagmeni Beach
URL: https://en.protothema.gr/2025/06/06/the-kasselites-who-turned-syriza-into-ramallah-the-39-permits-for-a-single-foreign-worker-the-ferry-routes-and-competition-and-why-everyones-going-crazy-over-vouliagmeni-beach/
Time Published: 2025-06-06T08:12:46Z
Full Content:
– Hello there, you’ve probably noticed we haven’t been talking about poor Stefanos for a while now, because really, what’s left to say or write? It’s not even fun anymore — even Liagas ditched him. But now… I can’t help but give him props for the stunts he pulled on SYRIZA, which managed to change its stance on the Preliminary Investigation Committee in just three hours (he sided with Karystianou). Honestly, the way he left them high and dry over at Koumoundourou was just beautiful — like going shopping in the morning, picking out a pair of shoes, then later looking at them in your mirror, not liking them, and returning them to get something else. Pure magic, right? They were left frozen… in Palestine– Meanwhile, as all this important stuff was going on, the SYRIZA delegation was in Ramallah, led by President Famellos. And when they found out (via news sites) that the Kasselites had left them out in the cold by withdrawing their signatures from Koumoundourou’s proposal, they were absolutely stunned. The shock was twofold — not only was SYRIZA’s proposal for the Preliminary Investigation Committee effectively dead, but the internal party storm this fiasco is about to unleash is massive. Not to mention that in Ramallah they barely had any internet connection to coordinate with SYRIZA’s parliamentary secretary Kalamatianos, who was attending the Conference of Parliamentary Presidents. And now, Gaza… at home– In Athens, however, as I told you, the Kasselites’ spectacular walkout sent intra-party tensions skyrocketing. SYRIZA members were already grumbling that “it’s going to be Palestine” at the upcoming Party Congress from June 12–15, since they had previously warned the leadership about the risks of co-signing anything with those… oddballs. And now the fallout is hitting not just Famellos, but also the Polakis–Pappas duo. High treason– Anyway, one of my sources told me that when Haritsis was asked whether they’d co-sign Mitsotakis’ so-called high treason, he replied with something like, “Guys, this stuff wouldn’t hold water even with first-year students — we’ll be a total laughingstock…” Cantina Conference (Chatzidakis)– Chatzidakis had plenty to say yesterday at the Cantina Academy conference — about Greek food, agricultural production, and the water shortages that, sadly, are headed our way. But what really stuck with me was one truly disheartening stat: in order for a foreign worker to get a work permit in Greece, the state requires 39 different permits and certifications before they can start. Now you understand why the workers from countries with whom Greece signs agreements… never actually arrive. Pierrakakis (Cantina)– Equally noteworthy was Pierrakakis’ intention to request a U.S. tariff exemption for Greek agricultural products (and yes, apparently there’s a shot the Americans might go for it). Tsiaras (OPEKEPE)– Beyond Pierrakakis’ comment at yesterday’s Cantina conference — that starting from January 1, 2026, the Independent Authority for Public Revenue (AADE) will take over OPEKEPE’s role — Tsiaras clarified that agricultural and livestock subsidy payments will continue as normal despite the system overhaul. Hardalias– Special mention of Attica’s agricultural production was also made by regional governor Hardalias, who opened (our own) Cantina conference. The Metlen factory and K.M.– One of the stops on K.M.’s (Kyriakos Mitsotakis’) tour in Volos yesterday was the old METKA factory in Nea Ionia. Now under the Metlen Group, the facility is expanding — with 50% of the work already done — as it becomes a key plant for Leopard tank production, among other things. Mitsotakis has also committed that 25% of every defense contract will go to Greek companies. Metlen’s strategic investment, budgeted at €45 million, includes new buildings and specialized machinery aimed at manufacturing advanced, high-value metal components. With K.M. was Development Minister Takis Theodorikakos, who, in coordination with Mytilineos, helped get the investment included in the strategic investment framework last December. Why everyone’s fighting over Vouliagmeni Beach– It’s not just the most popular organized beach in Attica — it’s also the most profitable. That explains the frenzy of big names (Prokopiou, Restis, Melissanidis, Kokkalis, etc.) crowding the ETAD tender for its lease. We’re talking about Vouliagmeni Beach, which can host up to 8,000 visitors daily. Last year alone, the beach received 420,000 visitors — meaning the operator could easily rake in around €5 million just from entrance tickets. Add in revenues from bars, the “Okeanida” restaurant, sports courts, and all the extra fees the private operator is sure to impose (sunbeds, ticket hikes, parking, etc.), plus new services (like food service), and this beach is a gold mine. The fact that it’s one of the only organized public-access beaches left doesn’t seem to interest ETAD (a subsidiary of the Hellenic Corporation of Assets and Participations — basically collateral from bankruptcy). I doubt ETAD even knows how many public properties it owns, but its CEO, H. Chatzigeorgiou, smartly starts with the easiest-to-monetize assets. They could have at least included some conditions to guarantee average citizens still have beach access under the new setup. But no — the only priority is to celebrate a lease deal worth €50–60 million for 20 (+10) years, a drop in the bucket compared to the €700 million debt ETAD created in the Mantonanakis affair. So the only hope left is the appeal filed with the Council of State by the Vouliagmeni Winter Swimmers Club, which will be heard in September. CEO Change (after two years) at the… immovable Fourlis -Dimitris Valachis ultimately stayed two years in the CEO seat at Fourlis, officially announced around this time in 2023 as the successor to Apostolos Petalas. The latter had steered the group for 17 years, and since Fourlis has shown it values stability, Valachis’ replacement sparked market chatter. Especially when it emerged that the new CEO would be Giannis Vassilakos, for years head of Kotsovolos and a key architect of the successful course of the chain that now belongs to PPC. Some sources now say a leadership change had been in the pipeline—though originally expected in the fall—as Valachis had completed a broad transformation plan for the Group and shareholders hadn’t provided clear guidance on the next steps. The same sources say the transition was accelerated by Vassilakos’ departure from Kotsovolos two months ago. At Fourlis, they held a high opinion of the well-known manager—also through his role at SELPE—which led to initial flirtation and then a formal offer that Vassilakos accepted, as such positions are scarce in the retail market. Valachis will oversee key Group projects until their completion and is reportedly leaving in a fully secured position, while Vassilakos will assume his duties at Fourlis on July 1. Meanwhile, the company’s general shareholders meeting on June 20 is expected to provide more details from Vassilis Fourlis himself about the CEO change and, more importantly, the Group’s new strategic direction for the years ahead, as Fourlis seems to be searching for its compass—a fact that’s also reflected in the stock market. The stock falls into the… unmovables category, even as the ASE rallies and many investors are left scratching their heads, since the Group is financially sound and remains a retail leader with IKEA as its ace in the hole. Competition or Fear of the Commission? -I don’t know why—call me suspicious or skeptical—but I find it hard to believe that competition will suddenly start working its magic on ferry ticket prices. Attica Holdings deployed two modern high-speed vessels with a total capacity of 2,000 seats on Cyclades routes, and word is they kicked off with an aggressive pricing strategy. In plain terms, Attica entered routes previously dominated by Sea Jet. It’s still early to gauge whether real competition is heating up, but let’s not forget that the Competition Commission carried out surprise inspections in the ferry sector last March and is now preparing a major investigation into the industry. The Mysteries of Kekropas -The mystery might be unveiled this morning at 9, during the annual General Shareholders Meeting in Paleo Psychiko. The game began on Tuesday, May 27. With above-average trading volume, the Kekropas stock climbed to €1.145. A week later, on Tuesday, June 3, it rose to €1.23 on heavy trades. On Wednesday, it reached €1.32. Yesterday, with 240,881 shares traded, KEKROPS shot up to €1.66, a +22% jump. It eventually closed at €1.65, up 21.32%. The rumors and “inside info” are swirling. Some believe one of the two main shareholders (Kokkalis or Peristeris) will bow out, selling their shares €0.50 above last week’s price. Others are concocting even more imaginative scenarios. The company’s market cap—which posted losses of €1.38 million in 2024—has now reached €32.8 million. National Bank and Piraeus Bank Recovered Dividend and Capital Return -The National Bank of Greece and Piraeus Bank quickly “absorbed” the losses from the €0.422 per share dividend cut by the former and the €0.298 per share capital return by the latter. NBG bounced back above €10.5, erasing Tuesday’s -2.76% drop. Similarly, Piraeus climbed above €5.6, recovering recent losses, with its market cap now exceeding €7 billion. Yesterday’s buying wave was accompanied by large block trades in both bank stocks. For NBG, three pre-agreed trades took place at prices between €10.3 and €10.52, totaling 2.174 million shares worth €22.513 million. As for Piraeus, four blocks were exchanged at €5.53–€5.57, with a total of 5.634 million shares changing hands, totaling €31.197 million. It’s Raining Blocks and Placements on the ASE -Lately, it’s been “raining” blocks and placements on the Athens Stock Exchange—that is, large equity placements into institutional portfolios. These packages act as new launchpads for the market. Obviously, the professional managers buying at these prices today are well aware of the market’s four-year upward trend and the fundamentals of the companies they’re investing in. In yesterday’s session, of the €227.7 million total turnover, €59.5 million came from block trades in blue-chip stocks—clearly not targeting retail investors. Over 60% of yesterday’s trading volume focused on Piraeus, National Bank, and Alpha Bank stocks. For the third consecutive session, the General Index closed in the green at +0.23%, landing at 1,839.22 points—close to the auction session high. Coca-Cola showed no willingness to support the market, closing down -1.16% at €46. Eurobank (+1.2%) traded at €2.78 with a turnover of “just” €15.4 million. OTE (+0.34%) joined in at €17.7 with €9.6 million in trades. All in all, of the €227.7 million in trading, €216 million was concentrated in FTSE25 stocks—leaving very little room for mid- and small-cap equities… Lavipharm Ramps Up Production -Lavipharm is building a new transdermal production unit at its Peania facility to fulfill third-party orders, with multinational pharma giant iNova Pharmaceuticals—owner of the Betadine brand—being the first major client. Until now, Lavipharm produced 45 million patches annually, exporting products worth around €25 million. Starting in 2026, the plant’s capacity will ramp up to 120 million patches. In liquid formulations, capacity will rise from 3.5 million bottles to 20 million, with Betadine as the flagship product. Lessons from Ukraine -Last Sunday, Ukraine successfully carried out Operation Spiderweb. We saw on TV an unprecedented drone attack on five Russian airbases, including Belaya base in Irkutsk—some 4,500 kilometers from the Ukrainian border. The operation, planned over 18 months using 117 AI-powered drones, disabled 41 Russian strategic aircraft and inflicted damages worth $7 billion. While Western intelligence agencies have offered more conservative assessments, it remains a striking asymmetric blow. Greek defense and resilience firms, part of a 30-member business delegation in Kyiv, had the opportunity to be briefed by Ukrainian companies on their new defense doctrine—which changes every two weeks. The doctrine focuses on drone warfare and establishing infantry UAV units staffed by young soldiers often skilled in video games. The “robot zone”—a 30-kilometer stretch operated solely by unmanned vehicles—is the new normal on the Russo-Ukrainian front. The Greek mission returned yesterday with plans to submit concrete proposals to the country’s political and military leadership aiming to redefine Greece’s defense doctrine, invest in UAV technologies, and train personnel to meet the demands of modern warfare. Recovery Fund Pays, Superfund Delivers -Projects totaling €3 billion will be delivered by mid-2026 by the Strategic Projects Preparation Facility (PPF) of the Hellenic Corporation of Assets and Participations (HCAP). By December this year, it will have completed €1 billion worth of projects. Yesterday, the Minister of Shipping and Island Policy signed off on a €9 million project to deepen the channel and harbor basin at the port of Stylida. On the same day, the Prime Minister inaugurated the fully renovated Health Center in Velestino, with €4 million in upgrades now offering quality services to local residents. These projects share a common thread: the PPF unit of HCAP, which prepared, tendered, and contracted them—along with funding from the Recovery and Resilience Facility. After Gold, Silver Takes Off Too -Silver is soaring to its highest price in 13 years, surpassing $35.8 per ounce and heading straight for $36. Since the start of the year, silver is up 24%, outperforming many major business groups. According to the Silver Institute, global demand is expected to outstrip supply for the fifth consecutive year. Meanwhile, tech stocks on Wall Street are having a field day. The S&P 500’s market cap has grown by roughly $7.5 trillion since its April 7 lows. Of that increase, 54% is thanks to the so-called “Magnificent Seven” tech stocks (Apple, Microsoft, Alphabet, Amazon, Nvidia, Tesla, Meta), which boosted their combined capitalization by $4 trillion. Notably, Tesla and Nvidia shares are up 53.6% and 42.6% respectively. Lagarde’s Political Future -Many believe that France’s conservative bloc urgently needs Christine Lagarde’s leadership to gain governability. She herself doesn’t deny that her career began in politics—she was the first female Finance Minister of a G8 country. Some thought she might leave her ECB post early to… move to Davos and take over the World Economic Forum from Klaus Schwab. Yesterday, the ECB chief shut down all such speculation, stating that she intends to serve out her term: “Let me say very clearly that I have always been and remain fully committed to fulfilling my mandate, and I am determined to complete my term.” Lagarde’s term ends in 2027. Truth be told, Christine Lallouette (her birth surname) has resigned early before—leaving her IMF post in 2019 to take the helm at the ECB. Even then, in a 2018 Financial Times interview, she had publicly denied interest in the job. Explore related questions
--------------------------------------------------

Title: Uber Considers Using Stablecoins for Cross-Border Money Transfers
URL: http://www.pymnts.com/cryptocurrency/2025/uber-considers-using-stablecoins-for-cross-border-money-transfers/
Time Published: 2025-06-06T00:46:32Z
Full Content:
Uber Technologies is reportedly considering using stablecoins to transfer money among different countries. Complete the form to unlock this article and enjoy unlimited free access to all PYMNTS content — no additional logins required. yesSubscribe to our daily newsletter, PYMNTS Today. By completing this form, you agree to receive marketing communications from PYMNTS and to the sharing of your information with our sponsor, if applicable, in accordance with our Privacy Policy and Terms and Conditions. Δ Stablecoins are appealing to global companies because they can help reduce the cost of moving money, Uber CEO Dara Khosrowshahi told Bloomberg on Thursday (June 5) in an interview conducted at the Bloomberg Tech conference in San Francisco. “That’s super interesting to us and we’re definitely going to take a look,” Khosrowshahi said, according to the report. Stablecoins can help solve the issues around settlement time, cost and dollar access that are often associated with cross-border transfers, PYMNTS reported Wednesday (June 4). While traditional cross-border transfers can take up to five business days and involve a daisy chain of correspondent banks, each clipping a fee, stablecoins enable money to move globally with internet speed and without intermediaries. Settlement costs can ultimately fall from $30 wires to sub-dollar fees. It was reported in May that Meta was considering adopting stablecoins as a way to make cross-border payments and that the company was in discussions with crypto firms. The company is looking at stablecoins as a way to make cross-border payments without the fees associated with wire transfers and other payment methods. Possible uses of stablecoins could include making small payouts to creators in other regions. It was reported in March that banks and FinTech also see stablecoins’ cross-border payments potential. The world’s largest ones are scrambling to roll out their own stablecoins because they anticipate that cryptocurrencies will transform the cross-border payments market. When stablecoin issuer Circle Internet Financial debuted on the New York Stock Exchange Thursday, its shares tripled in price. While its initial public offering was priced at $31 per share, the stock opened at $69.50 and ultimately closed at $83.23. “Our transformation into being a public company is a significant and powerful milestone — the world is ready to start upgrading and moving to the internet financial system,” Circle Co-Founder and CEO Jeremy Allaire said in a post on X. In May, Circle announced that its stablecoin-powered cross-border payments network, the Circle Payments Network (CPN), was live. The network is designed to facilitate the use of stablecoins for mainstream cross-border payments. Uber Considers Using Stablecoins for Cross-Border Money Transfers Businesses Limit Pass-Through of Tariff Costs Due to ‘Price-Sensitive’ Customers Google Releases Upgraded Gemini 2.5 Pro in Preview Circle Is America’s First Publicly Traded Stablecoin Issuer. Now What? We’re always on the lookout for opportunities to partner with innovators and disruptors.
--------------------------------------------------