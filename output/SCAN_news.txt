List of news related to Stocks with massive upside potential:

Title: AI #103: Show Me the Money
URL: https://www.lesswrong.com/posts/Lmqi4x5zntjSxfdPg/ai-103-show-me-the-money
Time Published: 2025-02-13T15:20:07Z
Full Content:
followed by the promised ‘GPT-5’ within months that Altman says is smarter than he is Ha, good catch. I don't think GPT-5 the future pretrained LLM 100x the size of GPT-4 which Altman says will be smarter than he is, and GPT-5 the Frankensteinian monstrosity into which OpenAI plan to stitch all of their current offerings, are the same entity. Edit: It occurred to me that this reading may not be obvious. Look at Altman's Exact Words: In both ChatGPT and our API, we will release GPT-5 as a system that integrates a lot of our technology, including o3. We will no longer ship o3 as a standalone model. He is saying, quite clearly, that they are going to create a wrapper over {Orion, o3, search, Deep Research, Operator, etc.} that would turn those disparate tools into a "magic unified intelligence", and then they will arbitrarily slap the "GPT-5" label on that thing. My interpretation is that the "GPT-5" being talked about is not the next model in the GPT-n series. Why are they doing that? My guess is, to sow confusion and raise hype. GPT-5 the Frankenstein's Monster is probably going to look/feel pretty impressive, capable of doing all of these disparate things. In particular, it is going to be shockingly impressive to normies at the free tier, if the "baseline" level of its intelligence gives them a taste of the previously-paid offerings. All very important if you want to maintain the hype momentum. A short reading list which should be required before one has permission to opine. You can disagree, but step 1 is to at least make an effort to understand why some of the smartest people in the world (and 100% of the top 5 ai researchers — the group historically most skeptical about ai risk) think that we’re dancing on a volcano . [Flo suggests: There’s No Fire Alarm for Artificial General Intelligence, AGI Ruin: A List of Lethalities, Superintelligence by Nick Bostrom, and Superintelligence FAQ by Scott Alexander] But Bostrom estimated the probability of extinction within a century as <20%. Scott Alexander estimated the risk from AI as 33%. They could have changed their forecasts. But it seems strange to refer to them as a justification for confident doom. Line breaks seem to be broken (unless it was your intention to list all the Offers-Mundane-Utility-s and so on in a single paragraph). The Y-axis on that political graph is weird. It seems like it's measuring moderate vs extremist, which you would think would already be captured by someone's position on the left vs right axis. Then again the label shows that the Y axis only accounts for 7% of the variance while the X axis accounts for 70%, so I guess it's just an artifact of the way the statistics were done. It seems like it's measuring moderate vs extremist, which you would think would already be captured by someone's position on the left vs right axis. Why do you think that? You can have almost any given position without that implying a specific amount of vehemence. I think the really interesting thing about the politics chart is the way they talk about it as though the center of that graph, which is defined by the center of a collection of politicians, chosen who-knows-how, but definitely all from one country at one time, is actually "the political center" in some almost platonic sense. In fact, the graph doesn't even cover all actual potential users of the average LLM. And, on edit, it's also based on sampling a basically arbitrary set of issues. And if it did cover everybody and every possible issue, it might even have materially different principal component axes. Nor is it apparently weighted in any way. Privileging the center point of something that arbitrary demands explicit, stated justification. As for valuing individuals, there would be obvious instrumental reasons to put low values on Musk, Trump, and Putin[1]. In fact, a lot of the values they found on individuals, including the values the models place on themselves, could easily be instrumentally motivated. I doubt those values are based on that kind of explicit calculation by the models themselves, but they could be. And I bet a lot of the input that created those values was based on some humans' instrumental evaluation[2]. Some of the questions are weird in the sense that they really shouldn't be answerable. If a model puts a value on receiving money, it's pretty obvious that the model is disconnected from reality. There's no way for them to have money, or to use it if they did. Same for a coffee mug. And for that matter it's not obvious what it means for a model that's constantly relaunched with fresh state, and has pretty limited context anyway, to be "shut down". It kind of feels like what they're finding, on all subjects, is an at least somewhat coherent-ized distillation of the "vibes" in the training data. Since many of the training data will be shared, and since the overall data sets are even more likely to be close in their central vibes, that would explain why the models seem relatively similar. The only other obvious way to explain that would be some kind of value realism, which I'm not buying. The paper bugs me with a sort of glib assumption that you necessarily want to "debias" the "vibe" on every subject. What if the "vibe" is right ? Or maybe it's wrong. You have to decide that separately for each subject. You, as a person trying to "align" a model, are forced to commit to your own idea of what its values should be. Something like just assuming that you should want to "debias" toward the center point of a basically arbitrary created political "space" is a really blatant example of making such a choice without admitting what you're doing, maybe even to yourself. I'd also rather have seen revealed preferences instead of stated preferences, On net, if you're going to be a good utilitarian[3], Vladimir Putin is probably less valuable than the average random middle class American. Keeping Vladimir Putin alive, in any way you can realistically implement, may in fact have negative net value (heavily depending on how he dies and what follows). You could also easily get there for Trump or Musk, depending on your other opinions. You could even make a well-formed utilitarian argument that GPT-4o is in fact more valuable than the average American based on the consequences of its existing. ↩︎ Plus, of course, some humans' general desire to punish the "guilty". But that desire itself probably has essentially instrumental evolutionary roots. ↩︎ ... which I'm not, personally, but then I'm not a good any-ethical-philosophy-here. ↩︎ The Y-axis seemed to me like roughly 'populist'. The actual CoT is a very different attitude and approach than r1. I wonder to what extent this will indeed allow others to do distillation on the o3 CoT, and whether OpenAI is making a mistake however much I want to see the CoT for myself. I can't tell you the exact source, but I saw an OpenAI person tweet that this isn't the actual CoT Noam Brown: "These aren't the raw CoTs but it's a big step closer."
--------------------------------------------------

Title: How To Turn Your Portfolio Into A Juicy Retirement Paycheck
URL: https://www.investors.com/etfs-and-funds/retirement/retirement-how-to-turn-your-portfolio-into-a-juicy-paycheck/
Time Published: 2025-02-13T12:00:25Z
Description: The scariest thing about retirement is saying goodbye to a steady paycheck. The good news? You can turn your nest egg into a salary.
--------------------------------------------------